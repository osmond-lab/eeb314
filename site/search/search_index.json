{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"Mathematical modelling in ecology and evolution (EEB314) <p>Mathematics is central to science because it provides a rigorous way to go from a set of assumptions to their logical consequences. In ecology &amp; evolution this might be how we think a virus will spread and evolve, how climate change will impact a threatened population, or how much genetic diversity we expect to see in a randomly mating population. In this course you'll learn how to build, analyze, and interpret mathematical models of increasing complexity through readings, lectures, computer labs, and a final project. Our focus is on deterministic dynamical models (recursions and differential equations), which requires us to learn and use some calculus and linear algebra.</p> <p>Please see the University of Toronto Academic Calendar for more details on the course prerequisites and additional information on the distribution/breadth requirements this course satisfies.</p> <p>Next taught: Fall 2025</p> <p>Previously taught: Fall 2024, Fall 2022 (EEB430), Fall 2021 (EEB430)</p>"},{"location":"acknowledgements/","title":"Acknowledgements","text":"<p>This course is based on a fantastic textbook by Sally Otto and Troy Day, A biologist's guide to mathematical modeling in ecology and evolution. I've been fortunate to have been taught by both of these excellent mentors and I hope I can do them and their book justice.</p> <p>I'm also very grateful to Leithen M'Gonigle who has slickly elaborated on Sally's lecture notes in LaTeX and kindly shared them with me.</p> <p>Thanks also to Ailene McPherson who, along with Sally and Leithen, continues to help me teach this course and inspires me to become a better teacher.</p> <p>Thanks are also due to Puneeth Deraje, who went above and beyond as the course's first TA, and Chris Carlson, who helped improve the labs as the course's second TA. </p> <p>And you wouldn't be reading this without the help of Tom Ouellette who did an amazing job creating this website as a course development TA.</p> <p>Finally, thanks to all the students who have taken the course and given me feedback (explicit or not) -- thanks for your patience!</p>"},{"location":"labs/schedule/","title":"Schedule","text":"<p>Under construction for Fall 2025 edition. Labs will be updated as we go.</p> Lab Tutorial Coding exercise 1 Building recursion equations Introduction to Jupyter, Python, and SymPy 2 tbd Simulations and cob-webs 3 tbd Equilibria, stability, and general solutions (univariate) 4 tbd Vectors and matrices 5 tbd Eigenvalues, eigenvectors, and linear multivariate solutions 6 tbd Demography 7 tbd Phase planes 8 tbd Equilibrium and stablity (nonlinear multivariate) 9 tbd More multivariate examples 10 tbd Invasion analysis I 11 tbd Invastion analysis II 12 tbd Invasion analysis III <p>The coding part of the labs are all available to view and download here.</p> <p>Students in the course are encouraged to view and run the coding aspect of the labs through the University of Toronto's JupyterHub here.</p>"},{"location":"labs/tutorial-01/","title":"Tutorial 1","text":""},{"location":"labs/tutorial-01/#tutorial-1-building-recursion-equations","title":"Tutorial 1: Building recursion equations","text":"Run notes interactively?      <p>Let's get some more practice building recursion equations.</p>"},{"location":"labs/tutorial-01/#review","title":"Review","text":"<p>In Lecture 1 we asked, how does immigration affect population size?</p> <p>We then built a model with a single variable, \\(n(t)\\), denoting population size at time \\(t\\). </p> <p>In discrete time the parameters were the average number of immigrants per time step (\\(M\\)), the average number of offspring per individual per time step (\\(B\\)), and the fraction of individuals that die each time step (\\(D\\)). </p> <p>Assuming migration, then birth, then death each time step, we drew the following life-cycle diagram:</p> <pre><code>graph LR;\n    A((n)) --migration--&gt; B((n'));\n    B --birth--&gt; C((n''));\n    C --death--&gt; A;</code></pre> <p>We then built an equation for the population size in the next generation, \\(n(t+1)\\), based on the life-cycle diagram above, by constructing an equation for each event </p> \\[n' = n(t) + M\\] \\[n'' = n' + Bn'\\] \\[n(t+1) = n'' - Dn''\\] <p>We then substituted \\(n''\\) and then \\(n'\\) into the equation for \\(n(t+1)\\) to write \\(n(t+1)\\) in terms of \\(n(t)\\)</p> \\[ \\begin{aligned} n(t+1) &amp;= n'' \u2212 Dn'' \\\\ &amp;= (n' + Bn') \u2212 D(n' + Bn') \\\\ &amp;= n'(1 + B \u2212 D \u2212 DB) \\\\ &amp;= (n(t) + M)(1 + B \u2212 D \u2212 DB) \\\\ \\end{aligned} \\] <p>This recursion equation correctly takes into account the order of the life cycle (migration, birth, death) and the point at which the census is taken (immediately after death).</p>"},{"location":"labs/tutorial-01/#problem","title":"Problem","text":"<p>Show that the six different life-cycle orders give four distinct recursion equations.</p>"},{"location":"lectures/lecture-01/","title":"Lecture 1","text":""},{"location":"lectures/lecture-01/#lecture-1-why-and-how-to-build-a-model","title":"Lecture 1: Why and how to build a model","text":"Run notes interactively?"},{"location":"lectures/lecture-01/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Why to build a model</li> <li>How to build a model</li> <li>Summary</li> </ol>"},{"location":"lectures/lecture-01/#1-why-to-build-a-model","title":"1. Why to build a model","text":"<p>Mathematics permeates ecology and evolution, from simple back-of-the-envelope calculations to the development of sophisticated mathematical models. This is because mathematics is a unique tool that can take what we already know (or assume to be true) and rigorously lead us to the logical conclusions.</p> <p>To see this (while also introducing you to the kind of models I work with), let's look at an example. See sections 1.2-1.4 of the text for another motivating example (within-patient HIV dynamics).</p>"},{"location":"lectures/lecture-01/#motivating-model-evolution-during-extreme-events","title":"Motivating model: evolution during extreme events","text":"<p>This is a model that I helped Dr. Kelsey Lyberger (then a PhD student at UC Davis) with in our paper, Lyberger et al 2021.</p> <sup>Kelsey Lyberger, doing Daphnia fieldwork I suppose.</sup> <p>Kelsey was interested in how populations respond to extreme climatic events, like lizards to hurricanes. It has long been clear that such events can impact the size of a population, e.g., by causing extra mortality, and may in fact put populations at risk of extinction. More recently it has become apparent that extreme events can also impose strong natural selection, and that populations can quickly adapt to the new environment. Some examples include:</p> <ul> <li>Hurricanes select on lizard limbs and toe pads</li> <li>Ice-storms select on sparrow body size</li> <li>Droughts select on Darwin finch beaks</li> <li>Droughts select on flowering time in Brassica</li> </ul> <sup>Lizard being blown off perch by a leaf blower. Source: colindonihue.com</sup> <p>Now, how should such rapid adaptive evolution impact population size? This is the question Kelsey set out to answer with a mathematical model.</p> Details of Kelsey's model <p>Kelsey assumed each individual has a quantitative genetic trait, such as lizard limb length, that is determined by many alleles of small effect plus some environmental noise. Fitness is assumed to be a bell-shaped function of the difference between the optimum trait value, \\(\\theta\\), and an individual's trait value, \\(z\\), which we write as \\(W(\\theta - z)\\). </p> <p>The change in population size, \\(N\\), from one generation to rate the next is the current population size times mean fitness, \\(\\overline{W}(\\theta - \\bar{z})\\), the latter depending on the population mean trait value, \\(\\bar{z}\\). This gives</p> <p>\\(\\Delta N = N \\overline{W}(\\theta - \\bar{z}).\\)</p> <p>The change in the mean trait value from one generation to the next is roughly the product of genetic variance in the trait, \\(V_g\\), and the strength of selection. The strength of selection is defined as the derivative of the natural logarithm of population mean fitness with respect to the mean trait value, \\(\\mathrm{d}\\ln\\overline{W}(\\theta - \\bar{z})/\\mathrm{d}\\bar{z}\\). This gives</p> <p>\\(\\Delta \\bar{z} = V_g \\frac{\\mathrm{d}}{\\mathrm{d}\\bar{z}}\\ln\\overline{W}(\\theta - \\bar{z}).\\)</p> <p>Together, these coupled recursion equations, \\(\\Delta N\\) and \\(\\Delta \\bar{z}\\), can be used to describe how evolution affects population size under an extreme event, which is modeled as a sudden but temporary change in the optimum phenotype, \\(\\theta\\).</p> <p>Below we numerically iterate the two equations above (dashed lines) and compare with stochastic simulations (solid lines) to recreate Figure 1 from Lyberger et al. With an activated kernel, you can tinker and run the code below to adjust the plot (the simulations take a few seconds). </p> <pre>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# choose parameter values\nVo, Ve, event_duration, seed = 0.75, 0, 1, 0\ngenerations, K, w, lmbda, event_time, initial_theta_t, dtheta_t, event_duration = 110, 500, 1, 2, 100, 0, 2.5, 1\nnreps = 10\n\n# recursions\ndef recursions(t0,n0,z0,lmbda,w,K,event_time,event_duration,initial_theta_t,dtheta_t,Vo,Ve,tmax=np.inf):\n\n    # initialize\n    t,nt,zt=t0,n0,z0\n\n    Vs = w**2 + Ve\n    Vg = (2*Vo - Vs + np.sqrt(4*Vo**2 + 12*Vo*Vs + Vs**2))/4\n    Vt = Vg + Vs\n\n    # yield and update\n    while t np.random.uniform(0,1) else False for p in prob_survival])\n        if len(survived) == 0:\n            break\n        members = members[survived]\n\n        # random mating\n        offspring = []\n        for m in np.random.choice(members, len(members)):\n            if len(offspring) &gt; K: #if more than K offspring already then choose K and stop matings\n                offspring = np.random.choice(offspring, K)\n                break\n            else:\n                n_off = np.random.poisson(lmbda)\n                mate = np.random.choice(members)\n                offspring += [(m + mate)/2 for _ in range(n_off)] #give offspring mean parental value\n\n        # sample new trait values for offspring\n        offspring = np.array(offspring)\n        offspring = np.random.normal(offspring, Vo) #add segregation variance\n\n        # record statistics\n        population_size.append(len(offspring))\n        mean_breeding_value.append(np.mean(offspring))\n\n        # update for next generation\n        members = offspring\n\n    return (\n        np.arange(0, generations-event_time+1), #times\n        np.array(population_size[event_time-1:]), #population size\n        np.array(mean_breeding_value[event_time-1:]) #mean trait value\n    )\n\n# initialize plot\nfig, ax = plt.subplots(2, sharex=True)\nfig.set_size_inches(8,6)\n\n# for each segregation (Vo) and environment variance (Ve) parameter combination\nfor Vo, Ve, c, lab in [[0.75, 0, 'black', 'with evolution'], [0, 1, 'red', 'without evolution']]:\n\n    # plot recursions\n    tnz = recursions(0,K,initial_theta_t,lmbda,w,K,event_time,event_duration,initial_theta_t,dtheta_t,Vo/2,Ve,generations)\n    tnzs = np.array([vals for vals in tnz])\n    ax[0].plot(tnzs[event_time:,0]-event_time,tnzs[event_time:,1], color=c, linestyle='--')\n    ax[1].plot(tnzs[event_time:,0]-event_time,tnzs[event_time:,2], color=c, linestyle='--')\n\n    # plot stochastic simulations\n    simulations = np.array([lyberger_model(Vo, Ve, event_duration, s, generations, K, w, lmbda, event_time, initial_theta_t, dtheta_t) for s in range(nreps)])\n    ax[0].plot(simulations[:,0].T, simulations[:,1].T, color=c, alpha=0.3);\n    ax[1].plot(simulations[:,0].T, simulations[:,2].T, color=c, alpha=0.3);\n\n    # hack together only one instance of the legend\n    ax[0].plot([np.min(simulations[:,0].T)], [np.min(simulations[:,1].T)], alpha=1,\n                  label = lab, color=c)\n    ax[1].plot([np.min(simulations[:,0].T)], [np.min(simulations[:,2].T)], alpha=1,\n                  label = lab, color=c)\n\n# add environmental event duration\nax[0].fill_between([0,event_duration], y1=K, alpha=0.2)\nax[1].fill_between([0,event_duration], y1=-0.2, y2=2, alpha=0.2)\n\n# add labels\nax[0].set_ylabel('Population size', fontsize=12)\nax[1].set_ylabel('Mean trait value', fontsize=12)\nax[1].set_xlabel('Generation', fontsize=14)\n\n# add legend\nplt.legend(frameon=False)\nplt.show()\n\n\n<p></p>\n<p>The key result, that you can see in the plot above, is that when extreme events are short, adaptive evolution (black lines) can paradoxically reduce population size (relative to the red lines, where there is no evolution). The reason for this is that, while during the extreme event (shaded section) evolution is adaptive, once the extreme event ends the population finds itself maladapted to the original environment. Adaptive evolution can therefore hamper population persistence, and this is an important thing to keep in mind when documenting rapid adaptive evolution in response to extreme events -- it is not necessarily a good thing for the species (or our conservation goals).</p>\n<p></p>"},{"location":"lectures/lecture-01/#2-how-to-build-a-model","title":"2. How to build a model","text":"<p>Although many problems require specific formulations and assumptions, there are a few general principles to constructing a model.</p>"},{"location":"lectures/lecture-01/#i-formulate-the-question","title":"i. Formulate the question","text":"<ul>\n<li>What do you want to know?</li>\n<li>Describe the model in the form of a question.</li>\n<li>Simplify, Simplify!</li>\n<li>Start with the simplest, biologically reasonable description of the problem.</li>\n</ul>\n<p>For example, we might ask: how does immigration affect population size?</p>"},{"location":"lectures/lecture-01/#ii-determine-the-basic-ingredients","title":"ii. Determine the basic ingredients","text":"<ul>\n<li>Define the variables in the model.</li>\n<li>Describe any constraints on the variables.</li>\n<li>Describe any interactions between variables.</li>\n<li>Decide whether you will treat time as continuous or discrete.</li>\n<li>Choose a time scale (i.e., decide what a time step equals in discrete time and specify whether rates will be measured per second, minute, day, year, generation, etc.).</li>\n<li>Define the parameters in the model.</li>\n<li>Describe any constraints on the parameters.</li>\n</ul>\n<p>In our example, there is a single variable, \\(n(t)\\), denoting population size at time \\(t\\). This must be non-negative, \\(n(t)\\geq0\\), to be biologically valid. </p>\n<p>We'll look at both continuous and discrete time, with an arbitrary time scale (if we wanted to plug in some numerical parameter values we'd have to specify this, but here we won't).</p>\n<p>In continuous time our parameters will be immigration rate (\\(m\\)), per capita birth rate (\\(b\\)), and per capita death rate (\\(d\\)). These must all be non-negative under our interpretation. The units of \\(m\\) are individuals/time while the units of \\(b\\) and \\(d\\) are simply 1/time (e.g., for birth we have the number of individuals produced per individual per time, so that individuals cancels out and we are left with 1/time).</p>\n<p>In discrete time the parameters will be the average number of immigrants per time step (\\(M\\)), the average number of offspring per individual per time step (\\(B\\)), and the fraction of individuals that die each time step (\\(D\\)). These must all be non-negative and \\(D\\) must also be less than or equal to 1 as it is a fraction. </p>"},{"location":"lectures/lecture-01/#iii-qualitatively-describe-the-biological-system","title":"iii. Qualitatively describe the biological system","text":"<ul>\n<li>For continuous-time models, draw a flow diagram to describe changes to the variables over time.</li>\n<li>For discrete-time models, draw a life-cycle diagram with all the events that occur each time unit.</li>\n<li>For discrete time models with multiple variables and events, construct a table listing the outcome of every event.</li>\n</ul>\n<p>For our continuous-time example we could draw the following:</p>\n\n<pre><code>graph LR;\n    A((n)) --birth--&gt; A;\n    B[ ] --migration--&gt; A;\n    A --death--&gt; C[ ];\n    style B height:0px;\n    style C height:0px;</code></pre>\n\n\n<p>For our discrete-time example, if we assume migration, then birth, then death each time step, we could draw the following:</p>\n\n<pre><code>graph LR;\n    A((n)) --migration--&gt; B((n'));\n    B --birth--&gt; C((n''));\n    C --death--&gt; A;</code></pre>\n\n\n<p>We'll see an example of a table of events later in the course.</p>"},{"location":"lectures/lecture-01/#iv-quantitatively-describe-the-biological-system","title":"iv. Quantitatively describe the biological system","text":"<ul>\n<li>Using the diagrams and tables as a guide, write down the equations. Perform checks. Are the constraints on the variables still met as time passes? Make sure that the units of the right-hand side equal those on the left-hand side.</li>\n<li>For continuous-time models, add rates for arrows coming in to the flow diagram and subtract rates for arrows coming out of the flow diagram. </li>\n</ul>\n<p>For example, in the model shown above the rate of change in the number of individuals, \\(\\frac{\\mathrm{d} n}{\\mathrm{d} t}\\), is</p>\n\\[\n\\begin{aligned}\n\\frac{\\mathrm{d} n}{\\mathrm{d} t} &amp;= m + b n(t) - d n(t)\\\\\n&amp;= m + (b - d) n(t)\n\\end{aligned}\n\\]\n<p>This is a differential equation.</p>\n<p>In discrete time, you must take into account the order of events when constructing equations. To build an equation for the population size in the next generation, \\(n(t+1)\\), based on the life-cycle diagram above, first construct an equation for each event </p>\n\\[n' = n(t) + M\\]\n\\[n'' = n' + Bn'\\]\n\\[n(t+1) = n'' - Dn''\\]\n<p>Next, substitute \\(n''\\) and then \\(n'\\) into the equation for \\(n(t+1)\\) to write \\(n(t+1)\\) in terms of \\(n(t)\\)</p>\n\\[\n\\begin{aligned}\nn(t+1) &amp;= n'' \u2212 Dn'' \\\\\n&amp;= (n' + Bn') \u2212 D(n' + Bn') \\\\\n&amp;= n'(1 + B \u2212 D \u2212 DB) \\\\\n&amp;= (n(t) + M)(1 + B \u2212 D \u2212 DB) \\\\\n\\end{aligned}\n\\]\n<p>We now have a recursion equation that correctly takes into account the order of the life cycle (migration, birth, death) and the point at which the census is taken (immediately after death).</p>"},{"location":"lectures/lecture-01/#v-analyze-the-equations","title":"v. Analyze the equations","text":"<ul>\n<li>Start by using the equations to simulate and graph the changes to the system over time. </li>\n<li>Choose and perform appropriate analyses.</li>\n<li>Make sure that the analyses can address the problem.</li>\n</ul>\n<p>We'll save the mathematical analyses for later in the course, but let's try simulating the recursion equations here to get a sense of the dynamics (we'll learn more about how to do this in the next lecture).</p>\n<pre>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# define a genarator to efficiently run the recursion\ndef recursion(n0, b, d, m, tmax):\n    t,n=0,n0 #initial conditions\n    while t &lt; tmax:\n        yield t,n #output current state\n        t += 1 #update time\n        n = (n + m) * (1 + b - d - b*d) #update population size using the recursion equation we derived above\n\n# simulate and plot\nfig, ax = plt.subplots()\n\nn0, b, d, tmax, dt = 1, 0.05, 0.1, 100, 0.1 #parameter values\nfor m in [0,1]:\n    tn = recursion(n0, b, d, m, tmax) #define recursion\n    tns = np.array([vals for vals in tn]) #get output\n    ax.scatter(tns[:,0], tns[:,1], label='M=%s'%m) #plot population size at each time\n\nax.set_xlabel('time, $t$')\nax.set_ylabel('population size, $n(t)$')\nplt.legend()\nplt.show()\n</pre>\n\n<p></p>\n<p>How does migration affect population size?</p>"},{"location":"lectures/lecture-01/#vi-checks-and-balances","title":"vi. Checks and balances","text":"<ul>\n<li>Check the results against data or any known special cases.</li>\n<li>Determine how general the results are.</li>\n<li>Consider alternatives to the simplest model.</li>\n<li>Extend or simplify the model, as appropriate, and repeat steps 2-5.</li>\n</ul>\n<p>If \\(b&gt;d\\) (or \\(B&gt;D\\)) the population grows without bound (try this in the code above). But competition should prevent unbounded population growth. We could therefore extend the model to include competition.</p>"},{"location":"lectures/lecture-01/#vii-relate-the-results-back-to-the-question","title":"vii. Relate the results back to the question","text":"<ul>\n<li>Do the results answer the biological question?</li>\n<li>Are the results counter-intuitive? Why?</li>\n<li>Interpret the results verbally, and describe conceptually any new insights into the biological process.</li>\n<li>Describe potential experiments.</li>\n</ul>\n<p>Immigration appears to increase the population size (compare \\(M=0\\) and \\(M=1\\) in the plot above), though we'd need to do the mathematical analysis to be more confident in this statement. </p>\n<p>There is some counter-intuitiveness in the discrete model: try creating the recursion equation for a different order of events in the lifecycle. The recursion equation depends on the order and will therefore lead to different dynamics.</p>\n<p>Potential experiments incude manipulating immigration in lab populations (e.g., bacteria) or comparing population sizes on islands that are different distances from the mainland.</p>\n<p></p>"},{"location":"lectures/lecture-01/#3-summary","title":"3. Summary","text":"<ul>\n<li>Mathematical models take us from assumptions to conclusions</li>\n<li>There is a recipe to build them</li>\n</ul>"},{"location":"lectures/lecture-02/","title":"Lecture 2","text":""},{"location":"lectures/lecture-02/#lecture-2-numerical-and-graphical-techniques-univariate","title":"Lecture 2: Numerical and graphical techniques (univariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-02/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Plots of variables over time</li> <li>Plots of variables as a function of themselves</li> <li>Summary</li> </ol> <p>Before we jump into more rigorous mathematical analyses, we\u2019re first going to learn how to get a feel for the dynamics of our models.</p> <p>To do so we\u2019re going to choose some particular numerical values for our parameters and then use our models to predict what happens over time.</p> <p>The downside of this approach is that we often won\u2019t know the parameter values to choose and, regardless, choosing particular values doesn\u2019t tell us about the dynamics of our model more generally.</p> <p>The upside is that this approach can highlight errors or reveal unexpected patterns that guide future mathematical analyses.</p> <p></p>"},{"location":"lectures/lecture-02/#1-plots-of-variables-over-time","title":"1. Plots of variables over time","text":"<p>The first plot is of the variables over time. We've already seen two examples of this in the previous lecture, for both evolution during extreme events and migration. In both cases we wrote recursive functions (actually, \"generator\"s in Python) to generate values of the variables at sequential time points.</p> <p>Here let's look at a simpler example to demonstrate the essence of the method by hand. Take the recursion for the migration example \\(n(t+1) = (n(t)+M)(1+B-D-BD)\\), ignore migration, \\(M=0\\), and create a new parameter, \\(R=1+B-D-BD\\). Our simplified recursion is then </p> <p>\\(n(t+1)=Rn(t)\\).</p> <p>This is called exponential growth (technically \"geometric\" growth in discrete time) and the only parameter of the model, \\(R\\), is referred to as the reproductive value.</p> <p>To plot a variable over time we</p> <ul> <li>choose an initial condition,</li> <li>choose parameter values, and</li> <li>numerically iterate the equation.</li> </ul> <p>Here let's choose initial condition \\(n(0)=100\\) and parameter value \\(R=2\\). Now we can iterate:</p> <p>\\(n(1) = Rn(0) = 2\\times100 = 200\\)</p> <p>\\(n(2) = Rn(1) = 2\\times200 = 400\\)</p> <p>\\(\\vdots\\)</p> <p>After doing this a few more times we can plot</p> <pre>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# recursion\ndef exp_growth(n0, R, tmax):\n    '''define generator for exponential growth'''\n\n    # set initial values\n    t,n=0,n0\n\n    # yield new values\n    while t &lt; tmax:\n        yield t,n\n\n        # update values\n        t += 1\n        n = n * R\n\n# evaluate \ntn = exp_growth(n0=100, R=2, tmax=10) #choose some parameter values and initial conditions\ntns = np.array([vals for vals in tn]) #get all the t, n(t) values\n\n# plot\nfig, ax = plt.subplots()\nax.plot(tns[:,0], tns[:,1], marker = '.', markersize = 10)\nax.set_xlabel('time step, $t$')\nax.set_ylabel('population size, $n(t)$')\nplt.show()\n</pre> <p></p> <p>We can get a feel for the model by adjusting initial conditions and parameter values. What happens to the population size when you set \\(0&lt;R&lt;1\\) in the code above?</p> <p></p>"},{"location":"lectures/lecture-02/#2-plots-of-variables-as-a-function-of-themselves","title":"2. Plots of variables as a function of themselves","text":"<p>OK, so now we\u2019ll move on to a plot that is easier to generate and is very useful for models with just one variable (which is what we\u2019ve been working with so far).</p> <p>Instead of plotting the variable as a function of time, we\u2019ll plot the variable as a function of the variable in the previous time, e.g., plotting \\(n(t+1)\\) as a function of \\(n(t)\\). We could do this for exponential growth but let's move on to something else for variety.</p> <p>Consider a population with two types of individuals, \\(n_A(t)\\) with allele \\(A\\) and \\(n_a(t)\\) with allele \\(a\\). The frequency of \\(A\\) in the population is \\(p(t) = \\frac{n_A(t)}{n_A(t) + n_a(t)}\\). This is the variable we wish to track.</p> <p>Let\u2019s assume that each individual with an \\(A\\) leaves \\(W_A\\) descendants in the next generation and each individual with an \\(a\\) leaves \\(W_a\\) descendants. These \\(W_i\\) are referred to as the absolute fitnesses as they determine the (absolute) numbers of individuals with an \\(A\\) and an \\(a\\) in the next generation, \\(n_i(t+1) = W_i n_i(t)\\), for \\(i=A\\) and \\(i=a\\). In other words, each allele grows exponentially with reproductive value \\(R=W_i\\). The frequency of \\(A\\) in the next generation is then</p> \\[ \\begin{aligned} p(t+1)  &amp;= \\frac{n_A(t+1)}{n_A(t+1) + n_a(t+1)} \\\\ &amp;= \\frac{W_A n_A(t)}{W_A n_A(t) + W_a n_a(t)} \\\\ &amp;= \\frac{W_A\\frac{n_A(t)}{n_A(t) + n_a(t)}}{W_A\\frac{n_A(t)}{n_A(t) + n_a(t)} + W_a\\frac{n_a(t)}{n_A(t) + n_a(t)}}\\\\ &amp;= \\frac{W_A p(t)}{W_A p(t) + W_a (1-p(t))}. \\end{aligned} \\] <p>This is the recursion equation we want to plot. It is a classic model called haploid selection. Below is some code that plots \\(p(t+1)\\) as a function of \\(p(t)\\).</p> <pre>\nimport sympy\n\n# Build cobweb plotting function\ndef cobweb_haploid(p0, WA, Wa, max=np.inf):\n    t, pnow, pnext = 0, p0, 0 #initial conditions\n    while t &lt;= max:\n        yield pnow, pnext #current value of p(t) and p_(t+1)\n        pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) #update p_(t+1)\n        yield pnow, pnext #current value of p(t) and p_(t+1)\n        pnow = pnext #update p(t)\n        t += 1 #update t\n\n# Build function for generating figure\ndef plot_haploid_selection(WA, Wa, p0=0.5, ax=None):\n    pt = sympy.symbols('pt') #define our variable p(t)\n\n    # Write out sympy equation\n    f = (WA * pt) / (WA * pt + Wa * (1 - pt)) #the recursion equation\n\n    # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation (turn it into a function)\n    t = np.linspace(0,1,100)\n    fy = sympy.lambdify(pt, f)(t)\n\n    # Build plot\n    if ax == None:\n        fig, ax = plt.subplots()\n    ax.plot(t, fy, color='black', label=f\"$W_A$ = {WA}, $W_a$ = {Wa}\") #plot p_(t+1) as function of p(t)\n    ax.plot(t, t, color='black', linestyle='--') #draw 1:1 line for reference\n\n    # Add cobweb\n    cobweb = np.array([p for p in cobweb_haploid(p0, WA, Wa, max=100)])\n    ax.plot(cobweb[:,0], cobweb[:,1])\n\n    # Annotate and label plot\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,1)\n    ax.set_xlabel(\"allele frequency at $t$, $p(t)$\")\n    ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\")\n    ax.legend(frameon=False)\n    return ax\n\n# Plot figure\nfig, ax = plt.subplots(1,2)\nfig.set_size_inches(12,4)\n\n# First cobweb with WA &gt; Wa\nplot_haploid_selection(WA = 1, Wa = 0.5, ax=ax[0])\n\n# Second cobweb with WA &lt; Wa\nplot_haploid_selection(WA = 0.5, Wa = 1, ax=ax[1])\n\nplt.show()\n</pre> <p></p> <p>There are three components to this plot. First, the solid curve gives the recursion itself (\\(p(t+1)\\) as a function of \\(p(t)\\)). Second, the dashed line shows where \\(p(t+1)=p(t)\\). And third, the blue lines show how the variable changes over multiple time steps. </p> <p>Foreshadowing what is to come, the dashed line is helpful for two reasons. First, it indicates where the variable does not change over time. So wherever the recursion (solid line) intersects with the dashed line is an equilibrium. Second, it reflects \\(p(t+1)\\) back onto \\(p(t)\\), updating the variable. For example, in the left panel above we start with an allele frequency of \\(p(t)=0.5\\), draw a blue vertical line to the recursion to find \\(p(t+1)\\), and then update \\(p(t)\\) to \\(p(t+1)\\) by drawing the horizontal blue line to the dashed line. Now we can ask what \\(p(t+1)\\) is given this updated value of \\(p(t)\\) by drawing another vertical blue line, and so on. Following the blue line we can therefore see where the system is heading, which tells us about the stability of the equilibria. </p> <p>What are the stable equilibria in the two panels above?</p> <p>We can do something very similar for difference and differential equations. Now we plot the rate of change in the variable as a function of the current value of the variable, e.g., plot \\(\\Delta n = n(t+1)-n(t)\\) or \\(dn/dt\\) as a function of \\(n(t)\\).</p> <p>Let's consider haploid selection in continuous time. To derive the differential equation let's first return to exponential growth and turn this into a difference equation,</p> \\[ \\begin{aligned} n(t+1) &amp;= R n(t)\\\\ n(t+1) - n(t) &amp;= R n(t) - n(t)\\\\ \\Delta n &amp;= (R-1)n(t). \\end{aligned} \\] <p>Now recall that \\(R=1+B-D-BD\\) so that \\(R-1=B-D-BD\\). And let's consider a small timestep \\(\\Delta t\\) during which there are \\(B\\Delta t\\) births and \\(D\\Delta t\\) deaths per individual. Then the difference equation over this timestep, \\(\\Delta n = n(t+\\Delta t)-n(t)\\), is</p> \\[ \\Delta n = (B\\Delta t - D\\Delta t - BD(\\Delta t)^2)n(t). \\] <p>We then divide both sides by \\(\\Delta t\\) and take the limit as \\(\\Delta t\\rightarrow 0\\) to get the differential equation</p> \\[ \\begin{aligned} \\frac{\\Delta n}{\\Delta t} &amp;= (B - D - BD\\Delta t)n(t)\\\\ \\lim_{\\Delta t\\rightarrow0}\\frac{\\Delta n}{\\Delta t} &amp;= (B - D)n(t)\\\\ \\frac{\\mathrm{d}n}{\\mathrm{d}t} &amp;= rn(t). \\end{aligned} \\] <p>This is exponential growth in continuous time where \\(r\\) is the per capita growth rate.</p> <p>Now, returning to haploid selection, consider that allele \\(A\\) has growth rate \\(r_A\\) and allele \\(a\\) has growth rate \\(r_a\\), this gives two differential equations for the respective population sizes</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}n_A}{\\mathrm{d}t} &amp;= r_An_A(t)\\\\ \\frac{\\mathrm{d}n_a}{\\mathrm{d}t} &amp;= r_an_a(t). \\end{aligned} \\] <p>We now summon up the qoutient rule from 1st year calculus to derive the differential equation for the frequency of \\(A\\) over time</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}p}{\\mathrm{d}t} &amp;= \\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(\\frac{n_A(t)}{n_A(t)+n_a(t)}\\right)\\\\ &amp;= \\frac{dn_A/dt(n_A(t)+n_a(t)) - n_A(t)(dn_A/dt+dn_a/dt)}{(n_A(t)+n_a(t))^2}\\\\ &amp;= \\frac{r_An_A(t)(n_A(t)+n_a(t)) - n_A(t)(r_An_A(t)+r_an_a(t))}{(n_A(t)+n_a(t))^2}\\\\ &amp;= r_Ap(t) - p(t)(r_Ap(t)+r_a(1-p(t)))\\\\ &amp;= p(t)(1 - p(t))(r_A - r_a)\\\\ &amp;= p(t)(1 - p(t))s. \\end{aligned} \\] <p>The new parameter \\(s=r_A-r_a\\) is called the selection coefficient of \\(A\\) relative to \\(a\\). The plot of \\(dp/dt\\) vs. \\(p\\) is below.</p> <pre>\n# Initialize sympy symbols\np0, s, t = sympy.symbols('p0, s, t')\np = sympy.Function('t')\n\n# Specify differential equation\ndiffeq = sympy.Eq(p(t).diff(t), s * p(t) * (1 - p(t)))\n\n# Convert differential equation RHS to pythonic function\ndp = sympy.lambdify((s, p(t)), diffeq.rhs)\n\n# Plot the curve\nfig, ax = plt.subplots()\n\nfor s_coeff in [0.01, -0.01]:\n    ax.plot(\n        np.linspace(0, 1, 100),\n        dp(s_coeff, np.linspace(0,1, 100)),\n        label=f\"s = {s_coeff}\"\n    )\n\nax.set_xlabel('allele frequency at $t, p$')\nax.set_ylabel('change in allele frequency, $\\mathrm{d}p/\\mathrm{d}t$')\nax.legend(frameon=False)\nplt.show()\n</pre> <p></p> <p>What does this tell us about how allele frequency will change when \\(s&gt;0\\) vs. \\(s&lt;0\\)? And what allele frequencies, \\(p\\), cause more rapid evolution?</p> <p>Now let's simplify the above plots and just indicate the direction (and magnitude) of change in \\(p(t)\\) with time. This is known as a phase-line diagram.</p> <pre>\ndef phase_line_haploid(p0, WA, Wa, max=np.inf):\n    'generator for p(t)'\n    t, pnow, pnext = 0, p0, 0 #initial conditions\n    while t &lt; max:\n        yield pnow #current value of p(t) and p_(t+1)\n        pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow))\n        pnow = pnext #update p(t)\n        t += 1 #update t\n\ndef plot_phase_line_haploid(WA, Wa, p0, max=20, ax=None):\n    'plot phase line'\n\n    # Set up figure\n    if ax==None:\n        fig, ax = plt.subplots()\n        fig.set_size_inches(8,0.25)\n    ax.axhline(0, color='black', linewidth=0.5)\n\n    # Plot phase-line\n    pts = [pt for pt in phase_line_haploid(p0, WA, Wa, max=max)] #pt values\n    ax.plot(\n        pts,\n        np.zeros(max) #dummy y values (0 for all x values) because we want to plot a 1d line\n    )\n\n    # Plot vector field\n    marker = '&gt;' if pts[2] &gt; pts[1] else '&lt;' #determine which direction to point based on first 2 time points\n    ax.scatter(\n        pts,\n        np.zeros(max),#dummy y again\n        marker=marker, s=150\n    )\n\n    # Remove background axes\n    ax.set_ylabel('$p$', rotation=0)\n    ax.set_xlabel(f\"$W_A$ = {WA}, $W_a$ = {Wa}, $p_0$ = {p0}\")\n    ax.get_yaxis().set_ticks([])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.set_xlim(0,1)\n    plt.show()\n\nplot_phase_line_haploid(WA=1, Wa=0.5, p0=0.01)\n\nplot_phase_line_haploid(WA=0.5, Wa=1, p0=0.99)\n</pre> <p></p> <p></p> <p>As above, we see the frequency of \\(A\\) approaches \\(p=1\\) when \\(W_A&gt;W_a\\) (i.e., \\(s&gt;0\\)) and \\(p=0\\) when \\(W_a&gt;W_A\\) (i.e., \\(s&lt;0\\)). We also notice, as above, the changes are fastest (fewer, longer arrows) at intermediate frequencies.</p> <p></p>"},{"location":"lectures/lecture-02/#3-summary","title":"3. Summary","text":"<p>To get a feel for a model it is helpful to plot some numerical examples:</p> <ul> <li>plot the variable as a function of time (\"simulate\")</li> <li>plot the variable (or change in variable) as a function of itself</li> </ul> <p>We've also just been introduced to two classic models in ecology and evolution, in both discrete and continuous time:</p> <ul> <li>exponential growth</li> <li>haploid selection</li> </ul>"},{"location":"lectures/lecture-03/","title":"Lecture 3","text":""},{"location":"lectures/lecture-03/#lecture-3-equilibria-univariate","title":"Lecture 3: Equilibria (univariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-03/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Equilibria</li> <li>Example: diploid selection</li> <li>Summary</li> </ol>"},{"location":"lectures/lecture-03/#1-equilibria","title":"1. Equilibria","text":"<p>An equilibrium is any state of a system which tends to persist unchanged over time.</p> <p>For discrete-time models, the equilibria are defined as those values of the variables where no changes occur from one time step to the next. </p> <p>For example, those values of allele frequency \\(p(t)\\) where</p> \\[ \\begin{aligned} \\Delta p &amp;= 0\\\\ p(t+1) - p(t) &amp;= 0\\\\ p(t+1) &amp;= p(t) \\end{aligned} \\] <p>Similarly, for continuous-time models, the equilibria are defined as those values of the variables for which the rate of change in the variables equals zero. </p> <p>For example, those values of allele frequency \\(p(t)\\) where</p> \\[ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = 0 \\] <p>What are the equilibria of the exponential population growth and haploid selection models discussed in the previous lecture?</p> <p></p>"},{"location":"lectures/lecture-03/#2-example-diploid-selection","title":"2. Example: diploid selection","text":""},{"location":"lectures/lecture-03/#the-model","title":"The model","text":"<p>Let's now consider selection on diploid individuals, where each individual is characterized by two alleles at a locus. This leads to three genotypes: \\(AA\\), \\(Aa\\), and \\(aa\\). The \\(Aa\\) genotype is called the heterozygote and the others are homozygotes.</p> <p>Let the number of individuals with each genotype be</p> <ul> <li>\\(n_{AA}(t) =\\) number of individuals with the \\(AA\\) genotype in generation \\(t\\)</li> <li>\\(n_{Aa}(t) =\\) number of individuals with the \\(Aa\\) genotype in generation \\(t\\)</li> <li>\\(n_{aa}(t) =\\) number of individuals wite the \\(aa\\) genotype in generation \\(t\\)</li> </ul> <p>The frequency of allele \\(A\\) is calculated by counting up all the \\(A\\) alleles in the population and dividing by the total number of alleles,</p> \\[ \\begin{aligned} p(t) &amp;= \\frac{2n_{AA}(t) + n_{Aa}(t)}{2(n_{AA}(t) + n_{Aa}(t) + n_{aa}(t))} \\\\ &amp;= \\frac{n_{AA}(t) + \\frac{1}{2}n_{Aa}(t)}{n_{AA}(t) + n_{Aa}(t) + n_{aa}(t)}. \\end{aligned} \\] <p>To determine the recursion equation, let's use the following life cycle diagram, where we census the population immediately after the diploid individuals are formed by union of the haploid gametes</p> <pre><code>    graph LR;\n    A((p'')) --gamete union--&gt; B((p));\n    B --selection--&gt; C((p'));\n    C --meiosis--&gt; A;</code></pre> <p>Now, let\u2019s assume that during selection each diploid individual has reproductive factor</p> <ul> <li>\\(W_{AA} =\\) reproductive factor of individuals with the \\(AA\\) genotype </li> <li>\\(W_{Aa} =\\) reproductive factor of individuals with the \\(Aa\\) genotype</li> <li>\\(W_{aa} =\\) reproductive factor of individuals with the \\(aa\\) genotype</li> </ul> <p>These reproductive factors are again referred to as the absolute fitnesses as they determine the (absolute) numbers of individuals after selection, \\(n_i' = W_i n_i(t)\\) for \\(i=AA\\), \\(i=Aa\\), and \\(i=aa\\).</p> <p>After selection these genotypes segregate into haploids via meiosis, go through the haploid phase of the life cycle, and then randomly pair to create diploids again. Random union and segregation shuffle alleles between genotypes but don't affect allele frequency. The frequency of \\(A\\) in the next generation is therefore</p> \\[ \\begin{aligned} p(t+1)  &amp;= \\frac{n_{AA}(t+1) + \\frac{1}{2}n_{Aa}(t+1)}{n_{AA}(t+1) + n_{Aa}(t+1) + n_{aa}(t+1)}\\\\ &amp;= \\frac{W_{AA}n_{AA}(t) + \\frac{1}{2}W_{Aa}n_{Aa}(t)}{W_{AA}n_{AA}(t) + \\frac{1}{2}W_{Aa}n_{Aa}(t) + W_{aa}n_{aa}(t)}. \\end{aligned} \\] <p>We want the recursion equation in terms of allele frequency, so we want to replace the \\(n_i\\)'s on the right hand side of this equation with \\(p\\)'s. To do this we note that given the random union of gametes the diploid offspring are in Hardy-Weinberg proportions, i.e.,</p> \\[ \\begin{aligned} n_{AA}(t) &amp;= p(t)^2 n(t) \\\\ n_{Aa}(t) &amp;= 2p(t) q(t) n(t) \\\\ n_{aa}(t) &amp;= q(t)^2 n(t) \\end{aligned} \\] <p>where \\(n(t) = n_{AA}(t) + n_{Aa}(t) + n_{aa}(t)\\) is the total population size.</p> <p>Substituting these Hardy-Weinberg proportions in and simplifying, the total population size cancels out and we can rewrite the above equation in terms of allele frequency alone,</p> \\[ \\begin{aligned} p(t+1)  &amp;= \\frac{W_{AA}p(t)^2 n(t) + W_{Aa}p(t) q(t) n(t)}{W_{AA}p(t)^2 n(t) + 2W_{Aa}p(t) q(t) n(t) + W_{aa}q(t)^2 n(t)}\\\\ &amp;= \\frac{W_{AA}p(t)^2 + W_{Aa}p(t) q(t)}{W_{AA}p(t)^2 + 2W_{Aa}p(t) q(t) + W_{aa}q(t)^2}. \\end{aligned} \\] <p>where we have used \\(q(t)=1-p(t)\\) for the frequency of \\(a\\) for convenience. This is a recursion equation for allele frequency in our model of diploid selection.</p>"},{"location":"lectures/lecture-03/#the-equilbria","title":"The equilbria","text":"<p>To find the equilibria we replace \\(p(t+1)\\) and \\(p(t)\\) with \\(\\hat p\\) and the \\(q(t)\\) with \\(\\hat{q}\\) and solve for these equilibrium values, \\(\\hat p\\) and \\(\\hat q\\),</p> \\[ \\begin{aligned} \\hat{p} &amp;= \\frac{\\hat{p}^2 W_{AA} + \\hat{p} \\hat{q} W_{Aa}}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}}\\\\ \\hat{p} &amp;= \\frac{\\hat{p}(\\hat p W_{AA} + \\hat{q} W_{Aa})}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}} \\end{aligned} \\] <p>We see that \\(\\hat{p}=0\\) is one equilibrium as then both sides of the equation are 0. If \\(\\hat p\\) is not 0 we can divide by \\(\\hat p\\) to get</p> \\[ \\begin{aligned} 1 &amp;= \\frac{\\hat p W_{AA} + \\hat{q} W_{Aa}}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}}\\\\ \\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa} &amp;= \\hat p W_{AA} + \\hat{q} W_{Aa}\\\\ 0 &amp;= (\\hat{p} - \\hat{p}^2) W_{AA} + (\\hat{q} - 2 \\hat{p} \\hat{q}) W_{Aa} - \\hat{q}^2 W_{aa}\\\\ 0 &amp;= \\hat{p}(1 - \\hat{p}) W_{AA} + \\hat{q}(1 - 2 \\hat{p}) W_{Aa} - \\hat{q}^2 W_{aa}\\\\ 0 &amp;= \\hat{q}(\\hat{p} W_{AA} + (1 - 2 \\hat{p}) W_{Aa} - \\hat{q} W_{aa}) \\end{aligned} \\] <p>And so \\(\\hat{q}=0\\implies\\hat{p}=1\\) is another equilibrium. Dividing by \\(\\hat{q}\\) and putting everything in terms of \\(p\\) we have</p> \\[ \\begin{aligned} 0 &amp;= \\hat{p} W_{AA} + (1 - 2 \\hat{p}) W_{Aa} - \\hat{q} W_{aa}\\\\ 0 &amp;= \\hat{p} W_{AA} + (1 - 2 \\hat{p}) W_{Aa} - (1 - \\hat{p}) W_{aa}\\\\ 0 &amp;= \\hat{p}(W_{AA} -2W_{Aa} + W_{aa}) + W_{Aa} - W_{aa}\\\\ W_{aa} - W_{Aa} &amp;= \\hat p(W_{AA} -2W_{Aa} + W_{aa})\\\\ \\frac{W_{Aa} - W_{aa}}{2W_{Aa} - W_{AA} - W_{aa}} &amp;= \\hat p\\\\ \\end{aligned} \\] <p>We therefore have three equilibria under diploid selection: </p> <ul> <li>\\(\\hat{p}=0\\)</li> <li>\\(\\hat p = \\frac{W_{Aa} - W_{aa}}{2W_{Aa} - W_{AA} - W_{aa}}\\)</li> <li>\\(\\hat p = 1\\)</li> </ul> <p>Since a frequency is bounded between 0 and 1, we must have \\(0 \\leq p \\leq 1\\). We therefore call \\(\\hat{p}=0\\) and \\(\\hat{p}=1\\) boundary equilibria. These bounds also imply the third equilibrium is only biologically valid when </p> \\[ 0 \\leq \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} \\leq 1. \\] <p>When \\(W_{Aa} = W_{aa}\\) this equilibrium reduces to \\(\\hat{p}=0\\) and when \\(W_{Aa} = W_{AA}\\) this reduces to \\(\\hat{p}=1\\) (check this for yourself). The third equilibrium will be an internal equilibrium, representing a population with both \\(A\\) and \\(a\\) alleles, when</p> \\[ 0 &lt; \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} &lt; 1. \\] <p>The equilibrium is positive when the numerator and denominator have the same sign (i.e., are both positive or both negative). Let's split this into two \"cases\". Case A will have a positive numerator, \\(W_{Aa} &gt; W_{aa}\\), and Case B will have a negative numerator, \\(W_{Aa} &lt; W_{aa}\\). So, in Case A, the equilibrium is positive when the denominator is positive, \\(2 W_{Aa} - W_{AA} - W_{aa} &gt; 0\\). Meanwhile in case B the equilibrium is positive when the denominator is negative, \\(2 W_{Aa} - W_{AA} - W_{aa} &lt; 0\\).</p> <p>Now we can rearrange the equilibrium to show that it is less than 1 when</p> \\[ \\begin{aligned} \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} &amp;&lt; 1\\\\ \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} - 1 &amp;&lt; 0\\\\ \\frac{W_{Aa} - W_{aa} - (2 W_{Aa} -W_{AA} - W_{aa})}{2 W_{Aa} -W_{AA} - W_{aa}} &amp;&lt; 0\\\\ \\frac{W_{AA} - W_{Aa}}{2 W_{Aa} -W_{AA} - W_{aa}} &amp;&lt; 0\\\\ \\frac{W_{Aa} - W_{AA}}{2 W_{Aa} -W_{AA} - W_{aa}} &amp;&gt; 0. \\end{aligned} \\] <p>Again, we need the numerator and denominator to have the same sign for this inequality to hold. In case A, where we've said that denominator is positive, this means we also need the numerator to be positive, \\(W_{Aa} &gt; W_{AA}\\). While in case B we said that the denominator is negative, so we also need the numerator to be negative, \\(W_{Aa} &lt; W_{AA}\\).</p> <p>Putting this all together, there is a biologically-relevant internal equilibrium when either</p> <ul> <li>Case A: \\(W_{Aa} &gt; W_{aa}\\) and \\(W_{Aa} &gt; W_{AA}\\) (which ensures \\(2 W_{Aa} - W_{AA} - W_{aa} &gt; 0\\); go ahead and check!)</li> <li>Case B: \\(W_{Aa} &lt; W_{aa}\\) and \\(W_{Aa} &lt; W_{AA}\\)  (which ensures \\(2 W_{Aa} - W_{AA} - W_{aa} &lt; 0\\))</li> </ul> <p>Case A therefore represents \"heterozygote advantage\", \\(W_{AA} &lt; W_{Aa} &gt; W_{aa}\\), while Case B represents \"heterozygote disadvantage\", \\(W_{AA} &gt; W_{Aa} &lt; W_{aa}\\). </p> <p></p>"},{"location":"lectures/lecture-03/#3-summary","title":"3. Summary","text":"<p>Equilibria are defined by the values of the variables that persist over time, i.e., where the change is zero.</p> <p>With diploid selection there are three equilibria, two external and one potentially internal -- in the next lecture we'll see which are stable.</p>"},{"location":"lectures/lecture-04/","title":"Lecture 04","text":""},{"location":"lectures/lecture-04/#lecture-4-one-locus-selection","title":"Lecture 4: One-locus selection","text":"Run notes interactively?"},{"location":"lectures/lecture-04/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Review of Hardy-Weinberg</li> <li>One-locus haploid selection in discrete time</li> <li>One-locus diploid selection in discrete time</li> <li>Comparing haploid and diploid selection</li> <li>One-locus haploid selection in continuous time</li> <li>Comparing haploid selection in discrete and continuous time</li> <li>The models we've covered</li> </ol>"},{"location":"lectures/lecture-04/#1-review-of-hardy-weinberg","title":"1. Review of Hardy-Weinberg","text":"<p>In Lecture 2 we saw that one round of random union among gametes (equivalently, random mating among diploids) causes the frequency of diploid genotypes at a locus with two alleles, \\(A\\) and \\(a\\), to become</p> <ul> <li>\\(AA\\): \\(p^2\\)</li> <li>\\(Aa\\): \\(2pq\\)</li> <li>\\(aa\\): \\(q^2\\)</li> </ul> <p>where \\(p\\) is the frequency of allele \\(A\\) and \\(q=1-p\\) is the frequency of allele \\(a\\). </p> <p>A population with these diploid genotype frequencies is said to be in Hardy-Weinberg equilibrium. Furthermore, we showed that under this model the allele frequencies do not change, \\(p' = p\\). </p> <p>But what if there is natural selection?</p> <p></p>"},{"location":"lectures/lecture-04/#2-one-locus-haploid-selection-in-discrete-time","title":"2. One-locus haploid selection in discrete time","text":"<p>We begin by examining a model where selection acts during the haploid phase of the life-cycle. This is a little simpler than diploid selection because there are only two haploid genotypes, \\(A\\) and \\(a\\). It is also a very relevant model for species with long haploid phases (eg, some algae and fungi) or with strong competition during the haploid phase (eg, pollen competiting for ovules).</p>"},{"location":"lectures/lecture-04/#life-cycle-diagram","title":"Life-cycle diagram","text":"<p>We first illustrate the structure of the model with a life-cycle diagram.</p> <pre><code>    graph LR;\n    A((p)) --gamete union--&gt; B((p'));\n    B --meiosis--&gt; C((p''));\n    C --selection--&gt; A;</code></pre> <p>We will census the population at the beginning of the haploid phase (immediately after meiosis).</p>"},{"location":"lectures/lecture-04/#deriving-the-equations","title":"Deriving the equations","text":"<p>Let the number of haploid individuals with each allele be</p> <ul> <li>\\(n_A(t) =\\) number of individuals with the \\(A\\) allele in generation \\(t\\)</li> <li>\\(n_a(t) =\\) number of individuals with the \\(a\\) allele in generation \\(t\\)</li> </ul> <p>The frequency of \\(A\\) is therefore \\(p(t) = \\frac{n_a(t)}{n_A(t) + n_a(t)}\\).</p> <p>Now, let\u2019s assume that during selection each haploid individual has reproductive factor</p> <ul> <li>\\(W_A =\\) reproductive factor of individuals with the \\(A\\) allele </li> <li>\\(W_a =\\) reproductive factor of individuals with the \\(a\\) allele</li> </ul> <p>These reproductive factors are referred to as the absolute fitnesses as they determine the (absolute) numbers of individuals after selection, \\(n_i' = W_i n_i(t)\\) for \\(i=A\\) and \\(i=a\\). To relate back to the models of population growth in Lecture 3, this is exponential growth of each allele.</p> <p>After selection these alleles randomly pair, go through the dipliod phase of the life cycle, and then segregate back into haploids after meiosis. From our analysis of Hardy-Weinberg we know random mating and segregation don't affect allele frequency. The frequency of \\(A\\) in the next generation is therefore</p> \\[ \\begin{aligned} p(t+1)  &amp;= \\frac{n_A(t+1)}{n_A(t+1) + v_a n_a(t)} \\\\ &amp;= \\frac{W_A n_A(t)}{W_A n_A(t) + W_a n_a(t)}. \\end{aligned} \\] <p>To make this a recursion equation we need to write \\(p(t+1)\\) in terms of \\(p(t)\\), so that if we knew \\(p\\) at some point in time we can recursively calculate it in all future times. To do this we divide both the numerator and denominator by the total number of individuals, \\(n_A(t) + n_a(t)\\), and simplify</p> \\[ \\begin{aligned} p(t+1)  &amp;= \\frac{W_A n_A(t)}{W_A n_A(t) + W_a n_a(t)} \\\\ &amp;= \\frac{W_A\\frac{n_A(t)}{n_A(t) + n_a(t)}}{W_A\\frac{n_A(t)}{n_A(t) + n_a(t)} + W_a\\frac{n_a(t)}{n_A(t) + n_a(t)}}\\\\ &amp;= \\frac{W_A p(t)}{W_A p(t) + W_a q(t)}. \\end{aligned} \\] <p>This is now a recursion equation for the allele frequency in our model of haploid selection.</p>"},{"location":"lectures/lecture-04/#simplifying","title":"Simplifying","text":"<p>Our current recursion is a function of two parameters, the absolute fitnesses \\(W_A\\) and \\(W_a\\). Now notice that if we divide both the numerator and denominator by one of these fitnesses, say \\(W_a\\), then we can reduce the recursion to a function of only one parameter, \\(w_A = W_A/W_a\\), the fitness of \\(A\\) relative to the fitness of \\(a\\),</p> \\[ \\begin{aligned} p(t+1) &amp;= \\frac{(W_A/W_a)p(t)}{(W_A/W_a)p(t) + (W_a/W_a)q(t)}\\\\ &amp;= \\frac{w_A p(t)}{w_A p(t) + q(t)}. \\end{aligned} \\] <p>Note that the allele frequency dynamics depend only on the relative fitnesses, and not on the absolute fitnesses, meaning that evolution does not depend on how the size of the population changes. It is therefore possible to study evolution while ignoring population dynamics under this simple model.</p> <p>Let's plot the recursion to get a sense of the dynamics.</p> <pre>\nimport matplotlib.pyplot as plt\n\n# calculate allele frequency over time with recursion\nw_A = 1.1 #relative fitness of allele A\np_now = 0.01 #initial allele frequency, p_0\nps = [] #list to hold allele frequencies over time\nts = range(100) #list of time steps\nfor t in ts: #for each time\n    ps.append(p_now) #add current allele frequency to list\n    p_now = w_A*p_now/(w_A*p_now+(1-p_now)) #update allele frequency with our recursion equation\n\n# plot\nplt.scatter(ts, ps) #plot the (t,p) pairs\nplt.xlabel(\"generation, $t$\") #label axes\nplt.ylabel(\"frequency of $A$ allele, $p(t)$\")\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-04/#3-one-locus-diploid-selection-in-discrete-time","title":"3. One-locus diploid selection in discrete time","text":"<p>Since we are all currently in the diploid phase of our life-cycle, it is natural to ask: Does selection in the diploid phase work the same way?</p>"},{"location":"lectures/lecture-04/#life-cycle-diagram_1","title":"Life-cycle diagram","text":"<p>We now have the following life-cycle diagram.</p> <pre><code>    graph LR;\n    A((p)) --gamete union--&gt; B((p'));\n    B --selection--&gt; C((p''));\n    C --meiosis--&gt; A;</code></pre> <p>We will census at the beginning of the diploid phase (immediately after gamete union).</p>"},{"location":"lectures/lecture-04/#deriving-the-equations_1","title":"Deriving the equations","text":"<p>Let the number of diploid individuals with each genotype be</p> <ul> <li>\\(n_{AA}(t) =\\) number of individuals with the \\(AA\\) genotype in generation \\(t\\)</li> <li>\\(n_{Aa}(t) =\\) number of individuals with the \\(Aa\\) genotype in generation \\(t\\)</li> <li>\\(n_{aa}(t) =\\) number of individuals wite the \\(aa\\) genotype in generation \\(t\\)</li> </ul> <p>The frequency of allele \\(A\\) is then</p> \\[ \\begin{aligned} p(t) &amp;= \\frac{2n_{AA}(t) + n_{Aa}(t)}{2n_{AA}(t) + n_{Aa}(t) + n_{aa}(t)} \\\\ &amp;= \\frac{n_{AA}(t) + \\frac{1}{2}n_{Aa}(t)}{n_{AA}(t) + n_{Aa}(t) + n_{aa}(t)} \\end{aligned} \\] <p>Now, let\u2019s assume that during selection each diploid individual has reproductive factor</p> <ul> <li>\\(W_{AA} =\\) reproductive factor of individuals with the \\(AA\\) genotype </li> <li>\\(W_{Aa} =\\) reproductive factor of individuals with the \\(Aa\\) genotype</li> <li>\\(W_{aa} =\\) reproductive factor of individuals with the \\(aa\\) genotype</li> </ul> <p>These reproductive factors are again referred to as the absolute fitnesses as they determine the (absolute) numbers of individuals after selection, \\(n_i' = W_i n_i(t)\\) for \\(i=AA\\), \\(i=Aa\\), and \\(i=aa\\).</p> <p>After selection these genotypes segregate into haploids via meiosis, go through the haploid phase of the life cycle, and then random join to create diploids again. Again, we know random union and segregation don't affect allele frequency. The frequency of \\(A\\) in the next generation is therefore</p> \\[ \\begin{aligned} p(t+1)  &amp;= \\frac{n_{AA}(t+1) + \\frac{1}{2}n_{Aa}(t+1)}{n_{AA}(t+1) + n_{Aa}(t+1) + n_{aa}(t+1)}\\\\ &amp;= \\frac{W_{AA}n_{AA}(t) + \\frac{1}{2}W_{Aa}n_{Aa}(t)}{W_{AA}n_{AA}(t) + \\frac{1}{2}W_{Aa}n_{Aa}(t) + W_{aa}n_{aa}(t)}. \\end{aligned} \\] <p>As above, we want a recursion equation in terms of allele frequency, so we want to replace the \\(n_i\\)'s in the right hand side of this equation with \\(p\\)'s. To do this we note that with the random union of gametes the diploid offspring are in Hardy-Weinberg proportions, so that</p> \\[ \\begin{aligned} n_{AA}(t) &amp;= p(t)^2 n(t) \\\\ n_{Aa}(t) &amp;= 2p(t) q(t) n(t) \\\\ n_{aa}(t) &amp;= q(t)^2 n(t) \\end{aligned} \\] <p>where \\(n(t) = n_{AA}(t) + n_{Aa}(t) + n_{aa}(t)\\) is the total population size.</p> <p>Substituting these Hardy-Weinberg proportions in and simplifying, the total population size cancels out and we can rewrite the above equation in terms of allele frequency alone,</p> \\[ \\begin{aligned} p(t+1)  &amp;= \\frac{W_{AA}p(t)^2 n(t) + W_{Aa}p(t) q(t) n(t)}{W_{AA}p(t)^2 n(t) + 2W_{Aa}p(t) q(t) n(t) + W_{aa}q(t)^2 n(t)}\\\\ &amp;= \\frac{W_{AA}p(t)^2 + W_{Aa}p(t) q(t)}{W_{AA}p(t)^2 + 2W_{Aa}p(t) q(t) + W_{aa}q(t)^2}. \\end{aligned} \\] <p>This is a recursion equation for allele frequency in our model of diploid selection.</p>"},{"location":"lectures/lecture-04/#simplifying_1","title":"Simplifying","text":"<p>As in the haploid selection case, we can divide by one of the absolute fitnesses, say \\(W_{aa}\\), to remove one parameter from the model</p> \\[ \\begin{aligned} p(t+1) &amp;= \\frac{(W_{AA}/W_{aa})p(t)^2 + (W_{Aa}/W_{aa})p(t) q(t)}{(W_{AA}/W_{aa})p(t)^2 + 2(W_{Aa}/W_{aa})p(t) q(t) + (W_{aa}/W_{aa})q(t)^2} \\\\ &amp;= \\frac{w_{AA}p(t)^2 + w_{Aa}p(t) q(t)}{w_{AA}p(t)^2 + 2w_{Aa}p(t) q(t) + q(t)^2} \\end{aligned} \\] <p>This recursion is a function of only two relative fitnesses, \\(w_{AA} = W_{AA}/W_{aa}\\) and \\(w_{Aa} = W_{Aa}/W_{aa}\\).</p> <p>Let's plot the recursion to get a sense of the dynamics.</p> <pre>\nimport matplotlib.pyplot as plt\n\n# calculate allele frequency over time with recursion\nw_AA = 1.2 #relative fitness of genotype AA\nw_Aa = 1.1 #relative fitness of genotype Aa \np_now = 0.01 #initial allele frequency, p_0\nps = [] #list to hold allele frequencies over time\nts = range(100) #list of time steps\nfor t in ts: #for each time\n    ps.append(p_now) #add current allele frequency to list\n    p_now = (w_AA*p_now**2 + w_Aa*p_now*(1-p_now))/(w_AA*p_now**2 + 2*w_Aa*p_now*(1-p_now) + (1-p_now)**2) #update allele frequency with our recursion equation\n\n# plot\nplt.scatter(ts, ps)\nplt.xlabel(\"generation, $t$\")\nplt.ylabel(\"frequency of $A$ allele, $p(t)$\")\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-04/#4-comparing-haploid-and-diploid-selection","title":"4. Comparing haploid and diploid selection","text":"<p>So, returning to our original question, let's compare evolution under haploid selection</p> \\[ p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a q(t)} \\] <p>to evolution under diploid selection</p> \\[ p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t) q(t)}{W_{AA}p(t)^2 + 2W_{Aa}p(t) q(t) + W_{aa}q(t)^2}. \\] <p>To facilitate this, let\u2019s assume the fitness of a diploid genotype is the product of the haploid fitnesses, i.e., \\(W_{AA} = W_A W_A\\), \\(W_{Aa} = W_A W_a\\), and \\(W_{aa} = W_a W_a\\).</p> <p>It then happens that our diploid recursion reduces to the haploid recursion,</p> \\[ \\begin{aligned} p(t+1) &amp;=  \\frac{W_A W_A p(t)^2 + W_A W_a p(t) q(t)}{W_A W_A p(t)^2 + 2W_A W_a p(t) q(t) + W_a W_a q(t)^2}\\\\ &amp;= \\frac{W_A p(t)(p(t) + W_a q(t))}{W_A p(t)(W_A p(t) + W_a q(t)) + W_a q(t)(W_A p(t) + W_a q(t))}\\\\ &amp;= \\frac{W_A p(t)}{W_A p(t) + W_a q(t)} \\end{aligned} \\] <p>This shows that we need twice as much selection under diploid selection relative to that under haploid selection (e.g., \\(W_{AA} = W_A W_A\\)) for evolution to proceed as quickly. Why is evolution slower under diploid selection?</p> <p>Let's check this visually (try breaking the assumption above, \\(W_{ij}=W_i W_j\\), to see what happens).</p> <pre>\nimport matplotlib.pyplot as plt\n\n# calculate allele frequency over time with recursion\nw_A = 1.1 #relative fitness of allele A in the haploid model\nw_AA = w_A*w_A #relative fitness of genotype AA in the diploid model\nw_Aa = w_A #relative fitness of genotype Aa in the diploid model\np_now_hap = 0.01 #initial allele frequency, p_0\np_now_dip = p_now_hap\nps_hap, ps_dip = [], [] #lists to hold allele frequencies over time\nts = range(100) #list of time steps\nfor t in ts: #for each time\n    ps_hap.append(p_now_hap) #add current allele frequency to list\n    p_now_hap = w_A*p_now_hap/(w_A*p_now_hap+(1-p_now_hap)) #update allele frequency with haploid recursion equation\n    ps_dip.append(p_now_dip) #add current allele frequency to list\n    p_now_dip = (w_AA*p_now_dip**2 + w_Aa*p_now_dip*(1-p_now_dip))/(w_AA*p_now_dip**2 + 2*w_Aa*p_now_dip*(1-p_now_dip) + (1-p_now_dip)**2) #update allele frequency with diploid recursion equation\n\n# plot\nplt.scatter(ts, ps_hap, label='haploid')\nplt.scatter(ts, ps_dip, s=10, label='diploid')\nplt.xlabel(\"generation, $t$\")\nplt.ylabel(\"frequency of $A$ allele, $p(t)$\")\nplt.legend() #add legend to know which points belong to which model\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-04/#5-one-locus-haploid-selection-in-continuous-time","title":"5. One-locus haploid selection in continuous time","text":"<p>To model haploid selection in continuous-time we assume that during the selective phase each haploid individual has growth rate</p> <ul> <li>\\(r_A =\\) growth rate of individuals with the \\(A\\) allele</li> <li>\\(r_a =\\) growth rate of individuals with the \\(a\\) allele</li> </ul> <p>and the relative numbers of each type don't change otherwise (ie, during union, diploidy, or meiosis).</p> <p>We therefore have exponential growth of both genotypes, \\(\\frac{\\mathrm{d} n_i}{dt} = r_i n_i\\).</p> <p>At any particular point in time, \\(t\\), the frequency of allele \\(A\\) is, \\(p(t) = n_A(t)/(n_A(t) + n_a(t))\\). </p> <p>We can therefore derive the rate of change in the frequency of allele \\(A\\), \\(\\mathrm{d}p/\\mathrm{d}t\\), using the qoutient rule (see Appendix 2 in the text for help with this and other math tricks)</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}p}{\\mathrm{d}t} &amp;= \\frac{\\mathrm{d}\\frac{n_A}{n_A + n_a}}{\\mathrm{d}t} \\\\ &amp;= \\frac{\\frac{\\mathrm{d}n_A}{\\mathrm{d}t} (n_A(t) + n_a(t)) - n_A(t) \\frac{\\mathrm{d}(n_A+n_a)}{\\mathrm{d}t}}{(n_A(t) + n_a(t))^2}\\\\ &amp;= \\frac{r_A n_A(t) (n_A(t) + n_a(t)) - n_A(t) (r_A n_A(t) + r_a n_a(t))}{(n_A(t) + n_a(t))^2}\\\\ &amp;= \\frac{r_A n_A(t) n_a(t) - r_a n_A(t) n_a(t))}{(n_A(t) + n_a(t))^2}\\\\ &amp;= \\frac{(r_A -r_a)n_A(t) n_a(t)}{(n_A(t) + n_a(t))^2}\\\\ &amp;= (r_A -r_a)p(t) q(t), \\end{aligned} \\] <p>where \\(s_c = r_A - r_a\\) is the continuous-time selection coefficient of allele \\(A\\).</p> <p>A similar equation can be derived for the model of diploid-selection in continuous time, but we will not study it (see Problem 3.16 in the text if you are curious).</p> <p></p>"},{"location":"lectures/lecture-04/#6-comparing-haploid-selection-in-discrete-and-continuous-time","title":"6. Comparing haploid selection in discrete and continuous time","text":"<p>Are the discrete- and continuous-time models of haploid selection as different as they look?</p> \\[ p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a q(t)} \\] \\[ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = s_c p(t) q(t) \\] <p>Not really. Discrete and continuous time models generally behave in a similar fashion when changes occur slowly over time. For this model of haploid selection, this implies that the discrete and continuous models will be similar when the fitnesses of the two alleles are nearly equal, i.e., when \\(W_A - W_a\\) is small. This is called \"weak selection\".</p> <p>In the discrete model, the change in the allele frequency is</p> \\[ \\begin{aligned} \\Delta p &amp;= p(t+1) - p(t) \\\\ &amp;= \\frac{W_A p(t)}{W_A p(t) + W_a q(t)} - p(t) \\\\ &amp;= \\frac{(W_A - W_a)p(t) q(t)}{W_A p(t) + W_a q(t)}. \\end{aligned} \\] <p>Now define \\(s_d = (W_A - W_a)/W_a\\) as the discrete time selection coefficient. Subbing this in as \\(W_A = s_d W_a + W_a\\) gives</p> \\[ \\begin{aligned} \\Delta p  &amp;= \\frac{(W_A-W_a)p(t) q(t)}{W_A p(t) + W_a q(t)}\\\\ &amp;= \\frac{(s_d W_a + W_a - W_a) p(t) q(t)}{(s_d W_a + W_a) p(t) + W_a q(t)}\\\\ &amp;= \\frac{s_d W_a p(t) q(t)}{s_d W_a p(t) + W_a (p(t) + q(t))}\\\\ &amp;= \\frac{s_d W_a p(t) q(t)}{s_d W_a p(t) + W_a}\\\\ &amp;= \\frac{s_d p(t) q(t)}{s_d p(t) + 1}\\\\ \\end{aligned} \\] <p>Now assume weak selection, i.e., that \\(s_d\\) is small. This implies \\(s_d p(t) + 1 \\approx 1\\). Making this approximation we have</p> \\[ \\Delta p \\approx s_d p(t) q(t) \\] <p>This is equivalent to the continuous-time model when the selection coefficients are equal, \\(s_c = s_d\\).</p> <p></p>"},{"location":"lectures/lecture-04/#7-the-models-weve-covered","title":"7. The models we've covered","text":"<p>To summarize the last two lectures, we've derived four of the most classic models in ecology and evolution:</p> Model Discrete time Continous time Exponential growth \\(n(t+1) = R n(t)\\) \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t} = r n(t)\\) Logistic growth \\(n(t+1) = (1 + r(1 - \\frac{n(t)}{K}))n(t)\\) \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t} = r(1 - \\frac{n(t)}{K})n(t)\\) Haploid selection \\(p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a q(t)}\\) \\(\\frac{\\mathrm{d}p}{\\mathrm{d}t} = s p(t)(1-p(t))\\) Diploid selection \\(p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t) q(t)}{W_{AA}p(t)^2 + 2 W_{Aa} p(t) q(t) + W_{aa} q(t)^2}\\) Not derived <p>See textbook Sections 3.4 and 3.5 for models of interacting species and epidemiology, respectively, which we won't cover in class.</p>"},{"location":"lectures/lecture-05/","title":"Lecture 05","text":""},{"location":"lectures/lecture-05/#lecture-5-numerical-and-graphical-techniques-i-univariate","title":"Lecture 5: Numerical and graphical techniques I (univariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-05/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Numerical and graphical techniques</li> <li>Plots of variables over time</li> <li>Plots of variables as a function of the variables themselves</li> <li>Summary</li> </ol>"},{"location":"lectures/lecture-05/#1-numerical-and-graphical-techniques","title":"1. Numerical and graphical techniques","text":"<p>Before we jump into more rigorous mathematical analyses, we\u2019re first going to learn how to get a feel for the dynamics of our models.</p> <p>To do so we\u2019re going to choose some particular numerical values for our parameters and then use our models to predict what happens over time.</p> <p>The downside of this approach is that we often won\u2019t know the parameter values to choose and, regardless, choosing particular values doesn\u2019t tell us about the dynamics of our model more generally.</p> <p>The upside is that this approach can highlight errors or reveal unexpected patterns that guide future mathematical analyses.</p> <p></p>"},{"location":"lectures/lecture-05/#2-plots-of-variables-over-time","title":"2. Plots of variables over time","text":""},{"location":"lectures/lecture-05/#exponential-growth-model","title":"Exponential growth model","text":"<p>In the discrete exponential growth model, there is one parameter, \\(R\\), the number of offspring per parent (\"reproductive factor\").</p> <p>In last week\u2019s lab we wrote a recursive function (actually, a generator) to generate values of \\(n(t)\\), the population size, at sequential time points.</p> <pre>\nimport numpy as np\n\ndef n(t0, n0, R, max=np.inf):\n    # Set the initial value of t and n(t)\n    t, nt = t0, n0\n\n    # Yield new values of n(t) if t hasn't gone past the max value\n    while t &lt; max: \n        yield nt \n\n        # Then update t and n(t)\n        t, nt = t + 1, nt * R\n</pre> <p>We then chose some parameter values (reproductive factor, \\(R = 2\\)) and initial conditions (initial population size, \\(n(0) = 1\\)) to get the values of \\(n(t)\\) from the initial (\\(t = 0\\)) to final (\\(t = 10\\)) time.</p> <pre>\nnt = n(t0=0, n0=1, R=2, max=10) #choose some parameter values\nnts = [n for n in nt] #get all the t, n(t) values\nnts\n</pre> <pre><code>[1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n</code></pre> <p>And we then plotted \\(n(t)\\) as a function of \\(t\\)</p> <pre>\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.plot(range(10), nts, marker = '.', markersize = 10)\nax.set_xlabel('time step, $t$')\nax.set_ylabel('population size, $n(t)$')\nplt.show()\n</pre> <p></p> <p>This allowed us to compare what happens for different values of the reproductive factor, \\(R\\).</p> <pre>\ncolors = ['black','blue','red']\nfig, ax = plt.subplots()\nfor i, R in enumerate([1.1,1,0.9]):\n    nt = n(t0=0, n0=100, R=R, max=10)\n    nts = [n for n in nt]\n    ax.plot(range(10), nts, color=colors[i], label=f\"R = {R}\", marker = '.', markersize = 10)\n\nax.set_xlabel('time step, $t$')\nax.set_ylabel('population size, $n(t)$')\nax.legend()\nplt.show()\n</pre> <p></p> <p>From this we can deduce that when \\(R&gt;1\\) the population grows, when \\(R&lt;1\\) the population declines, and when \\(R=1\\) the population size remains constant.</p>"},{"location":"lectures/lecture-05/#logistic-growth-model","title":"Logistic growth model","text":"<p>In the discrete logistic growth model there are two parameters, the intrinsic growth rate, \\(r\\), and the carrying capacity, \\(K\\). The behaviour doesn\u2019t change much with different values of \\(K\\) but it is extremely sensitive to the value of \\(r\\), as you may remember from Lab 2.</p> <pre>\ndef n(n0, r, k, max=np.inf):\n    t, nt = 0, n0\n    while t &lt; max:\n        yield nt\n        t, nt = t + 1, nt + r * nt * (1 - nt / k)\n\n# Initialize plots        \nfig, ax = plt.subplots(1, 2, sharex=True, sharey=True)\nfig.set_size_inches(12,4)\n\n# Logistic growth with smaller r values\nfor r in [0.40, 0.70, 1.80, 2.10]:\n    ax[0].plot( #plot lines connecting values for visual clarity\n        range(25),\n        [nt for nt in n(1, r, 1000, max=25)],\n        label = f\"r = {r}\",\n        marker = '.', markersize = 10 \n    )\n\n# Logistic growth with larger r values\nfor r in [2.70, 3.0995]:\n    ax[1].plot(\n        range(25),\n        [nt for nt in n(1, r, 1000, max=25)],\n        label = f\"r = {r}\",\n        marker = '.', markersize = 10         \n    )\n\n# Add titles and annotations\nax[0].set_title('smaller $r$')\nax[1].set_title('larger $r$')\nfor i in range(2):\n    ax[i].set_xlabel('time step, $t$')\n    ax[i].set_ylabel('population size, $n(t)$')\n    ax[i].legend()\n\nfig.tight_layout()\nplt.show()\n</pre> <p></p> <p>Bifurcation diagrams and chaos</p> <p>We can examine how this model behaves as we change \\(r\\) by making a bifurcation diagram, which plots the values the system takes on after a long time for a given parameter value. Check out the very complex and potentially strange dynamics in the plot below. What does it mean?</p> <pre>\n# Sample the periodicity of the oscillations \n# by taking unique values after reaching carrying capacity (here we use t between 30 and 75)\ndef log_map(r, n0=900, k=1000):    \n    return np.unique([nt for t, nt in enumerate(n(n0, r, k, max=75)) if t &gt; 30])\n\n# Compute the logistic map for different growth rates in discrete time\nr, Nr = np.array([]), np.array([]) #list of r and n(t) values we will plot\nfor i in np.linspace(1.5, 3, 1000): #these are the r values we will simulate\n    nl = log_map(i) #get the unique values after carrying capacity\n    r = np.hstack((r, [i for _ in range(len(nl))])) #add the r value to plotting list (repeat the value of r for each unique n(t) value (for plotting))\n    Nr = np.hstack((Nr, nl)) #add the n(t) values to plotting list\n\n# Plot the logistic map on a black background (why not?)\nfig, ax = plt.subplots()\nax.patch.set_facecolor('black')\nax.scatter(r, Nr, s=0.075, color='white')\nplt.xlabel('intrinsic growth rate, $r$')\nplt.ylabel('population size, $n(t)$')\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-05/#3-plots-of-variables-as-a-function-of-the-variables-themselves","title":"3. Plots of variables as a function of the variables themselves","text":"<p>OK, so now we\u2019ll move on to a plot that is easier to generate and is very useful for models with just one variable (which is what we\u2019ve been working with so far).</p> <p>Instead of plotting the variable as a function of time, we\u2019ll plot the variable as a function of the variable in the previous time, e.g., plotting \\(n(t+1)\\) as a function of \\(n(t)\\).</p>"},{"location":"lectures/lecture-05/#haploid-selection","title":"Haploid selection","text":"<p>Let's start with our model of haploid selection</p> \\[ p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a (1-p(t))} \\] <p>and plot for two different sets of parameter values, where \\(A\\) has a higher or lower fitness than \\(a\\). </p> <pre>\nimport sympy\n\n# Build cobweb plotting function\ndef cobweb_haploid(p0, WA, Wa, max=np.inf):\n    t, pnow, pnext = 0, p0, 0 #initial conditions\n    while t &lt;= max:\n        yield pnow, pnext #current value of p(t) and p_(t+1)\n        pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) #update p_(t+1)\n        yield pnow, pnext #current value of p(t) and p_(t+1)\n        pnow = pnext #update p(t)\n        t += 1 #update t\n\n# Build function for generating figure\ndef plot_haploid_selection(WA, Wa, p0=0.5, ax=None):\n    pt = sympy.symbols('pt') #define our variable p(t)\n\n    # Write out sympy equation\n    f = (WA * pt) / (WA * pt + Wa * (1 - pt)) #the recursion equation\n\n    # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation (turn it into a function)\n    t = np.linspace(0,1,100)\n    fy = sympy.lambdify(pt, f)(t)\n\n    # Build plot\n    if ax == None:\n        fig, ax = plt.subplots()\n    ax.plot(t, fy, color='black', label=f\"$W_A$ = {WA}, $W_a$ = {Wa}\") #plot p_(t+1) as function of p(t)\n    ax.plot(t, t, color='black', linestyle='--') #draw 1:1 line for reference\n\n    # Add cobweb\n    cobweb = np.array([p for p in cobweb_haploid(p0, WA, Wa, max=100)])\n    ax.plot(cobweb[:,0], cobweb[:,1])\n\n    # Annotate and label plot\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,1)\n    ax.set_xlabel(\"allele frequency at $t$, $p(t)$\")\n    ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\")\n    ax.legend(frameon=False)\n    return ax\n\n# Plot figure\nfig, ax = plt.subplots(1,2)\nfig.set_size_inches(12,4)\n\n# First cobweb with WA &gt; Wa\nplot_haploid_selection(WA = 1, Wa = 0.5, ax=ax[0])\n\n# Second cobweb with WA &lt; Wa\nplot_haploid_selection(WA = 0.5, Wa = 1, ax=ax[1])\n\nplt.show()\n</pre> <p></p> <p>There are three components to this plot. First, the solid curve gives the recursion itself (\\(p(t+1)\\) as a function of \\(p(t)\\)). Second, the dashed line shows where \\(p(t+1)=p(t)\\). And third, the blue lines show how the variable changes over multiple time steps. </p> <p>Foreshadowing what is to come (Lecture 7), the dashed line is helpful for two reasons. First, it indicates where the variable does not change over time. So wherever the recursion (solid line) intersects with the dashed line is an equilibrium. Second, it reflects \\(p(t+1)\\) back onto \\(p(t)\\), updating the variable. For example, in the left panel above we start with an allele frequency of \\(p(t)=0.5\\), draw a blue vertical line to the recursion to find \\(p(t+1)\\), and then update \\(p(t)\\) to \\(p(t+1)\\) by drawing the horizontal blue line to the dashed line. Now we can ask what \\(p(t+1)\\) is given this updated value of \\(p(t)\\) by drawing another vertical blue line, and so on. Following the blue line we can therefore see where the system is heading, which tells us about the stability of the equilibria. What are the stable equilibria in the two panels above?</p>"},{"location":"lectures/lecture-05/#diploid-selection","title":"Diploid selection","text":"<p>To demonstrate the utility of this method, let\u2019s move on to the slightly more complex model of diploid selection</p> \\[ p(t+1) = \\frac{W_{AA} p(t)^2 + W_{Aa}p(t) q_t}{W_{AA}p(t)^2 + W_{Aa}2p(t)q_t + W_{aa}q_t^2} \\] <p>To show some different behaviour than above, this time let's set \\(W_{AA} &lt; W_{Aa} &gt; W_{aa}\\) and plot for two different starting frequencies, \\(p_0\\).</p> <pre>\ndef cobweb_diploid(p0, WAA, WAa, Waa, max=np.inf):\n    t, pnow, pnext = 0, p0, 0 #initial conditions\n    while t &lt;= max:\n        yield pnow, pnext #current value of p(t) and p(t+1)\n        pnext = (WAA * pnow**2 + WAa * pnow * (1 - pnow)) / (WAA * pnow**2 + WAa * 2 * pnow * (1 - pnow) + Waa * (1 - pnow)**2) #update p(t+1)\n        yield pnow, pnext #current value of p(t) and p(t+1)\n        pnow = pnext #update p(t)\n        t += 1 #update t\n\n# Build function for generating figure\ndef plot_diploid_selection(WAA, WAa, Waa, ax=None, p0=0.5):\n    pt = sympy.symbols('pt') #define our variable p(t)\n\n    # Write out sympy equation\n    f = (WAA * pt**2 + WAa * pt * (1- pt) ) / (WAA * pt**2 + WAa * 2 * pt * (1 - pt) + Waa * (1 - pt)**2) #the recursion equation\n\n    # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation\n    x = np.linspace(0,1,100)\n    fy = sympy.lambdify(pt, f)(x)\n\n    # Build plot\n    if ax == None:\n        fig, ax = plt.subplots()\n\n    # Add cobweb\n    cobweb = np.array([p for p in cobweb_diploid(p0, WAA, WAa, Waa, max=100)])\n    ax.plot(cobweb[:,0], cobweb[:,1])\n\n    # Annotate and label plot\n    ax.plot(x, fy, color='black', label=f\"$W_A$$_A$ = {WAA}, $W_A$$_a$ = {WAa}, $W_a$$_a$ = {Waa}\")\n    ax.plot(x, x, color='black', linestyle='--')\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,1)\n    ax.set_xlabel(\"allele frequency at $t$, $p(t)$\")\n    ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\")\n    ax.legend(frameon=False)\n    return ax\n\n# Plot figure\nfig, ax = plt.subplots(1,2)\nfig.set_size_inches(12,4)\n\n# First cobweb from low starting condition\nplot_diploid_selection(WAA=1, WAa=2, Waa=1, p0=0.05, ax=ax[0])\n\n# Second cobweb from high starting condition\nplot_diploid_selection(WAA=1, WAa=2, Waa=1, p0=0.95, ax=ax[1])\n\nplt.show()\n</pre> <p></p> <p>How many equilibria are there? Which appear to be stable?</p>"},{"location":"lectures/lecture-05/#differencedifferential-equations","title":"Difference/differential equations","text":"<p>We can do something very similar for difference and differential equations.</p> <p>Now we plot the change in the variable as a function of the current value of the variable, e.g., plot \\(\\Delta n\\) or \\(dn/dt\\) as a function of \\(n(t)\\).</p> <p>For example, in our model of haploid selection we have</p> \\[ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = sp(1-p) \\] <p>and our plot looks like:</p> <pre>\n# Initialize sympy symbols\np0, s, t = sympy.symbols('p0, s, t')\np = sympy.Function('t')\n\n# Specify differential equation\ndiffeq = sympy.Eq(p(t).diff(t), s * p(t) * (1 - p(t)))\n\n# Convert differential equation RHS to pythonic function\ndp = sympy.lambdify((s, p(t)), diffeq.rhs)\n\n# Plot the curve\nfig, ax = plt.subplots()\n\nfor s_coeff in [0.01, -0.01]:\n    ax.plot(\n        np.linspace(0, 1, 100),\n        dp(s_coeff, np.linspace(0,1, 100)),\n        label=f\"s = {s_coeff}\"\n    )\n\nax.set_xlabel('allele frequency at $t, p$')\nax.set_ylabel('change in allele frequency, $\\mathrm{d}p/\\mathrm{d}t$')\nax.legend(frameon=False)\nplt.show()\n</pre> <p></p> <p>What does this tell us about how allele frequency will change when \\(s&gt;0\\) vs. \\(s&lt;0\\)? And what allele frequencies, \\(p\\), cause more rapid evolution?</p> <p></p>"},{"location":"lectures/lecture-05/#4-summary","title":"4. Summary","text":"<p>To get a feel for our model it is helpful to graph some numerical examples:</p> <ul> <li>Plot the variable as a function of time (\"simulate\")</li> <li>Plot the variable (or change in variable) as a function of itself (only works for models with one variable)</li> </ul> <p>Next lecture we\u2019ll look at a graphical technique for models with multiple variables...</p>"},{"location":"lectures/lecture-06/","title":"Lecture 06","text":""},{"location":"lectures/lecture-06/#lecture-6-numerical-and-graphical-techniques-ii-multivariate","title":"Lecture 6: Numerical and graphical techniques II (multivariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-06/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Numerical and graphical techniques</li> <li>Phase-line diagrams</li> <li>Phase-plane diagrams</li> </ol>"},{"location":"lectures/lecture-06/#1-numerical-and-graphical-techniques","title":"1. Numerical and graphical techniques","text":"<p>In the last lecture we talked about two numerical/graphical approaches to get a better understanding of our models:</p> <ul> <li>Plotting a variable as a function of time (eg, \\(p(t)\\) as a function of \\(t\\))</li> <li>Plotting a variable as a function of itself (eg, \\(p(t+1)\\) as a function of \\(p(t)\\)). </li> </ul> <p>The latter works well for models with one variable.</p> <p>In this lecture we\u2019re going to talk about a third numerical technique, a phase-plane diagram, which is especially useful for models that have two variables.</p> <p></p>"},{"location":"lectures/lecture-06/#2-phase-line-diagrams","title":"2. Phase-line diagrams","text":"<p>Before looking at models with two variables, let\u2019s first consider some with only one.</p> <p>Consider again haploid selection where</p> \\[ p(t+1) = \\frac{W_Ap(t)}{W_Ap(t) + W_a(1-p(t))} \\] <p>Last time we plotted \\(p_{t + 1}\\) as a function of \\(p(t)\\) and used this to examine the dynamics starting from any initial value. We called this plot a cob-web plot.</p> <pre>\nimport sympy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Build cobweb plotting function\ndef cobweb_haploid(p0, WA, Wa, max=np.inf):\n    t, pnow, pnext = 0, p0, 0 #initial conditions\n    while t &lt;= max:\n        yield pnow, pnext #current value of p(t) and p_(t+1)\n        pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) #update p_(t+1)\n        yield pnow, pnext #current value of p(t) and p_(t+1)\n        pnow = pnext #update p(t)\n        t += 1 #update t\n\n# Build function for generating figure\ndef plot_haploid_selection(WA, Wa, p0=0.5, ax=None):\n    pt = sympy.symbols('pt') #define our variable p(t)\n\n    # Write out sympy equation\n    f = (WA * pt) / (WA * pt + Wa * (1 - pt)) #the recursion equation\n\n    # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation (turn it into a function)\n    t = np.linspace(0,1,100)\n    fy = sympy.lambdify(pt, f)(t)\n\n    # Build plot\n    if ax == None:\n        fig, ax = plt.subplots()\n    ax.plot(t, fy, color='black', label=f\"$W_A$ = {WA}, $W_a$ = {Wa}\") #plot p_(t+1) as function of p(t)\n    ax.plot(t, t, color='black', linestyle='--') #draw 1:1 line for reference\n\n    # Add cobweb\n    cobweb = np.array([p for p in cobweb_haploid(p0, WA, Wa, max=100)])\n    ax.plot(cobweb[:,0], cobweb[:,1])\n\n    # Annotate and label plot\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,1)\n    ax.set_xlabel(\"allele frequency at $t$, $p(t)$\")\n    ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\")\n    ax.legend(frameon=False)\n    return ax\n\n# Plot figure\nfig, ax = plt.subplots(1,2)\nfig.set_size_inches(12,4)\n\n# First cobweb with WA &gt; Wa\nplot_haploid_selection(WA = 1, Wa = 0.5, ax=ax[0])\n\n# Second cobweb with WA &lt; Wa\nplot_haploid_selection(WA = 0.5, Wa = 1, ax=ax[1])\n\nplt.show()\n</pre> <p></p> <p>Now let's simplify the cob-web plot and just indicate the direction (and magnitude) of change in \\(p(t)\\) with time. This is known as a phase-line diagram with a vector field (the arrows).</p> <pre>\ndef phase_line_haploid(p0, WA, Wa, max=np.inf):\n    'generator for p(t)'\n    t, pnow, pnext = 0, p0, 0 #initial conditions\n    while t &lt; max:\n        yield pnow #current value of p(t) and p_(t+1)\n        pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow))\n        pnow = pnext #update p(t)\n        t += 1 #update t\n\ndef plot_phase_line_haploid(WA, Wa, p0, max=20, ax=None):\n    'plot phase line'\n\n    # Set up figure\n    if ax==None:\n        fig, ax = plt.subplots()\n        fig.set_size_inches(8,0.25)\n    ax.axhline(0, color='black', linewidth=0.5)\n\n    # Plot phase-line\n    pts = [pt for pt in phase_line_haploid(p0, WA, Wa, max=max)] #pt values\n    ax.plot(\n        pts,\n        np.zeros(max) #dummy y values (0 for all x values) because we want to plot a 1d line\n    )\n\n    # Plot vector field\n    marker = '&gt;' if pts[2] &gt; pts[1] else '&lt;' #determine which direction to point based on first 2 time points\n    ax.scatter(\n        pts,\n        np.zeros(max),#dummy y again\n        marker=marker, s=150\n    )\n\n    # Remove background axes\n    ax.set_ylabel('$p$', rotation=0)\n    ax.set_xlabel(f\"$W_A$ = {WA}, $W_a$ = {Wa}, $p_0$ = {p0}\")\n    ax.get_yaxis().set_ticks([])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.set_xlim(0,1)\n    plt.show()\n\nplot_phase_line_haploid(WA=1, Wa=0.5, p0=0.01)\n\nplot_phase_line_haploid(WA=0.5, Wa=1, p0=0.99)\n</pre> <p></p> <p></p> <p>As in the cob-web plots, we see the allele frequency approaches \\(p=1\\) when \\(W_A&gt;W_a\\) and \\(p=0\\) when \\(W_a&gt;W_A\\). We also notice, as above, the changes are fastest (fewer, longer arrows) at intermediate frequencies.</p> <p>Similarly, with the more complex model of diploid selection</p> \\[ p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t)q(t)}{W_{AA}p(t)^2 + W_{Aa}p(t)q(t) + W_{aa}q(t)^2} \\] <p>we can draw a phase-line diagram and vector field for a set of parameter values.</p> <pre>\ndef phase_line_diploid(p0, WAA, WAa, Waa, max=np.inf):\n    'generator for p(t)'\n    t, pnow, pnext = 0, p0, 0 #initial conditions\n    while t &lt; max:\n        yield pnow #current value of p(t) and p(t+1)\n        pnext = (WAA * pnow**2 + WAa * pnow * (1 - pnow)) / (WAA * pnow**2 + WAa * 2 * pnow * (1 - pnow) + Waa * (1 - pnow)**2) #update p(t+1)\n        pnow = pnext #update p(t)\n        t += 1 #update t\n\ndef plot_phase_line_diploid(WAA, WAa, Waa, p0, max=20, ax=None):\n    'plot phase line'\n\n    # set up figure\n    if ax==None:\n        fig, ax = plt.subplots()\n        fig.set_size_inches(8,0.25)\n    ax.axhline(0, color='black', linewidth=0.5)\n\n    # Plot phase-line\n    pts = [pt for pt in phase_line_diploid(p0, WAA, WAa, Waa, max=max)]\n    ax.plot(\n        pts,\n        np.zeros(max),\n        alpha=1\n    )\n\n    # Plot phase-line markers\n    marker = '&gt;' if pts[2] &gt; pts[1] else '&lt;'\n    ax.scatter(\n        pts,\n        np.zeros(max),\n        marker=marker, s=150\n    )\n\n    ax.set_xlabel(f\"$WAA$ = {WAA}, $WAa$ = {WAa}, $Waa$ = {Waa}, $p_0$ = {p0}\")\n\n    return ax\n\n# Plot figure\nfig, ax = plt.subplots()\nfig.set_size_inches(8,0.25)\n\nplot_phase_line_diploid(WAA=1, WAa=2, Waa=1, p0=0.99, max=100, ax=ax) #higher starting allele frequency\nplot_phase_line_diploid(WAA=1, WAa=2, Waa=1, p0=0.01, max=100, ax=ax) #low starting allele frequency\n\n# Remove background axes\nax.set_ylabel('$p$', rotation=0)\nax.get_yaxis().set_ticks([])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.set_xlim(0,1)\nplt.show()\n</pre> <p></p> <p>Notice that this time we chose two initial frequencies for the same plot (orange vs blue), to show that under heterozygote advantage (\\(W_{AA}&lt;W_{Aa}&gt;W_{aa}\\)) the allele frequency approaches an intermediate value from either direction. </p> <p></p>"},{"location":"lectures/lecture-06/#3-phase-plane-diagrams","title":"3. Phase-plane diagrams","text":"<p>Now let\u2019s extend this technique from one to two variables.</p>"},{"location":"lectures/lecture-06/#lotka-volterra-model","title":"Lotka-Volterra model","text":"<p>We'll introduce a new model for this purpose, the Lotka-Volterra model of competition (see section 3.4.1 in the text).</p> <p>This is an extension of the logistic growth model (Lecture 3) to include competition between multiple species (in our case two).</p> <p>Let the population size of each species be \\(n_1(t)\\) and \\(n_2(t)\\). These are our two variables.</p> <p>And let them have different intrinsic growth rates, \\(r_1\\) and \\(r_2\\), and carrying capacities, \\(K_1\\) and \\(K_2\\).</p> <p>To model competition, we\u2019ll assume that, for an individual of species \\(i\\), an individual of species \\(j\\) is the competitive equivalent of \\(\\alpha_{ij}\\) individuals of species \\(i\\). We then have</p> \\[ n_1(t+1) = n_1(t)\\left( 1 + r_1 \\left(1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1}\\right)\\right) \\] \\[ n_2(t+1) = n_2(t) \\left(1 + r_2 \\left(1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2}\\right)\\right) \\] <p>Often individuals of the same species will use more similar resources and therefore competition will be less severe with individuals of the other species, \\(0 &lt; \\alpha_{ij} &lt; 1\\), but not always. And, in fact, we could model other types of interactions (eg, mutualism) by making some of the interactions beneficial, \\(\\alpha_{ij} &lt; 0\\).</p>"},{"location":"lectures/lecture-06/#phase-planes-and-vector-fields","title":"Phase-planes and vector fields","text":"<p>So why did we introduce the Lotka-Volterra model? Well, phase-plane diagrams are plots of one variable against another (\\(n_1\\) vs. \\(n_2\\)), on which we can plot vector fields, vectors originating from many different starting conditions that indicate the direction and magnitude of change in the two variables. With this we can graphically investigate the dynamics of the Lotka-Volterra model by first defining the rates of change in our two variables, \\(\\Delta n_1\\) and \\(\\Delta n_2\\) and then choosing some parameter values to explore.</p> \\[ \\Delta n_1 \\equiv n_1(t+1) - n_1(t) = n_1(t)r_1\\left(1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1}\\right) \\] \\[ \\Delta n_2 \\equiv n_2(t+1) - n_2(t) = n_2(t)r_2\\left(1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2}\\right) \\] <p>Let's plot a phase-plane for the Lotka-Volterra with the following parameter values: \\(r_1 = 0.5, r_2 = 0.5, K_1 = 1000, K_2 = 1000, \\alpha_{12} = 0.5, \\alpha_{21} = 0.5\\).</p> <pre>\n# Define a function to plot the phase plane and vector field for n1 and n2\ndef plot_vector_field(dn1, dn2, xlim=(0,1200), ylim=(0,1200), n_steps=25, width=8, height=6, show=False, axes_labels=[None, None]):\n    # Set x and y ranges \n    xrange, yrange = np.linspace(xlim[0], xlim[1], n_steps), np.linspace(ylim[0], ylim[1], n_steps)\n\n    # Initialize 2D grid with x,y values and additional grids to track derivatives\n    X, Y = np.meshgrid(xrange, yrange)\n    U, V = np.zeros(X.shape), np.zeros(Y.shape)\n\n    # Compute the gradient at each x,y position\n    for i in range(len(xrange)):\n        for j in range(len(xrange)):\n            U[i,j] = sympy.lambdify((n1, n2), dn1)(X[i,j], Y[i,j]) #change in n1\n            V[i,j] = sympy.lambdify((n1, n2), dn2)(X[i,j], Y[i,j]) #change in n2\n\n    # Plot figure\n    fig, ax = plt.subplots()\n    fig.set_size_inches(width, height)\n    ax.set_xlabel(axes_labels[0])\n    ax.set_ylabel(axes_labels[1])\n    ax.quiver(X,Y,U,V, linewidth=1) #from point X,Y draw arrow moving U in x-axis and V in y-axis\n\n    if show == True:\n        plt.show()\n    else:\n        return ax\n</pre> <pre>\n# Initialize the sympy variables\nn1, n2 = sympy.symbols('n1, n2')\n\n# Choose the parameter values\nr1, r2 = 0.5, 0.5\nk1, k2 = 1000, 1000\na12, a21 = 0.5, 0.5\n\n# Specify the difference equations\ndn1 = r1 * n1 * (1 - (n1 + a12 * n2) / k1)\ndn2 = r2 * n2 * (1 - (n2 + a21 * n1) / k2)\n\n# Plot the vector field\nplot_vector_field(dn1, dn2, axes_labels=[\"number of species 1, $n_1$\", \"number of species 2, $n_2$\"])\n\nplt.show()\n</pre> <p></p> <p>With this approach we see that the dynamics appear to be approaching a value near \\(n_1 = 700, n_2 = 700\\) from nearly any initial condition.</p>"},{"location":"lectures/lecture-06/#null-clines","title":"Null clines","text":"<p>To better understand the dynamics, we can ask for what values of our variables (\\(n_1, n_2\\)) is the change in our variables zero (\\(\\Delta n_1 = 0\\), \\(\\Delta n_2 = 0\\)). These values are known as null clines.</p> <p>Concretely, going back to our previous formula for the change in \\(n_1\\) and \\(n_2\\) in the Lotka-Volterra model</p> \\[ \\Delta n_1 = n_1(t)r_1\\left(1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1}\\right) \\] \\[ \\Delta n_2 = n_2(t)r_2\\left(1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2}\\right) \\] <p>We want to know when \\(\\Delta n_1\\) and \\(\\Delta n_2\\) are 0. Solving for these inequalities shows that</p> \\[ \\Delta n_1 = 0 \\Longrightarrow n_1(t) = 0, 1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1} = 0 \\] \\[ \\Delta n_2 = 0 \\Longrightarrow n_2(t) = 0, 1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2} = 0 \\] <p>Plotting these null clines on the phase-plane diagram, we get</p> <pre>\n# Initialize plot and ranges\nax = plot_vector_field(dn1, dn2, axes_labels=[\"number of species 1, $n_1$\", \"number of species 2, $n_2$\"])\nxrange, yrange = np.linspace(0, 1200, 100), np.linspace(0, 1200, 100)\n\ndef plot_nullclines(ax):\n    #plot the null clines for species 1 (blue)\n    nullcline_1 = [list(i.values())[0] for i in sympy.solve(sympy.Eq(dn1, 0))]\n    ax.plot(xrange, sympy.lambdify(n1, nullcline_1[1])(xrange), color=plt.cm.tab10(0)) # this null cline is a function of n1 (i.e. x)\n    ax.plot([nullcline_1[0] for _ in xrange], yrange, color=plt.cm.tab10(0))\n\n    # #plot the null clines for species 2 (red)\n    nullcline_2 = [list(i.values())[0] for i in sympy.solve(sympy.Eq(dn2, 0))]\n    ax.plot(sympy.lambdify(n2, nullcline_2[0])(yrange), yrange, color=plt.cm.tab10(1)) # this null cline is a function of n2 (i.e. y)\n    ax.plot(xrange, [nullcline_2[1] for _ in yrange], color=plt.cm.tab10(1))\n\n    ax.set_ylim(-10, 1210)\n    ax.set_xlim(-10, 1210)\n    return ax\n\nplot_nullclines(ax)\nplt.show()\n</pre> <p></p> <p>The null clines (blue for \\(n_1\\) and orange for \\(n_2\\)) help us understand the dynamics. In each area bounded by null clines the vectors point in the same general direction (eg, in the top right area they point down and to the left). This helps us see where the dynamics are heading -- in this case most initial conditions head to the intersection of the non-zero null clines for \\(n_1\\) and \\(n_2\\), near \\(n_1=700\\) and \\(n_2=700\\). Note that where the null cline of one variable intersects a null cline of the other variable neither variable is changing, indicating equilibria.</p> <p>We can also make phase diagrams for continuous-time models, just using differential equations in place of difference equations.</p> <p>We\u2019ll see an example of that for another model, of predator and prey, in Lab 3.</p>"},{"location":"lectures/lecture-07/","title":"Lecture 07","text":""},{"location":"lectures/lecture-07/#lecture-7-equilibria","title":"Lecture 7: Equilibria","text":"Run notes interactively?"},{"location":"lectures/lecture-07/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Equilibria</li> <li>Exponential growth</li> <li>Logistic growth</li> <li>Haploid selection</li> <li>Diploid selection</li> <li>Summary</li> </ol>"},{"location":"lectures/lecture-07/#1-equilibria","title":"1. Equilibria","text":"<p>An equilibrium is any state of a system which tends to persist unchanged over time.</p> <p>For discrete-time models, the equilibria are defined as those values of the variables where no changes occur from one time step to the next. </p> <p>For example, those values of allele frequency \\(p(t)\\) where</p> \\[ \\begin{aligned} \\Delta p &amp;= 0\\\\ p(t+1) - p(t) &amp;= 0\\\\ p(t+1) &amp;= p(t) \\end{aligned} \\] <p>Similarly, for continuous-time models, the equilibria are defined as those values of the variables for which the rate of change in the variables equals zero. </p> <p>For example, those values of allele frequency \\(p(t)\\) where</p> \\[ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = 0 \\] <p>What are the equilibria for the following models?</p> Model Discrete time Continous time Exponential growth \\(n(t+1) = R n(t)\\) \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t} = r n(t)\\) Logistic growth \\(n(t+1) = (1 + r(1 - \\frac{n(t)}{K}))n(t)\\) \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t} = r(1 - \\frac{n(t)}{K})n(t)\\) Haploid selection \\(p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a q(t)}\\) \\(\\frac{\\mathrm{d}p}{\\mathrm{d}t} = s p(t)(1-p(t))\\) Diploid selection \\(p(t+1) = \\frac{p(t)^2W_{AA} + p(t) q(t)W_{Aa}}{p(t)^2W_{AA} + 2 p(t) q(t)W_{Aa} +  q(t)^2W_{aa}}\\) Not derived <p></p>"},{"location":"lectures/lecture-07/#2-exponential-growth","title":"2. Exponential growth","text":"<p>Here, we will solve for the equilibria in both the discrete- and continuous-time exponential-growth models.</p>"},{"location":"lectures/lecture-07/#discrete-time","title":"Discrete time","text":"\\[ n(t+1) = Rn(t) \\] <p>Set \\(n(t+1) = n(t) = \\hat n\\) and solve for \\(\\hat{n}\\)</p> \\[ \\begin{aligned} \\hat n &amp;= R\\hat n\\\\ \\hat n &amp;= 0 \\end{aligned} \\]"},{"location":"lectures/lecture-07/#continuous-time","title":"Continuous time","text":"\\[ \\frac{\\mathrm{d}n}{\\mathrm{d}t} = r n(t) \\] <p>Set \\(\\mathrm{d}n/\\mathrm{d}t = 0\\) and \\(n(t) = \\hat n\\) and solve for \\(\\hat n\\)</p> \\[ \\begin{aligned} 0 &amp;= r \\hat{n}\\\\ \\hat n &amp;= 0 \\end{aligned} \\] <p>So the only equilibrium in both discrete- and continuous-time exponential growth is extinction, \\(\\hat{n}=0\\).</p> <p>Special case of parameters</p> <p>Notice above that \\(R=1\\) and \\(r=0\\) also satisfy the conditions for an equilibrium. These are called special cases of parameters. Here this refers to the case where individuals perfectly replace themselves so that the population remains constant from any starting value of \\(n\\).</p> <p></p>"},{"location":"lectures/lecture-07/#3-logistic-growth","title":"3. Logistic growth","text":"<p>Here, we will solve for the equilibria in both the discrete- and continuous-time logistic-growth models.</p>"},{"location":"lectures/lecture-07/#discrete-time_1","title":"Discrete time","text":"\\[ n(t+1) = \\left(1 + r\\left(1 - \\frac{n(t)}{K}\\right)\\right)n(t) \\] <p>As above, we substitute \\(n(t+1) = n(t) = \\hat n\\) and want to solve for \\(\\hat{n}\\).</p> \\[ \\hat n = \\left(1 + r\\left(1 - \\frac{\\hat n}{K}\\right)\\right)\\hat{n}  \\] <p>Notice that one equilibrium is \\(\\hat n = 0\\). However, this isn't the only equilibrium because dividing both sides by \\(\\hat n\\) results in</p> \\[ \\begin{aligned} 1 &amp;= 1 + r\\left(1 - \\frac{\\hat n}{K}\\right)\\\\ 0 &amp;= r\\left(1 - \\frac{\\hat n}{K}\\right) \\end{aligned} \\] <p>Here we have a special case of parameters, \\(r=0\\), or</p> \\[ \\begin{aligned} 0 &amp;= 1 - \\frac{\\hat n}{K}\\\\ \\hat n &amp;= K \\end{aligned} \\] <p>There are therefore two equilibria: extinction, \\(\\hat{n}=0\\), or carrying capacity, \\(\\hat{n}=K\\).</p>"},{"location":"lectures/lecture-07/#continuous-time_1","title":"Continuous time","text":"\\[ \\frac{\\mathrm{d}n}{\\mathrm{d}t} = r \\left(1 - \\frac{n}{K}\\right)n \\] <p>We set \\(\\mathrm{d}n/\\mathrm{d}t=0\\) and \\(n=\\hat{n}\\)</p> \\[ 0 =  r \\left(1 - \\frac{\\hat n}{K}\\right)\\hat{n} \\] <p>which is the same equation we had above in discrete-time, so the equilibria (\\(\\hat n = 0,K\\)) and the special case of parameters (\\(r = 0\\)) are also the same.</p> <p></p>"},{"location":"lectures/lecture-07/#4-haploid-selection","title":"4. Haploid selection","text":"<p>Here, we will solve for the equilibria in both the discrete- and continuous-time haploid-selection models.</p>"},{"location":"lectures/lecture-07/#discrete-time_2","title":"Discrete time","text":"\\[ p(t+1) = \\frac{p(t)W_A}{p(t)W_A + q(t)W_a} \\] <p>Replace \\(p(t+1)\\) and \\(p(t)\\) with \\(\\hat p\\) and replace \\(q(t)\\) with \\(\\hat q\\) and solve for \\(\\hat p\\) and \\(\\hat q\\)</p> \\[ \\begin{aligned} \\hat{p} &amp;= \\frac{\\hat p W_A}{\\hat p W_A + \\hat q W_a}\\\\ \\end{aligned} \\] <p>We first see that \\(\\hat{p}=0\\) is an equilibrium. But there is more, since dividing by \\(\\hat p\\) gives</p> \\[ \\begin{aligned} 1 &amp;= \\frac{W_A}{\\hat p W_A + \\hat q W_a}\\\\ \\hat p W_A + \\hat q W_a &amp;= W_A\\\\ \\hat q W_a &amp;= (1-\\hat p) W_A \\end{aligned} \\] <p>At this point we use \\(q=1-p\\) to write this in terms of \\(p\\) only</p> \\[ (1-\\hat p) W_a = (1-\\hat p) W_A \\] <p>So \\(\\hat p =1\\) is another equilibrium. </p> <p>And finally, dividing by \\((1-\\hat p)\\) gives a special case of parameters, \\(W_A=W_a\\).</p> <p>To summarize, the allele frequency will not change from one generation to the next in our discrete-time haploid-selection model when</p> <ul> <li>\\(\\hat p = 0 \\Longrightarrow\\) the population is \"fixed\" for the \\(a\\) allele</li> <li>\\(\\hat p = 1 \\Longrightarrow\\) the population is fixed for the \\(A\\) allele</li> <li>\\(W_A = W_a \\Longrightarrow\\) the two alleles have equal fitness (\"neutrality\")</li> </ul>"},{"location":"lectures/lecture-07/#continuous-time_2","title":"Continuous time","text":"\\[ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = sp(t)(1-p(t)) \\] <p>In the continuous-time model, we set the derivative equal to zero and \\(p(t)=\\hat{p}\\)</p> \\[ \\begin{aligned} 0 &amp;= s\\hat p(1 -\\hat p) \\end{aligned} \\] <p>And we again find the same equilibria (\\(\\hat p=0,1\\)) and special case of parameters (\\(s=0\\), i.e., neutrality).</p> <p></p>"},{"location":"lectures/lecture-07/#5-diploid-selection","title":"5. Diploid selection","text":""},{"location":"lectures/lecture-07/#discrete-time_3","title":"Discrete time","text":"<p>Here, we will solve for the equilibria in the discrete-time diploid-selection model</p> \\[ p(t+1) = \\frac{p(t)^2 W_{AA} + p(t)  q(t) W_{Aa}}{p(t)^2 W_{AA} + 2 p(t) q(t) W_{Aa} + q(t)^2 W_{aa}} \\] <p>We replace \\(p(t+1)\\) and \\(p(t)\\) with \\(\\hat p\\) and \\(q(t)\\) with \\(\\hat{q}\\) and solve for \\(\\hat p\\) and \\(\\hat q\\)</p> \\[ \\begin{aligned} \\hat{p} &amp;= \\frac{\\hat{p}^2 W_{AA} + \\hat{p} \\hat{q} W_{Aa}}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}}\\\\ \\hat{p} &amp;= \\frac{\\hat{p}(\\hat p W_{AA} + \\hat{q} W_{Aa})}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}} \\end{aligned} \\] <p>We see that \\(\\hat{p}=0\\) is one equilibrium. Moving on, dividing by \\(\\hat p\\) gives</p> \\[ \\begin{aligned} 1 &amp;= \\frac{\\hat p W_{AA} + \\hat{q} W_{Aa}}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}}\\\\ \\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa} &amp;= \\hat p W_{AA} + \\hat{q} W_{Aa}\\\\ 0 &amp;= (\\hat{p} - \\hat{p}^2) W_{AA} + (\\hat{q} - 2 \\hat{p} \\hat{q}) W_{Aa} - \\hat{q}^2 W_{aa}\\\\ 0 &amp;= \\hat{p}(1 - \\hat{p}) W_{AA} + \\hat{q}(1 - 2 \\hat{p}) W_{Aa} - \\hat{q}^2 W_{aa}\\\\ 0 &amp;= \\hat{q}(\\hat{p} W_{AA} + (1 - 2 \\hat{p}) W_{Aa} - \\hat{q} W_{aa}) \\end{aligned} \\] <p>And so \\(\\hat{q}=0\\implies\\hat{p}=1\\) is another equilibrium. Dividing by \\(\\hat{q}\\) and putting everything in terms of \\(p\\) we have</p> \\[ \\begin{aligned} 0 &amp;= \\hat{p} W_{AA} + (1 - 2 \\hat{p}) W_{Aa} - \\hat{q} W_{aa}\\\\ 0 &amp;= \\hat{p} W_{AA} + (1 - 2 \\hat{p}) W_{Aa} - (1 - \\hat{p}) W_{aa}\\\\ 0 &amp;= \\hat{p}(W_{AA} -2W_{Aa} + W_{aa}) + W_{Aa} - W_{aa}\\\\ W_{aa} - W_{Aa} &amp;= \\hat p(W_{AA} -2W_{Aa} + W_{aa})\\\\ \\frac{W_{Aa} - W_{aa}}{2W_{Aa} - W_{AA} - W_{aa}} &amp;= \\hat p\\\\ \\end{aligned} \\] <p>We therefore have three equilibria under diploid selection: \\(\\hat{p}=0,\\frac{W_{Aa} - W_{aa}}{2W_{Aa} - W_{AA} - W_{aa}},1\\).</p> <p>Since a frequency is bounded between 0 and 1, we must have \\(0 \\leq p \\leq 1\\). We therefore call \\(\\hat{p}=0\\) and \\(\\hat{p}=1\\) boundary equilibria.</p> <p>These bounds also imply the third equilibrium is only biologically valid when </p> \\[ 0 \\leq \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} \\leq 1 \\] <p>When \\(W_{Aa} = W_{aa}\\) this equilibrium reduces to \\(\\hat{p}=0\\) and when \\(W_{Aa} = W_{AA}\\) this reduces to \\(\\hat{p}=1\\) (check this for yourself).</p> <p>The third equilibrium will be an internal equilibrium, representing a population with both \\(A\\) and \\(a\\) alleles, when</p> \\[ 0 &lt; \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} &lt; 1 \\] <p>The equilibrium is positive when the numerator and denominator have the same sign (i.e., are both positive or both negative).</p> <p>Let's split this into two \"cases\". Case A will have a positive numerator, \\(W_{Aa} &gt; W_{aa}\\), and Case B will have a negative numerator, \\(W_{Aa} &lt; W_{aa}\\).</p> <p>So, in Case A, the equilibrium is positive when the denominator is positive, \\(2 W_{Aa} - W_{AA} - W_{aa} &gt; 0\\).</p> <p>While in case B the equilibrium is positive when the denominator is negative, \\(2 W_{Aa} - W_{AA} - W_{aa} &lt; 0\\).</p> <p>Now we can rearrange the equilibrium to show that it is less than 1 when</p> \\[ \\begin{aligned} \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} &amp;&lt; 1\\\\ \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} - 1 &amp;&lt; 0\\\\ \\frac{W_{Aa} - W_{aa} - (2 W_{Aa} -W_{AA} - W_{aa})}{2 W_{Aa} -W_{AA} - W_{aa}} &amp;&lt; 0\\\\ \\frac{W_{AA} - W_{Aa}}{2 W_{Aa} -W_{AA} - W_{aa}} &amp;&lt; 0\\\\ \\frac{W_{Aa} - W_{AA}}{2 W_{Aa} -W_{AA} - W_{aa}} &amp;&gt; 0\\\\ \\end{aligned} \\] <p>Again we need the numerator and denominator to have the same sign for this inequality to hold.</p> <p>In case A, where we've said that denominator is positive, this means we also need the numerator to be positive, \\(W_{Aa} &gt; W_{AA}\\).</p> <p>While in case B we said that the denominator is negative, so we also need the numerator to be negative, \\(W_{Aa} &lt; W_{AA}\\).</p> <p>Putting this all together, there is a biologically-relevant internal equilibrium when either</p> <ul> <li>Case A: \\(W_{Aa} &gt; W_{aa}\\) and \\(W_{Aa} &gt; W_{AA}\\) (which ensures \\(2 W_{Aa} - W_{AA} - W_{aa} &gt; 0\\); go ahead and check!)</li> <li>Case B: \\(W_{Aa} &lt; W_{aa}\\) and \\(W_{Aa} &lt; W_{AA}\\)  (which ensures \\(2 W_{Aa} - W_{AA} - W_{aa} &lt; 0\\))</li> </ul> <p>Case A therefore represents \"heterozygote advantage\", \\(W_{AA} &lt; W_{Aa} &gt; W_{aa}\\), while Case B represents \"heterozygote disadvantage\", \\(W_{AA} &gt; W_{Aa} &lt; W_{aa}\\). </p> <p></p>"},{"location":"lectures/lecture-07/#6-summary","title":"6. Summary","text":"<p>In summary, the equilibria for the models we have looked at are:</p> Model Discrete-time equilibria Continuous-time equilibria Exponential growth \\(\\hat n = 0\\) \\(\\hat n = 0\\) Logistic growth \\(\\hat n = 0, \\hat n = K\\) \\(\\hat n = 0, \\hat n = K\\) Haploid selection \\(\\hat p = 0, \\hat p = 1\\) \\(\\hat p = 0, \\hat p = 1\\) Diploid selection \\(\\hat p = 0, \\hat p = 1, \\hat p = \\frac{W_{Aa} - W_{aa}}{2W_{Aa} - W_{AA} - W_{aa}}\\) Not derived <p>Make sure that you understand how to determine equilibria in discrete- and continuous-time and can derive the equilibria of the models above on your own.</p>"},{"location":"lectures/lecture-08/","title":"Lecture 08","text":""},{"location":"lectures/lecture-08/#lecture-8-local-stability-univariate","title":"Lecture 8: Local stability (univariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-08/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Stability</li> <li>Local stability analysis in continuous-time one-variable models</li> <li>Local stability analysis in discrete-time one-variable models</li> <li>Summary</li> </ol>"},{"location":"lectures/lecture-08/#1-stability","title":"1. Stability","text":"<p>When a variable is exactly at an equilibrium its value will never change. But what happens when we are not exactly at, but just near an equilibrium?</p> <p>Starting near an equilibrium, if the system moves towards the equilibrium over time the equilibrium is said to be locally stable.</p> <p>In contrast, if the system moves away from the equilibrium over time the equilibrium is said to be unstable.</p> <p>An equilibrium point is said to be globally stable if any starting point leads to it.</p> <p>We've seen these possibilities already graphically, eg., in phase-line plots for discrete-time diploid selection: when the heterozygote has the lowest fitness the fixation equilibria are locally (but not globally) stable and the polymorphic equilibrium is unstable.</p> <pre>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef phase_line_diploid(p0, WAA, WAa, Waa, max=np.inf):\n    'generator for p_t'\n    t, pnow, pnext = 0, p0, 0 #initial conditions\n    while t &lt; max:\n        yield pnow #current value of p(t) and p(t+1)\n        pnext = (WAA * pnow**2 + WAa * pnow * (1 - pnow)) / (WAA * pnow**2 + WAa * 2 * pnow * (1 - pnow) + Waa * (1 - pnow)**2) #update p(t+1)\n        pnow = pnext #update p(t)\n        t += 1 #update t\n\ndef plot_phase_line_diploid(WAA, WAa, Waa, p0, max=20, ax=None):\n    'plot phase line'\n\n    # set up figure\n    if ax==None:\n        fig, ax = plt.subplots()\n        fig.set_size_inches(8,0.25)\n\n    # Plot phase-line\n    ax.axhline(0, color='black', linewidth=0.5)\n\n    # Plot vector field\n    pts = [pt for pt in phase_line_diploid(p0, WAA, WAa, Waa, max=max)]\n    marker = '&gt;' if pts[2] &gt; pts[1] else '&lt;'\n    ax.scatter(\n        pts,\n        np.zeros(max),\n        marker=marker, s=50, c='black'\n    )\n\n    ax.set_xlabel(f\"$WAA$ = {WAA}, $WAa$ = {WAa}, $Waa$ = {Waa}\")\n\n    return ax\n\n# Plot figure\nfig, ax = plt.subplots()\nfig.set_size_inches(8,0.25)\n\n# phase line and vector field\nplot_phase_line_diploid(WAA=2, WAa=1, Waa=2, p0=0.45, max=12, ax=ax) #higher starting allele frequency\nplot_phase_line_diploid(WAA=2, WAa=1, Waa=2, p0=0.55, max=12, ax=ax) #low starting allele frequency\n\n# equilibria\nplt.scatter([0,1],[0,0],s=200,c='black')\nplt.scatter([0.5],[0],s=200,c='white',edgecolors='black')\n\n# Remove background axes\nax.set_ylabel('$p$', rotation=0)\nax.get_yaxis().set_ticks([])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.set_xlim(-0.05,1.05)\nplt.show()\n</pre> <p></p> <p>Our goal in this lecture is to mathematically determine whether a small perturbation away from an equilibrium point will grow or shrink in magnitude over time \\(\\Longrightarrow\\) local stability analysis.</p>"},{"location":"lectures/lecture-08/#motivating-example","title":"Motivating example","text":"<p>Consider logistic growth in continuous time, with \\(K = 1000\\) and \\(r = 0.5\\).</p> <p>For populations started near carrying capacity, a plot of \\(\\mathrm{d}n/\\mathrm{d}t\\) vs. \\(n\\) shows that they move closer to the carrying capacity over time, since the rate of change in \\(n\\) is positive when \\(n&lt;K\\) and negative when \\(K&lt;n\\).</p> <p><pre>\nr,k = 0.5,1000\nxs = np.linspace(0,1.5k,100)\nys = [rn*(1-n/k) for n in xs]\nplt.plot(xs,ys)\nplt.plot(xs,[0 for _ in xs], color='black')\nplt.xlabel('population size, \\(n\\)')\nplt.ylabel('rate of change in population size, \\(dn/dt\\)')\nplt.show()\n</pre></p> <p></p> <p>If we drew the same plot for \\(r=-0.5\\) we see that the population size then moves away from \\(n=K\\), as the rate of change in \\(n\\) is negative when \\(n&lt;K\\) and positive when \\(K&lt;n\\). </p> <p><pre>\nr,k=-0.5,1000\nxs = np.linspace(0,1.5k,100)\nys = [rn*(1-n/k) for n in xs]\nplt.plot(xs,ys)\nplt.plot(xs,[0 for _ in xs], color='black')\nplt.xlabel('population size, \\(n\\)')\nplt.ylabel('rate of change in population size, \\(dn/dt\\)')\nplt.show()\n</pre></p> <p></p> <p>The key difference between these plots near the equilibrium of interest, \\(\\hat n=K\\), is that </p> <pre><code>1. in the first case ($r=0.5$), where the equilibrium is stable, $\\mathrm{d}n/\\mathrm{d}t$ goes from positive to negative, meaning its slope is negative \n2. in the second case ($r = -0.5$), where the equilibrium is unstable, $\\mathrm{d}n/\\mathrm{d}t$, goes from negative to positive, meaning its slope is positive\n</code></pre> <p>This suggests that we can determine the local stability of an equilibrium by looking at the slope of the differential equation at that equilibrium.</p> <p></p>"},{"location":"lectures/lecture-08/#2-local-stability-analysis-in-continuous-time-one-variable-models","title":"2. Local stability analysis in continuous-time one-variable models","text":"<p>To determine local stability mathematically, we focus on a small perturbation (\\(\\epsilon\\)) away from an equilibrium (\\(\\hat{x}\\)) and determine whether this perturbation will grow or shrink.</p> <p>If, at time \\(t\\), the population is a small distance from equilibrium, \\(x = \\hat{x} + \\epsilon\\), the rate of change in \\(x\\) will be \\(\\mathrm{d}x/\\mathrm{d}t = \\mathrm{d}(\\hat{x} + \\epsilon)/\\mathrm{d}t\\).</p> <p>Let us call this derivative \\(f(x)=\\mathrm{d}x/\\mathrm{d}t\\), which we can write as \\(f(\\hat{x} + \\epsilon) =  \\mathrm{d}(\\hat{x} + \\epsilon)/\\mathrm{d}t\\).</p> <p>To work with this arbitrary function, \\(f\\), let's introduce a pretty remarkable mathematical fact, the Taylor Series.</p> <p>Taylor Series</p> <p>Any function \\(f(x)\\) can be written as an infinite series of derivatives evaluated at \\(x=a\\)</p> \\[ f(x) = \\sum_{k = 0}^{\\infty}\\frac{f^{(k)}(a)}{k!}(x-a)^k \\] <p>where \\(f^{(k)}(a)\\) is the \\(k^{\\mathrm{th}}\\) derivative of the function with respect to \\(x\\), evaluated at point \\(a\\). (See section P1.3 in the text for more information).</p> <p>In this case we want to write \\(f(\\hat{x} + \\epsilon)\\) as a Taylor Series evaluated at the equilibrium, \\(\\hat{x} + \\epsilon = \\hat{x}\\)</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}(\\hat{x} + \\epsilon)}{\\mathrm{d}t} &amp;= f(\\hat{x} + \\epsilon)\\\\ &amp;= f(\\hat{x}) + f^{(1)}(\\hat{x})(\\hat{x} + \\epsilon - \\hat{x}) + \\frac{f^{(2)}(\\hat{x})}{2}(\\hat{x} + \\epsilon - \\hat{x})^2 + \\cdots\\\\ &amp;= f(\\hat{x}) + f^{(1)}(\\hat{x})\\epsilon + \\frac{f^{(2)}(\\hat{x})}{2}\\epsilon^2 + \\cdots\\\\ \\end{aligned} \\] <p>Now, to work with this infinite series we will make an assumption, that we are very the equilibrium, meaning the deviation is small, \\(\\epsilon&lt;&lt;1\\). This means that \\(\\epsilon^2\\) is even smaller, and \\(\\epsilon^3\\) even smaller than that, and so on. By considering small \\(\\epsilon\\) we can therefore cut-off our infinite series by ignoring any term with \\(\\epsilon\\) to a power greater than 1. This is called a \"first order\" Taylor series approximation of \\(f\\) around \\(\\epsilon=0\\). This assumption is what limits us to determining only local stability. Global stability would require us to consider large deviations from the equilibrium as well, which is not possible for even mildly complicated functions, \\(f\\).</p> <p>OK, so making this assumption of small \\(\\epsilon\\) we have</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}(\\hat{x} + \\epsilon)}{\\mathrm{d}t} &amp;= f(\\hat{x}) + f^{(1)}(\\hat{x})\\epsilon\\\\ \\end{aligned} \\] <p>Since \\(\\hat{x}\\) is an equilibrium, \\(f(\\hat{x})\\) equals zero, leaving just \\(f^{(1)}(\\hat{x})\\epsilon\\) on the right-hand side.</p> <p>Furthermore, the left-hand side can be expanded and simplified (\\(\\hat{x}\\) is a constant that does not change in time)</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}(\\hat{x}+\\epsilon)}{\\mathrm{d}t} &amp;= \\frac{\\mathrm{d}\\hat{x}}{\\mathrm{d}t} + \\frac{\\mathrm{d}\\epsilon}{\\mathrm{d}t}\\\\ &amp;= \\frac{\\mathrm{d}\\epsilon} {\\mathrm{d}t} \\end{aligned} \\] <p>Combining the above, the deviation from the equilibrium will change over time at a rate</p> \\[ \\frac{\\mathrm{d} \\epsilon}{\\mathrm{d}t} = f^{(1)}(\\hat{x}) \\epsilon \\] <p>This is the same as exponential growth with growth rate \\(r=f^{(1)}(\\hat{x})\\).</p> <p>The deviation will therefore</p> <ul> <li>grow if \\(f^{(1)}(\\hat{x})&gt;0\\) \\(\\implies\\hat{x}\\) unstable</li> <li>shrink if \\(f^{(1)}(\\hat{x})&lt;0\\) \\(\\implies\\hat{x}\\) locally stable</li> </ul> <p>Stability in continuous time therefore requires the slope of the differential equation to be negative at the equilibrium, \\(f^{(1)}(\\hat{x}) = \\left.\\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(\\frac{\\mathrm{d}x}{\\mathrm{d}t}\\right)\\right|_{x=\\hat{x}} &lt; 0\\).</p>"},{"location":"lectures/lecture-08/#eg-logistic-growth","title":"E.g., logistic growth","text":"<p>Let's again look at the model of logistic growth in continuous time, where</p> \\[ f(n) = \\frac{\\mathrm{d}n}{\\mathrm{d}t} = rn \\left(1 - \\frac{n}{K}\\right) \\] <p>The derivative of \\(f\\) with respect to \\(n\\) is</p> \\[ f'(n) = r - 2 r\\frac{n}{K} \\] <p>Plugging in \\(n=K\\) gives</p> \\[ f'(K) = r - 2 r = -r \\] <p>This implies that \\(r&gt;0\\) causes local stability of \\(\\hat{n}=K\\). We can check this is consistent with a graphical analysis, below.</p> <pre>\ndef f(n,r,k):\n    'differential equation for logistic growth'\n    return n*r*(1-n/k)\n\ndef plot_logistic_de(r,k,ax=None):\n    'plot differential equation for logistic growth as function of n'\n    xs = np.linspace(0,k*1.5,100) #n values\n    if ax == None:\n        fig, ax = plt.subplots() \n    # 0 line\n    ax.plot(xs, [0 for _ in xs], color='black', linestyle='--')\n    # differential equation\n    ax.plot(xs, [f(x,r,k) for x in xs], color='black')\n    #aesthetics\n    ax.set_xlabel('$n$')\n    ax.set_ylabel('$dn/dt$')\n    ax.set_title('$r=$%.1f'%r)\n    return ax\n</pre> <pre>\nplot_logistic_de(r=0.5, k=1000)\nplt.show()\nplot_logistic_de(r=-0.5, k=1000)\nplt.show()\n</pre> <p></p> <p></p> <p></p>"},{"location":"lectures/lecture-08/#3-local-stability-analysis-in-discrete-time-one-variable-models","title":"3. Local stability analysis in discrete-time one-variable models","text":"<p>In discrete time we instead work with a recursion equation, \\(x(t+1) = f(x(t))\\).</p> <p>Again, consider a system that is a small distance from the equilibrium at time \\(t\\): \\(x(t) = \\hat{x} + \\epsilon(t)\\).</p> <p>At time \\(t+1\\), the population will be at \\(x(t+1) = \\hat{x} + \\epsilon(t+1) = f(\\hat{x} + \\epsilon(t))\\).</p> <p>Taking the Taylor Series of \\(f(\\hat{x} + \\epsilon(t))\\) around \\(\\epsilon(t)=0\\) and truncating to first order (under our assumption of small \\(\\epsilon\\)) we have</p> \\[ \\begin{aligned} \\hat{x} + \\epsilon(t+1) &amp;= f(\\hat{x} + \\epsilon(t))\\\\ &amp;= f(\\hat{x}) + f^{(1)}(\\hat{x})(\\hat{x} + \\epsilon(t) - \\hat{x})\\\\ &amp;= f(\\hat{x}) + f^{(1)}(\\hat{x})\\epsilon(t) \\end{aligned} \\] <p>We know that \\(f(\\hat{x}) = \\hat{x}\\) because \\(\\hat{x}\\) is an equilibrium, which implies</p> \\[ \\epsilon(t+1) = f^{(1)}(\\hat{x})\\epsilon(t) \\] <p>This is the recursion for exponential growth, with reproductive factor \\(\\lambda = f^{(1)}(\\hat{x})\\). </p> <p>Based on our knowledge of discrete-time exponential growth, we therefore know that the deviation from equilibrium will:</p> <ul> <li> <p>move from one side of the equilibrium to the other (i.e., oscillate) if \\(\\lambda\\) is negative</p> <ul> <li>grow if \\(\\lambda&lt;-1\\) \\(\\implies\\hat{x}\\) unstable</li> <li>shrink if \\(-1&lt;\\lambda&lt;0\\) \\(\\implies\\hat{x}\\) locally stable</li> </ul> </li> <li> <p>stay on the same side of the equilibrium (i.e., not oscillate) if \\(\\lambda\\) is positive</p> <ul> <li>shrink if \\(0&lt;\\lambda&lt;1\\) \\(\\implies\\hat{x}\\) locally stable</li> <li>grow if \\(1&lt;\\lambda\\) \\(\\implies\\hat{x}\\) unstable</li> </ul> </li> </ul> <p>Local stability in discrete time therefore requires the slope of the recursion to be between -1 and 1 at the equilibrium, \\(-1&lt;f^{(1)}(\\hat{x})=\\left.\\frac{\\mathrm{d}x_{t+1}}{\\mathrm{d}x_t}\\right|_{x_t=\\hat x}&lt;1\\).</p>"},{"location":"lectures/lecture-08/#eg-logistic-growth_1","title":"E.g., logistic growth","text":"<p>To see how this works, let's look at the logistic growth model in discrete time. Here the recursion is</p> \\[ f(n) = n \\left(1 + r\\left(1 - \\frac{n}{K}\\right)\\right) \\] <p>We first take the derivative of \\(f\\) with respect to \\(n\\)</p> \\[ f'(n) = 1 + r - 2 r \\frac{n}{K} \\] <p>Then we plug in the equilirbium value of interest, \\(n=K\\)</p> \\[ \\begin{aligned} f'(K) &amp;= 1 + r - 2 r \\\\ &amp;= 1 - r \\end{aligned} \\] <p>This will be negative when \\(r &gt; 1\\), creating oscillations.</p> <p>The equilibrium will be stable when \\(-1 &lt; 1 - r &lt; 1 \\implies 0 &lt; r &lt; 2\\).</p> <p>This is consistent with what we've seen in cob-web plots, as below.</p> <pre>\n# logistic growth recursion\ndef f(nt,r,k):\n    return nt * (1 + r * (1 - nt / k))\n\n# Build cobweb plotting function\ndef cobweb_logistic(n0, r, k, max=np.inf):\n    t, nnow, nnext = 0, n0, 0 #initial conditions\n    while t &lt; max:\n        yield nnow, nnext #current value of n(t) and n(t+1)\n        nnext = f(nnow,r,k)\n        yield nnow, nnext #current value of n(t) and n(t+1)\n        nnow = nnext #update n(t)\n        t += 1 #update t\n\n# Plot\ndef plot_logistic_with_cobweb(r, k, n0, ncobs=10, ax=None):\n    # Plot the curves (add an additional curve past equilibrium to show stability)\n    xs = np.linspace(0,k*1.5,100) #x values\n    if ax == None:\n        fig, ax = plt.subplots() \n    #1:1 line\n    ax.plot(xs, xs, color='black', linestyle='dashed')\n    # recursion\n    ax.plot(xs, [f(x,r,k) for x in xs], color='black')\n    # cobweb\n    cobweb = np.array([i for i in cobweb_logistic(n0, r, k, ncobs)])\n    plt.plot(cobweb[:,0], cobweb[:,1], color='blue')\n    #aesthetics\n    ax.set_ylim(0,None)\n    ax.set_xlim(0,None)\n    ax.set_xlabel('$n_t$')\n    ax.set_ylabel('$n_{t+1}$')\n    ax.set_title('$r=$%.1f'%r)\n    return ax\n</pre> <pre>\nplot_logistic_with_cobweb(r=0.5, k=1000, n0=900)\nplot_logistic_with_cobweb(r=1.5, k=1000, n0=900)\nplot_logistic_with_cobweb(r=2.5, k=1000, n0=900)\nplt.show()\n</pre> <p></p> <p></p> <p></p> <p></p>"},{"location":"lectures/lecture-08/#4-summary","title":"4. Summary","text":"<p>Local stability analysis for continuous- and discrete-time models with one variable:</p> <ol> <li>take the derivative of the differential equation or recursion with respect to the variable, \\(f^{(1)}(x)\\)</li> <li>plug in the equilibrium value of the variable, \\(f^{(1)}(\\hat x)\\)</li> <li>determine the sign (and magnitude in discrete-time)</li> </ol>"},{"location":"lectures/lecture-09/","title":"Lecture 09","text":""},{"location":"lectures/lecture-09/#lecture-9-general-solutions-univariate","title":"Lecture 9: General solutions (univariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-09/#lecture-overview","title":"Lecture overview","text":"<ol> <li>General solutions</li> <li>Linear models in discrete time</li> <li>Nonlinear models in discrete time</li> <li>Linear models in continuous time</li> <li>Nonlinear models in continuous time</li> <li>Summary</li> </ol>"},{"location":"lectures/lecture-09/#1-general-solutions","title":"1. General solutions","text":"<p>Last week we learned how to find equilibria and determine their local stability in models with one variable (univariate).</p> <p>Those analyses describe the long-term dynamics of our models, i.e., what we expect after a long time has passed.</p> <p>This week we\u2019ll look at some simple cases where we can describe the entire dynamics, including the short-term, by solving for the variable as a function of time, \\(x_t = f(t)\\) </p> <p>This is called a general solution.</p> <p></p>"},{"location":"lectures/lecture-09/#2-linear-models-in-discrete-time","title":"2. Linear models in discrete time","text":"<p>With a single variable, \\(x\\), in discrete time all linear models can be written </p> \\[ x_{t+1} = a x_t + b \\] <p>There are two cases that we will consider separately: 1) \\(b = 0\\) and 2) \\(b \\neq 0\\).</p>"},{"location":"lectures/lecture-09/#brute-force-iteration","title":"Brute force iteration","text":"<p>When \\(b = 0\\) we can use brute force iteration</p> \\[ \\begin{aligned} x_t &amp;= a x_{t-1}\\\\ &amp;= a a x_{t-2}\\\\ &amp;= a a a x_{t-3}\\\\  &amp;\\vdots\\\\ &amp;= a\\cdots a x_0\\\\ &amp;= a^t x_0 \\end{aligned} \\] <p>This is the general solution for exponential growth in discrete time, with reproductive factor \\(a\\). We can see that our variable will oscillate around the equilibrium (\\(\\hat{x}=0\\)) if \\(a&lt;0\\) and will either approach the equilibrium (\\(|a|&lt;1\\)) or depart from it (\\(|a|&gt;1\\)), consistent with our local stability analysis. See this for yourself by playing with the value of \\(a\\) in the plot below.  </p> <pre>\nimport matplotlib.pyplot as plt\na, x0 = 0.99, 10 #define parameter values and initial condition\nts = range(1000) #time values\nxs = [a**t * x0 for t in ts] #variable values from general solution\nplt.scatter(ts, xs) #plot discretely\nplt.ylabel('$x(t)$')\nplt.xlabel('$t$')\nplt.show()\n</pre> <p></p>"},{"location":"lectures/lecture-09/#solving-affine-models","title":"Solving affine models","text":"<p>When \\(b \\neq 0\\) (which gives us what is called an affine model) we need to use a transformation, much like we did when determining local stability.</p> <p>Step 1: Solve for the equilibrium</p> \\[ \\begin{aligned} \\hat{x} &amp;= a \\hat{x} + b \\\\ \\hat{x} &amp;= \\frac{b}{1 - a} \\end{aligned} \\] <p>Note</p> <p>Note that if \\(a=1\\) there is no equilibrium for \\(b\\neq0\\), and instead you can use brute force iteration to show that \\(x_t = x_0 + b t\\).</p> <p>Step 2: Define \\(\\delta_t = x_t - \\hat{x}\\), the deviation of our variable from the equilibrium (this is our transformation).</p> <p>Step 3: Write the recursion equation for the transformed variable</p> \\[ \\begin{aligned} \\delta_{t+1} &amp;= x_{t+1} - \\hat{x} \\\\ &amp;= a x_t + b - \\hat{x} \\\\ &amp;= a(\\delta_t + \\hat{x}) + b - \\hat{x}\\\\ &amp;= a \\left(\\delta_t + \\frac{b}{1 - a}\\right) + b - \\frac{b}{1 - a}\\\\ &amp;= a \\delta_t \\end{aligned} \\] <p>Step 4: This is the same recursion we derived above for \\(x\\) when \\(b=0\\). So the general solution for the transformed variable is \\(\\delta_t = a^t \\delta_0\\).</p> <p>Step 5: Reverse transform back to \\(x_t\\)</p> \\[ \\begin{aligned} x_t &amp;= \\delta_t + \\hat{x}\\\\ &amp;= a^t \\delta_0 + \\hat{x}\\\\ &amp;= a^t (x_0 - \\hat{x}) + \\hat{x}\\\\ &amp;= a^t x_0 + (1 - a^t)\\hat{x} \\end{aligned} \\] <p>This says that our variable moves from \\(x_0\\) towards/away from \\(\\hat{x}\\) by a factor \\(a\\) per time step. Note that if \\(b=0\\) then \\(\\hat{x}=0\\) and this reduces to what we derived above, \\(x_t=a^t x_0\\).</p> <p>Below we plot the general solution for a given value of \\(a\\) and \\(b\\) from a number of different intitial conditions. Try playing with the values of \\(a\\) and \\(b\\) and observe the different dynamics.</p> <pre>\na, b, x0 = 0.99, 1, 10 #define parameter values and initial condition\nts = range(1000) #time values\nxs = [a**t * x0 + (1-a**t)*b/(1-a) for t in ts] #variable values from general solution\nplt.scatter(ts, xs) #plot discretely\nplt.ylabel('$x(t)$')\nplt.xlabel('$t$')\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-09/#3-nonlinear-models-in-discrete-time","title":"3. Nonlinear models in discrete time","text":"<p>Unfortunately there is no recipe to solve nonlinear models in discrete time, even with one variable.</p> <p>In fact, most of the time there is no general solution.</p> <p>To get a sense of why that might be, remember the chaos of logistic growth!</p> <pre>\nimport numpy as np\n\n# Generator for logistic growth\ndef n(n0, r, k, max=np.inf):\n    t, nt = 0, n0\n    while t &lt; max:\n        yield nt\n        t, nt = t + 1, nt + r * nt * (1 - nt / k)\n\n# Sample the periodicity of the oscillations by taking unique values after reaching carrying capacity\ndef log_map(r, n0=900, k=1000):    \n    return np.unique([nt for t, nt in enumerate(n(n0, r, k, max=75)) if t &gt; 30])\n\n# Compute the logistic map for different growth rates in discrete time\nr, Nr = np.array([]), np.array([])\nfor i in np.linspace(1.5, 3, 1000):\n    nl = log_map(i)\n    r = np.hstack((r, [i for _ in range(len(nl))]))\n    Nr = np.hstack((Nr, nl))\n\n# Plot the logistic map on a black background\nfig, ax = plt.subplots()\nax.patch.set_facecolor('black')\nax.scatter(r, Nr, s=0.075, color='white')\nax.set_xlabel('intrinsic growth rate, $r$')\nax.set_ylabel('population size, $n$')\nplt.show()\n</pre> <p></p>"},{"location":"lectures/lecture-09/#solving-with-transformations","title":"Solving with transformations","text":"<p>Sometimes, however, you can find a transformation that works.</p> <p>For example, with haploid selection we have</p> \\[ p_{t+1} = \\frac{W_A p_t}{W_A p_t + W_a q_t} \\] <p>Brute force iteration will create a giant mess.</p> <p>But what about if we let \\(f_t = p_t/q_t\\)?</p> <p>Noting that \\(q_{t+1} = 1 - p_{t+1} = (W_a p_t)/(W_A p_t + W_a q_t)\\) we have</p> \\[ \\begin{aligned} f_{t+1} &amp;= \\frac{p_{t+1}}{q_{t+1}}\\\\ &amp;= \\frac{W_A p_t}{W_a q_t}\\\\ &amp;= \\frac{W_A}{W_a} f_t \\end{aligned} \\] <p>This implies that \\(f_t = (W_A/W_a)^t f_0\\)!</p> <p>Converting back to \\(p_t\\) we see</p> \\[ p_t = \\frac{f_t}{1-f_t} = \\frac{W_A^t p_0}{W_A^t p_0 + W_a^t q_0} \\]"},{"location":"lectures/lecture-09/#solving-with-conceptualization","title":"Solving with conceptualization","text":"<p>An alternative way to derive this general solution is to think about (\"conceptualize\") the \\(A\\) and \\(a\\) alleles as two competing populations that each grow exponentially according to their fitness</p> \\[ \\begin{aligned} n_A(t) &amp;= W_A^t n_A(0)\\\\ n_a(t) &amp;= W_a^t n_a(0)    \\end{aligned} \\] <p>Then the frequency of allele \\(A\\) at time \\(t\\) is</p> \\[ p_t = \\frac{n_A(t)}{n_A(t) + n_a(t)} = \\frac{W_A^t n_A(0)}{W_A^t n_A(0) + W_a^t n_a(0)} \\] <p>Dividing numerator and denominator by the total initial population size \\(n_A(0) + n_a(0)\\)</p> \\[ p_t = \\frac{W_A^t p_0}{W_A^t p_0 + W_a^t q_0} \\] <p>Below we plot this general solution for a given \\(W_A\\), \\(W_a\\), and \\(p_0\\).</p> <pre>\nWA, Wa, p0 = 1.1, 1, 0.01 #define parameter values and initial condition\nts = range(100) #time values\nps = [(WA**t * p0)/(WA**t * p0 + Wa**t * (1-p0)) for t in ts] #variable values from general solution\nplt.scatter(ts, ps) #plot discretely\nplt.ylabel('allele frequency, $p(t)$')\nplt.xlabel('generation, $t$')\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-09/#4-linear-models-in-continuous-time","title":"4. Linear models in continuous time","text":"<p>In continuous time, a linear differential equation of one variable can be written</p> \\[ \\frac{\\mathrm{d}x}{\\mathrm{d}t} = a x + b \\] <p>Let's first look at the case where \\(b=0\\).</p>"},{"location":"lectures/lecture-09/#separation-of-variables","title":"Separation of variables","text":"<p>Here we can use a method called seperation of variables.</p> <p>That is, our differential equation can be written \\(\\mathrm{d}x/\\mathrm{d}t = f(x) g(t)\\), i.e., we can separate the variables \\(x\\) and \\(t\\).</p> <p>We can then re-write the equation as  \\(\\mathrm{d}x/f(x) = g(t)\\mathrm{d}t\\) and take the indefinite integral of both sides.</p> <p>In our case we have \\(f(x)=a x\\) and \\(g(t)=1\\) so</p> \\[ \\begin{aligned} \\int \\frac{\\mathrm{d}x}{a x} &amp;= \\int \\mathrm{d}t \\\\ \\frac{\\ln(x)}{a} + c_1 &amp;= t + c_2\\\\ \\ln(x) &amp;= a t + c \\; \\text{(where } c = c_2 - c_1\\text{)}\\\\ x_t &amp;= e^{a t} e^{a c} \\end{aligned} \\] <p>Plugging in \\(t=0\\) we have \\(x_0 = e^{a c}\\) and so our general solution is</p> \\[ x_t = x_0 e^{at} \\] <p>This is the general solution for exponential growth in continuous time with growth rate \\(a\\). We see that variable will either converge on (\\(a&lt;0\\)) or depart from (\\(a&gt;0\\)) the equilibrium (\\(\\hat x=0\\)), consistent with our local stability analysis. </p> <pre>\nimport matplotlib.pyplot as plt\na, x0 = 0.01, 10 #define parameter values and initial condition\nts = range(1000) #time values\nxs = [exp(a*t) * x0 for t in ts] #variable values from general solution\nplt.plot(ts, xs) #plot continuously\nplt.ylabel('$x(t)$')\nplt.xlabel('$t$')\nplt.show()\n</pre> <p></p>"},{"location":"lectures/lecture-09/#using-transformations","title":"Using transformations","text":"<p>Now let's consider the case where \\(b\\neq0\\).</p> <p>This can also be solved by the method of separation of variables but let's do  it with a transformation, like we did in discrete time.</p> <p>Step 1: Solve for the equilibrium, </p> \\[ \\begin{aligned} 0 &amp;= a \\hat{x} + b\\\\ \\hat{x} &amp;= -b/a \\end{aligned} \\] <p>Step 2: Define \\(\\delta = x - \\hat{x}\\) as the deviation of the variable from equilibrium.</p> <p>Step 3: Derive the differential equation for \\(\\delta\\)</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\delta}{\\mathrm{d}t} &amp;= \\frac{\\mathrm{d}(x - \\hat{x})}{\\mathrm{d}t}\\\\ &amp;= \\frac{\\mathrm{d}x}{\\mathrm{d}t} - \\frac{\\mathrm{d}\\hat{x}}{\\mathrm{d}t}\\\\ &amp;= \\frac{\\mathrm{d}x}{\\mathrm{d}t}\\\\ &amp;= a x + b\\\\ &amp;= a (\\delta + \\hat{x}) + b\\\\ &amp;= a(\\delta + -b/a) + b\\\\ &amp;= a \\delta \\end{aligned} \\] <p>Step 4: This is the same differential equation we had above for \\(x\\) when \\(b=0\\). The general solution is therefore \\(\\delta\\) is \\(\\delta_t = \\delta_0 e^{a t}\\).</p> <p>Step 5: Replace \\(\\delta\\) with \\(x - \\hat{x}\\) to back transform to our original variable</p> \\[ x_t = e^{a t} x_0 + (1 - e^{a t})\\hat{x} \\] <p>Similar to the discrete case (but without the oscillations), this tells us there is an exponential approach to (\\(a&lt;0\\)) or departure from (\\(a&gt;0\\)) the equilibrium, \\(\\hat{x}\\).</p> <p>Below we plot this general solution for given values of \\(a\\) and \\(b\\) and a range of initial conditions, \\(x_0\\).</p> <pre>\na, b, x0 = -0.01, 1, 10 #define parameter values and initial condition\nts = range(1000) #time values\nxs = [exp(a*t) * x0 + (1-exp(a*t))*(-b/a) for t in ts] #variable values from general solution\nplt.plot(ts, xs) #plot continuously\nplt.ylabel('$x(t)$')\nplt.xlabel('$t$')\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-09/#5-nonlinear-models-in-continuous-time","title":"5. Nonlinear models in continuous time","text":"<p>Some nonlinear differential equations can also be solved. But there is no general recipe.</p>"},{"location":"lectures/lecture-09/#separation-of-variables_1","title":"Separation of variables","text":"<p>Sometimes separation of variables works, as in the case of logistic growth and haploid selection (which are equivalent in form in continuous-time!).</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}p}{\\mathrm{d}t} &amp;= s p (1-p)\\\\ \\frac{\\mathrm{d}p}{s p (1-p)} &amp;= \\mathrm{d}t\\\\ \\left(\\frac{1}{s p} + \\frac{1/s}{1-p}\\right) \\mathrm{d}p &amp;= \\mathrm{d}t \\;\\text{(method of partial fractions, rule A.19 in the text)}\\\\ \\int \\frac{1}{s p} \\mathrm{d}p + \\int \\frac{1}{s(1-p)} \\mathrm{d}p &amp;= \\int \\mathrm{d}t\\\\ \\ln(p)/s - \\ln(1 - p)/s + c_1 &amp;= t + c_2 \\\\ \\ln\\left(\\frac{p}{1-p}\\right) &amp;= s t + s c \\; \\text{(where } c = c_2 - c_1\\text{)}\\\\ \\frac{p}{1-p} &amp;= e^{st} e^{sc} \\end{aligned} \\] <p>Plugging in \\(t=0\\) we have \\(p_0/(1-p_0) = e^{sc}\\). Then solving this linear equation for \\(p\\)</p> \\[ p_t = \\frac{e^{st} p_0}{1 - p_0 + e^{st} p_0} \\] <p>This shows essentially the same dynamics as haploid selection in discrete time.</p> <pre>\ns, p0 = 0.1, 0.01 #define parameter values and initial condition\nts = range(100) #time values\nps = [(exp(s*t) * p0)/(1 - p0 + exp(s*t)*p0) for t in ts] #variable values from general solution\nplt.plot(ts, ps) #plot continuously\nplt.ylabel('allele frequency, $p(t)$')\nplt.xlabel('generation, $t$')\nplt.show()\n</pre> <p></p>"},{"location":"lectures/lecture-09/#alternative-methods","title":"Alternative methods","text":"<p>Separation of variables does not always work as it may not be possible to solve the integrals.</p> <p>However, separation of variables is not the only method.</p> <p>Box 6.2 in the text describes how to solve three forms of differential equations that are not amenable to separation of variables (ie, that cannot be written like \\(\\mathrm{d}x/\\mathrm{d}t = f(x) g(t)\\)).</p> <p></p>"},{"location":"lectures/lecture-09/#6-summary","title":"6. Summary","text":"<p>Today we've covered how to find the general solution for some univariate models.</p> <p>We now have three methods to analyze univariate models:</p> <ul> <li>numerical and graphical analyses (for particular parameter values)</li> <li>finding equilibria and determining their local stability (general long-term dynamics)</li> <li>finding the general solution (general short- and long-term dynamics)</li> </ul>"},{"location":"lectures/lecture-10/","title":"Lecture 10","text":""},{"location":"lectures/lecture-10/#lecture-10-linear-algebra-i","title":"Lecture 10: Linear algebra I","text":"Run notes interactively?"},{"location":"lectures/lecture-10/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Motivation</li> <li>What are vectors?</li> <li>What is a matrix?</li> <li>Vector and matrix operations</li> </ol>"},{"location":"lectures/lecture-10/#1-motivation","title":"1. Motivation","text":"<p>Until now, we have been dealing with problems in a single variable changing over time.</p> <p>Often, dynamical systems involve more than one variable (ie, they are multivariate). For instance, we may be interested in how the numbers of two species change as they interact (e.g., compete) with one another.</p> <p>As a simple example with more than one variable, consider a model tracking the number of birds on two islands. Let the number of birds on island 1 be \\(n_1\\) and let the number of birds on island 2 be \\(n_2\\). We assume the birds migrate between the islands at per capita rates \\(m_{12}\\) and \\(m_{21}\\), the birds on each island give birth at per capita rates \\(b_1\\) and \\(b_2\\), the birds on each island die at per capita rates \\(d_1\\) and \\(d_2\\), and new birds arrive on each island at rates \\(m_1\\) and \\(m_2\\). This is captured in the following flow diagram</p> <pre><code>graph LR;\n    A1((n1)) --b1 n1--&gt; A1;\n    B1[ ] --m1--&gt; A1;\n    A1 --d1 n1--&gt; C1[ ];\n\n    A2((n2)) --b2 n2--&gt; A2;\n    B2[ ] --m2--&gt; A2;\n    A2 --d2 n2--&gt; C2[ ];\n\n    A1 --m12 n1--&gt; A2;\n    A2 --m21 n2--&gt; A1;\n\n    style B1 height:0px;\n    style C1 height:0px;\n    style B2 height:0px;\n    style C2 height:0px;</code></pre> <p>The rate of change in \\(n_1\\) and \\(n_2\\) are then described by the following system of differential equations</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &amp;= (b_1 - d_1 - m_{12})n_1 + m_{21} n_2 + m_1 \\\\ \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &amp;= m_{12} n_1 + (b_2 - d_2 - m_{21})n_2 + m_1 \\end{aligned} \\] <p>These equations are linear functions of the variables (i.e., they contain only constant multiples of \\(n_1\\) and \\(n_2\\) and nothing more complicated such as \\(x^2\\) or \\(e^x\\)).</p> <p>Linear systems of equations like these can also be written in matrix form</p> \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} \\end{pmatrix}  &amp;= \\begin{pmatrix} b_1 - d_1 - m_{12} &amp; m_{21} \\\\ m_{12} &amp; b_2 - d_2 - m_{21} \\end{pmatrix} \\begin{pmatrix} n_1 \\\\ n_2 \\end{pmatrix}  + \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}\\\\ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} &amp;= \\mathbf{M}\\vec{n} + \\vec{m} \\end{aligned} \\] <p>Not only is this a nice compact expression, there are rules of linear algebra that can help us conveniently solve this (and any other) set of linear equations.</p> <p>So let's get to know these rules.</p> <p></p>"},{"location":"lectures/lecture-10/#2-what-are-vectors","title":"2. What are vectors?","text":"<p>Vectors are lists of elements (elements being numbers, parameters, functions, or variables).</p> <p>A column vector has elements arranged from top to bottom</p> \\[ \\begin{equation*} \\begin{pmatrix}   5 \\\\   2 \\end{pmatrix}, \\begin{pmatrix}   1 \\\\   5 \\\\   9 \\\\   7 \\end{pmatrix}, \\begin{pmatrix}   x \\\\   y \\end{pmatrix}, \\begin{pmatrix}   x \\\\   y \\\\   z \\end{pmatrix}, \\begin{pmatrix}   x_1 \\\\   x_2 \\\\   \\vdots \\\\   x_n \\end{pmatrix} \\end{equation*} \\] <p>A row vector has elements arranged from left to right</p> \\[ \\begin{pmatrix}5 &amp; 2\\end{pmatrix}, \\begin{pmatrix} 1 &amp; 5 &amp; 9 &amp; 7\\end{pmatrix}, \\begin{pmatrix} x &amp; y \\end{pmatrix}, \\begin{pmatrix} x &amp; y &amp; z\\end{pmatrix}, \\begin{pmatrix} x_1 &amp; x_2 &amp; \\cdots &amp; x_n \\end{pmatrix} \\] <p>We will indicate vectors by placing an arrow on top of the symbol</p> \\[ \\vec{x} = \\begin{pmatrix} x_1 &amp; x_2 &amp; \\cdots &amp; x_n \\end{pmatrix} \\] <p>The number of elements in the vector indicates its dimension, \\(n\\).</p> <p>For example, the row vector \\(\\begin{pmatrix}x &amp; y\\end{pmatrix}\\) has dimension \\(n=2\\).</p> <p>You can represent a vector as an arrow in \\(n\\) dimensions, connecting the origin with a point whose coordinates are given by elements in the vector. For example, the vector \\(\\vec{v} = \\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) can be depicted as below</p> <pre>\nimport matplotlib.pyplot as plt #import plotting library\n\nplt.arrow(0, 0, #starting x and y values of arrow\n          1, 2, #change in x and y \n          head_width=0.1, color='black') #aesthetics\nplt.xlim(0,2.5) #set bounds on x axis\nplt.ylim(0,2.5) #set bounds on y axis\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-10/#3-what-is-a-matrix","title":"3. What is a matrix?","text":"<p>An \\(m \\times n\\) matrix has \\(m\\) rows and \\(n\\) columns</p> \\[ \\begin{equation*} \\begin{pmatrix}   x_{11}  &amp; x_{12} &amp; \\cdots &amp; x_{1n}\\\\   x_{21}  &amp; x_{22} &amp; \\cdots &amp; x_{2n}\\\\   \\vdots &amp; \\vdots &amp;        &amp; \\vdots\\\\   x_{m1}  &amp; x_{m2} &amp; \\cdots &amp; x_{mn}\\\\ \\end{pmatrix}, \\begin{pmatrix}   a &amp; b \\\\   c &amp; d \\end{pmatrix}, \\begin{pmatrix}   75 &amp; 67 \\\\   66 &amp; 34 \\\\   12 &amp; 14 \\end{pmatrix}, \\begin{pmatrix}   1 &amp; 0 &amp; 0 \\\\   0 &amp; 1 &amp; 0 \\\\   0 &amp; 0 &amp; 1 \\end{pmatrix} \\end{equation*} \\] <p>We will indicate matrices by bolding the symbol (and using capital letters)</p> \\[ \\mathbf{X} = \\begin{pmatrix}   x_{11}  &amp; x_{12} &amp; \\cdots &amp; x_{1n}\\\\   x_{21}  &amp; x_{22} &amp; \\cdots &amp; x_{2n}\\\\   \\vdots &amp; \\vdots &amp;        &amp; \\vdots\\\\   x_{m1}  &amp; x_{m2} &amp; \\cdots &amp; x_{mn}\\\\ \\end{pmatrix} \\] <p>A matrix with an equal number of rows and columns, \\(m=n\\), is a square matrix.</p> <p>A matrix with zeros everywhere except along the diagonal is called a diagonal matrix</p> \\[ \\begin{pmatrix}   a &amp; 0 &amp; 0 \\\\   0 &amp; b &amp; 0 \\\\   0 &amp; 0 &amp; c \\end{pmatrix} \\] <p>And a special case of this with 1s along the diagonal is called the identity matrix</p> \\[ \\begin{pmatrix}   1 &amp; 0 &amp; 0 \\\\   0 &amp; 1 &amp; 0 \\\\   0 &amp; 0 &amp; 1 \\end{pmatrix} \\] <p>A matrix with all zeros below the diagonal is called an upper trianglular matrix</p> \\[ \\begin{pmatrix}   a &amp; b &amp; c \\\\   0 &amp; d &amp; e \\\\   0 &amp; 0 &amp; f \\end{pmatrix} \\] <p>A matrix with all zeros above the diagonal is called an lower trianglular matrix</p> \\[ \\begin{pmatrix}   a &amp; 0 &amp; 0 \\\\   b &amp; d &amp; 0 \\\\   c &amp; e &amp; f \\end{pmatrix} \\] <p>It is sometimes useful to chop a matrix up into multiple blocks, creating a block matrix</p> \\[ \\begin{pmatrix}   a &amp; b &amp; c \\\\   d &amp; e &amp; f \\\\   g &amp; h &amp; i \\end{pmatrix} =  \\begin{pmatrix}   \\mathbf{A} &amp; \\mathbf{B} \\\\   \\mathbf{C} &amp; \\mathbf{D} \\end{pmatrix} \\] <p>where \\(\\mathbf{A}=\\begin{pmatrix} a &amp; b \\\\ d &amp; e\\end{pmatrix}\\), \\(\\mathbf{B}=\\begin{pmatrix} c\\\\ f\\end{pmatrix}\\), \\(\\mathbf{C}=\\begin{pmatrix} g &amp; h \\end{pmatrix}\\), and \\(\\mathbf{D}=\\begin{pmatrix} i\\end{pmatrix}\\).</p> <p>This is especially helpful when the block form has off-diagonal submatrices consisting of all zeros.</p> <p>For instance, when \\(\\mathbf{B}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\\) or \\(\\mathbf{C}=\\begin{pmatrix} 0 &amp; 0 \\end{pmatrix}\\), we have a block triangular matrix.</p> <p>And when \\(\\mathbf{B}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\\) and \\(\\mathbf{C}=\\begin{pmatrix} 0 &amp; 0 \\end{pmatrix}\\),  we have a block diagonal matrix.</p> <p>Finally, it is sometimes useful to transpose a matrix, which exchanges the rows and columns (an element in row \\(i\\) column \\(j\\) moves to row \\(j\\) column \\(i\\))</p> \\[ \\begin{pmatrix}   a_1 &amp; a_2 &amp; a_3 \\\\   b_1 &amp; b_2 &amp; b_3 \\end{pmatrix}^\\intercal =  \\begin{pmatrix}   a_1 &amp; b_1  \\\\   a_2 &amp; b_2  \\\\   a_3 &amp; b_3 \\end{pmatrix} \\] <p>Like vectors, matrices have a graphical/geometrical interpretation: they stretch and rotate vectors (as we will see shortly).</p> <p></p>"},{"location":"lectures/lecture-10/#4-vector-and-matrix-operations","title":"4. Vector and matrix operations","text":""},{"location":"lectures/lecture-10/#addition","title":"Addition","text":"<p>Vector and matrix addition (and subtraction) is straightforward, entry-by-entry:</p> \\[ \\begin{equation*} \\begin{pmatrix}   a \\\\   b \\end{pmatrix} + \\begin{pmatrix}   c \\\\   d \\end{pmatrix} = \\begin{pmatrix}   a+c \\\\   b+d \\end{pmatrix} \\end{equation*} \\] \\[ \\begin{equation*} \\begin{pmatrix}   a &amp; b \\\\   c &amp; d \\end{pmatrix} + \\begin{pmatrix}   e &amp; f \\\\   g &amp; h \\end{pmatrix} = \\begin{pmatrix}   a+e &amp; b+f \\\\   c+g &amp; d+h \\end{pmatrix} \\end{equation*} \\] <p>Warning</p> <p>The vectors or matrices added together must have the same dimension!</p> <p>Geometrically, adding vectors is like placing the second vector at the end of the first. Below we add the black and red vectors together to get the blue vector.</p> <pre>\nimport matplotlib.pyplot as plt #import plotting library\n\nv1 = [1,2] #vector 1\nv2 = [1,0] #vector 2\nv12 = [i+j for i,j in zip(v1,v2)] #sum of the two vectors\n\n#first vector\nplt.arrow(0, 0, #starting x and y values of arrow\n          v1[0], v1[1], #change in x and y \n          head_width=0.1, color='black') #aesthetics\n\n#second vector placed at the end of first vector\nplt.arrow(v1[0], v1[1], #starting x and y values of arrow\n          v2[0], v2[1], #change in x and y \n          head_width=0.1, color='red') #aesthetics\n\n#sum of the vectors\nplt.arrow(0, 0, #starting x and y values of arrow\n          v12[0], v12[1], #change in x and y \n          head_width=0.1, color='blue') #aesthetics\n\nplt.xlim(0,2.5) #set bounds on x axis\nplt.ylim(0,2.5) #set bounds on y axis\nplt.show()\n</pre> <p></p>"},{"location":"lectures/lecture-10/#multiplication","title":"Multiplication","text":"<p>Vector and matrix multiplication by a scalar (which may be a constant, a variable, or a function, but not a matrix or a vector) is also straightforward, we just multiply every element by the scalar:</p> \\[ \\begin{equation*} \\alpha * \\begin{pmatrix}   a \\\\   b \\end{pmatrix} = \\begin{pmatrix}   \\alpha a \\\\   \\alpha b \\end{pmatrix} \\end{equation*} \\] \\[ \\begin{equation*} \\alpha * \\begin{pmatrix}   a &amp; b\\\\   c &amp; d \\end{pmatrix} = \\begin{pmatrix}   \\alpha a &amp; \\alpha b\\\\   \\alpha c &amp; \\alpha d \\end{pmatrix} \\end{equation*} \\] <p>Geometrically, multiplying by a scalar stretches (if \\(\\alpha&gt;1\\)) or compresses (if \\(\\alpha&lt;1\\)) a vector. Below we multiply the black vector by \\(1/2\\) to get the red vector.</p> <pre>\nimport matplotlib.pyplot as plt #import plotting library\n\nv1 = [1,2] #vector 1\nalpha = 1/2 #scalar\nv2 = [i*alpha for i in v1] #multiplication by a scalar\n\n#original vector\nplt.arrow(0, 0, #starting x and y values of arrow\n          v1[0], v1[1], #change in x and y \n          head_width=0.1, color='black') #aesthetics\n\n#stretched vector\nplt.arrow(0, 0, #starting x and y values of arrow\n          v2[0], v2[1], #change in x and y \n          head_width=0.1, color='red') #aesthetics\n\nplt.xlim(0,2.5) #set bounds on x axis\nplt.ylim(0,2.5) #set bounds on y axis\nplt.show()\n</pre> <p></p> <p>Multiplying vectors and matrices together is a bit trickier, but is based on the fact that a row vector times a column vector is equal to the sum of the products of their respective entries</p> \\[ \\begin{equation*} \\begin{pmatrix} a &amp; b &amp; c \\end{pmatrix} \\begin{pmatrix}   x \\\\   y \\\\   z \\end{pmatrix} = ax + by + cz \\end{equation*} \\] <p>This is referred to as the dot product. (There are other types of products for vectors and matrices, which we won't cover in this class.)</p> <p>To multiply a matrix by a vector, this procedure is repeated first for the first row of the matrix, then for the second row of the matrix, etc:</p> \\[ \\begin{equation*} \\begin{pmatrix}   a &amp; b &amp; c \\\\   d &amp; e &amp; f \\\\   g &amp; h &amp; i \\end{pmatrix} \\begin{pmatrix}   x \\\\   y \\\\   z \\end{pmatrix} = \\begin{pmatrix}   ax + by + cz \\\\   dx + ey + fz \\\\   gx + hy + iz \\\\ \\end{pmatrix} \\end{equation*} \\] <p>Geometrically, multiplying a vector by a matrix stretches and rotates a vector. Below we multiply the black vector my a matrix to get the red vector.</p> <pre>\nimport matplotlib.pyplot as plt #import plotting library\nfrom sympy import *\n\nv = Matrix([[2],[1]]) #column vector\nM = Matrix([[1,-1],[1,1/4]]) #matrix\nu = M*v\n\n#original vector\nplt.arrow(0, 0, #starting x and y values of arrow\n          float(v[0]), float(v[1]), #change in x and y \n          head_width=0.1, color='black') #aesthetics\n\n#stretched and rotated vector\nplt.arrow(0, 0, #starting x and y values of arrow\n          float(u[0]), float(u[1]), #change in x and y \n          head_width=0.1, color='red') #aesthetics\n\nplt.xlim(0,2.5) #set bounds on x axis\nplt.ylim(0,2.5) #set bounds on y axis\nplt.show()\n</pre> <p></p> <p>To multiply a matrix by a matrix, this procedure is then repeated first for the first column of the second matrix and then for the second column of the second matrix, etc:</p> \\[ \\begin{equation*} \\begin{pmatrix}   a &amp; b \\\\   c &amp; d \\end{pmatrix} \\begin{pmatrix}   e &amp; f \\\\   g &amp; h \\end{pmatrix} = \\begin{pmatrix}   ae + bg &amp; af + bh \\\\   ce + dg &amp; cf + dh \\end{pmatrix} \\end{equation*} \\] <p>Warning</p> <p>An \\(m \\times n\\) matrix (or vector) \\(\\mathbf{A}\\) can be multiplied on the right by \\(\\mathbf{B}\\) only if \\(\\mathbf{B}\\) is an \\(n \\times p\\) matrix (or vector). The resulting matrix (or vector) will then be \\(m \\times p\\).</p> <p>As opposed to basic algebra, matrix multiplication is not commutative. That is, \\(\\mathbf{AB}\\) does not generally equal \\(\\mathbf{BA}\\).</p> <p>This means that if we want to multiply both sides of an equation, e.g., \\(\\mathbf{AB} = \\mathbf{C}\\), by \\(\\mathbf{D}\\), we need to do so on the same side, \\(\\mathbf{ABD} = \\mathbf{CD}\\) or \\(\\mathbf{DAB} = \\mathbf{DC}\\). We therefore often need to specify that we are multiplying by a matrix \"on the left\" or \"on the right\".</p> <p>On the other hand, matrix multiplication does satisfy the following laws:</p> <ul> <li>\\((\\mathbf{AB})\\mathbf{C} = \\mathbf{A}(\\mathbf{BC})\\) (associative law)</li> <li>\\(\\mathbf{A}(\\mathbf{B+C}) = \\mathbf{AB}+\\mathbf{AC}\\) (distributive law)</li> <li>\\((\\mathbf{A}+\\mathbf{B})\\mathbf{C} = \\mathbf{AC}+\\mathbf{BC}\\) (distributive law)</li> <li>\\(\\alpha(\\mathbf{AB}) = (\\alpha\\mathbf{A})\\mathbf{B} = \\mathbf{A}(\\alpha\\mathbf{B}) = (\\mathbf{A}\\mathbf{B})\\alpha\\) (commutative law for scalars)</li> </ul> <p>Multiplication between the identity matrix and any vector, \\(\\vec{v}\\), or square matrix, \\(\\mathbf{M}\\), has no effect (it is like a \"1\" in normal algebra)</p> \\[ \\mathbf{I}\\vec{v}=\\vec{v} \\] \\[ \\mathbf{I}\\mathbf{M}=\\mathbf{M}\\mathbf{I}=\\mathbf{M} \\]"},{"location":"lectures/lecture-11/","title":"Lecture 11","text":""},{"location":"lectures/lecture-11/#lecture-11-linear-algebra-ii","title":"Lecture 11: Linear algebra II","text":"Run notes interactively?"},{"location":"lectures/lecture-11/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Matrix operations</li> <li>Solving systems of linear equations</li> </ol>"},{"location":"lectures/lecture-11/#1-matrix-operations","title":"1. Matrix operations","text":""},{"location":"lectures/lecture-11/#trace","title":"Trace","text":"<p>The trace of a matrix is the sum of the diagonal elements</p> \\[ \\mathrm{Tr}\\left( \\begin{pmatrix}   a &amp; b &amp; c \\\\   d &amp; e &amp; f \\\\   g &amp; h &amp; i \\\\ \\end{pmatrix}\\right) = a + e + i    \\]"},{"location":"lectures/lecture-11/#determinant","title":"Determinant","text":"<p>The determinant of a \\(2 \\times 2\\) matrix is:</p> \\[ \\begin{equation*} \\text{Det}\\left( \\begin{pmatrix}   a &amp; b \\\\   c &amp; d \\end{pmatrix}\\right) = \\begin{vmatrix}   a &amp; b\\\\   c &amp; d \\end{vmatrix} =ad-bc \\end{equation*} \\] <p>Note</p> <p>You should remember how to calculate the determinant of a 2x2 matrix.</p> <p>The determinant of an \\(n \\times n\\) matrix can be obtained by working along the first row, multiplying the first element of the first row by the determinant of the matrix created by deleting the first row and first column minus the second element of the first row times the determinant of the matrix created by deleting the first row and second column plus the third element... and so on</p> \\[ \\begin{vmatrix} \\mathbf{M} \\end{vmatrix} = \\sum_{j=1}^n (-1)^{j+1} m_{1j} \\begin{vmatrix} \\mathbf{M}_{1j} \\end{vmatrix} \\] <p>where \\(m_{ij}\\) is the element in the \\(i^{\\mathrm{th}}\\) row and \\(j^{\\mathrm{th}}\\) column and \\(\\mathbf{M}_{ij}\\) is the matrix \\(\\mathbf{M}\\) with the \\(i^{\\mathrm{th}}\\) row and the \\(j^{\\mathrm{th}}\\) column deleted.</p> <p>More generally, we can move along any row \\(i\\)</p> \\[ |\\mathbf{M}| = (-1)^{i+1}\\sum_{j=1}^{n}(-1)^{j+1}m_{ij}  |\\mathbf{M_{ij}}| \\] <p>or any column \\(j\\)</p> \\[ |\\mathbf{M}| = (-1)^{j+1}\\sum_{i=1}^{n}(-1)^{i+1}m_{ij}  |\\mathbf{M_{ij}}| \\] <p>A few useful rules emerge from this:</p> <ul> <li>The determinant of a matrix is the same as the determinant of its transpose, \\(|\\mathbf{M}| = |\\mathbf{M}^\\intercal|\\)</li> <li>The determinant of a diagonal or triangular matrix is the product of the diagonal elements, \\(|\\mathbf{M}| = \\prod_{i=1}^n m_{ii} = m_{11}m_{22}\\cdots m_{nn}\\)</li> <li>The determinant of a block-diagonal or -triangular matrix is the product of the determinants of the diagonal submatrices</li> </ul> <p>It also suggests that rows or columns with lots of zeros are very helpful when calculating the determinant, for example</p> \\[ \\begin{aligned} \\begin{vmatrix}   m_{11} &amp; 0 &amp; 0 \\\\   m_{21} &amp; m_{22} &amp; m_{23} \\\\   m_{31} &amp; m_{32} &amp; m_{33} \\\\ \\end{vmatrix} =  m_{11}  \\begin{vmatrix}   m_{22} &amp; m_{23} \\\\   m_{32} &amp; m_{33} \\\\ \\end{vmatrix}\\\\ \\end{aligned} \\] <p>And why would we want to calculate the determinant of a matrix? </p> <p>Well, when the determinant is zero, \\(|\\mathbf{M}|=0\\), it means that the rows are not linearly independent, that is, some row \\(\\vec{r}_k\\) can be written as \\(a_1 \\vec{r}_1 + \\cdots + a_{k-1} \\vec{r}_{k-1} + a_{k+1} \\vec{r}_{k+1} + \\cdots + a_n \\vec{r}_n\\), where the \\(a_i\\) are scalars. </p> <p>As a result, when we multiply a vector by a matrix with a determinant of zero we lose some information, and therefore cannot reverse the operation (as we will see when we discuss inverses). This is analagous to mutliplying by 0 in normal algebra -- if we multiply a bunch of different numbers by zero we have no way of knowing what the original numbers were.</p> <p>Geometrically, mutliplying multiple vectors by a matrix whose deteriminant is zero causes them to fall along a line. Below we multiply the two black vectors by a matrix whose determinant is zero to get the two red vectors, which line on the same line.</p> <pre>\nmatplotlib.pyplot as plt #import plotting library\nfrom sympy import *\n\nv1 = Matrix([[2],[1]]) #column vector 1\nv2 = Matrix([[1],[1]]) #column vector 2\nM = Matrix([[1/2,1],[1,2]]) #matrix with determinant of zero\n\n#original vectors\nfor v in [v1,v2]:\n    plt.arrow(0, 0, #starting x and y values of arrow\n              float(v[0]), float(v[1]), #change in x and y \n              head_width=0.1, color='black') #aesthetics\n\n#stretched and rotated vectors\nfor v in [M*v1,M*v2]:\n    plt.arrow(0, 0, #starting x and y values of arrow\n              float(v[0]), float(v[1]), #change in x and y \n              head_width=0.1, color='red') #aesthetics\n\nplt.xlim(0,5) #set bounds on x axis\nplt.ylim(0,5) #set bounds on y axis\nplt.show()\n</pre> <p></p>"},{"location":"lectures/lecture-11/#inverse","title":"Inverse","text":"<p>In the last lecture we discussed matrix addition/subtraction and multiplication. We did not yet discuss division. In fact, for matrices, there is no such thing as division! The analogy is the inverse.</p> <p>A square \\(m\\times m\\) matrix \\(\\mathbf{M}\\) is invertible if it may be multiplied by another matrix to get the identity matrix.  We call this second matrix, \\(\\mathbf{M}^{-1}\\) the inverse of the first</p> \\[ \\mathbf{M}\\mathbf{M}^{-1} = \\mathbf{I} = \\mathbf{M}^{-1}\\mathbf{M} \\] <p>Geometrically, the inverse reverses the stretching and rotating that the original matrix does to a vector</p> \\[\\mathbf{M}^{-1}(\\mathbf{M}\\vec{v}) = (\\mathbf{M}^{-1}\\mathbf{M})\\vec{v} = \\mathbf{I}\\vec{v} = \\vec{v}\\] <p>There are rules to find the inverse of a matrix (when it is invertible). For a 2x2 matrix we do the following</p> \\[ \\begin{align} \\mathbf{M}^{-1}  =&amp;\\begin{pmatrix}   a &amp; b \\\\   c &amp; d \\end{pmatrix}^{-1}\\\\ &amp;=\\frac{1}{\\mathrm{Det}(\\mathbf{M})} \\begin{pmatrix}   d  &amp; -b \\\\   -c &amp; a \\end{pmatrix}\\\\ &amp;= \\begin{pmatrix}   \\frac{d}{ad-bc}  &amp; \\frac{-b}{ad-bc} \\\\   \\frac{-c}{ad-bc} &amp; \\frac{a}{ad-bc} \\end{pmatrix} \\end{align} \\] <p>Note</p> <p>Remember how to take the inverse of a 2x2 matrix.</p> <p>Warning</p> <p>As you can see from the 2x2 case above, when the determinant is zero no inverse exists (and this is true for any square matrix). We therefore call a matrix whose determinant is zero non-invertible or singular. You can connect this fact back to the previous lecture, where we saw that mutliplying vectors by a matrix whose determinant is zero caused them to collapse upon one another, losing information. The fact that a matrix whose determinant is zero has no inverse means that we cannot reverse the original matrix multiplication, just like we can't reverse mutliplication by zero in normal algebra.</p> <p>Larger matrices are more difficult to invert, except if they are diagonal, in which case we simply invert each of the diagonal elements</p> \\[ \\mathbf{M}^{-1} =  \\begin{pmatrix} 1/m_{11} &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; 1/m_{22} &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ 0 &amp; 0 &amp; \\cdots &amp; 1/m_{nn}\\\\ \\end{pmatrix} \\] <p></p>"},{"location":"lectures/lecture-11/#2-solving-systems-of-linear-equations","title":"2. Solving systems of linear equations","text":"<p>With all this linear algebra knowledge in hand, let's use it!</p> <p>Let's return to our model for the number of birds on two islands</p> <pre><code>graph LR;\n    A1((n1)) --b1 n1--&gt; A1;\n    B1[ ] --m1--&gt; A1;\n    A1 --d1 n1--&gt; C1[ ];\n\n    A2((n2)) --b2 n2--&gt; A2;\n    B2[ ] --m2--&gt; A2;\n    A2 --d2 n2--&gt; C2[ ];\n\n    A1 --m12 n1--&gt; A2;\n    A2 --m21 n2--&gt; A1;\n\n    style B1 height:0px;\n    style C1 height:0px;\n    style B2 height:0px;\n    style C2 height:0px;</code></pre> \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} \\end{pmatrix}  &amp;= \\begin{pmatrix} b_1 - d_1 - m_{12} &amp; m_{21} \\\\ m_{12} &amp; b_2 - d_2 - m_{21} \\end{pmatrix} \\begin{pmatrix} n_1 \\\\ n_2 \\end{pmatrix}  + \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}\\\\ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} &amp;= \\mathbf{M}\\vec{n} + \\vec{m} \\end{aligned} \\] <p>The equilibria, \\(\\hat{\\vec{n}}\\), are then found by setting \\(\\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t}=0\\), subtracting \\(\\vec{m}\\) from both sides, and multiplying by the inverse matrix \\(\\mathbf{M}^{-1}\\) on the left</p> \\[ \\begin{align*} 0 &amp;= \\mathbf{M}\\hat{\\vec{n}} + \\vec{m}\\\\ -\\vec{m} &amp;= \\mathbf{M}\\hat{\\vec{n}}\\\\ -\\mathbf{M}^{-1}\\vec{m} &amp;= \\mathbf{M}^{-1}\\mathbf{M}\\hat{\\vec{n}}\\\\ -\\mathbf{M}^{-1}\\vec{m} &amp;= \\hat{\\vec{n}} \\end{align*} \\] <p>We can write the left hand side in terms of our parameters by calculating the inverse of this 2x2 matrix and multiplying by the vector</p> \\[ \\begin{align} \\hat{\\vec{n}}  &amp;=-\\mathbf{M}^{-1}\\vec{m}\\\\ &amp;=-\\frac{1}{|\\mathbf{M}|} \\begin{pmatrix} b_2 - d_2 - m_{21} &amp; -m_{21} \\\\ -m_{12} &amp; b_1 - d_1 - m_{12} \\end{pmatrix} \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}\\\\ &amp;= -\\frac{1}{(b_1 - d_1 - m_{12})(b_2 - d_2 - m_{21})-m_{21}m_{12}} \\begin{pmatrix} (b_2 - d_2 - m_{21})m_1 -m_{21}m_2 \\\\ -m_{12}m_1 + (b_1 - d_1 - m_{12})m_2 \\end{pmatrix}\\\\ &amp;= \\begin{pmatrix} -\\frac{(b_2 - d_2 - m_{21})m_1 -m_{21}m_2}{(b_1 - d_1 - m_{12})(b_2 - d_2 - m_{21})-m_{21}m_{12}} \\\\ -\\frac{-m_{12}m_1 + (b_1 - d_1 - m_{12})m_2}{(b_1 - d_1 - m_{12})(b_2 - d_2 - m_{21})-m_{21}m_{12}} \\end{pmatrix} \\end{align} \\] <p>Ta-da! Using linear algebra we solved for both equilibria, \\(\\hat{n}_1\\) and \\(\\hat{n}_2\\), with a single equation. In future lectures we'll see how we can use linear algebra to calculate the local stability and derive general solutions.</p>"},{"location":"lectures/lecture-12/","title":"Lecture 12","text":""},{"location":"lectures/lecture-12/#lecture-12-linear-algebra-iii","title":"Lecture 12: Linear algebra III","text":"Run notes interactively?"},{"location":"lectures/lecture-12/#lecture-overview","title":"Lecture overview","text":"<ol> <li>What are eigenvalues and eigenvectors?</li> <li>Why should be care about eigenvalues and eigenvectors?</li> <li>Finding eigenvalues</li> <li>Finding eigenvectors</li> </ol>"},{"location":"lectures/lecture-12/#1-what-are-eigenvalues-and-eigenvectors","title":"1. What are eigenvalues and eigenvectors?","text":"<p>A number \\(\\lambda\\) is an eigenvalue of matrix \\(\\textbf{M}\\) if there exists a non-zero vector, \\(\\vec{v}\\), that satisfies</p> \\[ \\mathbf{M} \\vec{v} = \\lambda \\vec{v} \\] <p>Every non-zero vector \\(\\vec{v}\\) satisfying this equation is a right eigenvector of \\(\\mathbf{M}\\) associated with eigenvalue \\(\\lambda\\).</p> <p>Every non-zero vector \\(\\vec{u}\\) satisfying</p> \\[ \\vec{u}\\mathbf{M} = \\lambda \\vec{u} \\] <p>is a left eigenvector of \\(\\mathbf{M}\\) associated with eigenvalue \\(\\lambda\\).</p> <p></p>"},{"location":"lectures/lecture-12/#2-why-should-we-care-about-eigenvalues-and-eigenvectors","title":"2. Why should we care about eigenvalues and eigenvectors?","text":"<p>Let's say \\(\\mathbf{M}\\) describes the dynamics of our biological variables, \\(\\vec{n}\\), which might be the numbers of different types of individuals or the frequency of alleles at different loci, and for concreteness let's just say we are working in discrete time, \\(\\vec{n}(t+1) = \\mathbf{M} \\vec{n}(t)\\) (but similar arguments hold for continuous time). </p> <p>Now notice that if our system, \\(\\vec{n}(t)\\), ever approaches a right eigenvalue, \\(\\vec{v}\\), the dynamics reduce to simple exponential growth \\(\\vec{n}(t+1) = \\mathbf{M} \\vec{n}(t) = \\mathbf{M} \\vec{v} = \\lambda \\vec{v}\\) at rate \\(\\lambda\\) in direction \\(\\vec{v}\\). In other words, the eigenvalues describe the rate at which our system grows or shrinks along their associated eigenvectors.</p> <p>More generally, even when \\(\\vec{n}\\) is not near a right eigenvector, the right eigenvectors provide a new coordinate system in which the dynamics of our system are easier to understand. </p> <p>To see this, let's take \\(\\mathbf{M} = \\begin{pmatrix} 1 &amp; 1 \\\\ 1/2 &amp; 3/2 \\end{pmatrix}\\). The eigenvalues of \\(\\mathbf{M}\\) are \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 1/2\\). The right eigenvectors associated with these two eigenvalues are \\(\\vec{v}_1 = \\begin{pmatrix} 1 \\\\ -1/2 \\end{pmatrix}\\) and \\(\\vec{v}_2 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\), respectively. </p> <p>The right eigenvectors are plotted as red vectors in the plot below. The initial state of the sytem, \\(\\vec{n}(0)\\), is given by the blue vector. Thinking of the eigenvectors as a new coordinate system for our dynamics, we draw dashed gray lines that are parallel to the two eigenvectors, connecting \\(\\vec{n}(0)\\) to the two eigenvectors. Where these dashed gray lines intersect the eigenvalues indicates the value of \\(\\vec{n}(0)\\) in the new coordinate system. </p> <p>To calculate the state of the system in the next time step, \\(\\vec{n}(1) = \\mathbf{M}\\vec{n}(0)\\), we can then simply multiply the values of \\(\\vec{n}(0)\\) in the new coordinate system by the eigenvalues. In this example, we multiply the distance of \\(\\vec{n}(0)\\) along the first eigenvector by \\(\\lambda_1\\) and the distance of \\(\\vec{n}(0)\\) along the second eigenvector by \\(\\lambda_2\\), which gives \\(\\vec{n}(1)\\) in orange. We can continue doing this to calculate \\(\\vec{n}(2)\\) (plotted in green), and so on...</p> <p>Not only do the right eigenvectors define a more convenient coordinate system for our dynamics, you may also notice in the plot below that the system approaches one of the eigenvectors, suggesting that the long-term dynamics of the system can be predicted based on only one of the eigenvalues and it's associated right eigenvector (a fact we will later prove). </p> <pre>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# define objects\nM = np.array([[1,1],[1/2,3/2]]) #matrix\ne1, e2 = np.linalg.eigvals(M) #eigenvalues\nvs = np.linalg.eig(M)[1] #eigenvectors\nv1 = vs[:,0]; v1 = v1/v1[0] #first eigenvector normalized by first entry\nv2 = vs[:,1]; v2 = v2/v2[0] #second eigenvector normalized by first entry\n\nn0 = np.array([2/3,1/4]) #initial values of our variables\nn1 = M @ n0 #n1 = M*n0\nn2 = M @ n1 #n2 = M*n1\n\n# plot\nxmin,xmax = -0.1,1.5 #x limits\nymin,ymax = -1,1.5 #y limits\nfig, ax = plt.subplots()\n\n# plot the eigenvectors\nfor i,v in enumerate([v1,v2]):\n    ax.plot([0,v[0]], # x values\n            [0,v[1]], # y values\n            c='r',\n            label='eigenvector $v_{%d}$'%(i+1)) #aesthetics\n\n# plot the state of the system\nfor i,n in enumerate([n0,n1,n2]):\n    ax.plot([0,n[0]],\n            [0,n[1]],\n           label='$n(%d)$'%i)\n    # parallel lines\n    for a,b in [[v1,v2],[v2,v1]]:\n        yshift = n[1] - a[1]/a[0] * n[0]\n        xshift = yshift/(b[1]/b[0]-a[1]/a[0])\n        yshift = xshift * b[1]/b[0]\n        ax.plot([0 + xshift, n[0]],\n                [0 + yshift, n[1]],\n                c='gray', ls='--', alpha=0.5)\n\n# aesthetics\nax.axis('off') #remove frame\nax.plot([0,0],[ymin,ymax], c='k') #x axis\nax.plot([xmin,xmax],[0,0], c='k') # yaxis\nax.legend()\n\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-12/#3-finding-eigenvalues","title":"3. Finding eigenvalues","text":"<p>To find the eigenvalues, first notice that if we try to use linear algebra to solve for the right eigenvector, \\(\\vec{v}\\), we find</p> \\[ \\begin{aligned}  \\mathbf{M}\\vec{v} &amp;= \\lambda\\vec{v}\\\\ \\mathbf{M}\\vec{v} - \\lambda\\vec{v} &amp;= \\vec{0}\\\\ \\mathbf{M}\\vec{v} - \\lambda\\mathbf{I}\\vec{v} &amp;= \\vec{0} \\\\ (\\mathbf{M} - \\lambda\\mathbf{I})\\vec{v} &amp;= \\vec{0}\\\\ \\vec{v} &amp;= (\\mathbf{M} - \\lambda\\mathbf{I})^{-1}\\vec{0} \\\\ \\vec{v} &amp;= \\vec{0} \\end{aligned} \\] <p>where \\(\\mathbf{I}\\) is the identity matrix and \\(\\vec{0}\\) is a vector of zeros.</p> <p>But above we've said that \\(\\vec{v}\\) is a non-zero vector! This is a contradiction.</p> <p>This contradiction implies that we did something wrong in our calculations. The only place we made any assumptions was in our last step, where we assumed \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) was invertible.</p> <p>We then conclude that \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) is non-invertible and therefore must have a determinant of zero, \\(|\\mathbf{M} - \\lambda\\mathbf{I}|=0\\). Interestingly, this last equation, \\(|\\mathbf{M} - \\lambda\\mathbf{I}|=0\\), gives us a way to solve for the eigenvalues, \\(\\lambda\\), without knowing the eigenvectors, \\(\\vec{v}\\)!</p> <p>The determinant of the \\(n\\times n\\) matrix \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) is an \\(n^{th}\\) degree polynomial in \\(\\lambda\\), which is called the characteristic polynomial of \\(\\mathbf{M}\\). Setting this polynomial equal to zero and solving for \\(\\lambda\\) gives the \\(n\\) eigenvalues of \\(\\mathbf{M}\\): \\(\\lambda_1,\\lambda_2,...,\\lambda_n\\).</p> <p>For example, in the \\(n=2\\) case we have</p> \\[ \\begin{aligned} \\mathbf{M} - \\lambda \\mathbf{I} &amp;= \\begin{pmatrix} m_{11} &amp; m_{12} \\\\ m_{21} &amp; m_{22} \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}\\\\ &amp;= \\begin{pmatrix} m_{11} &amp; m_{12} \\\\ m_{21} &amp; m_{22} \\end{pmatrix} -  \\begin{pmatrix} \\lambda &amp; 0 \\\\ 0 &amp; \\lambda \\end{pmatrix}\\\\ &amp;= \\begin{pmatrix} m_{11} - \\lambda &amp; m_{12} \\\\ m_{21} &amp; m_{22} - \\lambda \\end{pmatrix} \\end{aligned} \\] <p>so that the characteristic polynomial is</p> \\[ \\begin{aligned} |\\mathbf{M} - \\lambda \\mathbf{I} | =&amp; (m_{11}-\\lambda)(m_{22}-\\lambda)-m_{21} m_{12}\\\\ =&amp;\\lambda^2 - (m_{11}+m_{22})\\lambda + (m_{11}m_{22}-m_{21}m_{12})\\\\ =&amp;\\lambda^2 - \\mathrm{Tr}(\\mathbf{M})\\lambda + \\mathrm{Det}(\\mathbf{M}) \\end{aligned} \\] <p>Setting this polynomial equal to zero, the two solutions can be found using the quadratic formula</p> \\[ \\lambda = \\frac{\\mathrm{Tr}(\\mathbf{M}) \\pm \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4\\mathrm{Det}(\\mathbf{M})}}{2} \\] <p>Note that this shows that even a 2x2 matrix composed of all real numbers can have complex eigenvalues if the value within the square root is negative, \\(\\mathrm{Tr}(\\mathbf{M})^2 - 4\\mathrm{Det}(\\mathbf{M}) &lt; 0\\).</p> <p>Note</p> <p>A complex number has the form \\(A + B i\\), where \\(A\\) and \\(B\\) are real numbers and \\(i=\\sqrt{-1}\\). We call \\(A\\) the \"real part\" and \\(B\\) the \"imaginary part\". See Box 7.3 in the text for more fun facts.</p> <p>For example, \\(\\mathbf{M} = \\begin{pmatrix} 1 &amp; -1 \\\\ 1 &amp; 1 \\end{pmatrix}\\) has \\(\\mathrm{Tr}(\\mathbf{M}) = 2\\) and \\(\\mathrm{Det}(\\mathbf{M}) = 2\\), so that the eigenvalues are \\(\\lambda = 1 \\pm \\sqrt{-1} = 1 \\pm i\\).</p> <p>We will see in future lectures that complex eigenvalues indicate some cycling in the dynamics of our system.</p> <p>The \\(n=2\\) example shows another interesting fact. When \\(\\mathrm{Det}(\\mathbf{M})=0\\) one of the eigenvalues is 0 (and this holds for larger \\(n\\) too). As we discussed in Lecture 11, if \\(\\mathrm{Det}(\\mathbf{M})=0\\) then multiplying a vector by \\(\\mathbf{M}\\) causes a loss of information (like multiplying by 0 in normal algebra). This can now be understood based on the geometric argument presented above: if one of the eigenvalues is zero then the state of the system immediately goes to zero in new coordinate system defined by the right eigenvectors.  </p> <p>Finding the determinant of \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) becomes trickier for larger matrices, but there are some helpful properties of determinants that come in handy (see Lecture 11).</p> <p>For instance, the eigenvalues of a diagonal or triangular matrix are simply the diagonal elements</p> \\[ \\begin{aligned} |\\mathbf{M} - \\lambda \\mathbf{I}| &amp;= \\begin{vmatrix} m_{11} - \\lambda &amp; 0 &amp; 0 \\\\ m_{21} &amp; m_{22} - \\lambda &amp; 0 \\\\ m_{31} &amp; m_{32} &amp; m_{33} - \\lambda \\end{vmatrix}\\\\ &amp;= (m_{11} - \\lambda) (m_{22} - \\lambda) (m_{33} - \\lambda) \\end{aligned} \\] <p>Similarly, the eigenvalues of a block-diagonal or block-triangular matrix are the eigenvalues of the submatrices along the diagonal</p> \\[ \\begin{aligned} \\mathbf{M} - \\lambda \\mathbf{I} &amp;= \\begin{pmatrix} \\begin{pmatrix} m_{11} - \\lambda &amp; 0 \\\\ m_{21} &amp; m_{22} - \\lambda \\end{pmatrix} &amp; \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\\\ \\begin{pmatrix} m_{31} &amp; m_{32} \\end{pmatrix} &amp; \\begin{pmatrix} m_{33} - \\lambda \\end{pmatrix} \\end{pmatrix}\\\\ |\\mathbf{M} - \\lambda \\mathbf{I}| &amp;= \\begin{vmatrix} m_{11} - \\lambda &amp; 0 \\\\ m_{21} &amp; m_{22} - \\lambda \\end{vmatrix} \\begin{vmatrix} m_{33} - \\lambda \\end{vmatrix}\\\\ &amp;= (m_{11} - \\lambda) (m_{22} - \\lambda) (m_{33} - \\lambda) \\end{aligned} \\] <p></p>"},{"location":"lectures/lecture-12/#4-finding-eigenvectors","title":"4. Finding eigenvectors","text":"<p>Now that we can find an eigenvalue, how do we find its eigenvectors?</p> <p>As we said above, if \\(\\vec{v}\\) is a right eigenvector of the matrix \\(\\mathbf{M}\\) corresponding to the eigenvalue \\(\\lambda\\), it satisfies</p> \\[ \\mathbf{M}\\vec{v} = \\lambda \\vec{v} \\] <p>We would like to use linear algebra to solve for \\(\\vec{v}\\) from \\(\\mathbf{M}\\vec{v} = \\lambda \\vec{v}\\), as we attempted above, but we can't since \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) is non-invertible.</p> <p>Instead we need to write out the system of equations represented by \\(\\mathbf{M}\\vec{v} = \\lambda \\vec{v}\\) and solve for one variable after another.</p> <p>For example, for a \\(2 \\times 2\\) matrix \\(\\mathbf{M}\\) with eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\) we know that a right eigenvector \\(\\vec{v}_1\\) associated with \\(\\lambda_1\\) must solve</p> \\[ \\begin{aligned} \\mathbf{M}\\vec{v}_1 &amp;= \\lambda_1 \\vec{v}_1\\\\ \\begin{pmatrix}   m_{11} &amp; m_{12} \\\\   m_{21} &amp; m_{22} \\end{pmatrix} \\begin{pmatrix}   v_1 \\\\   v_2 \\end{pmatrix} &amp;= \\lambda_1 \\begin{pmatrix}   v_1 \\\\   v_2 \\end{pmatrix} \\end{aligned} \\] <p>Carrying out the matrix multiplication, we can write down a system of equations corresponding the the rows</p> \\[ \\begin{aligned} m_{11} v_1 + m_{12} v_2 &amp;= \\lambda_1 v_1 \\\\ m_{21} v_1 + m_{22} v_2 &amp;= \\lambda_1 v_2 \\end{aligned} \\] <p>This system of equations determines the elements of the right eigenvector, \\(\\vec{v}_1\\), associated with \\(\\lambda_1\\).</p> <p>Note from the matrix form above that we can multiply \\(\\vec{v}_1 = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}\\) by any constant and that will also be a solution. This means there are an infinite number of eigenvectors associated with an eigenvalue and we can set one of the elements to an arbitrary value. A typical choice is to set the first entry equal to one, \\(v_1 = 1\\).</p> <p>Now we have just one unknown, \\(v_2\\), so we can choose either of the equations above to solve for \\(v_2\\). We pick the first, giving</p> \\[ \\begin{aligned} m_{11} v_1 + m_{12} v_2 &amp;= \\lambda_1 v_1 \\\\ m_{11} 1 + m_{12} v_2 &amp;= \\lambda_1 1 \\\\ v_2 &amp;= (\\lambda_1 - m_{11}) / m_{12} \\end{aligned} \\] <p>We therefore have right eigenvector \\(\\vec{v}_1 =  \\begin{pmatrix} 1 \\\\ (\\lambda_1 - m_{11})/m_{12} \\end{pmatrix}\\) associated with the eigenvalue \\(\\lambda_1\\). </p> <p>Because we've done this quite generally, we also now know that the right eigenvector associated with the second eigenvalue, \\(\\lambda_2\\), is \\(\\vec{v}_2 = \\begin{pmatrix} 1 \\\\ (\\lambda_2 - m_{11})/m_{12} \\end{pmatrix}\\).</p> <p>Solving for the left eigenvectors is done following the same method. For eigenvalue \\(\\lambda_1\\) we want to find the vector \\(\\vec{u}_1\\) that solves</p> \\[ \\begin{aligned} \\vec{u}_1\\mathbf{M} &amp;= \\lambda_1 \\vec{u}_1\\\\ \\begin{pmatrix}   u_1 &amp; u_2 \\end{pmatrix} \\begin{pmatrix}   m_{11} &amp; m_{12} \\\\   m_{21} &amp; m_{22} \\end{pmatrix}  &amp;= \\lambda_1 \\begin{pmatrix}   u_1 &amp; u_2 \\end{pmatrix} \\end{aligned} \\] <p>The system of equations is</p> \\[ \\begin{aligned} u_1 m_{11} + u_2 m_{21} &amp;= \\lambda_1 u_1 \\\\ u_1 m_{12} + u_2 m_{22} &amp;= \\lambda_1 u_2 \\end{aligned} \\] <p>Once again we can set the first element to any value, say \\(u_1 = 1\\), and use one of the equations to solve for the second element</p> \\[ \\begin{aligned} 1 m_{11} + u_2 m_{21} &amp;= \\lambda_1 1 \\\\ u_2 m_{21} &amp;= \\lambda_1 - m_{11} \\\\ u_2 &amp;= (\\lambda_1 - m_{11})/m_{21} \\\\ \\end{aligned} \\] <p>So the left eigenvector associated with eigenvalue \\(\\lambda_1\\) is \\(\\vec{u}_1 = \\begin{pmatrix} 1 &amp; (\\lambda_1 - m_{11})/m_{21} \\end{pmatrix}\\).</p> <p>Again, because we've done this quite generally, we know that the left eigenvector associated with \\(\\lambda_2\\) is \\(\\vec{u}_2 = \\begin{pmatrix} 1 &amp; (\\lambda_2 - m_{11})/m_{21} \\end{pmatrix}\\).</p> <p>If you'd like practice finding eigenvalues and eigenvectors, try finding the eigenvalues and eigenvectors of the matrix given in section 2, \\(\\mathbf{M} = \\begin{pmatrix} 1 &amp; 1 \\\\ 1/2 &amp; 3/2 \\end{pmatrix}\\).</p>"},{"location":"lectures/lecture-13/","title":"Lecture 13","text":""},{"location":"lectures/lecture-13/#lecture-13-general-solutions-linear-multivariate","title":"Lecture 13: General solutions (linear multivariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-13/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Discrete time</li> <li>Continuous time</li> </ol>"},{"location":"lectures/lecture-13/#1-discrete-time","title":"1. Discrete time","text":""},{"location":"lectures/lecture-13/#motivating-example","title":"Motivating example","text":"<p>Let's return to our model of the number of birds on two islands from Lecture 10, which was the motivation for learning linear algebra in the first place. Let's adjust the model slightly, removing foreign immigration and switching to discrete time (assuming migration, then birth, then death). </p> <pre><code>graph LR;\n    A1((n1)) --b1 n1--&gt; A1;\n    A1 --d1 n1--&gt; C1[ ];\n\n    A2((n2)) --b2 n2--&gt; A2;\n    A2 --d2 n2--&gt; C2[ ];\n\n    A1 --m12 n1--&gt; A2;\n    A2 --m21 n2--&gt; A1;\n\n    style C1 height:0px;\n    style C2 height:0px;</code></pre> <p>The number of birds on the two islands in the next time step are then</p> \\[ \\begin{aligned} n_1(t+1) &amp;= (n_1(t)(1-m_{12}) + n_2(t)m_{21})(1+b_1)(1-d_1) \\\\ n_2(t+1) &amp;= (n_1(t)m_{12} + n_2(t)(1-m_{21}))(1+b_2)(1-d_2) \\\\ \\end{aligned} \\] <p>As we noted in Lecture 10, we can write a system of linear equations (in this case recursion equations) in matrix form</p> \\[ \\begin{aligned} \\vec{n}(t+1) &amp;= \\mathbf{M}\\vec{n}(t) \\end{aligned} \\] <p>where in this example</p> \\[ \\mathbf{M} =  \\begin{pmatrix}  (1-m_{12})(1+b_1)(1-d_1) &amp; m_{21}(1+b_1)(1-d_1) \\\\  m_{12}(1+b_2)(1-d_2) &amp; (1-m_{21})(1+b_2)(1-d_2) \\end{pmatrix} \\] <p>The question we now want to answer is, how do the numbers of birds on the two islands change over time?</p>"},{"location":"lectures/lecture-13/#general-formulation","title":"General formulation","text":"<p>Instead of analyzing this specific model, let's investigate the dynamics of any system of linear equations in discrete time. We will then return to our motivating example.</p> <p>If there are \\(n\\) variables to keep track of, \\(x_1\\), \\(x_2\\), ..., \\(x_n\\), then there will be \\(n\\) recursion equations</p> \\[ \\begin{aligned} x_1(t+1) &amp;= m_{11} x_1(t) + m_{12} x_2(t) + \\cdots + m_{1n} x_n(t) \\\\ x_2(t+1) &amp;= m_{21} x_1(t) + m_{22} x_2(t) + \\cdots + m_{2n} x_n(t) \\\\    &amp;\\vdots \\\\ x_n(t+1) &amp;= m_{n1} x_1(t) + m_{n2} x_2(t) + \\cdots + m_{nn} x_n(t) \\end{aligned} \\] <p>e.g., in the motivating example above we have \\(n=2\\).</p> <p>These equations can be written in matrix form</p> \\[ \\begin{aligned} \\begin{pmatrix} x_1(t+1) \\\\ x_2(t+1) \\\\ \\vdots \\\\ x_n(t+1) \\end{pmatrix} &amp;=  \\begin{pmatrix}          m_{11} &amp; m_{12} &amp; \\cdots &amp; m_{1n} \\\\           m_{21} &amp; m_{22} &amp; \\cdots &amp; m_{2n}\\\\          \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\          m_{n1} &amp; m_{n2} &amp; \\cdots &amp; m_{nn}  \\end{pmatrix} \\begin{pmatrix}           x_1(t) \\\\ x_2(t) \\\\ \\vdots \\\\ x_n(t)  \\end{pmatrix}\\\\ \\vec{x}(t+1) &amp;= \\mathbf{M} \\vec{x}(t) \\end{aligned} \\] <p>How does \\(\\vec{x}\\) change over time?</p>"},{"location":"lectures/lecture-13/#general-solution","title":"General solution","text":"<p>Since this is just a multivariate version of exponential growth, we can derive the general solution by brute force iteration</p> \\[ \\begin{aligned}  \\vec{x}(t) &amp;= \\mathbf{M}\\vec{x}(t-1)\\\\  &amp;= \\mathbf{M}^2\\vec{x}(t-2)\\\\ &amp; \\vdots \\\\ &amp;= \\mathbf{M}^t\\vec{x}(0)  \\end{aligned} \\] <p>However, in most cases it will be hard to compute \\(\\mathbf{M}^t\\) (if you don't believe me, try calculating even just \\(\\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}^3\\)). Fortunately there is a trick involving eigenvalues and eigenvectors.</p> <p>Recall the equation for the eigenvalues, \\(\\lambda\\), and right eigenvectors, \\(\\vec{v}\\),</p> \\[ \\mathbf{M} \\vec{v} = \\lambda \\vec{v} \\] <p>For our \\(n\\)-dimensional model, there will often be \\(n\\) eigenvalues and eigenvectors that satisfy this equation (sometimes the \\(n\\) eigenvalues are not all distinct).</p> <p>We can actually write all \\(n\\) of these equations in matrix form</p> \\[ \\begin{aligned} \\mathbf{M} \\begin{pmatrix} \\vec{v}_1 &amp; \\vec{v}_2 &amp; \\cdots &amp; \\vec{v}_n \\end{pmatrix}  &amp;= \\begin{pmatrix} \\lambda_1 \\vec{v}_1 &amp; \\lambda_2 \\vec{v}_2 &amp; \\cdots &amp; \\lambda_n \\vec{v}_n \\end{pmatrix}\\\\ \\mathbf{M} \\mathbf{A} &amp;= \\mathbf{A} \\mathbf{D} \\end{aligned} \\] <p>where the columns of \\(\\mathbf{A}\\) are the right eigenvectors and \\(\\mathbf{D}\\) is a diagonal matrix of the eigenvalues</p> \\[ \\mathbf{D} =  \\begin{pmatrix}  \\lambda_1 &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; \\lambda_2 &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\  0 &amp; 0 &amp; \\cdots &amp; \\lambda_n\\\\   \\end{pmatrix} \\] <p>Now here is the trick: multiply both sides of \\(\\mathbf{M} \\mathbf{A} = \\mathbf{A} \\mathbf{D}\\) by \\(\\mathbf{A}^{-1}\\) on the right</p> \\[ \\begin{aligned} \\mathbf{M} \\mathbf{A} &amp;= \\mathbf{A} \\mathbf{D}\\\\ \\mathbf{M} \\mathbf{A}\\mathbf{A}^{-1} &amp;= \\mathbf{A} \\mathbf{D}\\mathbf{A}^{-1}\\\\   \\mathbf{M} &amp;= \\mathbf{A} \\mathbf{D} \\mathbf{A}^{-1} \\end{aligned} \\] <p>Note</p> <p>We can only take the inverse of our \\(n\\times n\\) matrix of right eigenvectors, \\(\\mathbf{A}\\), when \\(\\mathbf{A}\\) is invertible, which requires the determinant be non-zero or, equivalently, the rows (or columns) of \\(\\mathbf{A}\\) to be linearly independent. Fortunately, in many cases the columns (the right eigenvectors) will be linearly independent. However, sometimes the right eigenvectors will not be linearly independent, which only occurs when there are less than \\(n\\) distinct eigenvalues. When the right eigenvectors of \\(\\mathbf{M}\\) are not linearly independent we call \\(\\mathbf{M}\\) defective. In that case we need to derive the general solution in another way, but we won't deal with that in this class.</p> <p>Subbing this alternate version of \\(\\mathbf{M}\\) into our general solution above, we see that most of the \\(\\mathbf{A}\\) and \\(\\mathbf{A}^{-1}\\) matrices cancel, leaving us with</p> \\[ \\begin{aligned}  \\vec{x}(t) &amp;= \\mathbf{M}^t\\vec{x}(0)\\\\ &amp;= (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1})^t\\vec{x}(0)\\\\ &amp;= (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) \\cdots (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) \\vec{x}(0)\\\\ &amp;= \\mathbf{A}\\mathbf{D}(\\mathbf{A}^{-1} \\mathbf{A})\\mathbf{D}(\\mathbf{A}^{-1}\\mathbf{A}) \\cdots (\\mathbf{A}^{-1}\\mathbf{A})\\mathbf{D}\\mathbf{A}^{-1} \\vec{x}(0)\\\\  &amp;= \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1} \\vec{x}(0)  \\end{aligned} \\] <p>And this is great because \\(\\mathbf{D}\\) is a diagonal matrix, meaning \\(\\mathbf{D}^t\\) is easy to calculate</p> \\[ \\mathbf{D}^t =   \\begin{pmatrix}     \\lambda_1^t &amp; 0 &amp; \\cdots &amp; 0 \\\\      0 &amp; \\lambda_2^t &amp; \\cdots &amp; 0\\\\     \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\     0 &amp; 0 &amp; \\cdots &amp; \\lambda_n^t \\end{pmatrix} \\] <p>It would not have been so easy to find \\(\\mathbf{M}^t\\)!</p> <p>Note that \\(\\mathbf{A}^{-1}\\) is a matrix whose rows are the left eigenvectors of \\(\\textbf{M}\\),  which can be seen by deriving \\(\\mathbf{M} = \\mathbf{A} \\mathbf{D}\\mathbf{A}^{-1}\\) starting  with the equation for the left eigenvectors \\(\\vec{u}\\mathbf{M}=\\lambda\\vec{u}\\).</p> <p>Note</p> <p>Another way to arrive at this general solition, \\(\\vec{x}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{x}(0)\\), is to consider a transformation from our current coordinate system, \\(\\vec{x}\\), to another defined by \\(\\vec{y} = \\mathbf{A}^{-1}\\vec{x}\\). </p> <p>If we then attempt to derive a recursion in our new coordinate system we find</p> \\[ \\begin{aligned} \\vec{y}(t+1) &amp;= \\mathbf{A}^{-1}\\vec{x}(t+1)\\\\ &amp;= \\mathbf{A}^{-1}\\mathbf{M}\\vec{x}(t)\\\\ &amp;= \\mathbf{A}^{-1}\\mathbf{M}\\mathbf{A}\\vec{y}(t)\\\\ &amp;= \\mathbf{A}^{-1}(\\mathbf{A} \\mathbf{D} \\mathbf{A}^{-1})\\mathbf{A}\\vec{y}(t)\\\\ &amp;= \\mathbf{D}\\vec{y}(t) \\end{aligned} \\] <p>Voila! In the new coordinate system we just move along each axis by a factor \\(\\lambda_i\\), just as we saw in Lecture 12. In this way the eigenvectors form a new, more convenient, coordinate system.</p> <p>To convert back to our original coordinate system we multiply both sides of the equation by \\(\\mathbf{A}\\) on the left and then use \\(\\vec{y} = \\mathbf{A}^{-1}\\vec{x}\\)</p> \\[ \\begin{aligned} \\mathbf{A}\\vec{y}(t+1) &amp;= \\mathbf{A}\\mathbf{D}\\vec{y}(t)\\\\ \\vec{x}(t+1) &amp;= \\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}\\vec{x}(t)\\\\ \\end{aligned} \\]"},{"location":"lectures/lecture-13/#long-term-dynamics","title":"Long-term dynamics","text":"<p>A major implication from the general solution is that only if all the eigenvalues have an absolute value less than one, \\(|\\lambda_i|&lt;1\\), will all entries of \\(\\mathbf{D}^t\\) decay to zero and hence \\(\\vec{x}(t)\\) decay to \\(\\vec{0}\\) (the equilibrium) as time increases.</p> <p>Further, as time increases \\(\\mathbf{D}^t\\) will be dominated by the eigenvalue with the largest absolute value, which we will call the leading eigenvalue.</p> <p>Note</p> <p>To see that \\(\\mathbf{D}^t\\) will be dominated by the eigenvalue with the largest absolute value as time increases, let the leading eigenvalue be \\(\\lambda_1\\) and factor it out of \\(\\mathbf{D}^t\\)</p> \\[ \\mathbf{D}^t = \\lambda_1^t  \\begin{pmatrix}  1 &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; (\\lambda_2/\\lambda_1)^t &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\  0 &amp; 0 &amp; \\cdots &amp; (\\lambda_n/\\lambda_1)^t\\\\   \\end{pmatrix} \\] <p>Since \\(|\\lambda_i/\\lambda_1|&lt;1\\) for all \\(i\\), for large \\(t\\) these all go to zero and we have</p> \\[ \\mathbf{D}^t \\approx \\tilde{\\mathbf{D}}^t  \\equiv \\lambda_1^t  \\begin{pmatrix}  1 &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; 0 &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\  0 &amp; 0 &amp; \\cdots &amp; 0\\\\   \\end{pmatrix} \\] <p>We can therefore approximate \\(\\vec{x}(t)\\) after a sufficient amount of time as </p> \\[ \\begin{aligned} \\tilde{\\vec{x}}(t) &amp;= \\mathbf{A}\\tilde{\\mathbf{D}}^t\\mathbf{A}^{-1}\\vec{x}(0)\\\\ &amp;= \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{x}(0) \\end{aligned} \\] <p>where \\(\\vec{v}_1\\) and \\(\\vec{u}_1\\) are the right and left eigenvectors associated with the leading eigenvalue, \\(\\lambda_1\\).</p> <p>Warning</p> <p>Finding the left eigenvectors via the inverse of \\(\\mathbf{A}\\) guarantees that the eigenvectors have been scaled such that \\(\\vec{u}_1\\vec{v}_1 = 1\\). If the eigenvectors have been derived in another way, make sure you scale them so that this is true, eg, make the left eigenvalue equal to \\(\\vec{u}_1/(\\vec{u}_1 \\vec{v}_1)\\). Otherwise the long-term approximation will be off by a constant factor.</p> <p>This means that, in the long-term,</p> <ul> <li>\\(\\vec{x}(t)\\) will grow like \\(\\lambda_1^t\\), where \\(\\lambda_1\\) is the leading eigenvalue</li> <li>each variable in \\(\\vec{x}(t)\\) will oscillate around the equilibrium if \\(\\lambda_1&lt;0\\)</li> <li>\\(\\vec{x}(t)\\) will approach the right eigenvector associated with the leading eigenvalue, \\(\\vec{v}_1\\)</li> <li>\\(\\vec{x}(t)\\) is weighted by a constant, \\(\\vec{u}_1 \\vec{x}(0)\\), describing the \"initial size\" of the system</li> </ul>"},{"location":"lectures/lecture-13/#complex-eigenvalues","title":"Complex eigenvalues","text":"<p>The final complication to discuss is complex eigenvalues. Specifically, what is \\(\\lambda^t\\) when \\(\\lambda = A + B i\\)?</p> <p>To do this, we can first use some simple geometry on the complex plane (a two-dimensional space with the real part, \\(A\\), on the x-axis and the imaginary part, \\(B\\), on the y-axis) to show that any complex number can be written</p> \\[ A + Bi = R (\\cos(\\theta) + i \\sin(\\theta)) \\] <p>where \\(R = \\sqrt{A^2 + B^2}\\) is the absolute value of \\(\\lambda\\) and \\(\\theta = \\arctan(B/A)\\) is the angle between \\(\\lambda\\) and the x-axis.</p> <pre>\nimport matplotlib.pyplot as plt\nimport math\n\nA,B = 1,1 #real and imaginary parts\n\nfig, ax = plt.subplots()\n\nax.arrow(0,0,A,B, head_width=0.05, color='black', length_includes_head=True) #eigenvalue as vector in complex plane\n\ndx = 0.05\nax.plot([0-dx/2,A-dx/2],[0+dx,B+dx],marker='o',c='b')\nax.text(A/2,B/2+3*dx,r'$R$',rotation=math.atan(B/A)*180/math.pi,c='b',fontsize=15,ha='center',va='center')\n\nax.plot([0,A],[B,B],marker='o',c='r')\nax.text(A/2,B+dx,r'$A=R \\cos(\\theta)$',c='r',fontsize=15,ha='center',va='center')\n\nax.plot([A,A],[0,B],marker='o',c='g')\nax.text(A+dx,B/2,r'$B=R \\sin(\\theta)$',c='g',fontsize=15,ha='center',va='center',rotation=90)\n\nax.set_xlabel('real part, $A$')\nax.set_ylabel('imaginary part, $B$')\nax.set_xlim(-dx,A+2*dx)\nax.set_ylim(-dx,B+2*dx)\n\ndx=A/4\nax.plot([0,dx],[0,0],c='orange')\nax.add_patch(Arc((0,0), width=2*dx, height=2*dx, theta1=0, theta2=math.atan(B/A)*180/math.pi, edgecolor='orange'))\nax.text(dx/2,dx/6,r'$\\theta$',fontsize=15,c='orange')\n\nplt.show()\n</pre> <p></p> <p>We then use Euler's equation, \\(\\cos(\\theta) + i \\sin(\\theta) = e^{i \\theta}\\), to write </p> \\[ A + Bi = R e^{i \\theta} \\] <p>And we can now take powers of \\(\\lambda\\) </p> \\[ \\lambda^t = R^t e^{i \\theta t} \\]"},{"location":"lectures/lecture-13/#summary","title":"Summary","text":"<p>To summarize, for any system of linear recursion equations, \\(\\vec{x}(t+1)\\), we can </p> <ul> <li>write the general solution in terms of the eigenvalues and eigenvectors, \\(\\vec{x}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{x}(0)\\)</li> <li>define the leading eigenvalue as the one with the largest absolute value</li> <li>say that the equilibrium is stable if the leading eigenvalue is less than 1</li> <li>approximate the long-term dynamics in terms of the leading eigenvalue and its associated eigenvectors, \\(\\vec{x}(t) = \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{x}(0)\\)</li> </ul>"},{"location":"lectures/lecture-13/#motivating-example-revisited","title":"Motivating example revisited","text":"<p>Now let's return to our motivating example of birds on islands. And let's imagine we have good estimates of the parameter values (after years of tough fieldwork!): \\(m_{12}=m_{21}=0.1\\), \\(b_1=b_2=0.2\\), \\(d_1=0.1\\), \\(d_2=0.2\\). To derive the general solution, giving the number of birds on the two islands in year \\(t\\), we first derive the eigenvalues and eigenvectors of \\(\\mathbf{M}\\). Using the techniques in Lecture 12 we find that the eigenvalues are \\(\\lambda_1\\approx1.03\\) and \\(\\lambda_2\\approx0.8\\). The associated right eigenvectors are \\(\\vec{v}_1\\approx\\begin{pmatrix} 1 \\\\ 0.57 \\end{pmatrix}\\) and \\(\\vec{v}_2\\approx\\begin{pmatrix} 1 \\\\ -1.57 \\end{pmatrix}\\). We therefore have </p> \\[ \\mathbf{D} = \\begin{pmatrix} 1.03 &amp; 0 \\\\ 0 &amp; 0.8 \\end{pmatrix} \\] <p>and </p> \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 1 \\\\ 0.57 &amp; -1.57 \\end{pmatrix} \\] <p>This year's census of the islands tells us that there are currently 100 birds on island 1 and 50 on island 2. Taking this as the starting point, \\(\\vec{n}(0) = \\begin{pmatrix} 100 \\\\ 50 \\end{pmatrix}\\), we can use our general solution to predict the number of birds on the two islands over time. Below we plot the predicted number of birds on the two islands over the next 100 years.</p> <pre>\nm12, m21, b1, b2, d1, d2 = 0.1, 0.1, 0.2, 0.2, 0.1, 0.2 #parameter values\n\n# general solution\nfrom sympy import *\nM = Matrix([[(1-m12)*(1+b1)*(1-d1), m21*(1+b1)*(1-d1)], #matrix\n            [m12*(1+b2)*(1-d2), (1-m21)*(1+b2)*(1-d2)]])\nA, D = M.diagonalize() #quick way to get matrix of right eigenvectors (A) and eigenvalues (D)\nn0 = Matrix([100,50]) #note this is made into a column vector automatically\nnt = A*D**t*A.inv()*n0 #general solution\n\n# plot\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nfor j in range(2): #for each island\n    ax.plot([nt.subs(t,i)[j] for i in range(100)], label='island %d'%(j+1), marker=\".\")\nax.legend()\nax.set_xlabel('years from now')\nax.set_ylabel('number of birds')\nplt.show()\n</pre> <p></p> <p>We see that the population grows, which we should have expected given that the leading eigenvalue, \\(\\lambda_1\\approx1.03\\), has an absolute value greater than 1. </p> <p>We also see that there are about 0.57 birds on island 2 for every 1 bird on island 1, as predicted by the right eigenvector associated with the leading eigenvector, \\(\\vec{v}_1 \\approx \\begin{pmatrix} 1 \\\\ 0.57 \\end{pmatrix}\\).</p> <p>More generally, our long-term prediction is \\(\\vec{n}(t) \\approx \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0)\\). Overlaying this approximation as black curves on the above plot shows that this works very well.</p> <pre>\n# long-term approximation\nu1 = Matrix(1,2,A.inv()[0:2]) #left eigenvector is first row in A inverse\nntapp = l1**t * v1[0] * u1 * n0\n\n# plot\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nfor j in range(2): #for each island\n    ax.plot([nt.subs(t,i)[j] for i in range(100)], label='island %d'%(j+1), marker=\".\") #general solution\n    ax.plot([ntapp.subs(t,i)[j] for i in range(100)], c='k') #long-term approx\nax.legend()\nax.set_xlabel('years from now')\nax.set_ylabel('number of birds')\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-13/#2-continuous-time","title":"2. Continuous time","text":"<p>Now let's consider a system of linear equations in continuous time, which we can write in matrix form as</p> \\[ \\frac{\\mathrm{d}\\vec{x}}{\\mathrm{d}t} = \\mathbf{M} \\vec{x} \\]"},{"location":"lectures/lecture-13/#general-solution_1","title":"General solution","text":"<p>Just as in the univariate case of exponential growth, the general solution is simply </p> \\[ \\vec{x}(t) = e^{\\mathbf{M}t}\\vec{x}(0) \\] <p>But now we have \\(e\\) to the power of a matrix, and \\(e^{\\mathbf{M}t} = \\sum_{i=0}^{\\infty} \\mathbf{M}^i t^i / i!\\) is complicated!</p> <p>Fortunately we can use the same transform as in the discrete time case, \\(\\vec{y}=\\mathbf{A}^{-1}\\vec{x}\\), to write the general solution as </p> \\[ \\vec{x}(t) = \\mathbf{A} e^{\\mathbf{D}t} \\mathbf{A}^{-1}\\vec{x}(0) \\] <p>This is much simpler because \\(e^{\\mathbf{D}t}\\) is just</p> \\[ e^{\\mathbf{D}t} = \\begin{pmatrix} e^{\\lambda_1 t} &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; e^{\\lambda_2 t} &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ 0 &amp; \\cdots &amp; 0 &amp; e^{\\lambda_n t} \\end{pmatrix} \\]"},{"location":"lectures/lecture-13/#long-term-dynamics_1","title":"Long-term dynamics","text":"<p>From this general solution we see that the system will approach equilibrium, \\(\\hat{\\vec{x}} = \\vec{0}\\), only if all the entries of \\(e^{\\mathbf{D}t}\\) approach zero as time increases, which requires that all the eigenvalues are negative.</p> <p>Taking \\(\\lambda_1\\) to be the eigenvalue with the largest value, which we will call the leading eigenvalue, we also see that after sufficient time \\(e^{\\mathbf{D}t}\\) becomes dominated by this entry</p> \\[ e^{\\mathbf{D}t} \\approx  \\begin{pmatrix} e^{\\lambda_1 t} &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; 0 &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ 0 &amp; \\cdots &amp; 0 &amp; 0 \\end{pmatrix} \\] <p>implying that the long-term dynamics can be approximated by </p> \\[ \\vec{x}(t) \\approx e^{\\lambda_1 t} \\vec{v}_1 \\vec{u}_1 \\vec{x}(0) \\] <p>where \\(\\vec{v}_1\\) and \\(\\vec{u}_1\\) are the right and left eigenvectors associated with the leading eigenvalue, \\(\\lambda_1\\).</p>"},{"location":"lectures/lecture-13/#complex-eigenvalues_1","title":"Complex eigenvalues","text":"<p>When we have complex eigenvalues we can again use Euler's equation to write</p> \\[ \\begin{aligned} e^{\\lambda t} &amp;= e^{(A + Bi) t}\\\\ &amp;= e^{At}e^{Bti}\\\\ &amp;= e^{At}(\\cos(Bt) + i\\sin(Bt)) \\end{aligned} \\] <p>Because both \\(\\cos(Bt)\\) and \\(\\sin(Bt)\\) are bounded (between -1 and 1), whether \\(e^{\\lambda t}\\) grows or shrinks in long term depends only on the real part, \\(A\\). So, in contrast to discrete time, when determining the leading eigenvalue in continuous time we only have to consider the real parts of the eigenvalues.</p>"},{"location":"lectures/lecture-13/#summary_1","title":"Summary","text":"<p>In summary, for any system of linear differential equations, \\(\\frac{\\mathrm{d}\\vec{x}}{\\mathrm{d}t}\\), we can </p> <ul> <li>write the general solution in terms of the eigenvalues and eigenvectors, \\(\\vec{x}(t) = \\mathbf{A} e^{\\mathbf{D}t} \\mathbf{A}^{-1}\\vec{x}(0)\\)</li> <li>define the leading eigenvalue as the eigenvalue with the largest real part</li> <li>say that the equilibrium is stable if the real part of the leading eigenvalue is negative</li> <li>approximate the long-term dynamics in terms of the leading eigenvalue and its associated eigenvectors, \\(\\vec{x}(t) \\approx e^{\\lambda_1 t} \\vec{v}_1 \\vec{u}_1 \\vec{x}(0)\\)</li> </ul>"},{"location":"lectures/lecture-13/#example","title":"Example","text":"<p>Let's consider a different example here in continuous time. Instead of thinking about birds on islands, let's think about a model of sexual selection. We will model the mean value of a male trait, \\(\\bar{z}\\), such as the length of a birds tail, and the mean value of female preference for that trait, \\(\\bar{p}\\) (if \\(\\bar{p}&gt;0\\) females tend to prefer larger male traits, if \\(\\bar{p}&lt;0\\) females tend to prefer smaller male traits). We assume the optimal male trait value in the absence of sexual selection is \\(\\theta\\), i.e., natural selection always pushes \\(\\bar{z}\\) towards \\(\\theta\\) (we'll take \\(\\theta=0\\), meaning \\(\\bar{z}\\) is measured relative to the optimum). We assume female choice is costly, i.e., natural selection always pushes \\(\\bar{p}\\) towards 0. Finally we will assume that male traits and female preference share some genetic basis, meaning that they will covary (e.g., there may be some alleles that increase the trait value when in males and increase the preference when in females, causing positive covariance). This covariance means that a change in male trait will cause a change in female preference, and vice-versa.</p> <p>We can describe the dynamics of \\(\\bar{z}\\) and \\(\\bar{p}\\) with a system of linear differential equations</p> \\[ \\begin{align} \\frac{\\mathrm{d}\\bar{z}}{\\mathrm{d}t} = G_z (a \\bar{p} - c \\bar{z}) - B b \\bar{p}\\\\ \\frac{\\mathrm{d}\\bar{p}}{\\mathrm{d}t} = B (a \\bar{p} - c \\bar{z}) - G_p b \\bar{p} \\\\ \\end{align} \\] <p>where \\(G_z\\) and \\(G_p\\) are the amounts of genetic variation in male traits and female preference (this is the \"fuel\" of evolution, so the rates of evolution are proportional to these variances), \\(B\\) is the covariance between male traits and female preference, \\(a\\) is the strength of sexual selection, and \\(c\\) and \\(b\\) are the strengths of natural selection on male traits and female preference.</p> <p>Choosing some parameter values and plotting the general solution, we see the mean male trait and mean female preference cycle over time, decaying towards zero.</p> <pre>\nGz, Gp, B, a, b, c, z0, p0, tmax = 0.15, 0.8, 0.32, 0.95, 0.3, 0.45, 1, 0, 1000 #parameter values\n\n# general solution\nfrom sympy import *\nM = Matrix([[-Gz*c, Gz*a-b*B],\n            [-B*c, -Gp*b+a*B]])\nA, D = M.diagonalize() #quick way to get matrix of right eigenvectors (A) and eigenvalues (D)\nn0 = Matrix([z0,p0]) #note this is made into a column vector automatically\nnt = A*exp(D*t)*A.inv()*n0 #general solution\n\n# plot\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.plot([re(nt.subs(t,i)[0]) for i in range(tmax)], label='male trait') #need to remove the imaginary part because of numerical error\nax.plot([re(nt.subs(t,i)[1]) for i in range(tmax)], label='female preference')\nax.legend()\nax.set_xlabel('time')\nax.set_ylabel('value')\nplt.show()\n</pre> <p></p> <p>This cycling occurs because initially the mean male trait is positive but there is no mean female preference. This implies that both natural and sexual selection favour smaller male traits, causing the mean to decline. But because of a correlated response, female preference also declines, favouring male traits less than 0. Eventually female preference becomes too costly and begins to increase back toward zero. This causes a correlated increase in the male trait, and so on. </p> <p>With these parameter values the eigenvalues are \\(\\lambda \\approx -0.002 \\pm 0.05 i\\).</p> <p>We could therefore have predicted this cycling based on these eigenvalues, as they are complex.</p> <p>We could also have predicted the eventual decay to zero (the equilibrium), as the real parts of both eigenvalues are negative.</p> <p>In this case the two eigenvalues have the same real part and therefore there is no one leading eigenvalue, meaning that we cannot use our long-term approximation, \\(\\vec{x}(t) \\approx e^{\\lambda_1 t} \\vec{v}_1 \\vec{u}_1 \\vec{x}(0)\\).</p>"},{"location":"lectures/lecture-14/","title":"Lecture 14","text":""},{"location":"lectures/lecture-14/#lecture-14-demography","title":"Lecture 14: Demography","text":"Run notes interactively?"},{"location":"lectures/lecture-14/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Demography</li> <li>Stage-structure</li> <li>Age-structure</li> </ol>"},{"location":"lectures/lecture-14/#1-demography","title":"1. Demography","text":"<p>We're now going to use what we've learned about linear multivariate models to describe the dynamics of a single population that is composed of different types of individuals. This area of research is called demography. We'll just consider discrete-time models here.</p> <p>In the last lecture we saw that for any discrete-time linear multivariate model</p> \\[ \\vec{n}(t+1) = \\mathbf{M}\\vec{n}(t) \\] <p>we can compute the general solution</p> \\[ \\vec{n}(t) = \\mathbf{M}^t\\vec{n}(0) \\] <p>or, more conveniently, in terms of the eigenvalues (\\(\\mathbf{D}\\)) and eigenvectors (\\(\\mathbf{A}\\))</p> \\[ \\vec{n}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{n}(0 ) \\] <p>Despite this progress, the eigenvalues and eigenvectors are often unobtainable (without specifying parameter values), leaving us to rely on the long-term approximation</p> \\[ \\vec{n}(t) = \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0) \\] <p>For this we just need to know the leading eigenvalue (\\(\\lambda_1\\)) and the corresponding right (\\(\\vec{v}_1\\)) and left (\\(\\vec{u}_1\\)) eigenvectors, respectively. (Remember that we have scaled the eigenvectors such that \\(\\vec{u}_1\\vec{v}_1=1\\).)</p> <p>These three components (\\(\\lambda_1\\), \\(\\vec{v}_1\\), \\(\\vec{u}_1\\)) are the key demographic quantities that we will investigate</p> <ul> <li>\\(\\lambda_1\\) is the long-term population growth rate</li> <li>\\(\\vec{v}_1\\) describes the stable stage-distribution</li> <li>\\(\\vec{u}_1\\) describes the relative reproductive values of each stage</li> </ul> <p>Note</p> <p>The long-term approximation, \\(\\vec{n}(t) = \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0)\\), is valid as long as the leading eigenvalue, \\(\\lambda_1\\), is</p> <ul> <li>real (no cycles in long-term)</li> <li>positive (no oscillations to negative numbers!)</li> <li>larger than all other eigenvalues (so that we can ignore the other eigenvalues/vectors)</li> </ul> <p>Fortunately we are guaranteed all these conditions in our demographic models since all entries of \\(\\mathbf{M}\\) are non-negative and all entries of \\(\\mathbf{M}^t\\) are positive for some value of \\(t\\). (This follows from something called the Perron-Frobenius Theorem.)</p> <p>The most general demographic model is called stage-structure: we consider some finite number of discrete stages that an individual can be in and we use an abitrary matrix of transition rates between stages (a projection matrix), \\(\\mathbf{M}\\), to project how the population size and composition changes over time.</p> <p>A common special case is age-structure: here we define the stages as the number of time steps an individual has been alive for, which leads to a simpler projection matrix (called a Leslie matrix) because individuals always transition to the next stage (or die).</p> <p></p>"},{"location":"lectures/lecture-14/#2-stage-structure","title":"2. Stage-structure","text":""},{"location":"lectures/lecture-14/#example-north-atlantic-right-whale","title":"Example: North Atlantic right whale","text":"<p>Let's consider an example (from 10.2 in the text). North Atlantic right whales were hunted to near extinction in the 1800s and early 1900s, after which their population size is thought to have slowly recovered (there are less than 400 now, and unfortunately appear to be in decline again). Because of their long life span, over which survival and reproductive rates vary enormously, a stage-structured model is very appropriate. Below we draw a flow diagram representing all the transitions between calves, sexually immature individuals, sexually mature individuals, and actively reproducing individuals.</p> <pre><code>graph LR;\n    C((Calves)) --sIC nC--&gt; I((Immature));\n\n    I --sII nI--&gt; I;\n    I --sMI nI--&gt; M((Mature));\n    I --sRI nI--&gt; R((Reproductive));\n\n    M --sMM nM--&gt; M;\n    M --sRM nM--&gt; R;\n\n    R --sRR nR--&gt; R;\n    R --sMR nR--&gt; M;\n    R --b nR--&gt; C;</code></pre> <p>Converting this flow diagram into a system of recursion equations, \\(\\vec{n}(t+1) = \\mathbf{M}\\vec{n}(t)\\), the projection matrix is</p> \\[ \\mathbf{M} = \\begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; b \\\\ s_{IC} &amp; s_{II} &amp; 0 &amp; 0\\\\ 0 &amp; s_{MI} &amp; s_{MM} &amp; s_{MR} \\\\ 0 &amp; s_{RI} &amp; s_{RM} &amp; s_{RR} \\end{pmatrix} \\] <p>If we now plug in some estimated parameter values from the literature (\\(b=0.3\\), \\(s_{IC}=0.92\\), \\(s_{II}=0.86\\), \\(s_{MI}=0.08\\), \\(s_{MM}=0.8\\), \\(s_{MR}=0.88\\), \\(s_{RI}=0.02\\), \\(s_{RM}=0.19\\), \\(s_{RR}=0\\)) we can numerically calculate the three key demographic quantities</p> \\[ \\begin{aligned} &amp;\\lambda_1 \\approx 1.003 \\\\ &amp;\\vec{v}_1 \\approx \\begin{pmatrix} 0.04 \\\\ 0.23 \\\\ 0.61 \\\\ 0.12\\end{pmatrix} \\\\ &amp;\\vec{u}_1 \\approx \\begin{pmatrix} 0.69 &amp; 0.76 &amp; 1.07 &amp; 1.15 \\end{pmatrix} \\end{aligned} \\] <p>These quantitives tells us, for example, that in the long-run the population is predicted to grow (\\(\\lambda_1&gt;1\\)), the majority of individuals are expected to be mature (the second last entry in \\(\\vec{v}_1\\) is the largest), and mature and reproductive individuals are expected to have much higher reproductive values than calves and immature individuals (the last two entries in \\(\\vec{u}_1\\) are much larger than the first two). </p> <p>Caveat</p> <p>The parameter values above were estimated before the current population decline -- changes in climate and human behaviour, eg boat traffic and fishing, have presumably altered the parameter values, making the predictions less accurate. For example, the increased incidence of entanglement in fishing nets has likely decreased survival rates to the point that the population is now expected to decline. See here for more info.</p> <p>We can now answer an important conservation question: </p> <p>Question</p> <p>If we wanted to increase the total population size in the future and we could add one individual to any stage, which stage should it be?</p> <p>We want to know what entry of \\(\\vec{n}(0)\\) to add 1 to to maximize \\(\\vec{n}(t) \\approx \\lambda^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0)\\). This will not affect the long-term growth rate (\\(\\lambda_1\\)) or the stable-stage distribution (\\(\\vec{v}_1\\)). We can therefore only increase \\(\\vec{u}_1\\vec{n}(0) = u_1 n_1(0) + u_2 n_2(0) + ... + u_m n_m(0)\\). And so we add 1 to the stage with the largest reproductive value, \\(u_i\\).</p> <p>So for these whales, where the reproductive values are \\(\\vec{u}_1 \\approx \\begin{pmatrix} 0.69 &amp; 0.76 &amp; 1.07 &amp; 1.15 \\end{pmatrix}\\), we should add a reproductively active individual. This is perhaps not surprising since those are the only individuals that produce offspring and if we introduced an individual in an earlier stage there is some chance that they would die before becoming reproductively active.</p> <p>One other question we might now consider is:</p> <p>Question</p> <p>If we wanted to increase the long-term population growth rate and we could increase any parameter a little bit, which parameter should it be?</p> <p>One way to answer this is to numerically calculate the long-term population growth rate, \\(\\lambda_1\\), for a range of values of one parameter, \\(z\\) (e.g., we might take \\(z=b\\), the fecundity of reproductive individuals).</p> <p>Or, we could try to analytically compute the rate of change in the leading eigenvalue with respect to \\(z\\), \\(\\mathrm{d}\\lambda_1/\\mathrm{d}z\\), while considering \\(\\mathbf{M}\\), \\(\\lambda_1\\), \\(\\vec{v}_1\\), and \\(\\vec{u}_1\\) to be functions of \\(z\\). </p> <p>Since \\(\\mathbf{M}\\vec{v}_1 = \\lambda_1 \\vec{v}_1\\) and \\(\\vec{u}_1\\mathbf{M} = \\vec{u}_1\\lambda_1\\) we have</p> \\[ \\begin{aligned} \\mathbf{M} \\vec{v}_1 &amp;= \\lambda_1 \\vec{v}_1 \\\\ \\vec{u}_1 \\mathbf{M} \\vec{v}_1 &amp;= \\vec{u}_1 \\lambda_1 \\vec{v}_1 \\\\ \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left(\\vec{u}_1 \\mathbf{M} \\vec{v}_1 \\right)&amp;= \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left(\\vec{u}_1 \\lambda_1 \\vec{v}_1 \\right)\\\\ \\frac{\\mathrm{d}\\vec{u}_1}{\\mathrm{d}z} \\mathbf{M} \\vec{v}_1 + \\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1 + \\vec{u}_1 \\mathbf{M} \\frac{\\mathrm{d}\\vec{v}_1}{\\mathrm{d}z} &amp;= \\frac{\\mathrm{d}\\vec{u}_1}{\\mathrm{d}z} \\lambda_1 \\vec{v}_1 + \\vec{u}_1 \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\vec{v}_1 + \\vec{u}_1 \\lambda_1 \\frac{\\mathrm{d}\\vec{v}_1}{\\mathrm{d}z}\\\\ \\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1 &amp;= \\vec{u}_1 \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\vec{v}_1 \\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} &amp;= \\frac{\\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1}{\\vec{u}_1 \\vec{v}_1} \\end{aligned}\\] <p>We call \\(\\mathrm{d}\\lambda_1/\\mathrm{d}z\\) the sensitivity of \\(\\lambda_1\\) to \\(z\\).</p> <p>This expression will be too complicated to understand in general in most cases, but we can evaluate at some particular value \\(z^*\\) (e.g., at the current estimate) to see how the growth rate changes as \\(z\\) increases from that value</p> \\[ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\bigg|_{z=z^*} = \\frac{\\vec{u}_1 \\left(\\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\big|_{z=z^*} \\right) \\vec{v}_1}{\\vec{u}_1 \\vec{v}_1} \\] <p>Returning to our question, we want to know which parameter, \\(z\\), gives the largest value of \\(\\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\big|_{z=z^*}\\).</p> <p>For these whales we can calculate \\(\\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\big|_{z=z^*}\\) for each parameter \\(z\\) (ie, for each entry in \\(\\mathbf{M}\\)) evaluated at it's current value \\(z^*\\). Doing that calculation we see that increasing the fraction of mature individuals that survive to become reproductively active, \\(s_{RM}\\), has the largest effect. This makes good sense because increasing \\(s_{RM}\\) increases the rate at which the most populous stage (from \\(\\vec{v}_1\\)) transitions to the stage with the highest reproductive value (from \\(\\vec{u}_1\\)).</p> <p>And finally, we can ask</p> <p>Question</p> <p>What are the predicted numbers of individuals in each stage over time?</p> <p>Here we simply plot the general solution (solid) and compare with the long-term approximation (dashed) out of interest. We see that the full solution very quickly converges to the long-term solution in this case. </p> <pre>\nb,sic,sii,smi,smm,smr,sri,srm,srr = 0.3,0.92,0.86,0.08,0.8,0.88,0.02,0.19,0 #parameter values\n\nimport numpy as np\n\nM = np.array([[0,0,0,b], #projection matrix\n              [sic,sii,0,0],\n              [0,smi,smm,smr],\n              [0,sri,srm,srr]])\n\n# calculate\neigs = np.linalg.eig(M) #eigenvalues and right eigenvectors\nix = np.argmax(np.abs(eigs[0])) #which is leading eigenvalue\nl1 = eigs[0][ix] #leading eigenvalue\nv1 = eigs[1][:,ix] #leading right eigenvector\nv1 = v1/np.sum(v1) #normalized to sum to 1\nus = np.linalg.inv(eigs[1]) #left eigenvectors\nu1 = us[ix] #leading left eigenvalue\nu1 = u1/np.dot(u1,v1) #normalized so u1*v1=1\n\nn0 = np.array([50,50,50,50]) #initial state\n\ndef ntfull(t):\n    '''full projection'''\n    return np.dot(np.linalg.matrix_power(M,t), n0)\n\ndef ntapp(t):\n    '''long term approximation'''\n    return l1**t * v1 * np.dot(u1, n0)\n\n# plot\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nlabels = ['calves','immature','mature','reproductive']\ncolors = plt.get_cmap('tab10')\nfor i,j in enumerate(range(len(M))): #for each stage\n    ax.plot([ntfull(t)[j] for t in range(100)], color=colors(i), label=labels[i])\n    ax.plot([ntapp(t)[j] for t in range(100)], color=colors(i), linestyle='--') #long-term approx\n\nax.legend()\nax.set_xlabel('years')\nax.set_ylabel('number of whales')\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-14/#3-age-structure","title":"3. Age-structure","text":"<p>Now let's look at the special case of age-structure.</p> <p>Here individuals in stage \\(i\\) at time \\(t\\) can only contribute to stage \\(i+1\\) (survival) and stage \\(1\\) (reproduction) at time \\(t+1\\).</p> <pre><code>graph LR;\n    1((1)) --p1 n1--&gt; 2((2));    \n    1 --m1 n1--&gt; 1;\n    2 --p2 n2--&gt; 3((3));\n    2 --m2 n2--&gt; 1;    \n    3 --p3 n4--&gt; 4((4));\n    3 --m3 n3--&gt; 1;\n    4 --m4 n4--&gt; 1;</code></pre> <p>Because of this, the projection matrix is simpler in the sense that it contains more zeros and has non-zero entries in very specific places</p> \\[ \\mathbf{L} =  \\begin{pmatrix}   m_1 &amp; m_2 &amp; m_3 &amp; \\cdots &amp; m_d \\\\ p_1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; p_2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp;  &amp; \\vdots &amp;  &amp; \\vdots \\\\   0 &amp; \\cdots &amp; 0 &amp; p_{d-1} &amp; 0\\\\ \\end{pmatrix} \\] <p>We call it a Leslie matrix and often denote it with \\(\\mathbf{L}\\). The first row contains the fecundities of each age group, \\(m_i\\), while the entries immediately below the diagonal give the fraction of individuals that survive each age group, \\(p_i\\).</p> <p>Because of the structure of the Leslie matrix, many expressions are now simpler.</p> <p>For example, the characteristic polynomial, \\(\\mathrm{Det}(\\mathbf{L}-\\mathbf{I}\\lambda)=0\\), can be calculated using the first row. After rearranging we get what is known as the Euler-Lotka equation</p> \\[ 1 = \\sum_{i=1}^{d} \\frac{l_i m_i}{\\lambda^i} \\] <p>where \\(l_1 = 1\\), \\(l_i = p_1 p_2 \\cdots p_{i-1}\\) is the fraction of individuals that survive from birth to age \\(i\\) and \\(d\\) is the number of ages (ie, \\(\\mathbf{L}\\) is a \\(d\\times d\\) matrix).</p> <p>Given the \\(l_i\\) and \\(m_i\\) we can use this equation to find the long-term population growth rate, \\(\\lambda_1\\).</p>"},{"location":"lectures/lecture-14/#example-stickleback","title":"Example: stickleback","text":"<p>For example, let's look at a model of stickleback, a small fish (see section 10.6 of the text).</p> <p>We assume stickleback do not live more than 4 years and estimate the Leslie matrix as</p> \\[ \\mathbf{L} =  \\begin{pmatrix} 2 &amp; 3 &amp; 4 &amp; 4\\\\ 0.6 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0.3 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0.1 &amp; 0 \\end{pmatrix} \\] <p>The Euler-Lotka equation is then</p> \\[ \\begin{aligned} 1 &amp;= \\sum_{i=1}^{4} \\frac{l_i m_i}{\\lambda^i}\\\\ 1 &amp;= \\frac{l_1 m_1}{\\lambda} + \\frac{l_2 m_2}{\\lambda^2} + \\frac{l_3 m_3}{\\lambda^3} + \\frac{l_4 m_4}{\\lambda^4}\\\\ 1 &amp;= \\frac{2}{\\lambda} + \\frac{1.8}{\\lambda^2} + \\frac{0.72}{\\lambda^3} + \\frac{0.072}{\\lambda^4}\\\\     \\end{aligned} \\] <p>This can be numerically solved to give our four eigenvalues: \\(\\lambda\\approx2.75, -0.3 \\pm 0.3i, -0.14\\).</p> <p>The long-term growth rate is the eigenvalue with the largest absolute value, \\(\\lambda_1=2.75\\).</p> <p>Note</p> <p>We won't spend the time on it in class, but with age-structure we can also write the stable age distribution, reproductive values, and sensitivities as functions of survival, fecundity, and rate of population growth.</p> <p>The proportion of individuals that are age \\(x\\) (in the long-run) is</p> \\[ v_x = \\frac{l_x \\lambda_1^{-(x-1)}}{\\sum_{i=1}^{d}l_i \\lambda_1^{-(i-1)}} \\] <p>The reproductive value of individuals that are age \\(x\\), relative to age \\(1\\), is</p> \\[ \\frac{u_x}{u_1} = \\frac{\\lambda_1^{x-1}}{l_x}\\sum_{i=x}^{d}\\frac{l_i m_i}{\\lambda_1^{i}} \\] <p>The sensitivities of the long-term growth rate to survival and reproduction at age \\(x\\) are</p> \\[ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_x} = \\frac{v_x u_{x+1}}{\\vec{u}_1 \\vec{v}_1} \\] \\[ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_x} = \\frac{v_x u_1}{\\vec{u}_1 \\vec{v}_1} \\] <p>For example, in our stickleback model the proportion of the population that is age \\(x=2\\), in the long-run, is</p> \\[ \\begin{aligned} v_x &amp;= \\frac{l_x \\lambda_1^{-(x-1)}}{\\sum_{i=1}^{d}l_i \\lambda_1^{-(i-1)}}\\\\ v_2 &amp;= \\frac{l_2 \\lambda_1^{-1}}{\\sum_{i=1}^{4}l_i \\lambda_1^{-(i-1)}}\\\\ &amp;\\approx \\frac{0.6 (2.75)^{-1}}{1 (2.75)^{0} + 0.6 (2.75)^{-1} + 0.18 (2.75)^{-2} + 0.018 (2.75)^{-3}}\\\\ &amp;\\approx 0.18 \\end{aligned} \\] <p>and the relative reproductive value of age \\(x=2\\) individuals is </p> \\[ \\begin{aligned} \\frac{v_x}{v_1} &amp;= \\frac{\\lambda_1^{x-1}}{l_x}\\sum_{i=x}^{d}\\frac{l_i m_i}{\\lambda_1^{i}}\\\\ \\frac{v_2}{v_1} &amp;= \\frac{\\lambda_1^{2-1}}{l_2}\\sum_{i=2}^{4}\\frac{l_i m_i}{\\lambda_1^{i}}\\\\ &amp;= \\frac{\\lambda_1}{l_2} \\left(\\frac{l_2 m_2}{\\lambda_1^{2}} + \\frac{l_3 m_3}{\\lambda_1^{3}} + \\frac{l_4 m_4}{\\lambda_1^{i}} \\right)\\\\ &amp;\\approx \\frac{2.75}{0.6} \\left(\\frac{(0.6) (3)}{2.75^{2}} + \\frac{(0.18) (4)}{2.75^{3}} + \\frac{(0.018) (4)}{2.75^{4}} \\right)\\\\ &amp;\\approx 1.25 \\end{aligned} \\] <p>Repeating these for the other ages we get the stable-age distribution</p> \\[ \\vec{v}_1 \\approx \\begin{pmatrix} 0.80 \\\\ 0.18 \\\\ 0.02 \\\\ 0.0007 \\end{pmatrix} \\] <p>and the reproductive values</p> \\[ \\vec{u}_1 \\approx \\begin{pmatrix} 1 &amp; 1.25 &amp; 1.51 &amp; 1.45 \\end{pmatrix} \\] <p>From these we see that the majority of the population is expected to be age 1 (from \\(\\vec{v}_1\\)) and age 3 has the highest reproductive value (from \\(\\vec{u}_1\\)).</p> <p>We can now use these vectors to calculate the sensitivities of population growth rate to survival</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_x} &amp;= \\frac{v_x u_{x+1}}{\\vec{u}_1 \\vec{v}_1}\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_1} &amp;\\approx 0.95\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_2} &amp;\\approx 0.25\\\\  \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_3} &amp;\\approx 0.03 \\end{aligned} \\] <p>and to fecundity</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_x} &amp;= \\frac{v_x u_1}{\\vec{u}_1 \\vec{v}_1}\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_1} &amp;\\approx 0.76\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_2} &amp;\\approx 0.17\\\\  \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_3} &amp;\\approx 0.02\\\\  \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_4} &amp;\\approx 0.0007 \\end{aligned} \\] <p>And so we see that, because such a large proportion of the population is expected to be age 1, increasing the survival and fecundity of that age has the largest effect.</p> <p>Finally, we can plot the predicted dynamics as we did for the whales above. In this case the population grows very quickly (clearly there is a need to also model density-dependence via competition, otherwise our model predicts that the universe will soon be stuffed-full with sticklback!), so we plot the number of fish of each age on a log scale. Again, we compare the full general solution (solid) to the long-term approximation (dashed) and see quick convergence.</p> <pre>\nm1,m2,m3,m4,p1,p2,p3 = 2,3,4,4,0.6,0.3,0.1 #parameter values\n\nimport numpy as np\n\nM = np.array([[m1,m2,m3,m4], #projection matrix\n              [p1,0,0,0],\n              [0,p2,0,0],\n              [0,0,p3,0]])\n\n# calculate\neigs = np.linalg.eig(M) #eigenvalues and right eigenvectors\nix = np.argmax(np.abs(eigs[0])) #which is leading eigenvalue\nl1 = eigs[0][ix] #leading eigenvalue\nv1 = eigs[1][:,ix] #leading right eigenvector\nv1 = v1/np.sum(v1) #normalized to sum to 1\nus = np.linalg.inv(eigs[1]) #left eigenvectors\nu1 = us[ix] #leading left eigenvalue\nu1 = u1/np.dot(u1,v1) #normalized so u1*v1=1\n\nn0 = np.array([25,25,25,25]) #initial state\n\ndef ntfull(t):\n    '''full projection'''\n    return np.dot(np.linalg.matrix_power(M,t), n0)\n\ndef ntapp(t):\n    '''long term approximation'''\n    return l1**t * v1 * np.dot(u1, n0)\n\n# plot\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nlabels = ['age 1','age 2','age 3','age 4']\ncolors = plt.get_cmap('tab10')\nfor i,j in enumerate(range(len(M))): #for each stage\n    ax.plot([ntfull(t)[j] for t in range(10)], color=colors(i), label=labels[i])\n    ax.plot([ntapp(t)[j] for t in range(10)], color=colors(i), linestyle='--') #long-term approx\n\nax.legend()\nax.set_xlabel('years')\nax.set_ylabel('number of stickleback')\nax.set_yscale('log')\nplt.show()\n</pre> <p></p>"},{"location":"lectures/lecture-15/","title":"Lecture 15","text":""},{"location":"lectures/lecture-15/#lecture-15-equilibria-and-stability-nonlinear-multivariate","title":"Lecture 15: Equilibria and stability (nonlinear multivariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-15/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Continuous time</li> <li>Discrete time</li> <li>Summary</li> </ol> <p>Now that we've covered linear multivariate models, we turn our attention to the most common type of model: one with multiple interacting variables.</p> <p>For example, a model of the number of susceptible, \\(S\\), and infected, \\(I\\), individuals often includes the interaction between these two variables, \\(SI\\), describing the rate at which these two classes of individuals meet one another (and potentially spread disease). </p> <p>Similarly, models of predator, \\(P\\), and prey, \\(N\\), abundance often include terms like \\(NP\\) describing the rate at which predators encounter prey (and potentially eat them).</p> <p>Let's first see how to deal with these models in general and then apply those techniques to specific circumstances like those mentioned above (in the next two lectures).</p> <p></p>"},{"location":"lectures/lecture-15/#1-continuous-time","title":"1. Continuous time","text":"<p>In general, if we have \\(n\\) interacting variables, \\(x_1, x_2, ..., x_n\\), we can write any continuous time model like</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}x_1}{\\mathrm{d}t} &amp;=  f_1(x_1, x_2, ..., x_n)\\\\ \\frac{\\mathrm{d}x_2}{\\mathrm{d}t} &amp;=  f_2(x_1, x_2, ..., x_n)\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}x_n}{\\mathrm{d}t} &amp;=  f_n(x_1, x_2, ..., x_n)   \\end{aligned} \\] <p>If we then want to find the equilibria, \\(\\hat{x}_1,\\hat{x}_2, ..., \\hat{x}_n\\), we set all these equations to 0 and solve for one variable at a time (note that solving for the equilibrium is not always possible in nonlinear models!).</p> <p>Now note that we can no longer write this system of equations in matrix form with a matrix composed only of parameters.</p> <p>In order to use what we've learned about eigenvalues and eigenvectors we're first going to have to linearize the system so that the corresponding matrices do not contain variables.</p> <p>As we saw in nonlinear univariate models, one useful way to linearize a system is to measure the system relative to equilibrium, \\(\\epsilon = n - \\hat{n}\\).</p> <p>Then assuming that the deviation from equilibrium, \\(\\epsilon\\), is small, we used a Taylor series expansion to approximate the nonlinear system with a linear system.</p> <p>To do that with multivariate models we'll need to know how to take a Taylor series expansion of multivariate functions</p> <p>Taylor series expansion of a multivariate function</p> <p>Taking the series of \\(f\\) around \\(x_1=a_1\\), \\(x_2=a_2\\), ..., \\(x_n=a_n\\) gives</p> \\[ \\begin{aligned} f(x_1, x_2, ..., x_n) &amp;= f(a_1, a_2, ..., a_n)\\\\  &amp;+ \\sum_{i=1}^{n} \\left( \\frac{\\partial f}{\\partial x_i} \\bigg|_{x_1=a_1, x_2=a_2, ..., x_n=a_n} \\right) (x_i - a_i)\\\\ &amp;+ \\sum_{i=1}^{n}\\sum_{j=1}^n \\left( \\frac{\\partial f}{\\partial x_i \\partial x_j} \\bigg|_{x_1=a_1, x_2=a_2, ..., x_n=a_n} \\right) (x_i - a_i)(x_j - a_j)\\\\ &amp;+ \\cdots \\end{aligned} \\] <p>where \\(\\frac{\\partial f}{\\partial x_i}\\) is the \"partial derivative\" of \\(f\\) with respect to \\(x_i\\), meaning that we treat all the other variables as constants when taking the derivative. </p> <p>Then when the difference between each variable and its value, \\(x_i-a_i\\), is small enough we can ignore all the terms with a \\((x_i-a_i)(x_j-a_j)\\), and we are left with a linear approximation of \\(f\\).</p> <p>So let \\(\\epsilon_i = x_i - \\hat{x}_i\\) be the deviation of variable \\(x_i\\) from its equilibrium value, \\(\\hat{x}_i\\).</p> <p>Then we can write a system of equations describing the change in the deviations for all of our variables</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &amp;=  f_1(x_1, x_2, ..., x_n)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &amp;=  f_2(x_1, x_2, ..., x_n)\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &amp;=  f_n(x_1, x_2, ..., x_n)   \\end{aligned} \\] <p>And then we can take a Taylor series around \\(x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n\\) to get a linear approximation of our system near the equilibrium</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &amp;\\approx  f_1(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &amp;\\approx  f_2(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &amp;\\approx  f_n(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i) \\end{aligned} \\] <p>And then note that all \\(f_i(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n)=0\\) by definition of a equilibrium, leaving</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &amp;\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &amp;\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &amp;\\approx  \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i) \\end{aligned} \\] <p>Each of the partial derivatives \\(\\frac{\\partial f_i}{\\partial x_j}\\) is evaluated at the equilibrium, so these are constants. And \\(x_i - \\hat{x}_i = \\epsilon_i\\). So we now have a linear system</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &amp;\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &amp;\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &amp;\\approx  \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i \\end{aligned} \\] <p>We can now write our approximate system around the equilibrium in matrix form</p> \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} \\\\ \\vdots \\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} &amp; \\frac{\\partial f_n}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix} \\end{aligned} \\] <p>This is a special matrix called the Jacobian</p> \\[ \\mathbf{J} =  \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} &amp; \\frac{\\partial f_n}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix} \\] <p>And now that we have a linear system around an equilibrium, we can assess its local stability just as we did with linear multivariate models (see Summary).</p> <p></p>"},{"location":"lectures/lecture-15/#2-discrete-time","title":"2. Discrete time","text":"<p>Note</p> <p>The short version of this section is that we can do the same thing in discrete time -- local stability is determined by the eigenvalues of the Jacobian, where the functions in that Jacobian are now our recursions, \\(x_i(t+1) = f_i(x_1(t), x_2(t), ..., x_n(t))\\).</p> <p>We can do something very similar for nonlinear multivariate models in discrete time</p> \\[ \\begin{aligned} x_1(t+1) &amp;=  f_1(x_1(t), x_2(t), ..., x_n(t))\\\\ x_2(t+1) &amp;=  f_2(x_1(t), x_2(t), ..., x_n(t))\\\\ &amp;\\vdots\\\\ x_n(t+1) &amp;=  f_n(x_1(t), x_2(t), ..., x_n(t))\\\\ \\end{aligned} \\] <p>Now the equilibria are found by setting all \\(x_i(t+1) = x_i(t) = \\hat{x}_i\\) and solving for the \\(\\hat{x}_i\\) one at a time.</p> <p>To linearize the system around an equilibrium we again measure the system in terms of deviation from the equilibrium, \\(\\epsilon_i(t) = x_i(t) - \\hat{x}_i\\), giving</p> \\[ \\begin{aligned} \\epsilon_1(t+1) &amp;=  f_1(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ \\epsilon_2(t+1) &amp;=  f_2(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ &amp;\\vdots\\\\ \\epsilon_n(t+1) &amp;=  f_n(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ \\end{aligned} \\] <p>Then taking the Taylor series of each \\(f_i\\) around \\(x_1(t) = \\hat{x}_1, ..., x_n(t) = \\hat{x}_n\\) we can approximate our system near the equilibrium as</p> \\[ \\begin{aligned}   \\epsilon_1(t+1) &amp;=  f_1(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n)  + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_1\\\\   \\epsilon_2(t+1) &amp;=  f_2(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_2\\\\   &amp;\\vdots\\\\   \\epsilon_n(t+1) &amp;=  f_n(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_n\\\\   \\end{aligned} \\] <p>And noting that \\(f_i(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) = \\hat{x}_i\\) and \\(x_i(t) - \\hat{x}_i = \\epsilon_i(t)\\) we have</p> \\[ \\begin{aligned}   \\epsilon_1(t+1) &amp;= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\   \\epsilon_2(t+1) &amp;= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\   &amp;\\vdots\\\\   \\epsilon_n(t+1) &amp;= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\   \\end{aligned} \\] <p>We can therefore write our approximation around the equilibrium in matrix form</p> \\[ \\begin{aligned} \\begin{pmatrix} \\epsilon_1(t+1) \\\\ \\epsilon_2(t+1) \\\\ \\vdots \\\\ \\epsilon_n(t+1) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} &amp; \\frac{\\partial f_n}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\begin{pmatrix} \\epsilon_1(t) \\\\ \\epsilon_2(t) \\\\ \\vdots \\\\ \\epsilon_n(t) \\end{pmatrix} \\end{aligned} \\] <p>As in continuous time, the dynamics are described by the Jacobian matrix</p> \\[ \\mathbf{J} =  \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} &amp; \\frac{\\partial f_n}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix} \\] <p>We therefore assess the local stability of an equilibrium by evaluating the Jacobian at that equilibrium and finding the eigenvalues (see Summary).</p> <p></p>"},{"location":"lectures/lecture-15/#3-summary","title":"3. Summary","text":"<p>We can determine the stability of nonlinear multivariate models with the eigenvalues of the Jacobian evaluated at an equilibrium. The recipe is</p> <ul> <li>Find the equilibrium of interest, \\(\\hat{x}_1, \\hat{x}_2, ... \\hat{x}_n\\)</li> <li>Calculate the Jacobian, \\(\\mathbf{J}\\)</li> <li>Evaluate the Jacobian at the equilibrium of interest, \\(\\mathbf{J}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n}\\)</li> <li>Calculate the characteristic polynomial \\(|\\mathbf{J}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} - \\lambda\\mathbf{I}|\\) </li> <li>Set the characteristic polynomial equal to 0 and solve for the \\(n\\) eigenvalues, \\(\\lambda\\)</li> </ul> <p>Stability reminder</p> <p>Continuous time</p> <ul> <li>the leading eigenvalue is the one with the largest real part</li> <li>if the leading eigenvalue has a negative real part the equilibrium is stable</li> <li>if any eigenvalue has a non-zero complex part there will be cycling</li> </ul> <p>Discrete time</p> <ul> <li>the leading eigenvalue is the one with the largest absolute value</li> <li>if the leading eigenvalue has an absolute value less than one the equilibrium is stable</li> <li>if any eigenvalue has a non-zero complex part there will be cycling</li> </ul>"},{"location":"lectures/lecture-16/","title":"Lecture 16","text":""},{"location":"lectures/lecture-16/#lecture-16-epidemiology","title":"Lecture 16: Epidemiology","text":"Run notes interactively?"},{"location":"lectures/lecture-16/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Epidemiology</li> </ol>"},{"location":"lectures/lecture-16/#1-epidemiology","title":"1. Epidemiology","text":"<p>In Lecture 15 we learned how to find equilibria and determine their stability in nonlinear multivariate models. To make this more concrete, let's consider a biological example (see Section 8.2 in the text). </p> <p>Consider a population composed of \\(S\\) susceptible individuals and \\(I\\) infected individuals. We assume new susceptible individuals arrive at rate \\(\\theta\\) via immigration and existing susceptibles die at per capita rate \\(d\\). We assume infected individuals die at an elevated per capita rate \\(d+v\\) and recover at per capita rate \\(\\gamma\\). So far this is a linear (affine) model. Finally, we assume susceptibles become infected at rate \\(\\beta S I\\). This is the non-linear part.</p> <p>We can describe this with the following flow diagram</p> <pre><code>graph LR;\n    A[ ] --theta--&gt; S((S));\n    S --beta S I--&gt; I((I));\n    S --d S--&gt; B[ ];    \n    I --\"(d + v) I\"--&gt; C[ ];\n    I --gamma I--&gt; S;\n\n    style A height:0px;\n    style B height:0px;\n    style C height:0px;</code></pre> <p>The corresponding system of differential equations is</p> \\[\\begin{aligned} \\frac{\\mathrm{d}S}{\\mathrm{d}t} &amp;= \\theta - \\beta S I - d S + \\gamma I \\\\ \\frac{\\mathrm{d}I}{\\mathrm{d}t} &amp;= \\beta S I - (d + v) I - \\gamma I  \\end{aligned}\\] <p>At equilibrium both derivatives are equal to zero </p> \\[\\begin{aligned} 0 &amp;= \\theta - \\beta \\hat{S} \\hat{I} - d \\hat{S} + \\gamma \\hat{I} \\\\ 0 &amp;= \\beta \\hat{S} \\hat{I} - (d + v) \\hat{I} - \\gamma \\hat{I}  \\end{aligned}\\] <p>To be systematic we could start with the first equation and solve for the first variable, \\(\\hat{S}\\), in terms of the remaining variables, \\(\\hat{I}\\). We could then sub that expression for \\(\\hat{S}\\) into the second equation, which would then be an equation for \\(\\hat{I}\\) alone. After solving for \\(\\hat{I}\\) we could then sub that solution into \\(\\hat{S}\\) and be done. But through experience we notice that there is an easier approach. </p> <p>Because the second equation is proportional to \\(\\hat{I}\\) we immediately know \\(\\hat{I}=0\\) is one potential equilibrium point. For this to work we also need the first equation to be zero. Subbing in \\(\\hat{I}=0\\) to that first equation and solving for \\(\\hat{S}\\) gives \\(\\hat{S}=\\theta/d\\). One equilibrium is therefore</p> \\[\\begin{aligned} \\hat{S} &amp;= \\theta/d \\\\ \\hat{I} &amp;= 0 \\end{aligned}\\] <p>which we call the \"disease-free\" equilibrium.</p> <p>Returning to the second equation, after factoring out \\(\\hat{I}\\) we are left with \\(0 = \\beta \\hat{S} - (d + v + \\gamma)\\), implying \\(\\hat{S} = (d + v + \\gamma)/\\beta\\). Plugging this into the first equation and solving for \\(\\hat{I}\\) we see that a second equilibrium is</p> \\[\\begin{aligned} \\hat{S} &amp;= (d + v + \\gamma)/\\beta \\\\ \\hat{I} &amp;= \\frac{\\theta - d(d + v + \\gamma)/\\beta}{d+v} \\end{aligned}\\] <p>which we call the \"endemic equilibrium\" because there is some non-zero amount of disease. Note that this equilibrium is only biologically valid when the numerator of \\(\\hat{I}\\) is positive which can be rearranged as \\(\\beta\\theta/d &gt; d + v + \\gamma\\).</p> <p>Now that we have the equilibria, the next step is to calculate the Jacobian. Letting \\(x_1=S\\) and \\(x_2=I\\) we have \\(f_1(x_1,x_2)=\\mathrm{d}S/\\mathrm{d}t\\) and \\(f_2(x_1,x_2)=\\mathrm{d}I/\\mathrm{d}t\\). The Jacobian is therefore</p> \\[\\begin{aligned} \\mathbf{J}  &amp;=  \\begin{pmatrix} \\frac{\\partial}{\\partial S}\\left(\\frac{\\mathrm{d}S}{\\mathrm{d}t}\\right) &amp; \\frac{\\partial}{\\partial I}\\left(\\frac{\\mathrm{d}S}{\\mathrm{d}t}\\right) \\\\ \\frac{\\partial}{\\partial S}\\left(\\frac{\\mathrm{d}I}{\\mathrm{d}t}\\right) &amp; \\frac{\\partial}{\\partial I}\\left(\\frac{\\mathrm{d}I}{\\mathrm{d}t}\\right) \\end{pmatrix}\\\\ &amp;= \\begin{pmatrix} -d-\\beta I &amp; -\\beta S+\\gamma \\\\ \\beta I &amp; \\beta S-(d+v+\\gamma) \\end{pmatrix} \\end{aligned}\\] <p>We can now determine the local stability of an equilibrium by evaluating the Jacobian at that equilibrium and calculating the eigenvalues. </p> <p>Let's do that first for the simpler disease-free equilibrium, where there are no infected individuals, \\(\\hat{I}=0\\), and the number of susceptibles is a balance of immigration and death, \\(\\hat{S} = \\theta/d\\). Plugging these into the Jacobian gives </p> \\[\\begin{aligned} \\mathbf{J}_\\mathrm{disease-free}  &amp;=  \\begin{pmatrix} -d &amp; -\\beta \\theta/d+\\gamma \\\\ 0 &amp; \\beta \\theta/d-(d+v+\\gamma) \\end{pmatrix} \\end{aligned}\\] <p>This is an upper triangular matrix, so the eigenvalues are just the diagonal elements, \\(\\lambda = -d, \\beta\\theta/d-(d+v+\\gamma)\\). Because all the parameters are rates they are all non-negative, and therefore the only eigenvalue that can have a positive real part (and therefore cause instability) is \\(\\lambda=\\beta\\theta/d-(d+v+\\gamma)\\). The equilibrium is unstable when this is positive, \\(\\beta\\theta/d-(d+v+\\gamma)&gt;0\\). Because this equilibrium has no infected individuals, instability in this case means the infected individuals will increase in number from rare -- ie, the disease can spread when rare. </p> <p>We can rearrange the instability condition to get a little more intuition. The disease will spread when rare whenever</p> \\[\\begin{aligned} \\beta\\theta/d - (d+v+\\gamma)&amp; &gt; 0 \\\\ \\beta\\theta/d &amp;&gt; d+v+\\gamma \\\\ \\frac{\\beta\\theta/d}{d+v+\\gamma} &amp;&gt; 1 \\end{aligned}\\] <p>The numerator is \\(\\beta\\) times the number of susceptibles at the disease-free equilibrium, \\(\\hat{S}=\\theta/d\\). This is the rate that a rare disease infects new individuals. The denominator is the rate at which the disease is removed from the population. Therefore a rare disease that infects faster than it is removed can spread. This ratio, in our case \\(\\frac{\\beta\\theta/d}{d+v+\\gamma}\\), is termed \\(R_0\\) and is a very key epidemiological quantity (you may remember estimates of \\(R_0\\) in the news from a certain recent virus...).</p> <p>Now for the endemic equilibrium. Plugging these values into the Jacobian and simplifying gives</p> \\[\\begin{aligned} \\mathbf{J}_\\mathrm{endemic}  &amp;=  \\begin{pmatrix} -\\frac{\\beta \\theta - d \\gamma}{d+v} &amp; -(d+v) \\\\ \\frac{\\beta \\theta - d (d+v+\\gamma)}{d+v} &amp; 0 \\end{pmatrix} \\end{aligned}\\] <p>Here, instead of calculating the eigenvalues explicitly, we will use the Routh-Hurwitz stability criteria for a 2x2 matrix.</p> <p>Routh-Hurwitz stability criteria for a 2x2 matrix</p> <p>When working with 2x2 matrices, there is a simple way to determine if both the eigenvalues have negative real parts (ie, if the equilibrium is stable) without having to calculate the eigenvalues themselves. These are called the Routh-Hurwitz stability criteria (and extend to larger matrices but we won't cover that here).</p> <p>Recall that for a 2x2 matrix, \\(\\mathbf{M}\\), the eigenvalues can be written </p> \\[\\lambda = \\frac{\\mathrm{Tr}(\\mathbf{M}) \\pm \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4\\mathrm{Det}(\\mathbf{M})}}{2}\\] <p>First notice that the product of the two eigenvalues is \\(\\mathrm{Det}(\\mathbf{M})\\) (you may want to check that for yourself). This means that the two eigenvalues have the same sign if and only if \\(\\mathrm{Det}(\\mathbf{M})&gt;0\\).</p> <p>Second, notice that the sum of the two eigenvalues is \\(\\mathrm{Tr}(\\mathbf{M})\\). </p> <p>We therefore know that the real parts of both eigenvalues will be negative (ie, the equilibrium will be stable) if and only if \\(\\mathrm{Det}(\\mathbf{M})&gt;0\\) and \\(\\mathrm{Tr}(\\mathbf{M})&lt;0\\).</p> <p>The determinant is \\(\\beta \\theta - d (d+v+\\gamma)\\), so for this to be positive we need \\(\\beta \\theta/d &gt; (d+v+\\gamma)\\), which was our validity condition (above) and also the instability condition on the disease-free equilibrium (\\(R_0&gt;1\\)). The trace is \\(-\\frac{\\beta \\theta - d \\gamma}{d+v}\\), so for this to be negative we need \\(\\beta \\theta/d &gt; \\gamma\\), which is guaranteed if the determinant is positive. So in conclusion, the endemic equilibrium is valid and stable whenever the disease can invade, \\(R_0&gt;1\\).</p>"},{"location":"lectures/lecture-17/","title":"Lecture 17","text":""},{"location":"lectures/lecture-17/#lecture-17-multi-locus-population-genetics","title":"Lecture 17: Multi-locus population genetics","text":"Run notes interactively?"},{"location":"lectures/lecture-17/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Multi-locus population genetics</li> </ol>"},{"location":"lectures/lecture-17/#1-multi-locus-population-genetics","title":"1. Multi-locus population genetics","text":"<p>In Lecture 15 we learned how to find equilibria and determine their stability in nonlinear multivariate models. In Lecture 16 we looked at a continuous-time example from epidemiology. Here, we'll extend the discrete-time dipliod selection model of Lecture 4 to multiple loci (see Section 8.3 in the text). </p> <p>Genomes contain many loci -- what new dynamics arise when we model more than one locus? Here we look at the simplest multi-locus model, with two loci each with two alleles. Let's denote the loci with letters, \\(A\\) and \\(B\\), and the alleles at each with numbers, \\(1\\) and \\(2\\). This gives a total of \\(2^2=4\\) haploid genotypes, which we'll give frequencies \\(x_1\\) to \\(x_4\\) like so</p> genotype frequency \\(A_1B_1\\) \\(x_1\\) \\(A_1B_2\\) \\(x_2\\) \\(A_2B_1\\) \\(x_3\\) \\(A_2B_2\\) \\(x_4\\) <p>with the constraint that the total frequency sums to one, \\(x_1+x_2+x_3+x_4 = 1\\).</p> <p>To determine how these frequencies change from one generation to the next, let's first determine the order of events in a life-cycle diagram</p> <pre><code>graph LR;\n    A((census)) --&gt; B((gamete union));\n    B --&gt; C((selection));\n    C --&gt; D((meiosis));    \n    D --&gt; A;</code></pre> <p>In words, we'll census the population in the gamete (haploid) phase while selection happens in the diploid phase.</p> <p>Next, to consider how the frequencies change through this life cyle, let's construct a life-cycle (mating) table. To do this, we need to consider what happens during meiosis when there are multiple loci. </p> <p>In the 1-locus models we analyzed earlier, meiosis meant Mendelian segregation: each allele is present in 1/2 of the gametes. Here, with 2 loci, things are slightly more complicated and we need to consider recombination. Every meiosis there is a chance, \\(r\\), of an odd number of crossover events between the two loci (\\(r\\) will increase with the distance between the loci). When this happens the pairing of the alleles at loci A and B get swapped. This only has an effect in diploid individuals that are heterozygous at both loci (\"double heterozygotes\"), here \\(A_1B_1\\) x \\(A_2B_2\\) and \\(A_1B_2\\) x \\(A_2B_1\\). Every time these individuals go through meiosis the original pairings are kept with probability \\(1-r\\) and the alternative pairings are created with probability \\(r\\). </p> <p>We can now fill in the following table</p> union frequency frequency after selection gamete frequency after meiosis (\\(A_1B_1\\), \\(A_1B_2\\), \\(A_2B_1\\), \\(A_2B_2\\)) \\(A_1B_1\\) x \\(A_1B_1\\) \\(x_1^2\\) \\(x_1^2 w_{11}/\\bar{w}\\) 1, 0, 0, 0 \\(A_1B_1\\) x \\(A_1B_2\\) \\(2x_1x_2\\) \\(2x_1x_2 w_{12}/\\bar{w}\\) 1/2, 1/2, 0, 0 \\(A_1B_1\\) x \\(A_2B_1\\) \\(2x_1x_3\\) \\(2x_1x_3 w_{13}/\\bar{w}\\) 1/2, 0, 1/2, 0 \\(A_1B_1\\) x \\(A_2B_2\\) \\(2x_1x_4\\) \\(2x_1x_4 w_{14}/\\bar{w}\\) \\((1-r)/2\\), \\(r/2\\), \\(r/2\\), \\((1-r)/2\\) \\(A_1B_2\\) x \\(A_1B_2\\) \\(x_2^2\\) \\(x_2^2 w_{22}/\\bar{w}\\) 0, 1, 0, 0 \\(A_1B_2\\) x \\(A_2B_1\\) \\(2x_2x_3\\) \\(2x_2x_3 w_{23}/\\bar{w}\\) \\(r/2\\), \\((1-r)/2\\), \\((1-r)/2\\), \\(r/2\\) \\(A_1B_2\\) x \\(A_2B_2\\) \\(2x_2x_4\\) \\(2x_2x_4 w_{24}/\\bar{w}\\) 0, 1/2, 0, 1/2 \\(A_2B_1\\) x \\(A_2B_1\\) \\(x_3^2\\) \\(x_3^2 w_{33}/\\bar{w}\\) 0, 0, 1, 0 \\(A_2B_1\\) x \\(A_2B_2\\) \\(2x_3x_4\\) \\(2x_3x_4 w_{34}/\\bar{w}\\) 0, 0, 1/2, 1/2 \\(A_2B_2\\) x \\(A_2B_2\\) \\(x_4^2\\) \\(x_4^2 w_{44}/\\bar{w}\\) 0, 0, 0, 1 <p>where \\(w_{ij}=w_{ji}\\) is the fitness of the diploid that is composed of haploid genotypes \\(i\\) and \\(j\\) and \\(\\bar{w}\\) is the population mean fitness, which is the sum of the frequencies after selection.</p> <p>We can build the recursion equations from this table by multiplying the frequency after selection by the gamete frequency after meiosis. For example, the frequency of \\(A_1B_1\\) in the next generation is found by multiplying the first entry in the final column by the frequency after selection and summing this up over rows, giving</p> \\[\\begin{align} x_1(t+1)  &amp;= x_1(t)^2 w_{11}/\\bar{w} + x_1(t)x_2(t) w_{12}/\\bar{w} + x_1(t)x_3(t) w_{13}/\\bar{w} + (1-r)x_1(t)x_4(t) w_{14}/\\bar{w} + r x_2(t)x_3(t) w_{23}/\\bar{w}\\\\ &amp;= x_1(t) (x_1(t) w_{11}/\\bar{w} + x_2(t) w_{12}/\\bar{w} + x_3(t) w_{13}/\\bar{w} + x_4(t) w_{14}/\\bar{w}) - r(x_1(t)x_4(t) w_{14}/\\bar{w} - x_2(t)x_3(t) w_{23}/\\bar{w})\\\\ &amp;= x_1(t) \\sum_{i=1}^{4} x_i(t) w_{1i}/\\bar{w} - r D^* \\end{align}\\] <p>where \\(D^*=x_1(t)x_4(t) w_{14}/\\bar{w} - x_2(t)x_3(t) w_{23}/\\bar{w}\\) is called linkage disequilibrium (the asterisk differentiates it from the same quantity measured prior to selection, \\(D=x_1(t)x_4(t) - x_2(t)x_3(t)\\)). Linkage disequilibrium is an important term in population genetics that measures the deviation of the association of alleles at two loci from that expected by chance under random assortment. For example, when \\(A_1\\) pairs with \\(B_1\\) more often than expected by chance then \\(x_1(t)x_4(t) &gt; x_2(t)x_3(t)\\) and \\(D&gt;0\\). </p> <p>The remaining equations are created in the same way, giving</p> \\[\\begin{align} x_2(t+1) &amp;= x_2(t) \\sum_{i=1}^{4} x_i(t) w_{2i}/\\bar{w} + r D^* \\\\ x_3(t+1) &amp;= x_3(t) \\sum_{i=1}^{4} x_i(t) w_{3i}/\\bar{w} + r D^* \\\\ x_4(t+1) &amp;= x_4(t) \\sum_{i=1}^{4} x_i(t) w_{4i}/\\bar{w} - r D^* \\\\ \\end{align}\\] <p>Now we have a system of recursion equations to work with. This system is nonlinear and four dimensional, which makes things relatively complex. For instance, it is impossible to find all the equilibria analytically. Here we'll not worry about that and just deal with a particularly simple equilibrium where \\(A_1B_1\\) is fixed, \\(\\hat{x}_1=1\\) and \\(\\hat{x}_2=\\hat{x}_3=\\hat{x}_4=0\\). This implies \\(\\bar{w}=w_{11}\\).</p> <p>To determine the stability of this equilbrium we need to calculate the Jacobian</p> \\[ \\mathbf{J} =  \\begin{pmatrix} \\frac{\\partial x_1(t+1)}{\\partial x_1(t)} &amp; \\frac{\\partial x_1(t+1)}{\\partial x_2(t)} &amp; \\frac{\\partial x_1(t+1)}{\\partial x_3(t)} &amp; \\frac{\\partial x_1(t+1)}{\\partial x_4(t)}\\\\ \\frac{\\partial x_2(t+1)}{\\partial x_1(t)} &amp; \\frac{\\partial x_2(t+1)}{\\partial x_2(t)} &amp; \\frac{\\partial x_2(t+1)}{\\partial x_3(t)} &amp; \\frac{\\partial x_2(t+1)}{\\partial x_4(t)}\\\\ \\frac{\\partial x_3(t+1)}{\\partial x_1(t)} &amp; \\frac{\\partial x_3(t+1)}{\\partial x_2(t)} &amp; \\frac{\\partial x_3(t+1)}{\\partial x_3(t)} &amp; \\frac{\\partial x_3(t+1)}{\\partial x_4(t)}\\\\ \\frac{\\partial x_4(t+1)}{\\partial x_1(t)} &amp; \\frac{\\partial x_4(t+1)}{\\partial x_2(t)} &amp; \\frac{\\partial x_4(t+1)}{\\partial x_3(t)} &amp; \\frac{\\partial x_4(t+1)}{\\partial x_4(t)}\\\\ \\end{pmatrix} \\] <p>(we omit writing out the derivatives for brevity) and then evaluate it at the focal equilibrium, \\(\\mathbf{J}_{x_1=1,x_2=x_3=x_4=0}\\). Local stability of the focal equilibrium is determined by the eigenvalues of this matrix, which are</p> \\[\\begin{aligned} \\lambda_1 &amp;= 0 \\\\ \\lambda_2 &amp;= w_{12}/w_{11} \\\\ \\lambda_3 &amp;= w_{13}/w_{11} \\\\ \\lambda_4 &amp;= (1-r)w_{14}/w_{11}  \\end{aligned}\\] <p>The first eigenvalue, \\(\\lambda_1 = 0\\), indicates that there is an axis along which or system does not change. This is due to the fact that the frequencies always sum to one, \\(x_1+x_2+x_3+x_4=1\\). We therefore effectively have a three dimensional model, e.g., we could just track \\(x_1\\), \\(x_2\\), and \\(x_3\\) because we know that \\(x_4 = 1 - x_1-x_2-x_3\\).</p> <p>The second and third eigenvalues, \\(\\lambda_2 = w_{12}/w_{11}\\) and \\(\\lambda_3 = w_{13}/w_{11}\\), are analogous to what we found in the 1 locus (univariate) case. Remembering that stability in discrete time requires that the eigenvalues are less than 1 in absolute value, we can interpret these eigenvalues as saying that the \\(B_2\\) allele can invade (instability) when it has higher fitness than the \\(B_1\\) allele (\\(w_{12}&gt;w_{11}\\)) and the \\(A_2\\) can invade when it has higher fitness than the \\(A_1\\) allele (\\(w_{13}&gt;w_{11}\\)). </p> <p>The fourth eigenvalue, \\(\\lambda_4 = (1-r)w_{14}/w_{11}\\), is the new part, which depends on recombination. Interestingly, here, even if \\(A_2B_2\\) has higher fitness than \\(A_1B_1\\), meaning \\(w_{14}&gt;w_{11}\\), it is possible that the \\(A_2B_2\\) genotype cannot invade. This is because, for a rare \\(A_2B_2\\) genotype, every generation it pairs with the common \\(A_1B_1\\) genotype and therefore gets broken apart into \\(A_1B_2\\) and \\(A_2B_1\\) by recombination with probability \\(r\\). In other words, recombination can hinder the spread of an adaptive combination of alleles. This is epitiomized by the scenario where having a single \"2\" allele is deleterious \\(w_{12}&lt;w_{11}\\) and \\(w_{13}&lt;w_{11}\\) (making \\(\\lambda_2&lt;1\\) and \\(\\lambda_3&lt;1\\)) but having two \"2\" alleles is beneficial, \\(w_{14}&gt;w_{11}\\). Such a scenario is called a fitness valley because of the plot below</p> <pre>\nimport matplotlib.pyplot as plt\n\nw11=1\nw12=0.9\nw13=0.8\nw14=1.1\n\nfig,ax=plt.subplots()\n\nax.plot([0,1,2],[w11,w12,w14],marker='o')\nax.plot([0,1,2],[w11,w13,w14],marker='o')\nax.text(0,w11,r'$A_1B_1$',va='bottom')\nax.text(1,w12,r'$A_1B_2$',va='top')\nax.text(1,w13,r'$A_2B_1$',va='top')\nax.text(2,w14,r'$A_2B_2$',va='bottom',ha='right')\n\nax.set_ylabel('fitness')\nax.set_xlabel('number of \"2\" alleles')\nax.set_xticks([0,1,2])\nplt.show()\n</pre> <p></p> <p>Here we've seen how recombination can slow the spread of the optimal genotype, \\(A_2B_2\\), potentially preventing fitness-valley crossing. There is also, however, a constructive aspect of recombination, not explored in this simple model: when the deleterious genotypes \\(A_1B_2\\) and \\(A_2B_1\\) are both present, there is a chance that they pair and recombine, giving rise to the optimal genotype \\(A_2B_2\\). The role of recombination in fitness-valley crossing is therefore a relatively interesting and complex problem.</p>"},{"location":"lectures/lecture-18/","title":"Lecture 18","text":""},{"location":"lectures/lecture-18/#lecture-18-evolutionary-invasion-analysis","title":"Lecture 18: Evolutionary invasion analysis","text":"Run notes interactively?"},{"location":"lectures/lecture-18/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Invasion fitness</li> <li>Evolutionarily singular strategies</li> <li>Evolutionarily stable strategies</li> <li>Evolutionary convergence</li> <li>Pairwise invasibility plots</li> </ol> <p>In the models we've discussed so far we've taken the parameters to be fixed. In reality, many of these parameters can evolve. For example, in our model of exponential growth in discrete time, \\(n(t+1)=n(t) R\\), we took \\(R\\) to be the same for all individuals for all time. But any mutation causing a larger \\(R\\) would increase in frequency, causing the value of \\(R\\) to increase over time. In this lecture we'll explore how to determine the direction of evolution and the stability of evolutionary endpoints for more complex models using a technique called evolutionary invasion analysis (also known as adaptive dynamics).</p> <p>The idea:</p> <ul> <li>determine which parameter(s) of our model an evolving trait affects</li> <li>take the population to be fixed for some \"resident\" trait value</li> <li>determine the equilibria and stability of the system with only the resident trait</li> <li>derive the growth rate of a new individual with a \"mutant\" trait value</li> <li>ask when the mutant trait value will invade</li> <li>look for potential evolutionary endpoints</li> <li>determine the stability of those endpoints</li> </ul> <p>The evolution of dispersal</p> <p>To motivate evolutionary invasion analysis, let's consider the evolution of dispersal. </p> <p>Imagine there are \\(S\\) sites, with a most one individual reproducing at each. We census the population at the time of reproduction. A reproducing individual has a large number \\(B\\) offspring and then dies. A fraction \\(d\\) of those offspring disperse and a fraction \\(1-c\\) of those survive. The survivors then equally divided among all sites. One individual in each site is then chosen at random to reproduce, which begins the life-cycle anew.</p> <p>The question is, how should dispersal, \\(d\\), evolve? There is a cost, \\(c\\), which selects against dispersal but dispersal also allows offspring to avoid competiting with their kin, which could select for more dispersal. We'll use evolutionary invasion analysis to sort this out.</p> <p></p>"},{"location":"lectures/lecture-18/#1-invasion-fitness","title":"1. Invasion fitness","text":"<p>Let's think about this analysis very generally (in discrete time).</p> <p>Let the number of individuals with the resident trait value be \\(n\\) and the number of individuals with the mutant trait value \\(n_m\\). (And we'll assume asexual reproduction for simplicity, so that residents produce residents and mutant produce mutants.) Let the potentially nonlinear dynamics of these two groups of individuals depend on their respective trait values, \\(z\\) and \\(z_m\\),</p> \\[ \\begin{aligned} n(t+1) &amp;= n(t) R(n(t), n_m(t), z, z_m)\\\\ n_m(t+1) &amp;= n_m(t) R_m(n(t), n_m(t), z, z_m) \\end{aligned} \\] <p>The Jacobian of this system is</p> \\[ \\begin{aligned} \\mathbf{J} &amp;=  \\begin{pmatrix} \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)} &amp; \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n_m(t)} \\\\ \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n(t)} &amp; \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)}  \\end{pmatrix} \\end{aligned} \\] <p>Now consider some non-zero resident equilibrium, \\(\\hat{n}&gt;0\\), without the mutant, \\(\\hat{n}_m=0\\). Assuming that the resident does not produce mutants, \\(\\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n(t)}\\big|_{n_m=0}=0\\), the Jacobian evaluated at this equilibrium simplifies to</p> \\[ \\begin{aligned} \\mathbf{J}\\big|_{n_m=0,n=\\hat{n}} &amp;=  \\begin{pmatrix} \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)} &amp; \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n_m(t)} \\\\ 0 &amp; \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)}  \\end{pmatrix}_{n_m=0,n=\\hat{n}} \\end{aligned} \\] <p>We can immediately see that the two eigenvalues of this upper triangular matrix are</p> \\[ \\begin{aligned} \\lambda_1 &amp;= \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)}\\bigg|_{n_m=0,n=\\hat{n}}\\\\ \\lambda_2 &amp;= \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)}\\bigg|_{n_m=0,n=\\hat{n}} \\end{aligned} \\] <p>The first, \\(\\lambda_1\\), determines whether the resident equilibrium, \\(\\hat{n}&gt;0\\), is stable in the absence of mutants. We'll take \\(0 &lt; \\lambda_1 &lt; 1\\) as a given (we're only interested in stable resident equilibria, we can disregard the others).</p> <p>The second, \\(\\lambda_2\\), determines whether the resident equilibrium is stable in the presence of a small number of mutants. We call \\(\\lambda_2\\) the invasion fitness, \\(\\lambda(z_m,z)\\). The mutant will invade whenever \\(\\lambda(z_m,z) &gt; 1\\).</p> <p>In some simple cases we might be able to use the invasion criterium, \\(\\lambda(z_m,z)&gt;1\\), to determine what values of \\(z_m\\) (relative to \\(z\\)) can invade. In most cases, however, \\(\\lambda(z_m,z)\\) will be complex enough that this will not be possible, so we rely on a simple approximation.</p> <p>The evolution of dispersal</p> <p>Let a fraction \\(d\\) of the resident offspring disperse and a fraction \\(d_m\\) of the mutant offspring disperse. And let there be \\(n(t)\\) residents, \\(n_m(t)\\) mutants, and \\(S-n(t)-n_m(t)\\geq0\\) empty sites. Then the probability a resident offspring replaces a resident is the number of resident offspring in a resident patch divided by the total number of offspring in that patch,</p> \\[ \\begin{aligned} p_{rr} &amp;= \\frac{B(1-d) + (n(t)-1)Bd(1-c)/S}{B(1-d) + (n(t)-1)Bd(1-c)/S + n_m(t)Bd_m(1-c)/S}\\\\ &amp;= \\frac{S(1-d) + (n(t)-1)d(1-c)}{S(1-d) + (n(t)-1)d(1-c) + n_m(t)d_m(1-c)} \\end{aligned} \\] <p>Here \\(B(1-d)\\) is the number of non-dispersing offspring produced by the resident in that patch, \\((n(t)-1)Bd(1-c)/S\\) is the number of resident offspring dispersing to the patch from elsewhere, and \\(n_m(t)Bd_m(1-c)/S\\) is the number of mutant offspring dispersing to the patch. The probability that a mutant offspring replaces a mutant \\(p_{mm}\\) is the same expression with \\(d\\) and \\(d_m\\) and \\(n\\) and \\(n_m\\) exchanged. The probability that a resident offspring wins an empty patch is </p> \\[ \\begin{aligned} p_{re} &amp;= \\frac{n(t)Bd(1-c)}{n(t)Bd(1-c) + n_m(t)Bd_m(1-c)}\\\\ &amp;= \\frac{n(t)d}{n(t)d + n_m(t)d_m} \\end{aligned} \\] <p>Then the number of resident and mutant individuals in the next generation are</p> \\[ \\begin{aligned} n(t+1) &amp;= n(t) p_{rr} + n_m(t) (1 - p_{mm}) + (S - n(t) - n_m(t)) p_{re}\\\\ n_m(t+1) &amp;= n(t) (1-p_{rr}) + n_m(t) p_{mm} + (S - n(t) - n_m(t)) (1-p_{re}) \\end{aligned} \\] <p>Now consider an equilibrium with no mutants, \\(\\hat{n}_m=0\\). Setting \\(n(t+1) = n(t) = \\hat{n}\\) gives \\(\\hat{n}=S\\).</p> <p>We determine the stability of this equilibrium with the Jacobian. From the general analysis above we know the two eigenvalues are</p> \\[ \\begin{aligned} \\lambda_1 &amp;= \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)}\\bigg|_{n_m=0,n=S} = 0\\\\ \\lambda_2 &amp;= \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)}\\bigg|_{n_m=0,n=S} = \\frac{1-d_m}{1-d_m + d(1-c)} + \\frac{d_m(1-c)}{1-d + d(1-c)} \\end{aligned} \\] <p>The first eigenvalue is less than 1 in absolute value, meaning the resident equilibrium is stable. The second eigenvalue is our invasion fitness</p> \\[ \\lambda(d_m,d) = \\frac{1-d_m}{1-d_m + d(1-c)} + \\frac{d_m(1-c)}{1-d + d(1-c)} \\] <p>In this relatively simple example, we can determine exactly what mutant trait values can invade by looking at where</p> \\[ \\lambda(d_m,d) - 1 = \\frac{(1-c)(d_m-d)(1-cd-d_m)}{(1-cd)(1-d_m+d(1-c))} \\] <p>is positive. The sign of \\(\\lambda(d_m,d) - 1\\) is the sign of \\((d_m-d)(1-cd-d_m)\\). So the mutant will invade if both terms are positive \\(d&lt;d_m&lt;1-cd\\) or both terms are negative \\(1-cd&lt;d_m&lt;d\\). In either case, the mutant will invade when it has a trait value between \\(1-cd\\) and \\(d\\).</p> <p></p>"},{"location":"lectures/lecture-18/#2-evolutionarily-singular-strategies","title":"2. Evolutionarily singular strategies","text":"<p>When the mutant trait value is very close to the resident trait value, we can approximate invasion fitness with a Taylor series around \\(z_m = z\\)</p> \\[ \\begin{aligned} \\lambda(z_m,z) &amp;\\approx \\lambda(z,z) + \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=z} (z_m-z)\\\\ &amp;= 1 + \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=z} (z_m-z) \\end{aligned} \\] <p>This allows us to determine which direction evolution will proceed from the current resident value:</p> <ul> <li>if \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}&gt;0\\) then invasion when \\(z_m&gt;z\\)</li> <li>if \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}&lt;0\\) then invasion when \\(z_m&lt;z\\)</li> </ul> <p>The direction of evolution by small steps is given by \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}\\), which we call the selection gradient.</p> <p>Potential evolutionary endpoints, also called evolutionarily singular strategies, are the resident trait values \\(z=\\hat{z}\\) where there is no directional selection</p> \\[ \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=\\hat{z}, z=\\hat{z}} = 0 \\] <p>The evolution of dispersal</p> <p>The selection gradient is</p> \\[ \\frac{\\partial \\lambda}{\\partial d_m}\\Big|_{d_m=d} = \\frac{(1-d-dc))(1-c)}{(1-cd)^2} \\] <p>The sign of this is the sign of 1-d-dc, meaning the dispersal rate will increase whenever d&lt;1/(1+c) and decrease whenever 1/(1+c)&lt;d. Setting the selection gradient to zero and solving for the evolutionarily singular strategy gives \\(\\hat{d} = 1/(1+c)\\).</p> <p></p>"},{"location":"lectures/lecture-18/#3-evolutionarily-stable-strategies","title":"3. Evolutionarily stable strategies","text":"<p>An evolutionarily singular strategy, \\(\\hat{z}\\), will only be an evolutionarily stable strategy (ESS), \\(z^*\\), if it cannot be invaded.</p> <p>Global evolutionary stability will be impossible to prove for most models and so we often focus on local stability, which requires that \\(\\lambda(z_m,z)|_{z=\\hat{z}}\\) is concave at \\(z_m=\\hat{z}\\) (i.e., \\(\\hat{z}\\) is a local fitness maximum),</p> \\[ \\frac{\\partial^2 \\lambda}{\\partial z_m^2}\\bigg|_{z_m=\\hat{z}, z=\\hat{z}} &lt; 0 \\] <p>The evolution of dispersal</p> <p>The second derivative of invasion fitness with respect to the mutant trait value evalulated at the singular strategy is</p> \\[ \\frac{\\partial^2 \\lambda}{\\partial d_m^2}\\Big|_{d_m=d=\\hat{d}} = -2(1-c)(1+c)^2 \\] <p>Because \\(0&lt;c&lt;1\\) this is always negative, which means the singular strategy is always evolutionarily stable.</p> <p></p>"},{"location":"lectures/lecture-18/#4-evolutionary-convergence","title":"4. Evolutionary convergence","text":"<p>There is one more characteristic of evolutionarily singular strategies that we care about, and that is whether evolution actually leads to that strategy or not. For evolution to move the trait value towards a singular strategy, \\(\\hat{z}\\), we need evolution to increase the trait value when it is less than \\(\\hat{z}\\) and decrease the trait value when it is greater than \\(\\hat{z}\\). In other words, we need the selection gradient \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}\\) to decrease as we move through \\(z=\\hat{z}\\)</p> \\[ \\frac{\\mathrm{d}}{\\mathrm{d} z}\\left( \\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z} \\right)_{z=\\hat{z}} &lt; 0 \\] <p>Singular strategies that satisfy this criteria are said to be convergence stable. </p> <p>Interestingly, not all evolutionarily stable strategies are convergence stable and not all convergence stable singular strategies are evolutionarily stable! Evolutionarily stable strategies that are not convergence stable are called Garden of Eden strategies. Singular strategies that are convergence stable but not evolutionarily stable are called evolutionary branching points. The latter are of particular interest because the system evolves towards a state where multiple strategies can invade and coexist, leading to diversification.</p> <p>The evolution of dispersal</p> <p>The derivative of the selection gradient evaluated at the singular strategy is</p> \\[ \\frac{\\mathrm{d}}{\\mathrm{d} d}\\left( \\frac{\\partial \\lambda}{\\partial d_m}\\Big|_{d_m=d} \\right)_{d=\\hat{d}} = -(1-c)(1+c)^3 \\] <p>Because \\(0&lt;c&lt;1\\) this is always negative, which means the singular strategy is always convergence stable.</p> <p></p>"},{"location":"lectures/lecture-18/#5-pairwise-invasibility-plots","title":"5. Pairwise invasibility plots","text":"<p>A helpful way to visualize the two types of stability of an evolutionarily singular strategy is called a pairwise invasibility plot (PIP). In this plot we have the resident trait value \\(z\\) on the x-axis, the mutant trait value \\(z_m\\) on the y-axis, and we color in the regions where the mutant can invade, \\(\\lambda(z_m,z)&gt;1\\).</p> <p>The four types of evolutionarily singular strategies \\(\\hat{z}\\) are then represented by the following PIPs</p> <pre>\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# dummy invasion fitness\ndef inv(x, y, slope, z=1):\n    if y &lt; x:\n      if y &gt; slope * x:\n        return z\n    if y &gt; x:\n      if y &lt; slope * x:\n        return z\n    return 1-z\n\n# evaluate\ndef compute_pip(slope=-2,z=1,xmin=-2,xmax=2,steps=100):\n    xs = np.linspace(xmin,xmax,steps)\n    X,Y = np.meshgrid(xs,xs) # X and Y values\n    # store the invasion success in a matrix\n    PIP = []\n    for y in xs:\n      row = []\n      for x in xs:\n        row.append(inv(x,y,slope,z))\n      PIP.append(row)\n    return X,Y,PIP\n\n# plot\ndef plotfun(X,Y,Z,slope=1,ax=None):\n    if ax==None:\n      fig, ax=plt.subplots(1,1,figsize=(5,5))\n    ax.contourf(X,Y,Z, colors=['white','black','blue','black'])\n    ax.plot(X[0],X[0],'k',lw=5)\n    ax.plot(X[0],X[0]*slope,'k',lw=5)\n    ax.set_xlim(min(X[0]),max(X[0]))\n    ax.set_ylim(min(X[0]),max(X[0]))\n    ax.set_xlabel('resident trait value $z$')\n    ax.set_ylabel('mutant trait value $z_m$')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig, axs = plt.subplots(2,2,figsize=(10,10))\nX,Y,Z = compute_pip(slope=-4,z=1)\nplotfun(X,Y,Z,slope=-4,ax=axs[0][0])\naxs[0][0].set_title('evolutionarily stable',fontsize=14)\naxs[0][1].text(2.1,0,'convergence stable',fontsize=14,rotation=270,verticalalignment='center')\nX,Y,Z = compute_pip(slope=4,z=0)\nplotfun(X,Y,Z,slope=4,ax=axs[0][1])\naxs[0][1].set_title('evolutionarily unstable',fontsize=14)\nX,Y,Z = compute_pip(slope=4,z=1)\nplotfun(X,Y,Z,slope=4,ax=axs[1][0])\naxs[1][1].text(2.1,0,'convergence unstable',fontsize=14,rotation=270,verticalalignment='center')\nX,Y,Z = compute_pip(slope=-4,z=0)\nplotfun(X,Y,Z,slope=-4,ax=axs[1][1])\n</pre> <p></p> <p>We can read a PIP by choosing a resident trait value (a point on the x-axis) and looking to see what mutant trait values can invade it (blue regions in that vertical slice). Choose one of the possible invading trait values and set this to be the new resident trait value. Continue indefinitely. </p> <p>When we assume mutants have trait values close to the resident, we restrict ourselves to moving along the 1:1 line. Then, we move to the right when there is blue directly above the 1:1 line but not below and we move to the left when there is blue directly below the 1:1 line but not above. Where there is blue directly above and below the 1:1 we are at a singular strategy that is a fitness minimum (it can be invaded in both directions). Where there is white directly above and below the 1:1 we are at a singular strategy that is a fitness maximum (it can't be invaded in either direction).</p> <p>Try reading each of the plots above. Prove to yourself that the top left has a convergence stable evolutionarily stable strategy, the top right has a branching point, the bottom left has a has a Garden of Eden, and the bottom right has an invasible repellor (a fitness minimum that is not convergence stable).</p> <p>The evolution of dispersal</p> <p>In our model of the evolution of dispersal, the singular strategy is always evolutionarily and convgence stable, and the PIP is below.</p> <pre>\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# invasion fitness\ndef inv_fun(dm,d,c):\n    invasionfitness = (1-dm)/(1-dm + d*(1-c)) + (dm*(1-c))/(1-d + d*(1-c))\n    if invasionfitness&gt;1:\n        return 1 # return 1 if mutant invades\n    return 0 # return 0 if mutant does not invade\n\n# evaluate\ndef compute_pip(c=0.5,dmin=0.01,dmax=0.99,steps=100):\n    ds = np.linspace(dmin,dmax,steps)\n    X,Y = np.meshgrid(ds,ds) # X and Y values\n    # store the invasion success in a matrix\n    PIP = []\n    for dm in ds:\n      row = []\n      for d in ds:\n        row.append(inv_fun(dm,d,c))\n      PIP.append(row)\n    return X,Y,PIP\n\n# plot\ndef plotfun(X,Y,Z):\n    fig, ax = plt.subplots(1,1,figsize=(5,5))\n    ax.set_xlabel('$d$')\n    ax.set_ylabel('$d_m$')\n    ax.contourf(X,Y,Z, colors=['white','black','blue','black'])\n    ax.plot(X[0],X[0],'k',lw=5)\n\nX,Y,Z=compute_pip()\nplotfun(X,Y,Z)\n</pre> <p></p>"},{"location":"lectures/lecture-19/","title":"Lecture 19","text":""},{"location":"lectures/lecture-19/#lecture-19-the-evolution-of-dominance","title":"Lecture 19: The evolution of dominance","text":"Run notes interactively?"},{"location":"lectures/lecture-19/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Model</li> <li>Resident equilibrium</li> <li>Mutant invasion</li> </ol> <p>So far we've considered invasion into an unstructured population, i.e., there is just one type of resident and one type of mutant. We can extend this analysis to consider structured populations, age- or spatial-structure for example. The general approach is the same: find a stable resident equilibria and evaluate the Jacobian at that equilibrium to determine the mutant's invasion fitness. The only difference is that our Jacobian is no longer a 2x2 matrix. </p> <p>Below we will demonstrate the general method with a specific example: the evolution of dominance. The motivating observation is that many deleterious alleles are recessive, meaning the wild-type allele partially shields a heterozygote individual from selection. There are many examples in humans, such as sickle cell anemia and cystic fibrosis. In these examples the fitness of heterozygous individuals is indistinguishable from those without any deleterious mutations. Could the dominance of a wild-type allele over a deleterious allele be the result of adaptive evolution? To ask this question we perform an evolutionary invasion analysis on a population with genetic structure. This type of model is often referred to as a modifier model within the field of population genetics.</p> <p></p>"},{"location":"lectures/lecture-19/#1-model","title":"1. Model","text":"<p>The model is an extension of the 2-locus model we developed in lecture 17 to include mutation. We consider two loci each with two alleles. At one locus we have alleles \\(A_1\\) and \\(A_2\\). We'll treat \\(A_1\\) as the wild-type allele and \\(A_2\\) as the deleterious allele and refer to this \\(A\\) locus as the selected locus. At the other locus we have alleles \\(B_1\\) and \\(B_2\\). We'll treat \\(B_1\\) as the resident allele and \\(B_2\\) as the mutant (modifier) and refer to this \\(B\\) locus as the modifier locus. The analysis will determine when \\(B_2\\) can invade. </p> <p>We consider diploid selection. Let the relative fitness of any individual with \\(A_1 A_1\\) be 1 and the relative fitness of any individual with \\(A_2 A_2\\) be \\(1-s\\), with \\(0&lt;s&lt;1\\). The relative fitnesses of the \\(A_1 A_2\\) heterozygotes are affected by their genotype at the \\(B\\) locus:</p> diploid genotype relative fitness \\(A_1A_2\\) \\(B_1B_1\\) \\(1 - h_{11}s\\) \\(A_1A_2\\) \\(B_1B_2\\) \\(1 - h_{12}s\\) \\(A_1A_2\\) \\(B_2B_2\\) \\(1 - h_{22}s\\) <p>We census the population in the haploid phase and keep track of the 4 haploid genotype frequencies:</p> genotype frequency \\(A_1B_1\\) \\(x_1\\) \\(A_1B_2\\) \\(x_2\\) \\(A_2B_1\\) \\(x_3\\) \\(A_2B_2\\) \\(x_4\\) <p>with the constraint that the total frequency sums to one, \\(x_1+x_2+x_3+x_4 = 1\\).</p> <p>We treat time as discrete and assume the following life-cycle:</p> <p> <pre><code>graph LR;\n    A((census)) --&gt; B((gamete union));\n    B --&gt; C((selection));\n    C --&gt; D((meiosis/recombination));    \n    D --&gt; E((mutation));\n    E --&gt; A</code></pre> </p> <p>After selection but before mutation the frequencies are as they were in lecture 17,</p> \\[\\begin{align} x_1' &amp;= x_1(t) \\sum_{i=1}^{4} x_i(t) w_{1i}/\\bar{w} - r D^* \\\\ x_2' &amp;= x_2(t) \\sum_{i=1}^{4} x_i(t) w_{2i}/\\bar{w} + r D^* \\\\ x_3' &amp;= x_3(t) \\sum_{i=1}^{4} x_i(t) w_{3i}/\\bar{w} + r D^* \\\\ x_4' &amp;= x_4(t) \\sum_{i=1}^{4} x_i(t) w_{4i}/\\bar{w} - r D^*, \\end{align}\\] <p>where \\(w_{ij}=w_{ji}\\) is the fitness of the diploid that is composed of haploid genotypes \\(i\\) and \\(j\\), \\(\\bar{w}\\) is the population mean fitness, and \\(D^*=x_1(t)x_4(t) w_{14}/\\bar{w} - x_2(t)x_3(t) w_{23}/\\bar{w}\\) is linkage disequilibrium after selection.</p> <p>We assume \\(A_1\\) mutates to \\(A_2\\) with probability \\(\\mu\\) and ignore back mutations (a relatively safe assumption given that the deleterious \\(A_2\\) allele will be rare and therefore will create few mutants). The recursion equations are then</p> \\[\\begin{align} x_1(t+1) &amp;= (1-\\mu)x_1'\\\\ x_2(t+1) &amp;= (1-\\mu)x_2'\\\\ x_3(t+1) &amp;= x3' + \\mu x_1'\\\\ x_4(t+1) &amp;= x4' + \\mu x_2'. \\end{align}\\] <p></p>"},{"location":"lectures/lecture-19/#2-resident-equilibrium","title":"2. Resident equilibrium","text":"<p>We cannot find all equilibria of this system of nonlinear equations, but we can find some. Here we are most interested in the \"resident\" equilibrium where there are no \\(B_2\\) alleles, \\(\\hat{x}_2=\\hat{x}_4=0\\), which reduces us back to a one locus model. We'll also take \\(h_{11}=1/2\\) for simplicity, meaning that the resident \\(B_1\\) allele makes the \\(A\\) locus additive. Then there is a relatively simple resident equilibrium, </p> \\[ \\begin{align} \\hat{x}_1 = 1 - \\frac{\\mu}{(1+\\mu)s/2} \\\\ \\hat{x}_3 = \\frac{\\mu}{(1+\\mu)s/2}, \\end{align} \\] <p>which says that the deleterious \\(A_2\\) allele is maintained at a balance between its removal by selection and its generation by mutation. This equilibrium is valid as long as \\(\\mu&lt;(1+\\mu)s/2\\), which means mutation is weak relative to selection.</p> <p></p>"},{"location":"lectures/lecture-19/#3-mutant-invasion","title":"3. Mutant invasion","text":"<p>To evaluate the stability of this equilibrium we use the Jacobian evaluated at the resident equilibrium. Because we have 4 equations this is a 4x4 matrix. But if we arrange the equations in the following order, \\(x_1(t+1), x_3(t+1), x_2(t+1), x_4(t+1)\\), then the Jacobian at the resident equilibrium can be written as an upper triangular block matrix, </p> \\[ \\mathbf{J}|_{x_1=\\hat{x}_1,x_3=\\hat{x}_3,x_3=x_4=0} = \\begin{bmatrix} \\mathbf{J}_\\mathrm{res} &amp; \\mathbf{V} \\\\ \\mathbf{0} &amp; \\mathbf{J}_\\mathrm{mut}\\end{bmatrix}. \\] <p>Here \\(\\mathbf{J}_\\mathrm{res}\\) describes the stability of the resident equilibrium in the absence of any \\(B_2\\) alleles and \\(\\mathbf{J}_\\mathrm{mut}\\) describes the stability of the resident equilibrium in the face of rare \\(B_2\\) alleles. One can show that the eigenvalues of \\(\\mathbf{J}_\\mathrm{res}\\) are always less than 1 when the resident equilibrium is valid, guaranteeing stability. </p> <p>We want to know whether \\(B_2\\) alleles can invade the resident equilibrium. This is determined by the leading eigenvalue of \\(\\mathbf{J}_\\mathrm{mut}\\), which we call the invasion fitness. Unfortunately the eigenvalues are a little complicated, but we can make some progress with a little trick. The equation for the eigenvalues of \\(\\mathbf{J}_\\mathrm{mut}\\) is </p> \\[ \\lambda^2 - \\mathrm{Tr}(\\mathbf{J}_\\mathrm{mut}) \\lambda + \\mathrm{Det}(\\mathbf{J}_\\mathrm{mut}) = 0. \\] <p>Now, given that \\(\\lambda\\) is the invasion fitness of a mutant with dominance coefficient \\(h_{12}\\) in a resident population with dominance coefficient \\(h_{11}=1/2\\), the selection gradient is \\(\\frac{\\partial\\lambda}{\\partial h_{12}}|_{h_{12}=1/2}\\). The trick is that we can get the selection gradient directly from the equation above without solving for invasion fitness. To make the dependence on \\(h_{12}\\) clear and simplify the notation, write \\(\\lambda=\\lambda(h_{12})\\), \\(-\\mathrm{Tr}(\\mathbf{J}_\\mathrm{mut})=a(h_{12})\\), and \\(\\mathrm{Det}(\\mathbf{J}_\\mathrm{mut})=b(h_{12})\\). We can then differentiate the equation above with respect to \\(h_{12}\\) and rearrange for the selection gradient (this is called implicit differentiation),</p> \\[ \\begin{align} \\frac{\\partial \\lambda^2}{\\partial h_{12}} + \\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) + a(h_{12}) \\frac{\\partial \\lambda}{\\partial h_{12}} + \\frac{\\partial b}{\\partial h_{12}} &amp;= 0 \\\\ 2\\lambda(h_{12})\\frac{\\partial \\lambda}{\\partial h_{12}} + \\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) + a(h_{12}) \\frac{\\partial \\lambda}{\\partial h_{12}} + \\frac{\\partial b}{\\partial h_{12}} &amp;= 0 \\\\ \\frac{\\partial \\lambda}{\\partial h_{12}}(2\\lambda(h_{12}) + a(h_{12})) &amp;= - \\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) - \\frac{\\partial b}{\\partial h_{12}} \\\\ \\frac{\\partial \\lambda}{\\partial h_{12}} &amp;= - \\frac{\\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) + \\frac{\\partial b}{\\partial h_{12}}}{2\\lambda(h_{12}) + a(h_{12})} \\end{align} \\] <p>We then evaluate at \\(h_{12}=1/2\\), which is where the mutant is equivalent to the resident, i.e., \\(\\lambda(1/2)=1\\),</p> \\[ \\begin{align} \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &amp;= - \\left(\\frac{\\frac{\\partial a}{\\partial h_{12}}\\lambda(h_{12}) + \\frac{\\partial b}{\\partial h_{12}}}{2\\lambda(h_{12})+a(h_{12})}\\right)_{h_{12}=1/2} \\\\ \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &amp;= - \\left(\\frac{\\left.\\frac{\\partial a}{\\partial h_{12}}\\right|_{h_{12}=1/2}\\lambda(1/2) + \\left.\\frac{\\partial b}{\\partial h_{12}}\\right|_{h_{12}=1/2}}{2\\lambda(1/2)+a(1/2)}\\right) \\\\ \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &amp;= - \\left(\\frac{\\left.\\frac{\\partial a}{\\partial h_{12}}\\right|_{h_{12}=1/2} + \\left.\\frac{\\partial b}{\\partial h_{12}}\\right|_{h_{12}=1/2}}{2+a(1/2)}\\right) \\\\ \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &amp;= - \\frac{2(4r + s(1-2r))(s(1+\\mu)-2\\mu)\\mu}{s(s(1+\\mu) - 2\\mu + r(2-s)(1-\\mu^2))}. \\end{align} \\] <p>Given biological validity, \\(\\mu&lt;(1+\\mu)s/2\\), this is always negative (recall that \\(r\\leq 1/2\\)). Therefore there is always selection to reduce the dominance coefficient, \\(h\\). This means that selection could be responsible for making deleterious mutations recesive! However, there is a massive caveat. While the selection gradient is negative it is also proportional to \\(\\mu\\), which is tiny. This means that selection for recessive deleterious mutations is exceptionaly weak and so is easily overwhelmed by other forces (eg, genetic drift, migration, etc). The biological reason for such weak selection is that selection only acts on the variation that is present, and deleterious mutations are very rare at mutation-selection balance, \\(\\mu/((1+\\mu)s/2)\\).  </p>"},{"location":"lectures/lecture-20/","title":"Lecture 20","text":""},{"location":"lectures/lecture-20/#lecture-20-probability-i-genetic-drift","title":"Lecture 20: Probability I (genetic drift)","text":"Run notes interactively?"},{"location":"lectures/lecture-20/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Discrete random variables</li> <li>Expectation of a discrete random variable</li> <li>Variance of a discrete random variable</li> <li>Independence</li> <li>Binomial random variable</li> <li>Genetic drift</li> </ol> <p>credits</p> <p>This lecture was created by PhD student Puneeth Deraje as part of a course development TA position -- thanks Puneeth! If you are following along with the text, this lecture does not follow along as closely.</p> <p>Until now we have been dealing with deterministic models, i.e., given the value of the variables at time \\(t\\), we know exactly what the values will be at the next time. However, life is a little bit more random than that. Stochasticity (i.e., chance) is inherent in nature and can significantly alter the outcomes. In order to capture the effect of this stochasticity, we need some tools from probability theory.</p> <p></p>"},{"location":"lectures/lecture-20/#1-discrete-random-variables","title":"1. Discrete random variables","text":""},{"location":"lectures/lecture-20/#example-1-a-fair-coin-toss","title":"Example 1: a fair coin toss","text":"<p>Let's toss a toonie. If you get a heads, you use that toonie to buy a chocolate, otherwise you put the toonie in a piggy bank for grad school :P. Let \\(X\\) denote the number of chocolates you have after the coin toss. Then \\(X\\) can be equal to 0 or 1.  </p> <p>In this case, tossing the coin is called an event and \\(X\\) is the random variable that records its outcome. The state space is the set of all outcomes, {0,1}. We call \\(X\\) a discrete random variable because there are a finite number of outcomes in the state space.</p> <p>We don't know the exact value of \\(X\\) until you actually toss the coin (i.e., until the event happens) -- we can not predict the outcome of the coin toss in that way. However, what we do know is that with a fair coin there is an equal chance of getting either a heads or tails. In other words, if we were to toss the coin a large number of times, say a million times, roughly half a million times we will see tails and the other half a million times we will see heads. </p> <p>Therefore, we say that \\(X\\) takes the value 0 with probability \\(\\frac{500000}{1000000}=0.5\\) and 1 with probability \\(\\frac{500000}{1000000}=0.5\\). We can write this as </p> \\[\\begin{aligned} \\Pr(X=0) &amp;= 0.5\\\\ \\Pr(X=1) &amp;= 0.5  \\end{aligned}\\] <p>Note that the sum of the probabilities of a random variable taking a value, over all values in the state space, is 1. Here, \\(\\Pr(X=0) + \\Pr(X=1) = 1\\).</p> <p>Note</p> <p>This way of thinking about the probability of a random variable \\(X\\) taking a value \\(x\\), as the fraction (frequency) of events where \\(X=x\\), is the frequentist interpretation of probability. In order for the frequency to match the probability you need to repeat the event a large number of times (ideally infinitely). </p>"},{"location":"lectures/lecture-20/#example-2-a-biased-coin-toss","title":"Example 2: a biased coin toss","text":"<p>Consider the same situation as above but with a biased toonie, i.e., one that shows heads in not half but in a fraction \\(p\\) of a very large number of events. Then</p> \\[\\begin{aligned} \\Pr(X=0) &amp;= 1-p \\\\ \\Pr(X=1) &amp;= p \\\\ \\end{aligned}\\] <p>This random variable \\(X\\) is called a Bernoulli random variable with parameter \\(p\\) and is denoted as \\(X\\sim\\mathrm{Ber}(p)\\). Above, with a fair coin, \\(X\\sim\\mathrm{Ber}(1/2)\\).</p>"},{"location":"lectures/lecture-20/#example-3-a-fair-die-roll","title":"Example 3: a fair die roll","text":"<p>Now suppose you roll a fair 6-sided die to decide how many chocolates you will buy (this is nice because you never get 0!). Then, if \\(X\\) denotes the number of chocolates, we have </p> \\[\\begin{aligned} \\Pr(X=1) &amp;= \\frac{1}{6}\\\\ \\Pr(X=2) &amp;= \\frac{1}{6} \\\\ \\Pr(X=3) &amp;= \\frac{1}{6}  \\\\ \\Pr(X=4) &amp;= \\frac{1}{6} \\\\ \\Pr(X=5) &amp;= \\frac{1}{6}  \\\\ \\Pr(X=6) &amp;= \\frac{1}{6}   \\end{aligned}\\] <p>The state space here is {1,2,3,4,5,6} and \\(\\sum_{x=1}^6 \\Pr(X=x) = 1\\).</p> <p></p>"},{"location":"lectures/lecture-20/#2-expectation-of-a-discrete-random-variable","title":"2. Expectation of a discrete random variable","text":"<p>Given this way of thinking about stochasticity, one might be interested in questions like \"What is the value of \\(X\\), on average?\" and \"How certain can I be about the outcome of \\(X\\)?\". </p> <p>In order to answer these questions we need to define two important properties of a random variable, its expectation and variance.</p> <p>The expectation of a random variable is the average value of outcomes if the event is repeated infinitely many times. </p>"},{"location":"lectures/lecture-20/#example-1-a-fair-coin-toss_1","title":"Example 1: a fair coin toss","text":"<p>Suppose we toss the fair coin a million times. By the frequentist definition of probability we should see tails (\\(X=0\\)) half a million times and heads (\\(X=1\\)) the other half a million times. The expected value of \\(X\\) is then</p> \\[\\begin{aligned} \\mathbb{E}(X) &amp;= \\frac{\\overbrace{0+0+...+0}^{\\text{0.5 million times}}+\\overbrace{1+1+...+1}^{\\text{0.5 million times}}}{\\text{1 million}}\\\\ &amp;= \\frac{ 0 \\times \\text{0.5 million} + 1 \\times \\text{0.5 million} }{\\text{1 million}}\\\\ &amp;= 0 \\times \\frac{\\text{0.5 million}}{\\text{1 million}} + 1 \\times \\frac{\\text{0.5 million}}{\\text{1 million}}\\\\ &amp;= 0 \\times (1/2) + 1 \\times (1/2) \\\\ &amp;= 0.5 \\end{aligned}\\]"},{"location":"lectures/lecture-20/#example-2-a-biased-coin-toss_1","title":"Example 2 : a biased coin toss","text":"<p>Suppose we repeat the whole procedure for a biased coin, i.e., \\(X\\sim\\mathrm{Ber}(p)\\). Then</p> \\[\\begin{aligned} \\mathbb{E}(X) &amp;= \\frac{\\overbrace{0+0+...+0}^{(1-p) \\text{ million times}}+\\overbrace{1+1+...+1}^{p \\text{ million times}}}{\\text{1 million}} \\\\ &amp;= 0 \\times (1-p) + 1 \\times p \\\\ &amp;= p \\end{aligned}\\]"},{"location":"lectures/lecture-20/#example-3-a-fair-die-roll_1","title":"Example 3: a fair die roll","text":"<p>If we do the same for a fair 6-sided die,</p> \\[\\begin{aligned} \\mathbb{E}(X)  &amp;= \\frac{\\overbrace{1+1+...+1}^{(1/6) \\text{ million times}} + \\overbrace{2+2+...+2}^{(1/6) \\text{ million times}} + \\overbrace{3+3+...+3}^{(1/6) \\text{ million times}} + \\overbrace{4+4+...+4}^{(1/6) \\text{ million times}} + \\overbrace{5+5+...+5}^{(1/6) \\text{ million times}} + \\overbrace{6+6+...+6}^{(1/6) \\text{ million times}}}{\\text{1 million}} \\\\ &amp;= 1\\times\\frac{1}{6} + 2\\times\\frac{1}{6} + 3\\times\\frac{1}{6} + 4\\times\\frac{1}{6} + 5\\times\\frac{1}{6} + 6\\times\\frac{1}{6}\\\\ &amp;= 3.5 \\end{aligned}\\] <p>Expectation</p> <p>In general, the expectation of a discrete random variable is \\(\\mathbb{E}(X) = \\sum_x x \\Pr(X=x)\\), where the sum is over the entire state space.</p>"},{"location":"lectures/lecture-20/#properties-of-expectation","title":"Properties of expectation","text":"<ol> <li>If \\(c\\) is a constant then \\(\\mathbb{E}(c)=c\\) and \\(\\mathbb{E}(cX)=c\\mathbb{E}(X)\\)</li> <li>If \\(X\\) and \\(Y\\) are two random variables then \\(\\mathbb{E}(X+Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\\)</li> <li>Suppose \\(X\\) is a random variable and \\(f\\) is any function then \\(\\mathbb{E}(f(X)) = \\sum_x f(x)P(X=x)\\). For example, let \\(f(x) = x^2\\) and \\(X\\sim\\mathrm{Ber}(p)\\) (e.g., a biased coin toss), then \\(\\mathbb{E}(f(X)) = \\sum_{x=0}^{1}x^2P(X=x)= 0^2(1-p)+1^2p = p\\).</li> </ol>"},{"location":"lectures/lecture-20/#3-variance-of-a-discrete-random-variable","title":"3. Variance of a discrete random variable","text":"<p>Suppose we have an extremely biased coin with \\(p=1\\). That is, the coin always shows heads. Can we, before tossing, say something about the outcome in this case? Yes, we know exactly what the outcome will be, heads. Similarly, if \\(p=0\\) we would know the exact outcome, tails. We say there is no uncertainity in either of these cases.</p> <p>Now, consider a slightly less biased coin with \\(p = 0.99\\). Although we can not say exactly what the outcome will be, it will very likely be heads. There is some small amount of uncertainity.</p> <p>Finally, suppose we toss a fair coin (i.e., \\(p = 0.5\\)). Then we have no idea at all what the outcome will be. There is very high uncertainity in this case.</p> <p>Variance</p> <p>The variance of a random variable \\(X\\), denoted \\(\\mathrm{Var}(X)\\), quantifies the uncertainity associated with the random variable and is given by</p> \\[\\begin{aligned} \\mathrm{Var}(X)  &amp;= \\mathbb{E}((X - \\mathbb{E}(X))^2 )\\\\ &amp;= \\mathbb{E}(X^2 - 2X\\mathbb{E}(X) +  \\mathbb{E}(X)^2)\\\\ &amp;= \\mathbb{E}(X^2) - \\mathbb{E}(2X\\mathbb{E}(X)) +  \\mathbb{E}(\\mathbb{E}(X)^2)\\\\ &amp;= \\mathbb{E}(X^2) - 2\\mathbb{E}(X)\\mathbb{E}(X) +  \\mathbb{E}(X)^2\\\\ &amp;= \\mathbb{E}(X^2) - \\mathbb{E}(X)^2 \\end{aligned}\\]"},{"location":"lectures/lecture-20/#example-a-biased-coin-toss","title":"Example: a biased coin toss","text":"<p>If \\(X\\sim\\mathrm{Ber}(p)\\) then</p> \\[\\begin{aligned}  \\mathrm{Var}(X)  &amp;= \\mathbb{E}(X^2) - \\mathbb{E}(X)^2 \\\\ &amp;= p - p^2 \\\\ &amp;= p(1-p) \\end{aligned}\\] <p>Below we plot the variance of \\(X\\) as a function of \\(p\\).</p> <pre>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\np_vals = np.arange(0,1,0.01)\nVar_values = [ p*(1-p) for p in p_vals ]\nplt.plot(p_vals, Var_values)\n\nplt.axvline(x=0.5,linestyle='dashed')\n\nplt.xlabel('$p$ = probability of getting a heads')\nplt.ylabel('Var($X$)')\nplt.show()\n</pre> <p></p> <p>Note that for \\(p=0\\) or \\(p=1\\) we have Var(\\(X\\)) = 0, i.e., there is no uncertainity. Further note that Var(\\(X\\)) = \\(p(1-p)\\) peaks at \\(p=0.5\\), i.e., the highest uncertainity is associated with the toss of a fair coin.</p>"},{"location":"lectures/lecture-20/#properties-of-variance","title":"Properties of variance","text":"<ol> <li>If \\(c\\) is a constant then Var\\((c)\\) = 0 </li> <li>If \\(a\\) and \\(b\\) are constants and \\(X\\) is a random variable then Var\\((aX + b)\\) = \\(a^2\\)Var\\((X)\\)</li> </ol>"},{"location":"lectures/lecture-20/#4-independence","title":"4. Independence","text":"<p>Let's now look at two random variables simultaneously.</p> <p>Let's play the same game as before. Toss a coin, if it's heads use the coin to buy a chocolate. If it's tails put the coin in the piggy bank. As before, let \\(X\\) be the number of chocolates after the toss, \\(X\\sim\\)Ber(\\(p\\)). This time we'll also keep track of how many coins we put in the piggy bank, \\(Y\\).</p> <p>Now if \\(X=0\\) then \\(Y=1\\) and if \\(X=1\\) then \\(Y=0\\). The value of \\(Y\\) depends on \\(X\\), and vice versa, so \\(X\\) and \\(Y\\) are not independent random variables.</p> <p>On the other hand, suppose you have two coins. You toss each coin and decide whether to use it to buy a chocolate. Let \\(X_1\\) be the outcome of tossing the first coin. Let \\(X_2\\) be the outcome of tossing the second coin. Then \\(X_1\\) does not depend on \\(X_2\\), and vice-versa, so \\(X_1\\) and \\(X_2\\) are independent random variables. </p> <p>Independence</p> <p>Two random variables \\(X\\) and \\(Y\\) are said to be independent if for every \\(x\\) and \\(y\\)</p> \\[\\Pr(X=x \\text{ and } Y=y) = \\Pr(X=x)\\Pr(Y=y)\\]"},{"location":"lectures/lecture-20/#properties-of-independent-random-variables","title":"Properties of independent random variables","text":"<p>If \\(X\\) and \\(Y\\) are independent random variables then</p> <ol> <li>\\(\\mathbb{E}(XY)=\\mathbb{E}(X)\\mathbb{E}(Y)\\)</li> <li>Var(\\(X+Y\\)) = Var(\\(X\\)) + Var(\\(Y\\))</li> </ol> <p></p>"},{"location":"lectures/lecture-20/#5-binomial-random-variable","title":"5. Binomial random variable","text":"<p>Let \\(X_1\\) and \\(X_2\\) be the outcome of two independent tosses of the same coin. We want know how many chocolates we have after two tosses, \\(X = X_1 + X_2\\). Recall that \\(X_1\\) and \\(X_2\\) are both Ber(\\(p\\)). Therefore, </p> \\[\\begin{aligned}  \\mathbb{E}(X) &amp;= \\mathbb{E}(X_1) + \\mathbb{E}(X_2)\\\\ &amp;= p + p \\\\ &amp;= 2p \\end{aligned}\\] <p>Since \\(X_1\\) and \\(X_2\\) are independent we also have</p> \\[\\begin{aligned}  \\mathrm{Var}(X)  &amp;= \\mathrm{Var}(X_1) +\\mathrm{Var}(X_2)\\\\ &amp;= p(1-p) + p(1-p)\\\\ &amp;= 2p(1-p) \\end{aligned}\\] <p>Now let's look at three special cases to see what this means. </p> <p>If \\(p=0\\) then \\(\\mathbb{E}(X) = 0\\) and \\(\\mathrm{Var}(X)=0\\). If the coin always shows tails then we know with certainity that we will have no chocolates after two tosses.</p> <p>Similarly, if \\(p=1\\) then \\(\\mathbb{E}(X) = 2\\) and \\(\\mathrm{Var}(X)=0\\). If the coin always shows heads then we know with certainity that we will have two chocolates after two tosses. </p> <p>Finally, if \\(p=0.5\\) then \\(\\mathbb{E}(X) = 1\\) and \\(\\mathrm{Var}(X)=1\\). With a fair coin we expect to have 1 chocolate but there is some uncertainity since we could end up with 0, 1, or 2.</p> <p>Binomial random variable</p> <p>In general we could have \\(n\\) coins. Let \\(X_i\\) denote the outcome of the \\(i^{\\mathrm{th}}\\) toss, which is Ber(\\(p\\)) and independent of all other tosses. Then the sum over all \\(n\\) Bernoulli random variables, \\(X = X_1 + X_2 + ... + X_n\\), is called a binomial random variable with parameters \\(n\\) and \\(p\\), and is denoted by Bin(\\(n,p\\)).</p> <p>The expectation of a binomial random variable is</p> \\[\\mathbb{E}(X) = np\\] <p>and its variance is</p> \\[\\mathrm{Var}(X) = np(1-p)\\] <p>Moreover, we can compute the probability of a binomial random variable \\(X\\sim\\)Bin(\\(n,p\\)) taking value \\(k\\), as </p> \\[P(X = k) = {n \\choose k} p^k (1-p)^{n-k}\\] <p>where \\({n \\choose k}\\) is read \"\\(n\\) choose \\(k\\)\" and is the number of ways of choosing \\(k\\) items from \\(n\\) options. Mathematically, </p> \\[\\begin{aligned} {n \\choose k}  &amp;= \\frac{n}{k} {n-1 \\choose k-1}\\\\ &amp;= \\frac{n}{k} \\frac{n-1}{k-1} {n-2 \\choose k-2}\\\\ &amp;= \\vdots\\\\ &amp;= \\frac{n (n-1) (n-2)\\cdots (n-k+1)}{k(k-1)(k-2)\\cdots 1}  \\end{aligned}\\] <p></p>"},{"location":"lectures/lecture-20/#6-genetic-drift","title":"6. Genetic drift","text":"<p>Recall our model of one-locus haploid selection from Lecture 4. If \\(p_t\\) is the frequency of allele \\(A\\) in the population at time \\(t\\) then we said that the frequency of \\(A\\) in the next generation is</p> \\[  p_{t+1} = \\frac{p_t W_A }{p_t W_A + (1-p_t)W_a}  \\] <p>where \\(W_A\\) is the fitness of \\(A\\) and \\(W_a\\) is the fitness of \\(a\\).</p> <p>One unspoken assumption here is that the population size is infinitely large (or at least sufficiently big). In reality, populations are not infinitely large and may even be quite small. This finiteness introduces stochasticity, which in evolution we call genetic drift. One well-known model that incorporates genetic drift is the Wright-Fisher model. This model assumes that there are \\(N\\) diploid individuals in generation \\(t\\), each of which produce an infinite number of haploid gametes, of which \\(2N\\) are randomly sampled to form the diploid individuals of generation \\(t+1\\). Here, with haploid selection, we assume the frequency of gametes changes due to selection before sampling for the next generation.</p> <pre><code>graph TD\n    A[N diploids at t] --&gt; B[Infinite haploids before selection];\n    B --&gt; C[Infinite haploids after selection];\n    C --&gt; D[N diploids at t+1];</code></pre> <p>So, let \\(p_t\\) be the frequency of allele \\(A\\) in generation \\(t\\). Each diploid individual contributes an infinite number of gametes, so the frequency of \\(A\\) in the gamete pool is also \\(p_t\\). Now selection acts on the gamete pool, altering the frequencies as in our original model, so that the frequency of \\(A\\) in the gamete pool is now \\(p_t' = \\frac{p_tW_A }{p_t W_A + (1-p_t)W_a}\\). We then sample \\(2N\\) gametes to form the next diploid generation. For each gamete we pick there is a probability \\(p'_t\\) that it has allele \\(A\\) and a probability \\(1-p'_t\\) that it has allele \\(a\\). Let \\(X\\) be the number of \\(A\\) alleles in a randomly chosen gamete. Then \\(X\\sim\\)Ber\\((p'_t)\\) and the total number of \\(A\\) alleles after \\(2N\\) samples, \\(n_A(t+1)\\), is the sum of \\(2N\\) Bernoulli random variables, implying \\(n_A(t+1)\\sim\\)Bin\\((2N,p'_t)\\). The frequency of allele \\(A\\) in the next generation is then</p> \\[\\begin{aligned}  p_{t+1}  &amp;= \\frac{n_A(t+1)}{2N}\\\\  &amp;= \\frac{\\text{Bin}(2N,p_t')}{2N} \\end{aligned}\\] <p>With this model we can no longer predict exactly what the frequency of \\(A\\) will be in the next generation. However, we can compute the expectation</p> \\[\\begin{aligned}  \\mathbb{E}(p_{t+1}) &amp;= \\mathbb{E}\\left(\\frac{\\text{Bin}(2N,p_t')}{2N}\\right)\\\\ &amp;= \\frac{1}{2N}\\mathbb{E}\\left(\\text{Bin}(2N,p_t')\\right)\\\\ &amp;= \\frac{2N p_t'}{2N}\\\\ &amp;= p_t'  \\end{aligned}\\] <p>and the variance</p> \\[\\begin{aligned}  \\text{Var}(p_{t+1})  &amp;= \\text{Var}\\left(\\frac{\\text{Bin}(2N,p_t')}{2N}\\right) \\\\ &amp;= \\frac{1}{4N^2}\\text{Var}(\\text{Bin}(2N,p_t')) \\\\ &amp;= \\frac{2Np_t'(1-p_t')}{4N^2}\\\\ &amp;= \\frac{p_t'(1-p_t')}{2N} \\end{aligned}\\] <p>The expectation is as before -- the population size is irrelevant. The variance, however, is largest when the population size is small and the expected frequency is near 0.5 (just as we saw for a general binomial random variable above).  </p> <p>Note that variance goes to 0 as \\(N\\) goes to infinity. Therefore, in an infinitely large population, the frequency of \\(A\\) in the next generation is \\(p_{t+1} = p_t'\\) with no uncertainity. This recovers the one-locus haploid selection model from earlier in the course. </p> <p>In the next lab we'll learn how to simulate this stochastic model of evolution.</p>"},{"location":"lectures/lecture-21/","title":"Lecture 21","text":""},{"location":"lectures/lecture-21/#lecture-21-probability-ii-demographic-stochasticity","title":"Lecture 21: Probability II (demographic stochasticity)","text":"Run notes interactively?"},{"location":"lectures/lecture-21/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Poisson random variable</li> <li>Demographic stochasticity</li> <li>Extinction</li> <li>Establishment</li> </ol> <p>credits</p> <p>This lecture was created by PhD student Puneeth Deraje as part of a course development TA position -- thanks Puneeth! If you are following along with the text, this lecture does not follow along as closely.</p> <p>In Lecture 18 we learned how to model stochasticity in population genetics (genetic drift). In this lecture we'll learn how to model stochasticity in population dynamics (demographic stochasticity). </p> <p>To do so we'll work with the simplest model of population dynamics possible, exponential growth (see Lecture 3). In discrete time this is</p> \\[n_{t+1} = R n_t\\] <p>where \\(n_t\\) is the number of individuals at time \\(t\\) and \\(R\\) is the reproductive factor. In this deterministic model every individual had a reproductive factor of exactly \\(R\\) at every time step. In reality there will be stochasticity in \\(R\\) across individuals and time. We can account for that by replacing \\(R\\) with a random variable. </p> <p></p>"},{"location":"lectures/lecture-21/#1-poisson-random-variable","title":"1. Poisson random variable","text":"<p>Recall the binomial random variable from the previous lecture, \\(X \\sim \\mathrm{Bin}(n,p)\\). The probability this random variable takes on value \\(k\\) is then </p> \\[\\Pr(X = k) = {n \\choose k} p^k (1-p)^{n-k}\\] <p>We could model the reproductive factor as a binomial random variable. For example, perhaps each individual at time \\(t\\) produces \\(n\\) offspring before dying, and each offspring survives to become an adult with probability \\(p\\). Using this model requires estimates of two parameters, \\(n\\) and \\(p\\). But there is a simpler way.</p> <p>Let the mean number of offspring produced be \\(\\lambda=np\\). Rearranging this in terms of \\(p\\), we can write the binomial as \\(X\\sim \\mathrm{Bin}(n,\\lambda/n)\\). Note that the mean is always \\(\\lambda\\), regardless of \\(n\\). Now imagine that individuals in the population we are modelling tend to have a very large number of offspring, very few of which survive. We can approximate this in the extreme by taking \\(n\\rightarrow \\infty\\). Let \\(Y\\) be this random variable, \\(Y = \\lim_{n \\rightarrow \\infty} \\mathrm{Bin}(n, \\lambda/n))\\). The distribution of \\(Y\\) is then </p> \\[ \\begin{aligned} \\Pr(Y=k) &amp;= \\lim_{n \\rightarrow \\infty} {n \\choose k} \\left(\\frac{\\lambda}{n}\\right)^k \\left(1-\\frac{\\lambda}{n}\\right)^{n-k} \\\\ &amp;= \\lim_{n \\rightarrow \\infty} \\frac{n(n-1)(n-2)...(n-k+1)}{k!} \\frac{\\lambda^k}{n^k} \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &amp;= \\frac{\\lambda^k}{k!}\\lim_{n \\rightarrow \\infty} \\frac{n(n-1)(n-2)...(n-k+1)}{n^k} \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &amp;= \\frac{\\lambda^k}{k!}\\lim_{n \\rightarrow \\infty} 1\\left(1-\\frac{1}{n}\\right)\\left(1-\\frac{2}{n}\\right)...\\left(1-\\frac{k-1}{n}\\right) \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &amp;= \\frac{\\lambda^k}{k!}\\lim_{n \\rightarrow \\infty} \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &amp;= \\frac{\\lambda^k e^{-\\lambda}}{k!} \\end{aligned} \\] <p>We call \\(Y\\) a Poisson random variable with mean \\(\\lambda\\) and denoted it by \\(Y\\sim\\mathrm{Poi}(\\lambda)\\). We can now model the reproductive factor as a random variable with only one parameter, \\(\\lambda\\). </p> <p>Note that the simpler Poisson distribution is a good approximation of the binomial distribution even for fairly moderate values of \\(n\\), as seen in the plot below.</p> <pre>\nimport sympy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef binomial(n,p,k):\n    return sympy.binomial(n,k) * p**k * (1-p)**(n-k)\n\ndef poisson(lam,k):\n    return lam**k * np.exp(-lam)/sympy.factorial(k)\n\nn = 100\np = 0.1\nlam = n*p\nks = range(n+1)\n\nfig, ax = plt.subplots()\n\nax.plot(ks, [binomial(n,p,k) for k in ks], label='binomial')\nax.plot(ks, [poisson(lam,k) for k in ks], label='Poisson')\n\nax.set_xlabel('number of successes')\nax.set_ylabel('probability')\nax.legend()\nplt.show()\n</pre> <p></p> <p>Two key properties of a Poisson random variable are</p> <p>1) the variance is equal to the mean</p> \\[  \\begin{aligned}  \\mathrm{Var}(Y) &amp;= \\lim_{n \\rightarrow \\infty} \\mathrm{Var}\\left(\\mathrm{Bin}\\left(n,\\frac{\\lambda}{n}\\right)\\right) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} n \\frac{\\lambda}{n} \\left(1-\\frac{\\lambda}{n}\\right) \\\\ &amp;= \\lambda  \\lim_{n \\rightarrow \\infty} \\left(1-\\frac{\\lambda}{n}\\right) \\\\ &amp;= \\lambda \\end{aligned} \\] <p>2) if \\(Y_1\\) and \\(Y_2\\) are two independent Poisson random variables with means \\(\\lambda_1\\) and \\(\\lambda_2\\), then their sum is distributed as a Poisson with mean \\(\\lambda_1+\\lambda_2\\)</p> \\[Y_1 + Y_2 \\sim \\mathrm{Poi}(\\lambda_1 + \\lambda_2)\\] <p></p>"},{"location":"lectures/lecture-21/#2-demographic-stochasticity","title":"2. Demographic stochasticity","text":"<p>We can now model demographic stochasticity with the Poisson distribution. </p> <p>Assume each individual in the population produces a Poisson number of surviving offspring with mean \\(\\lambda\\), independent of all other individuals in the populations, and then dies. Write the number of offspring for individual \\(i\\) as \\(X_i\\sim\\mathrm{Poi}(\\lambda)\\). Let the current number of individuals be \\(n_t\\). Then the population size in the next generation, \\(n_{t+1}\\), is distributed like the sum of \\(n_t\\) independent and identical Poisson's</p> \\[\\begin{aligned} n_{t+1} &amp;\\sim \\sum_{i=1}^{n_t}\\mathrm{Poi}(\\lambda)\\\\ &amp;= \\mathrm{Poi}\\left(\\sum_{i=1}^{n_t} \\lambda\\right) \\\\ &amp;= \\mathrm{Poi}(\\lambda n_t) \\\\ \\end{aligned}\\] <p>This implies that the expected population size in the next generation is \\(\\lambda n_t\\), as is the variance. In Lab 11 we'll simulate this to get a better sense of the resulting dynamics.</p> <p></p>"},{"location":"lectures/lecture-21/#3-extinction","title":"3. Extinction","text":"<p>Let us now look at one property of this model -- the probability of extinction.</p> <p>Consider a given individual at time \\(t\\). Let \\(\\eta\\) be the probability this individual does not have descendants in the long-term, i.e., that its lineage goes extinct.</p> <p>To find \\(\\eta\\) we note that the probability that this lineage goes extinct, \\(\\eta\\), is the probability that this individual has \\(k\\) surviving offspring (for all values of \\(k\\)) and all of those offspring lineages go extinct (with probability \\(\\eta^k\\)). Given the probability of having \\(k\\) surviving offspring is \\(\\lambda^k e^{-\\lambda}/k!\\), this implies</p> \\[ \\begin{aligned} \\eta  &amp;= \\sum_{k=0}^{\\infty} \\frac{e^{-\\lambda}\\lambda^k}{k!} \\eta^k \\\\ &amp;= e^{-\\lambda} \\sum_{k=0}^{\\infty} \\frac{(\\eta \\lambda)^k}{k!} \\\\ &amp;= e^{-\\lambda}e^{\\lambda \\eta} \\\\ &amp;= e^{-\\lambda (1-\\eta)} \\end{aligned} \\] <p>One solution is \\(\\eta=1\\), certain extinction. But there can be a second biologically valid solution (between 0 and 1), meaning that extinction is not certain. This occurs when the mean number of surviving offspring is greater than one, \\(\\lambda&gt;1\\), as shown in plot below (the solutions are where the curve intersects the 1:1 line).</p> <pre>\nxs = np.linspace(0,1,100)\nfig,ax=plt.subplots()\n\nlam = 2\n\nax.plot(xs, xs)\nax.plot(xs, [np.exp(-lam*(1-x)) for x in xs])\n\nax.set_xlabel(r'$\\eta$')\nax.set_ylabel(r'$e^{-\\lambda (1-\\eta)}$')\n\nplt.show()\n</pre> <p></p> <p>Above we calculated the probability a single lineage goes extinct, \\(\\eta\\). From this, the probability that the entire population goes extinct is the probability that all \\(n_t\\) lineages go extinct, \\(\\eta^{n_t}\\). </p> <p>The two major conclusions from this are</p> <p>1) when the mean number of surviving offspring per parent is \\(\\lambda=1\\) the deterministic model predicts a constant population size but the stochastic model says that extinction is certain.</p> <p>2) when the mean number of surviving offspring per parent is \\(\\lambda&gt;1\\) the deterministic model predicts exponential growth but the stochastic model says there is still some non-zero probability of extinction.</p> <p></p>"},{"location":"lectures/lecture-21/#4-establishment","title":"4. Establishment","text":"<p>Before moving on, the above result about extinction is mathematically identical to a classic result in population genetics concerning the establishment of a beneficial allele.</p> <p>Consider a population at equilibrium such that the mean number of offspring per parent is 1. And now consider a beneficial allele that tends to have more offspring, \\(\\lambda&gt;1\\). Then we know from above that there is some non-zero probability this lineage does not go extinct, \\(\\eta&lt;1\\). Writing the above equation about extinction in terms of the establishment probability, \\(p=1-\\eta\\), we have</p> \\[\\begin{aligned} \\eta &amp;= e^{-\\lambda (1-\\eta)}\\\\ 1 - p &amp;= e^{-\\lambda p}\\\\ p &amp;= 1 - e^{-\\lambda p} \\end{aligned}\\] <p>Now assume that the beneficial allele increases the number of offspring only slightly, so that \\(\\lambda=1+s\\) with \\(s\\) small. We can then solve for \\(p\\) explicitly using a Taylor series expansion of \\(e^{-\\lambda p}=e^{-(1+s) p}\\) around \\(s=0\\)</p> \\[  \\begin{aligned} p &amp;= 1 - e^{-(1+s)p} \\\\ p &amp;\\approx 1 - \\left(1 - (1+s)p + \\frac{(1+s)^2p^2}{2}\\right) \\\\ p &amp;\\approx (1+s)p - \\frac{(1+s)^2p^2}{2} \\\\ 1 &amp;\\approx 1 + s - \\frac{(1+s)^2p}{2} \\\\ 0 &amp;\\approx s - \\frac{(1+s)^2p}{2} \\\\ p &amp;\\approx \\frac{2s}{(1+s)^2}\\\\ p &amp;\\approx 2s \\end{aligned} \\] <p>This says that the probability a weakly beneficial allele establishes (i.e., is not lost by genetic drift) is roughly twice its selective advantage.</p>"},{"location":"lectures/lecture-22/","title":"Lecture 22","text":""},{"location":"lectures/lecture-22/#lecture-22-probability-iii-the-coalescent","title":"Lecture 22: Probability III (the coalescent)","text":"Run notes interactively?"},{"location":"lectures/lecture-22/#lecture-overview","title":"Lecture overview","text":"<ol> <li>The coalescent</li> </ol> <p>All the models we have studied so far have looked forward in time, into the future. But this doesn't have to always be the case, we can also look back in time, modeling the past. Here we look at one particularly powerful example from population genetics.</p> <p></p>"},{"location":"lectures/lecture-22/#1-the-coalescent","title":"1. The coalescent","text":"<p>Let's consider a population composed of \\(N\\) diploid individuals, i.e., with \\(2N\\) alleles at a given locus. We want to model the history of these alleles -- from which alleles in the previous generations do they descend? </p> <p>One of the simplest ways to model this is to treat all of the alleles as equivalent (e.g., no fitness differences) so that a given allele in the current generation picks its \"parent\" allele at random from amongst the \\(2N\\) alleles in the previous generation.</p> <p>We can simulate this and plot the result, arranging the \\(2N\\) alleles horizontally, stacking previous generations on top, and drawing lines to connect \"children\" and \"parent\" alleles.</p> <pre>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef simulate(N,tmax):\n\n    p = np.arange(2*N) #initial configuration\n    t = 0\n    ps = np.empty((tmax,2*N), dtype='int')\n    while t &lt; tmax:\n        ps[t] = p\n        p = np.random.randint(0,2*N,2*N) #parents of current generation\n        t += 1\n\n    return ps\n</pre> <pre>\ndef plot_lineages(ps,ax,alpha):\n\n    # plot lineages\n    for t,p in enumerate(ps[1:]): #loop over generations\n        ax.plot([ps[0],ps[t+1]], [t,t+1], marker='o', color='k', alpha=alpha) #connect children with parents\n    ax.scatter(ps[0], [t+1 for _ in ps[0]], marker='o', color='k', alpha=alpha) #plot all alleles of last gen\n\n    # simplify presentation\n    # ax.axis('off')\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.xaxis.set_ticks([])\n    ax.yaxis.set_ticks(range(len(ps)))\n    ax.set_xlabel('alleles')\n    ax.set_ylabel('generations ago')\n</pre> <pre>\nfig, ax = plt.subplots()\n\nps = simulate(N=5,tmax=4)\nplot_lineages(ps,ax,alpha=0.5)\n\nplt.show()\n</pre> <p></p> <p>One of the most important aspects of this model is that we can choose to think about the history of just some subset of the alleles in the current generation. This is helpful because 1) we do not need to model the entire population and 2) this has a close connection to data (since we almost never sample every individual in a population).</p> <p>To see this visually, we can choose a few \"sample\" alleles from the current generation and highlight their \"lineages\".</p> <pre>\nfig, ax = plt.subplots()\n\nplot_lineages(ps,ax,alpha=0.1) #plot full population in background\n\n# plot sample lineages\nfor i in [0,5,9]: #samples\n    path = [i] #lineage of sample i\n    for p in ps[1:]: #loop over generations\n        path.append(p[i]) #add parent \n        i = p[i] #make parent the child\n    ax.plot(path,range(len(path)), marker='o')\n\nplt.show()\n</pre> <p></p>"},{"location":"lectures/lecture-22/#time-for-two-lineages-to-coalesce","title":"Time for two lineages to coalesce","text":"<p>When two sample lineages meet at a most recent common ancestor we say they coalesce (\"come together\"). This is where the name of the model comes from, the coalescent. (Note that some reserve that name for the continuous-time limit of this model, but we won't be so strict.) </p> <p>The time it takes for two lineages to coalesce determines how similar those alleles are. The more distant their most recent common ancestor the more mutations that are expected to have accumulated. Thus the time to coalescence influences the amount of genetic diversity we expect to see.  </p> <p>The first question we will ask with this model is: how many generations does it take for two lineages to coalesce? Let's call this random variable \\(T_2\\). We want to know how this random variable is distributed.</p> <p>Consider one generation at a time. We choose the parent of one of the alleles at random. We do the same for the second allele. The probability that the two lineages coalesce in the previous generation is the probability that the parent of the second allele is the same as the parent of the first allele, \\(p_2=1/(2N)\\). </p> <p>Let \\(X=1\\) if the two lineages coalesce in the previous generation, \\(X=0\\) if they don't coalesce. Then \\(X\\) is a Bernoulii random variable, \\(X\\sim\\text{Ber}(p_2)\\). We can now rephrase our question mathematically: how many Bernoulli trials do we have to perform to get one success?</p> <p>Geometric random variable</p> <p>Let \\(X\\) be the number of Bernoulli trials (with success probability \\(p\\)) that it takes to get 1 success. Then the probability we need to perform \\(X=k\\) trials is the probability of having \\(k-1\\) failures (which happens with probability \\((1-p)^{k-1}\\)) followed by a success (which happens with probability \\(p\\))</p> \\[\\Pr(X=k) = (1-p)^{k-1}p\\] <p>This random variable \\(X\\) is called a \"geometric random variable\" with parameter \\(p\\) and denoted \\(X\\sim\\text{Geo}(p)\\). After evaluating a few infinite sums it can be shown to have expectation</p> \\[\\mathbb{E}(X) = \\frac{1}{p}\\] <p>and variance</p> \\[\\text{Var}(X) = \\frac{1-p}{p^2}\\] <p>Returning to the coalescent, the time for two sample lineages to coalesce is therefore geometrically distributed, \\(T_2\\sim\\text{Geo}(p_2)\\). We then can expect to wait </p> \\[\\begin{aligned} \\mathbb{E}(T_2)  &amp;= 1/p_2\\\\ &amp;=2N \\end{aligned}\\] <p>generations until coalescence of the two lineages. However, the variance around this expectation is </p> \\[\\begin{aligned} \\text{Var}(T_2)  &amp;= \\frac{1-p_2}{p^2}\\\\ &amp;=2N(2N-1) \\end{aligned}\\] <p>which is roughly \\((2N)^2\\) in a large population, \\(2N&gt;&gt;1\\). Since \\(N^2\\) can be very large relative to \\(N\\), this means that there is a lot of noise around the expectation.</p> <p>We can see how noisey the coalescence time is by sampling from a geometric distribution for a given value of \\(N\\). For example, below we sample the coalescence time 1000 times with \\(N=1000\\) and plot those times in a histogram. We often see coalescence times ranging from very near 1 to well over 10000 (run the code a few times if you'd like).</p> <pre>\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nN = 1000 #diploid population size\np = 1/(2*N) #probability 2 lineages coalesce in the previous generation\nts = np.random.geometric(p,1000) #sample from a geometric with param p 1000 times\nax.hist(ts, bins=100) #histogram of coalescence times\n\nax.set_xlabel('number of generations until 2 lineages coalesce')\nax.set_ylabel('number of realizations')\nplt.show()\n</pre> <p></p>"},{"location":"lectures/lecture-22/#time-for-n-lineages-to-coalesce","title":"Time for \\(n\\) lineages to coalesce","text":"<p>Considering only two samples is a special case that gives us some intuition about the model. But in general we want to know, how long until \\(n\\) samples all share a common ancestor? Let this time be random variable \\(M_n\\). We want to know something about \\(M_n\\). </p> <p>We start by assuming that only one pair of lineages can coalesce each generation, which is valid when the population size is large, \\(N&gt;&gt;1\\), and the sample is relatively small \\(n&lt;&lt;N\\). Then we can write \\(M_n\\) as the time it takes to go from \\(n\\) to \\(n-1\\) lineages, \\(T_n\\), plus the time it takes to go from \\(n-1\\) to \\(n-2\\) lineages, \\(T_{n-1}\\), and so on down to the time it takes to go from 2 to 1 lineages, \\(T_2\\)</p> \\[M_n = T_n + T_{n-1} + ... + T_2\\] <p>The next step is to find out something about the \\(T_i\\). We do this by noting that when there are \\(i\\) lineages the probability of no coalescence in the previous generation is</p> \\[1-p_i = \\left(1-\\frac{1}{2N}\\right)\\left(1-\\frac{2}{2N}\\right)...\\left(1-\\frac{i-1}{2N}\\right)\\] <p>In words, we choose any parent for the first lineage, the second lineage does not have the same parent with probability \\(1-\\frac{1}{2N}\\), the third lineage does not have the same parent as either of the first two lineages with probability \\(1-\\frac{2}{2N}\\), and so on to the \\(i^\\text{th}\\) lineage.</p> <p>This is a complicated expression but we can simplify by taking a Taylor series around \\(1/N=0\\) and approximating to first order, which gives</p> \\[\\begin{aligned} 1-p_i  &amp;\\approx 1 - \\frac{1}{2N}\\sum_{j=1}^{i-1}j\\\\ &amp;= 1 - \\frac{1}{2N}\\frac{i(i-1)}{2}\\\\ &amp;= 1 - \\frac{{i \\choose 2}}{2N}\\\\ \\end{aligned}\\] <p>so that the probability that there is coalescence is</p> \\[p_i \\approx \\frac{{i \\choose 2}}{2N}\\] <p>This makes good sense. When \\(N\\) is large there can be at most 1 coalescent event per generation among the sample lineages. Since there are \\({i \\choose 2}\\) ways to choose a pair of lineages from \\(i\\) lineages, each of which coalesce with probability \\(1/(2N)\\), the probability of coalescence is \\({i \\choose 2}/(2N)\\).</p> <p>We can now treat the \\(T_i\\) as geometric random variables with parameter \\(p_i\\), \\(T_i\\sim\\text{Geo}(p_i)\\).</p> <p>This means that \\(M_n\\), the time for \\(n\\) samples to coalesce into 1, is the sum of \\(n-1\\) independent geometric random variables. Unfortunately this doesn't produce a nice distribution (in contrast to the sum of Poisson random variables we saw in the last lecture). However, we can calculate the expectation since we know the expectation of a sum is the sum of the expectations</p> \\[\\begin{aligned} \\mathbb{E}(M_n)  &amp;= \\mathbb{E}(T_n + T_{n-1} + ... + T_2)\\\\ &amp;= \\mathbb{E}(T_n) + \\mathbb{E}(T_{n-1}) + ... + \\mathbb{E}(T_2)\\\\ &amp;= 1/p_n + 1/p_{n-1} + ... + 1/p_2\\\\ &amp;\\approx \\frac{2N}{{n \\choose 2}} + \\frac{2N}{{n-1 \\choose 2}} + ... + \\frac{2N}{{2 \\choose 2}}\\\\ &amp;= 2N \\sum_{i=2}^n \\frac{1}{{i \\choose 2}} \\\\ &amp;= 4N \\frac{n-1}{n} \\\\ \\end{aligned}\\] <p>Since \\((n-1)/n&lt;1\\) we see that all \\(n\\) sample lineages are expected to coalesce in less than \\(4N\\) generations. This is pretty remarkable given we expect to wait more than half that time just for 2 sample lineages to coalesce! The reason for this is that coalescence happens faster when there are more lineages (since there are more pairs to choose from). So if we take a large sample then most of the lineages will coalesce very quickly and most of the time spent waiting for the most recent common ancestor will be once few lineages remain. Below we take advantage of a great Python package, <code>msprime</code>, to quickly simulate the coalescent with \\(n\\) samples and plot the history of those samples (a coalescent tree).</p> <pre>\nimport msprime\nn = 10\nts = msprime.sim_ancestry(n) #simulate coalescent with n samples\nts.first().draw_svg(node_labels={}, size=(500,300)) #plot tree without node labels\n</pre> <p></p>"},{"location":"lectures/schedule/","title":"Schedule","text":"<p>Under construction for Fall 2025 edition. Lectures will be updated as we go.</p> Lecture Topic Background reading 1 Why and how to build a model Preface (2p.), Chapter 1 (14p.), Chapter 2 (34p.) 2 Numerical and graphical techniques (univariate) 4.1-4.3 (23p.) 3 Equilibria (univariate) 5.1-5.2 (12p.) 4 Stability (univariate) 5.3 (13p.) 5 General solutions (univariate) Chapter 6 (18p.) - Test 6 Vectors and matrices P2.1-P2.4 (14p.) 7 Linear algebra P2.5-P2.7 (9p.) 8 General solutions (linear multivariate) I P2.8-P2.9 (13p.), 9.1-9.2 (18p.) 9 General solutions (linear multivariate) II 10 Demography Chapter 10 (37p.) - Test 11 Numerical and graphical techniques (multivariate) 4.4-4.5 (12p.) 12 Equilibria (nonlinear multivariate) Chapter 8 (37p.) 13 Stability (nonlinear multivariate) Chapter 8 (37p.) 14 Epidemiology 294-301 (7p.) 15 Multi-locus population genetics 322-330 (8p.) - Test 16 Evolutionary invasion analysis I 12.1-12.4 (30p.) 17 Evolutionary invasion analysis II 12.1-12.4 (30p.) 18 The evolution of dominance 12.5 (17p.) 19 Evolutionary invasion analysis III 12.1-12.4 (30p.) 20 Example - Test"},{"location":"syllabus/assignments/","title":"Assignments","text":""},{"location":"syllabus/assignments/#homeworks","title":"Homeworks","text":"<p>A list of homework problems will be released after class on Wednesday, with the solutions due by the start of class on Monday.</p>"},{"location":"syllabus/assignments/#labs","title":"Labs","text":"<p>Each computer lab contains a series of problems and the solutions are due by the end of the day (ideally by the end of the lab).</p>"},{"location":"syllabus/course_structure/","title":"Course structure","text":""},{"location":"syllabus/course_structure/#learning-objectives","title":"Learning objectives","text":"<p>Mathematics is central to science because it provides a rigorous way to go from a set of assumptions (what we take to be true) to their logical consequences (what we want to know). In ecology &amp; evolution this might be how we think SARS-CoV-2 may spread and evolve given a set of vaccination rates and travel restrictions, how caribou population sizes are predicted to respond to forecasted rates of climate change, or something much more abstract like the expected amount of genetic diversity in a randomly mating population. In this course we'll learn how to build, analyze, and interpret mathematical models of increasing complexity through readings, lectures, computer labs, and a final project. By the end of the course you will be able to:</p> <ul> <li> build a model: go from a verbal description of a biological system to a set of equations</li> <li> analyze a model: manipulate a set of equations into a mathematical expression of interest</li> <li> interpret a model: translate mathematical expressions back into biological meaning</li> </ul>"},{"location":"syllabus/course_structure/#weekly-tasks","title":"Weekly tasks","text":"<ul> <li> read text</li> <li> attend two lectures</li> <li> attend one lab</li> </ul>"},{"location":"syllabus/course_structure/#grading-scheme","title":"Grading scheme","text":"<ul> <li>in-class tests: 4 x 20%</li> <li>final project: 20%</li> </ul>"},{"location":"syllabus/exams/","title":"Exams","text":""},{"location":"syllabus/exams/#previous-midterms","title":"Previous midterms","text":"<p>Roughly covers univariate lectures and labs.</p> <ul> <li>2024, solutions </li> <li>2022, solutions </li> <li>2021, solutions </li> </ul>"},{"location":"syllabus/exams/#previous-finals","title":"Previous finals","text":"<p>Covers all material, with a focus on the remaining lectures and labs.</p> <ul> <li>2024, solutions </li> <li>2022, solutions </li> <li>2021, solutions </li> </ul>"},{"location":"syllabus/final_project/","title":"Final project","text":""},{"location":"syllabus/final_project/#construct-your-own-model","title":"Construct your own model","text":"<p>In this project you will use the tools you've learned in class and apply them to a model that you develop. The model can be about any phenomenon in ecology and evolution, as long as you make up the model.</p> <p>You'll do the final project in two parts.</p> <ul> <li> <p> Part 1</p> <ul> <li>Describe your biological question and why this interests you </li> <li>Describe your model in words (ie, the main assumptions) and explain the main structure with a diagram (eg, flow or life cycle diagram) </li> <li>Write down the equations that you will analyze</li> <li>Describe what your analysis might reveal (ie, your hypothesis) </li> <li>Max 2 pages </li> <li>Example</li> </ul> </li> <li> <p> Part 2 </p> <ul> <li>Re-iterate your biological question and why this interests you</li> <li>Describe your model assumptions in detail, defining all parameters and variables </li> <li>Write down the equations for your model </li> <li>Analyze your model </li> <li>Explain how the results address your original question</li> <li>Suggest how the model could be improved or extended</li> <li>Max 4 pages (not including any code that you used, which can be included as a link or additional file)</li> <li>Example</li> </ul> </li> </ul> <p>Tip</p> <p>If you are having trouble coming up with a new model, take one of the models that we've analysed in the course and change one or more of its underlying assumptions to get a new set of equations. Then analyse these equations. Discuss the differences between the assumptions used and also  between the results obtained.</p>"},{"location":"syllabus/general_info/","title":"General info","text":""},{"location":"syllabus/general_info/#land-acknowledgement","title":"Land acknowledgement","text":"<p>I wish to acknowledge this land on which the University of Toronto operates. For thousands of years it has been the traditional land of the Huron-Wendat, the Seneca, and the Mississaugas of the Credit. Today, this meeting place is still the home to many Indigenous people from across Turtle Island and I am grateful to have the opportunity to work on this land. For more information see University of Toronto's land acknowledgement.</p>"},{"location":"syllabus/general_info/#group-norms","title":"Group norms","text":"<p>The University of Toronto is committed to equity, human rights, and respect for diversity. All members of the learning environment in this course should strive to create an atmosphere of mutual respect where all members of our community can express themselves, engage with each other, and respect one another\u2019s differences. U of T does not condone discrimination or harassment against any persons or communities. Please contact me  if you have any concerns. For more information see the Code of Student Conduct.</p>"},{"location":"syllabus/general_info/#accessibility","title":"Accessibility","text":"<p>The University provides academic accommodations for students with disabilities in accordance with the terms of the Ontario Human Rights Code. This occurs through a collaborative process that acknowledges a collective obligation to develop an accessible learning environment that both meets the needs of students and preserves the essential academic requirements of the University\u2019s courses and programs. Students with diverse learning styles and needs are welcome in this course. If you have a disability that may require accommodations, please feel free to get in touch with me and/or the Accessibility Services office.</p>"},{"location":"syllabus/general_info/#religious-observances","title":"Religious observances","text":"<p>The University provides reasonable accommodation of the needs of students who observe religious holy days other than those already accommodated by ordinary scheduling and statutory holidays. Students have a responsibility to alert members of the teaching staff in a timely fashion to upcoming religious observances and anticipated absences and I will make every reasonable effort to avoid scheduling tests, examinations or other compulsory activities at these times. Please reach out to me as early as possible to communicate any anticipated absences related to religious observances, and to discuss any possible related implications for course work.</p>"},{"location":"syllabus/general_info/#family-care-responsibilities","title":"Family care responsibilities","text":"<p>The University of Toronto strives to provide a family-friendly environment. You may wish to inform me if you are a student with family responsibilities. If you are a student parent or have family responsibilities, you also may wish to visit the Family Care Office website.</p>"},{"location":"syllabus/instructors/","title":"Instructors","text":""},{"location":"syllabus/instructors/#professor","title":"Professor","text":"<p>Matthew Osmond (he/him)</p> <ul> <li>email: mm.osmond@utoronto.ca</li> <li>office: Earth Sciences Centre (ESC) 3041</li> <li>website: osmond-lab.github.io</li> </ul>"},{"location":"syllabus/instructors/#teaching-assistant","title":"Teaching assistant","text":"<p>Erik Curtis (he/him)</p> <ul> <li>email: erik.curtis@mail.utoronto.ca</li> </ul>"},{"location":"syllabus/resources/","title":"Resources","text":"<p>There are many resources available at the University of Toronto to help you succeed in this course. Below are a few:</p> <ul> <li>Writing Center</li> <li>Academic integrity</li> <li>More on academic integrity</li> <li>CTSI list of supports</li> <li>Academic success module</li> <li>Get help with Quercus</li> </ul>"},{"location":"syllabus/textbook/","title":"Textbook","text":"<p>Otto &amp; Day 2007. A biologist's guide to mathematical modeling in ecology and evolution.</p> <ul> <li>UofT library e-copies</li> <li>UofT library physical-copies</li> <li>buy your own copy</li> </ul>"},{"location":"syllabus/when_and_where/","title":"When and where","text":""},{"location":"syllabus/when_and_where/#lectures","title":"Lectures","text":"<ul> <li>Monday 10:10 - 11:00 AM, Koffler House (KP) 113</li> <li>Wednesday 10:10 - 11:00 AM, Sidney Smith (SS) 1084</li> </ul>"},{"location":"syllabus/when_and_where/#labs","title":"Labs","text":"<ul> <li>Wednesday, 3:10 - 5:00 PM, Sidney Smith (SS), room 561</li> </ul>"},{"location":"syllabus/final_project/partII_example/","title":"partII example","text":""},{"location":"syllabus/final_project/partII_example/#final-project-part-ii-example","title":"Final project part II - example","text":"Run notes interactively?      <p>Note</p> <pre><code>This is a made-up example inspired by the research of Sydney Ackermann, an MSc student in the Osmond lab, who was in turn inspired by the models of [Roze &amp; Michod](https://doi.org/10.1086/323590) and [Pichugin et al.](https://doi.org/10.1371/journal.pcbi.1005860).\n</code></pre>"},{"location":"syllabus/final_project/partII_example/#biological-question","title":"Biological question","text":"<p>Note</p> <p>In this case, this is just repeated from part I, so that part II tells the whole story from question to answer. Update and refine as needed.</p> <p>I am interested in the origin of multicellularity and the reason why particular life-cycles predominate. For instance, many bacterial species exist as single cells which reproduce via binary fission while many multicellular organisms grow to large sizes and reproduce via single-celled offspring. However, there are many more options and it is not clear if or why these strategies are evolutionarily optimal or how multicellularity evolved from a binary fission ancestor. Here I ask the question, which multicellular life-cycles can invade a unicellular population producing via binary fission?</p>"},{"location":"syllabus/final_project/partII_example/#model","title":"Model","text":"<p>Note</p> <p>This is an improved, expanded version of the model description from part I, with more detail and now including the equations to be analyzed.</p>"},{"location":"syllabus/final_project/partII_example/#unicellular-resident","title":"Unicellular resident","text":"<p>Here I first model a unicellular population where individuals divide, die, and compete in continuous time. The variable in this model is the number of individuals, \\(n\\). The parameters are the birth rate (\\(b_1\\)), death rate (\\(d_1\\)), and amount of competition. To describe competition I will assume there are a total of \\(k\\) places for individuals to exist. When a cell divides, each of the two resulting offspring randomly choose one of these \\(k\\) locations. If an offspring lands on an empty spot it survives. If an offspring lands on an occupied spot it \"wins\" that spot with probability 1/2, killing the resident, and otherwise dies. The amount of competition is therefore controlled by parameter \\(k\\).</p> <p>This model can be described by the following flow diagram</p> <pre><code>graph LR;\n    A((n)) --\"b1 n (1 - n/k)\" --&gt; A;\n    A --d1 n--&gt; B[ ];\n\n    style B height:0px;</code></pre> <p>The corresponding differential equation is </p> \\[ \\frac{\\mathrm{d}n}{\\mathrm{d}t} = b_1 n \\left(1-\\frac{n}{k}\\right) - d_1 n \\] <p>which can be solved for equilibrium, \\(\\hat{n}\\).</p>"},{"location":"syllabus/final_project/partII_example/#multicellular-invader","title":"Multicellular invader","text":"<p>Next, imagine an invading multicellular population. </p>"},{"location":"syllabus/final_project/partII_example/#111","title":"1+1+1","text":"<p>For instance, imagine a life-cycle where offspring are unicellular, they divide to form an individual with two cells, and then divide again to produce three single-celled offspring. We call this strategy 1+1+1 because those are the sizes of the offspring produced.</p> <p>In this case we need to track the number of individuals with one, \\(n_1\\), and two, \\(n_2\\), cells (i.e., this is a structured population). I assume that each cell in an individual with \\(i\\) cells divides at rate \\(b_i\\) and that individuals with \\(i\\) cells die at rate \\(d_i\\). I also assume that this invading population is so rare (and \\(k\\) so large) that offspring produced by this life-history strategy never land on other individuals with this same strategy. The offspring may, however, land on an individual from the unicellular population, which will reduce the growth rate of the invader.</p> <p>The dynamics of this invading multicellular population can be described by the following flow diagram</p> <pre><code>graph LR;\n    A((n1)) --b1 n1--&gt; B((n2));\n    A --d1 n1--&gt; C1[ ];\n    B --d2 n2--&gt; C2[ ];\n    B --\"6b2 n2 (1-n/(2k))\"--&gt; A;\n\n    style C1 height:0px;\n    style C2 height:0px;</code></pre> <p>To determine whether this population can invade a unicellular resident at equilibrium, I will calculate the leading eigenvalue from the system of (linear in \\(n_i\\)) differential equations</p> \\[ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} = \\mathbf{M}\\vec{n} \\] <p>where \\(\\vec{n}=\\begin{pmatrix}n_1 \\\\ n_2\\end{pmatrix}\\) and </p> \\[ \\mathbf{M}= \\begin{pmatrix}  -b_1-d_1 &amp; 6b_2(1-\\hat{n}/(2k)) \\\\  b_1 &amp; -2b_2-d_2  \\end{pmatrix} \\]"},{"location":"syllabus/final_project/partII_example/#1-2","title":"1 + 2","text":"<p>The other multicellular strategy that has offspring sizes summing to 3 is 1+2. In this case the transition matrix is</p> \\[ \\mathbf{M}= \\begin{pmatrix}  -b_1-d_1 &amp; 2b_2(1-\\hat{n}/(2k)) \\\\  b_1 &amp; -2b_2-d_2+2b_2(1-\\hat{n}/(2k))  \\end{pmatrix} \\]"},{"location":"syllabus/final_project/partII_example/#1-1-1-1","title":"1 + 1 + 1 + 1","text":"<p>We can also consider larger multicellular strategies, like 1+1+1+1. In this case we need to keep track of the number of groups of size 1, 2, and 3. This gives a 3x3 projection matrix</p> \\[ \\mathbf{M}= \\begin{pmatrix}  -b_1-d_1 &amp; 0 &amp; 12b_3(1-\\hat{n}/(2k)) \\\\  b_1 &amp; -2b_2-d_2 &amp; 0 \\\\ 0 &amp; 2b_2 &amp; -3b_3-d_3 \\end{pmatrix} \\]"},{"location":"syllabus/final_project/partII_example/#1-1-2","title":"1 + 1 + 2","text":"<p>For the 1+1+2 strategy we have </p> \\[ \\mathbf{M}= \\begin{pmatrix}  -b_1-d_1 &amp; 0 &amp; 6b_3(1-\\hat{n}/(2k)) \\\\  b_1 &amp; -2b_2-d_2 &amp; 3b_3(1-\\hat{n}/(2k)) \\\\ 0 &amp; 2b_2 &amp; -3b_3-d_3 \\end{pmatrix} \\]"},{"location":"syllabus/final_project/partII_example/#1-3","title":"1 + 3","text":"<p>For the 1+3 strategy we have </p> \\[ \\mathbf{M}= \\begin{pmatrix}  -b_1-d_1 &amp; 0 &amp; 3b_3(1-\\hat{n}/(2k)) \\\\  b_1 &amp; -2b_2-d_2 &amp; 0 \\\\ 0 &amp; 2b_2 &amp; -3b_3-d_3+3b_3(1-\\hat{n}/(2k)) \\end{pmatrix} \\]"},{"location":"syllabus/final_project/partII_example/#2-2","title":"2 + 2","text":"<p>For the 2+2 strategy we have </p> \\[ \\mathbf{M}= \\begin{pmatrix}  -b_1-d_1 &amp; 0 &amp; 0 \\\\  b_1 &amp; -2b_2-d_2 &amp; 6b_3(1-\\hat{n}/(2k)) \\\\ 0 &amp; 2b_2 &amp; -3b_3-d_3 \\end{pmatrix} \\]"},{"location":"syllabus/final_project/partII_example/#results","title":"Results","text":""},{"location":"syllabus/final_project/partII_example/#unicellular-resident_1","title":"Unicellular resident","text":"<p>I first solve for the unicellular resident equilibrium</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}n}{\\mathrm{d}t} &amp;= b_1 n \\left(1-\\frac{n}{k}\\right) - d_1 n \\\\ 0 &amp;= b_1 \\hat{n} \\left(1-\\frac{\\hat{n}}{k}\\right) - d_1 \\hat{n} \\\\ 0 &amp;= \\hat{n} \\left(b_1 \\left(1-\\frac{\\hat{n}}{k}\\right) - d_1\\right) \\\\ \\end{aligned} \\] <p>This implies that \\(\\hat{n}=0\\) or</p> \\[ \\begin{aligned} 0 &amp;= b_1 \\left(1-\\frac{\\hat{n}}{k}\\right) - d_1 \\\\ d_1/b_1 &amp;= 1-\\frac{\\hat{n}}{k} \\\\ 1 - d_1/b_1 &amp;= \\frac{\\hat{n}}{k} \\\\ k(1 - d_1/b_1) &amp;= \\hat{n} \\\\ \\end{aligned} \\] <p>This non-zero equilibrium is biologically valid whenever \\(0\\leq\\hat{n}\\implies d_1\\leq b_1\\). This non-zero equilibrium is stable when </p> \\[ \\begin{aligned} \\frac{\\mathrm{d}}{\\mathrm{d}n}\\left(\\frac{\\mathrm{d}n}{\\mathrm{d}t}\\right)_{n=\\hat{n}} &amp;&lt; 0\\\\ b_1 \\left(1-\\frac{\\hat{n}}{k}\\right) - b_1 \\hat{n}/k - d_1 &amp;&lt; 0\\\\ - b_1 (1-d_1/b_1) &amp;&lt; 0\\\\ - (b_1 - d_1) &amp;&lt; 0\\\\ b_1 &amp;&gt; d_1\\\\ \\end{aligned} \\] <p>We will assume \\(b_1&gt;d_1\\) such that this non-zero equilibrium is both biologically valid and stable.</p> <pre>\n# check our calculations\n\nfrom sympy import *\nvar('b1,d1,b2,d2,b3,d3,k,n')\n\ndndt = n*b1*(1-n/k) - d1*n #equation\neq = solve(dndt,n) #equilibrium\nprint(eq)\n\ndiff(dndt,n).subs(n,eq[1]).simplify() #stability condition\n</pre>"},{"location":"syllabus/final_project/partII_example/#multicellular-invader_1","title":"Multicellular invader","text":""},{"location":"syllabus/final_project/partII_example/#1-1-1","title":"1 + 1 + 1","text":"<p>Let's now look at the growth rate of a rare 1+1+1 strategy, by calculating the leading eigenvalue of \\(\\mathbf{M}\\). Because this is a 2x2 matrix we know that the eigenvalues solve</p> \\[ \\lambda^2 - \\mathrm{Tr}(\\mathbf{M})\\lambda + |\\mathbf{M}| = 0 \\] <p>giving </p> \\[ \\lambda = \\frac{\\mathrm{Tr}(\\mathbf{M}) \\pm \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4|\\mathbf{M}|}}{2} \\] <p>For stability (ie, the multicellular strategy does not invade) we need \\(|\\mathbf{M}|&gt;0\\) and \\(\\mathrm{Tr}(\\mathbf{M})&lt;0\\) (these are the Routh-Hurwitz conditions). The first requires</p> \\[ \\begin{aligned} |\\mathbf{M}| &amp;&gt; 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 6b_2(1-\\hat{n}/(2k))b_1 &amp;&gt; 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 6b_2(1-(1 - d_1/b_1)/2)b_1 &amp;&gt; 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 3b_2(1+d_1/b_1)b_1 &amp;&gt; 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 3b_2(b_1+d_1) &amp;&gt; 0\\\\ (b_1+d_1)(d_2 - b_2) &amp;&gt; 0\\\\ \\end{aligned} \\] <p>Since all \\(b_i\\) and \\(d_i\\) are non-negative (since they are rates) this implies stability when \\(d_2&gt;b_2\\).</p> <p>The second condition requires </p> \\[ \\begin{aligned} \\mathrm{Tr}(\\mathbf{M}) &amp;&lt; 0 \\\\ (-b_1-d_1-2b_2-d_2) &amp;&lt; 0 \\\\ b_1+d_1+2b_2+d_2 &amp;&gt; 0 \\\\ \\end{aligned} \\] <p>which is true as long as at least one of these rates is non-zero, which we already assumed to be the case for resident stability (\\(b_1&gt;d_1\\)). </p> <p>To conclude, this 1+1+1 strategy will invade the unicellular strategy whenever \\(b_2&gt; d_2\\).</p> <pre>\n# check our calculations\n\nM = Matrix([\n    [-b1 - d1, 6*b2*(1-n/(2*k))],\n    [b1, -2*b2 - d2]])\n\ndet = M.det().subs(n,eq[1]).simplify()\ntr =  M.trace().subs(n,eq[1]).simplify()\nprint(det)\nprint(tr)\n\n(det - (b1+d1)*(d2-b2)).simplify() #check our simplified expression of the determinant\n</pre>"},{"location":"syllabus/final_project/partII_example/#1-2_1","title":"1 + 2","text":"<p>We can take the same approach for a rare 1+2 invader. In this case the determinant condition reduces to</p> \\[ \\begin{aligned} |\\mathbf{M}| &amp;&gt; 0\\\\ (-b_1-d_1)(-2b_2-d_2+2b_2(1-\\hat{n}/(2k))) - 2b_2(1-\\hat{n}/(2k))b_1 &amp;&gt; 0\\\\ (b_1 + d_1)(d_2 - b_2 d_1/b_1) &amp;&gt; 0\\\\ \\end{aligned} \\] <p>and the trace determinant condition reduces to </p> \\[ \\begin{aligned} \\mathrm{Tr}(\\mathbf{M}) &amp;&lt; 0 \\\\ (-b_1-d_1-d_2+2b_2(1-\\hat{n}/(2k))) &amp;&lt; 0 \\\\ -b_1-d_1-d_2-b_2(1-d_1/b_1) &amp;&lt; 0 \\\\ \\end{aligned} \\] <p>The latter is always true, so the critical condition for invasion comes from the former, which can be rearranged as \\(b_2/b_1 &gt; d_2/d_1\\).</p> <pre>\n# check our calculations\n\nM = Matrix([\n    [-b1 - d1, 2*b2*(1-n/(2*k))],\n    [b1, -2*b2 - d2 + 2*b2*(1-n/(2*k))]])\n\ndet = M.det().subs(n,eq[1]).simplify()\ntr =  M.trace().subs(n,eq[1]).simplify()\nprint(det)\nprint(tr)\n\n(det - ((b1 + d1)*(d2 - b2*d1/b1))).simplify() #check our simplified expression of the determinant\n</pre>"},{"location":"syllabus/final_project/partII_example/#larger-multicellular-strategies","title":"Larger multicellular strategies","text":"<p>In the remaining cases we are dealing with 3x3 matrices, and so the analytical expressions for the eigenvalues are not easy to interpret. Instead we plot the real part of the leading eigenvalue (ie, the invasion growth rate) as a function of the cell division rate in groups of size 2, \\(b_2\\), for a specific set of paramter values (see figure caption). We also plot the invasion growth rates of two smaller multicellular strategies (analysed above) for comparison.  </p> <pre>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_invasion_rate(M, nhat, pvals, b2s, ax, label=''):\n    '''plot the invasion growth rate as a function of b2'''\n    evs = M.eigenvals(multiple=True) #eigenvalues\n    evseq = [ev.subs(n,nhat) for ev in evs] #eigenvalues at resident equilibrium\n    evseqp = [ev.subs(pvals) for ev in evseq] #evaluate at chosen parameter vaues\n    rs = [max([re(ev.subs(b2,i)).n() for ev in evseqp]) for i in b2s] #find the max of the real parts of each eigenvalue for each value of b2 \n    ax.plot(b2s, rs, label=label) #plot\n</pre> <pre>\n# projection matrices for each multicellular strategy\n\nM111 = Matrix([\n    [-b1 - d1, 6*b2*(1-n/(2*k))],\n    [b1, -2*b2 - d2]])\n\nM12 = Matrix([\n    [-b1 - d1, 2*b2*(1-n/(2*k))],\n    [b1, -2*b2 - d2 + 2*b2*(1-n/(2*k))]])\n\nM1111 = Matrix([\n    [-b1 - d1, 0, 12*b3*(1-n/(2*k))],\n    [b1, -2*b2 - d2, 0],\n    [0, 2*b2, -3*b3-d3]])\n\nM112 = Matrix([\n    [-b1 - d1, 0, 6*b3*(1-n/(2*k))],\n    [b1, -2*b2 - d2, 3*b3*(1-n/(2*k))],\n    [0, 2*b2, -3*b3-d3]])\n\nM13 = Matrix([\n    [-b1 - d1, 0, 3*b3*(1-n/(2*k))],\n    [b1, -2*b2 - d2, 0],\n    [0, 2*b2, -3*b3-d3+3*b3*(1-n/(2*k))]])\n\nM22 = Matrix([\n    [-b1 - d1, 0, 0],\n    [b1, -2*b2 - d2, 6*b3*(1-n/(2*k))],\n    [0, 2*b2, -3*b3-d3]])\n</pre> <pre>\n# chose parameter values\npvals = {'b1':1.0,'d1':0.1,'d2':0.1,'d3':0.1,'k':100,'b3':1.0}\nb2s = np.linspace(0,2,100) #range to plot over\nnhat = eq[1]\n\n#plot\nfig, ax = plt.subplots()\n\nplot_invasion_rate(M=M111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1')\nplot_invasion_rate(M=M12, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+2')\nplot_invasion_rate(M=M1111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1+1')\nplot_invasion_rate(M=M112, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+2')\nplot_invasion_rate(M=M13, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+3')\nplot_invasion_rate(M=M22, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='2+2')\n\nax.plot(b2s, [0 for _ in b2s], c='k') #0 line for reference (invade if above this line)\n\nplt.xlabel(r'$b_2$')\nplt.ylabel('invasion growth rate')\nplt.legend()\nplt.show()\n</pre> <p></p> <p>Figure 1. The invasion growth rate (real part of leading eigenvalue) versus the rate of cell division in groups of size 2, \\(b_2\\), for 6 multicellular strategies (see legend) invading a unicellular strategy, 1+1. Parameter values: \\(b_1=b_3=1\\), \\(d_i=0.1\\), and \\(k=100\\).</p>"},{"location":"syllabus/final_project/partII_example/#discussion","title":"Discussion","text":""},{"location":"syllabus/final_project/partII_example/#answering-the-question","title":"Answering the question","text":"<p>Above we have shown that any of the 6 chosen multicellular strategies can invade a unicellular strategy given certain conditions. For the smaller multicellular strategies (1+1+1 and 1+2) we were able to show these conditions analytically, in general. For the larger multicellular strategies (1+1+1+1, 1+1+2, 1+3, and 2+2), where the eigenvalues are more complicated, we turned to a numerical example. </p> <p>For 1+1+1, invasion requires cell division to be faster than death in groups of size 2, \\(b_2&gt;d_2\\). Essentially, the benefit a cell receives from being in a group (\\(b_2\\)) needs to outweigh the costs (\\(d_2\\)). It is interesting that this does not depend on the cell division and death rates in groups of size 1, \\(b_1\\) and \\(d_1\\). This is likely because all offspring of both the unicellular and multicellular strategy are size 1, so both strategies are affected equally by changes in \\(b_1\\) and \\(d_1\\).</p> <p>For 1+2, invasion requires the cell division rate in groups of size 2 relative to that in groups of size 1 to be greater than the death rate in groups of size 2 relative to that in groups of size 1, \\(b_2/b_1&gt;d_2/d_1\\). This may be the case if groups of cells cooperate by sharing resources or protecting each other from the environment, which would increase \\(b_2/b_1\\) or decrease \\(d_2/d_1\\).</p> <p>For larger multicellular strategies, our numerical example indicates that the invasion rate increases with the number of offspring.  Those strategies with 2 offspring (1+3 and 2+2) behave much like 1+2 while the strategy with 3 offspring behaves much like 1+1+1. It is the strategy with 4 offspring (1+1+1+1) that has the highest invasion growth rate and thus appears most like to invade a unicellular ancestor under this model. As shown in the figure below, this is despite the fact that the 1+1+1+1 strategy does not have the highest growth rate in the absence of competitors. Instead, it appears that having more (and hence smaller) offspring at the time of fragmenting makes a strategy more competitive, presumably because this increases the chances of winning a spot. </p> <pre>\n# chose parameter values\npvals = {'b1':1.0,'d1':0.1,'d2':0.1,'d3':0.1,'k':100,'b3':1.0}\nb2s = np.linspace(0,2,100) #range to plot over\nnhat = 0 #no unicellular individuals\n\n#plot\nfig, ax = plt.subplots()\n\nplot_invasion_rate(M=M111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1')\nplot_invasion_rate(M=M12, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+2')\nplot_invasion_rate(M=M1111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1+1')\nplot_invasion_rate(M=M112, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+2')\nplot_invasion_rate(M=M13, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+3')\nplot_invasion_rate(M=M22, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='2+2')\n\nax.plot(b2s, [0 for _ in b2s], c='k') #0 line for reference (invade if above this line)\n\nplt.xlabel(r'$b_2$')\nplt.ylabel('invasion growth rate')\nplt.legend()\nplt.show()\n</pre> <p></p> <p>Figure 2. The growth rate (real part of leading eigenvalue) versus the rate of cell division in groups of size 2, \\(b_2\\), for 6 multicellular strategies (see legend) growing from rare in the absence of competition. Parameter values: \\(b_1=b_3=1\\), \\(d_i=0.1\\), and \\(k=100\\).</p>"},{"location":"syllabus/final_project/partII_example/#limitations-and-extensions","title":"Limitations and extensions","text":"<p>There are a number of ways this analysis and model could be improved. For one, I have only looked at the invasion of a multicellular strategy into the unicellular resident. It therefore remains unclear if an invading multicellular strategy will completely displace the unicellular strategy or the both will coexist. Looking at the reverse situation, where the unicellular strategy invades a multicellular strategy would help answer this. Further, it is unclear how the multicellular strategies compete with each other. Exploring this would help us understand how and why complex multicellular organisms, like ourselves, have evolved. Another interesting direction is to allow competition to depend on the size of a group, for instance by making larger groups more likely to win spots resided by smaller groups. And finally, it would be interesting to include different cell types, like cooperators and cheaters, as multicellularity opens the door to within-group conflict, which may affect which strategies invade best.</p>"},{"location":"syllabus/final_project/partI_example/","title":"Final project part I - example","text":"<p>Note</p> <p>This is a made-up example inspired by the research of Sydney Ackermann, an MSc student in the Osmond lab, who was in turn inspired by the models of Roze &amp; Michod and Pichugin et al..</p>"},{"location":"syllabus/final_project/partI_example/#biological-question","title":"Biological question","text":"<p>I am interested in the origin of multicellularity and the reason why particular life-cycles predominate. For instance, many bacterial species exist as single cells which reproduce via binary fission while many multicellular organisms grow to large sizes and reproduce via single-celled offspring. However, there are many more options and it is not clear if or why these strategies are evolutionarily optimal or how multicellularity evolved from a binary fission ancestor. Here I ask the question, which multicellular life-cycles can invade a unicellular population producing via binary fission?</p>"},{"location":"syllabus/final_project/partI_example/#model-description","title":"Model description","text":"<p>I will start by modeling a unicellular population where individuals divide, die, and compete in continuous time. The variable in this model is the number of individuals, \\(n\\). The parameters are the birth rate (\\(b_1\\)), death rate (\\(d_1\\)), and amount of competition. To describe competition I will assume there are a total of \\(k\\) places for individuals to exist. When a cell divides, each of the two resulting offspring randomly choose one of these \\(k\\) locations. If an offspring lands on an empty spot it survives. If an offspring lands on an occupied spot it \"wins\" that spot with probability 1/2, killing the resident, and otherwise dies. The amount of competition is therefore controlled by parameter \\(k\\).</p> <p>This model can be described by the following flow diagram</p> <pre><code>graph LR;\n    A((n)) --\"b1 n (1 - n/k)\" --&gt; A;\n    A --d1 n--&gt; B[ ];\n\n    style B height:0px;</code></pre> <p>From the resulting differential equation I will determine the equilibrium number of unicellular individuals, \\(\\hat{n}\\).</p> <p>Next I will model the dynamics of an invading multicellular population. For instance, imagine a life-cycle where offspring are unicellular, they divide to form an individual with two cells, and then divide again to produce three single-celled offspring. In this case we need to track the number of individuals with one, \\(n_1\\), and two, \\(n_2\\), cells (i.e., this is a structured population). I assume that each cell in an individual with \\(i\\) cells divides at rate \\(b_i\\) and that individuals with \\(i\\) cells die at rate \\(d_i\\). I also assume that this invading population is so rare (and \\(k\\) so large) that offspring produced by this life-history strategy never land on other individuals with this same strategy. The offspring may, however, land on an individual from the unicellular population, which will reduce the growth rate of the invader.</p> <p>The dynamics of this invading multicellular population can be described by the following flow diagram</p> <pre><code>graph LR;\n    A((n1)) --b1 n1--&gt; B((n2));\n    A --d1 n1--&gt; C1[ ];\n    B --d2 n2--&gt; C2[ ];\n    B --\"6b2 n2 (1-n/(2k))\"--&gt; A;\n\n    style C1 height:0px;\n    style C2 height:0px;</code></pre> <p>The goal is then to see if the growth rate of this invading multicellular population is positive (i.e., it can invade) or negative (i.e., it cannot invade). This will require calculating the leading eigenvalue from the system of (linear) differential equations describing the rate of change in \\(n_i\\).</p>"},{"location":"syllabus/final_project/partI_example/#equations","title":"Equations","text":"<p>The equation for the unicellular population size is</p> \\[\\frac{\\mathrm{d}n}{\\mathrm{d}t} = b_1 n \\left(1 - \\frac{n}{k}\\right) - d_1 n\\] <p>and the equations for the invading multicellular population described above are</p> \\[\\begin{aligned} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &amp;= -(b_1 + d_1) n_1 + 6 b_2 n_2 \\left(1 - \\frac{n}{2k}\\right)\\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} &amp;= b_1 n_1 - (2b_2 + d_2) n_2 \\end{aligned}\\]"},{"location":"syllabus/final_project/partI_example/#hypothesis","title":"Hypothesis","text":"<p>I hypothesize that, provided the \\(b_2\\), \\(b_3\\), ... are large enough relative to \\(b_1\\) (ie, that cells divide fast enough in multicellular organisms) and the \\(d_2\\), \\(d_3\\), ... are small enough relative to \\(d_1\\) (ie, that multicellular individuals don't die too quickly), there will be multicellular life-cycles that can invade a unicellular population. </p>"}]}