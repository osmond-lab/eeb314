{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"Mathematical modelling in ecology and evolution (EEB314) <p>Mathematics is central to science because it provides a rigorous way to go from a set of assumptions to their logical consequences. In ecology &amp; evolution this might be how we think a virus will spread and evolve, how climate change will impact a threatened population, or how much genetic diversity we expect to see in a randomly mating population. In this course you'll learn how to build, analyze, and interpret mathematical models of increasing complexity through readings, lectures, computer labs, and a final project. Our focus is on deterministic dynamical models (recursions and differential equations), which requires us to learn and use some calculus and linear algebra.</p> <p>Please see the University of Toronto Academic Calendar for more details on the course prerequisites and additional information on the distribution/breadth requirements this course satisfies.</p> <p>Next taught: Fall 2025</p> <p>Previously taught: Fall 2024, Fall 2022 (EEB430), Fall 2021 (EEB430)</p>"},{"location":"acknowledgements/","title":"Acknowledgements","text":"<p>This course is based on a fantastic textbook by Sally Otto and Troy Day, A biologist's guide to mathematical modeling in ecology and evolution. I've been fortunate to have been taught by both of these excellent mentors and I hope I can do them and their book justice.</p> <p>I'm also very grateful to Leithen M'Gonigle who has slickly elaborated on Sally's lecture notes in LaTeX and kindly shared them with me.</p> <p>Thanks also to Ailene McPherson who, along with Sally and Leithen, continues to help me teach this course and inspires me to become a better teacher.</p> <p>Thanks are also due to Puneeth Deraje, who went above and beyond as the course's first TA, and Chris Carlson, who helped improve the labs as the course's second TA. </p> <p>And you wouldn't be reading this without the help of Tom Ouellette who did an amazing job creating this website as a course development TA.</p> <p>Finally, thanks to all the students who have taken the course and given me feedback (explicit or not) -- thanks for your patience!</p>"},{"location":"labs/schedule/","title":"Schedule","text":"<p>Under construction for Fall 2025 edition. Labs will be updated as we go.</p> Lab Tutorial Coding exercise 1 Building recursion equations Introduction to Jupyter, Python, and SymPy 2 Solving for equilibria Simulating recursion equations 3 Determining local stability Finding general solutions 4 Phase planes Vectors and matrices 5 Long-term dynamics of linear multivariate models Eigenvalues, eigenvectors, and linear multivariate solutions 6 Linear multivariate practice Demography 7 Lotka-Volterra competition I Lotka-Volterra competition II 8 tbd tbd 9 tbd tbd 10 tbd tbd 11 tbd tbd <p>The links for the coding part of the labs are for students at the University of Toronto. They are also all available to view and download here.</p>"},{"location":"labs/tutorial-01/","title":"Tutorial 1","text":""},{"location":"labs/tutorial-01/#tutorial-1-building-recursion-equations","title":"Tutorial 1: Building recursion equations","text":"Run notes interactively?      <p>Let's get some more practice building recursion equations.</p>"},{"location":"labs/tutorial-01/#review","title":"Review","text":"<p>In Lecture 1 we asked, how does immigration affect population size?</p> <p>We then built a model with a single variable, \\(n(t)\\), denoting population size at time \\(t\\). </p> <p>In discrete time the parameters were the average number of immigrants per time step (\\(M\\)), the average number of offspring per individual per time step (\\(B\\)), and the fraction of individuals that die each time step (\\(D\\)). </p> <p>Assuming migration, then birth, then death each time step, we drew the following life-cycle diagram:</p> <pre><code>graph LR;\n    A((n)) --migration--&gt; B((n'));\n    B --birth--&gt; C((n''));\n    C --death--&gt; A;</code></pre> <p>We then built an equation for the population size in the next generation, \\(n(t+1)\\), based on the life-cycle diagram above, by constructing an equation for each event </p> \\[n' = n(t) + M\\] \\[n'' = n' + Bn'\\] \\[n(t+1) = n'' - Dn''\\] <p>We then substituted \\(n''\\) and then \\(n'\\) into the equation for \\(n(t+1)\\) to write \\(n(t+1)\\) in terms of \\(n(t)\\)</p> \\[ \\begin{aligned} n(t+1) &amp;= n'' \u2212 Dn'' \\\\ &amp;= (n' + Bn') \u2212 D(n' + Bn') \\\\ &amp;= n'(1 + B \u2212 D \u2212 DB) \\\\ &amp;= (n(t) + M)(1 + B \u2212 D \u2212 DB) \\\\ \\end{aligned} \\] <p>This recursion equation correctly takes into account the order of the life cycle (migration, birth, death) and the point at which the census is taken (immediately after death).</p>"},{"location":"labs/tutorial-01/#problem","title":"Problem","text":"<p>Show that the six different life-cycle orders give four distinct recursion equations.</p>"},{"location":"labs/tutorial-02/","title":"Tutorial 2","text":""},{"location":"labs/tutorial-02/#tutorial-2-solving-for-equilibria","title":"Tutorial 2: Solving for equilibria","text":"Run notes interactively?      <p>Let's get more familiar with solving for equilibria using a model of population growth.</p>"},{"location":"labs/tutorial-02/#logistic-growth","title":"Logistic growth","text":"<p>Our model for exponential growth in discrete time was \\(n(t+1) = R n(t)\\). When we plotted this over time for \\(R&gt;1\\) we saw the population size increase quickly without bound. This is obviously quite unrealistic, eventually competition between individuals will slow and prevent population growth. To model this we make the reproductive factor \\(R\\) a function of the current population size \\(n(t)\\) and, in the simplest case, assume \\(R\\) declines linearly with \\(n(t)\\), </p> \\[R(n(t)) = 1 + r\\left(1 - \\frac{n(t)}{K}\\right).\\] <p>We call \\(r\\) the intrinsic growth rate (ie, growth rate when rare) and \\(K\\) the carrying capacity. </p> <p>Below we plot the reproductive factor as a function of \\(n(t)\\) for a few different values of \\(r\\) and \\(K\\).</p> <pre>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Reproductive factor for logistic growth\ndef logistic_discrete(nt, r, K):\n    '''reproductive factor in discrete logistic model with growth rate r and carrying capacity k'''\n    return 1 + r * (1 - nt/K) \n\n# Compare a few different growth rates and carrying capacities\nfig, ax = plt.subplots()\nfor r, K in zip([1, 2, 1], [100, 100, 50]): #for each pair of r and K values\n    nt = np.linspace(0, 200) #for a range of population sizes from 0 to 200\n    R = logistic_discrete(nt, r, K) #calculate the reproductive factor\n    ax.plot(nt, R, label=f\"r = {r}, K = {K}\") #and plot\n\nax.plot(nt, [1 for i in nt], '--', color='gray') #1 line for reference\nax.set_xlabel('Population size, $n(t)$')\nax.set_ylabel('Reproductive factor, $R$')\nax.legend(frameon=False)\nplt.ylim(0,None)\nplt.show()\n</pre> <p></p> <p>Our recursion equation is now</p> \\[n(t+1) = \\left(1 + r\\left(1-\\frac{n(t)}{K}\\right)\\right)n(t).\\] <p>This is the recursion equation for logistic growth.</p> <p>This recursion is a non-linear function of \\(n(t)\\) (non-linear means that there is a term in the equation where the term is taken to some power other than 1; here if we expand out the recursion we get a \\(n(t)^2\\) term). This reflects the fact that logistic growth models an interaction between individuals (competition).</p> <p>To get a better sense of the model let's make a cob-web plot for \\(K=1000\\) and \\(r=0.5\\):</p> <pre>\nfrom sympy import *\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr, K = 0.5, 1000 #choose parameter values\nn = symbols('n') #define our variable n(t)\n\n# Write out sympy equation\nf = n*(1 + r*(1 - n/K)) #the recursion equation\n\n# Compute function over a set of points in [0,2K] by 'lambdifying' sympy equation\nx = np.linspace(0,2*K,100)\nfy = lambdify(n, f)(x)\n\n# Build function for generating figure\ndef plot_n(x,fy):\n    fig, ax = plt.subplots()\n    ax.plot(x, fy, color='black') #n_{t+1} as function of n_t\n    ax.plot(x, x, color='black', linestyle='--') #1:1 line for reference\n    ax.set_xlim(0,2*K)\n    ax.set_ylim(0,2*K)\n    ax.set_xlabel(\"population size at $t$, $n_t$\")\n    ax.set_ylabel(\"population size at $t+1$, $n_{t+1}$\")\n    return ax\n\n# Plot figure\nax = plot_n(x,fy)\n\n# make generator\ndef nt(n0, r, K, max=oo):\n    t, nnow, nnext = 0, n0, 0 #initial conditions\n    while t &lt; max:\n        yield nnow, nnext #current value of n(t) and n(t+1)\n        nnext = nnow*(1 + r*(1 - nnow/K)) #update n(t+1)\n        yield nnow, nnext #current value of n(t) and n(t+1)\n        nnow = nnext #update n(t)\n        t += 1 #update t\n\n# Initialize generator\nnts = nt(n0=100, r=r, K=K, max=10)\n\n# Compute x,y pairs by iterating through generator\nns = np.array([[x,y] for x,y in nts])\n\n# Plot 'cobwebs'\nax.plot(ns[:,0], ns[:,1])\n\nax.set_title('r=%s'%r)\n\nplt.show()\n</pre> <p></p> <p>We see population size move towards an internal equilibrium. Things are not so straightforward when we increase \\(r\\)!</p> <pre>\nr,K = 2.7,1000 #choose parameter values\nn = symbols('n') #define our variable n(t)\n\n# Write out sympy equation\nf = n*(1 + r*(1 - n/K)) #the recursion equation\n\n# Compute function over a set of points in [0,2K] by 'lambdifying' sympy equation\nx = np.linspace(0,2*K,100)\nfy = lambdify(n, f)(x)\n\n# Plot figure\nax = plot_n(x,fy)\n\n# Initialize generator\nnts = nt(n0=100, r=r, K=K, max=10)\n\n# Compute x,y pairs by iterating through generator\nns = np.array([[x,y] for x,y in nts])\n\n# Plot 'cobwebs'\nax.plot(ns[:,0], ns[:,1],)\n\nplt.show()\n</pre> <p></p>"},{"location":"labs/tutorial-02/#problem","title":"Problem","text":"<p>Solve for the equilibria, \\(\\hat n\\). Check this is consistent with the plots above.</p>"},{"location":"labs/tutorial-03/","title":"Tutorial 3","text":""},{"location":"labs/tutorial-03/#tutorial-3-determining-local-stability","title":"Tutorial 3: Determining local stability","text":"Run notes interactively?      <p>Let's get more familiar with determining local stability using logistic population growth.</p>"},{"location":"labs/tutorial-03/#logistic-growth-in-discrete-time","title":"Logistic growth in discrete time","text":"<p>A reminder that the recursion is</p> \\[ n(t+1) = \\left(1 + r\\left(1-\\frac{n(t)}{K}\\right)\\right)n(t), \\] <p>where \\(r\\) is the intrinsic growth rate and \\(K\\) the carrying capacity.</p> <p>We can define the right-hand side as \\(f(n)\\) for convenience,</p> \\[ f(n) = n \\left(1 + r\\left(1 - \\frac{n}{K}\\right)\\right). \\] <p>We first take the derivative of \\(f\\) with respect to \\(n\\)</p> \\[ f'(n) = 1 + r - 2 r \\frac{n}{K} \\] <p>Then we plug in the equilirbium value of interest, \\(n=K\\)</p> \\[ \\begin{aligned} f'(K) &amp;= 1 + r - 2 r \\\\ &amp;= 1 - r \\end{aligned} \\] <p>This will be negative when \\(r &gt; 1\\), creating oscillations.</p> <p>The equilibrium will be stable when \\(-1 &lt; 1 - r &lt; 1 \\implies 0 &lt; r &lt; 2\\).</p> <p>This is consistent with what we've seen in cob-web plots, which you can verify by playing with the parameter values at the top of the code below.</p> <pre>\nfrom sympy import *\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# parameter values\nr = 0.5 #growth rate\nK = 1000 #carrying capacity\n\n# Write out sympy equation\nn = symbols('n') #define our variable n(t)\nf = n*(1 + r*(1 - n/K)) #the recursion equation\n\n# Compute function over a set of points in [0,2K] by 'lambdifying' sympy equation\nx = np.linspace(0,2*K,100)\nfy = lambdify(n, f)(x)\n\n# Build function for generating figure\ndef plot_n(x,fy):\n    fig, ax = plt.subplots()\n    ax.plot(x, fy, color='black') #n_{t+1} as function of n_t\n    ax.plot(x, x, color='black', linestyle='--') #1:1 line for reference\n    ax.set_xlim(0,2*K)\n    ax.set_ylim(0,2*K)\n    ax.set_xlabel(\"population size at $t$, $n(t)$\")\n    ax.set_ylabel(\"population size at $t+1$, $n(t+1)$\")\n    return ax\n\n# Plot figure\nax = plot_n(x,fy)\n\n# make generator\ndef nt(n0, r, K, max=oo):\n    t, nnow, nnext = 0, n0, 0 #initial conditions\n    while t &lt; max:\n        yield nnow, nnext #current value of n(t) and n(t+1)\n        nnext = nnow*(1 + r*(1 - nnow/K)) #update n(t+1)\n        yield nnow, nnext #current value of n(t) and n(t+1)\n        nnow = nnext #update n(t)\n        t += 1 #update t\n\n# Initialize generator\nnts = nt(n0=100, r=r, K=K, max=10)\n\n# Compute x,y pairs by iterating through generator\nns = np.array([[x,y] for x,y in nts])\n\n# Plot 'cobwebs'\nax.plot(ns[:,0], ns[:,1])\n\nax.set_title('r=%s'%r)\n\nplt.show()\n</pre> <p></p>"},{"location":"labs/tutorial-03/#problem","title":"Problem","text":"<p>In continuous time the logistic growth model is </p> \\[ \\frac{\\mathrm{d}n}{\\mathrm{d}t} = n r \\left(1 - \\frac{n}{K}\\right). \\] <p>What are the equilibria and when are they stable? How do these answers differ in continuous vs discrete time?</p>"},{"location":"labs/tutorial-04/","title":"Tutorial 4","text":""},{"location":"labs/tutorial-04/#tutorial-4-phase-planes","title":"Tutorial 4: Phase planes","text":"Run notes interactively?      <p>Here we'll learn how to build the two-dimensional version of a phase line, called a phase plane. This is a useful way to visualize two-dimensional models.</p>"},{"location":"labs/tutorial-04/#example","title":"Example","text":"<p>Take our 2-island bird example,</p> \\[ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} = \\mathbf{M}\\vec{n} + \\vec{m}, \\] <p>where </p> \\[ \\begin{aligned} \\vec{n} &amp;= \\begin{pmatrix} n_1 \\\\ n_2 \\end{pmatrix}\\\\ \\mathbf{M} &amp;= \\begin{pmatrix} b_1 - d_1 - m_{12} &amp; m_{21} \\\\ m_{12} &amp; b_2 - d_2 - m_{21} \\end{pmatrix}\\\\ \\vec{m} &amp;= \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}, \\end{aligned} \\] <p>with \\(n_i\\) the number of birds on island \\(i\\) at time \\(t\\), \\(b_i\\) the birth rate on island \\(i\\), , \\(d_i\\) the death rate on island \\(i\\), \\(m_{ij}\\) the rate at which birds on island \\(j\\) migrate to island \\(i\\), and \\(m_i\\) the rate at which birds arrive to island \\(i\\) from elsewhere.</p> <p>To visualize the dynamics we will build a phase plane. We start by finding the nullclines, which are the values of the variables that make the change in each variable zero. In this case there are two nullclines, which we can solve for in terms of \\(n_2\\),</p> \\[ \\begin{align} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &amp;= 0\\\\ (b_1 - d_1 - m_{12})n_1 + m_{21} n_2 + m_1 &amp;= 0\\\\ m_{21} n_2 &amp;= -m_1 - (b_1 - d_1 - m_{12})n_1\\\\ n_2 &amp;= \\frac{-m_1 - (b_1 - d_1 - m_{12})n_1}{m_{21}} \\end{align} \\] <p>and</p> \\[ \\begin{align} \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} &amp;= 0\\\\ (b_2 - d_2 - m_{21})n_2 + m_{12} n_1 + m_2 &amp;= 0\\\\ (b_2 - d_2 - m_{21})n_2  &amp;= -m_{12} n_1 - m_2\\\\ n_2  &amp;= \\frac{-m_{12} n_1 - m_2}{b_2 - d_2 - m_{21}}. \\end{align} \\] <p>These two nullclines are lines along which \\(n_1\\) or \\(n_2\\) does not change. Let's plot them for some parameter values.</p> <pre>\nimport matplotlib.pyplot as plt\nfrom sympy import *\nimport numpy as np\n\n# define the variables\nn1, n2 = symbols('n1, n2')\n\n# Choose the parameter values\nb1, b2 = 1, 1\nd1, d2 = 1.1, 1.1\nm12, m21 = 0.05, 0.05\nm1, m2 = 5, 5\n\n# define differential equations\ndn1dt = (b1 - d1 - m12) * n1 + m21 * n2 + m1\ndn2dt = m12 * n1 + (b2 - d2 - m21) * n2 + m2\n\n# get the nullclines\nnullcline_1 = solve(Eq(dn1dt, 0),n2)[0]\nnullcline_2 = solve(Eq(dn2dt, 0),n2)[0]\n\n# plot\nn1s = np.linspace(0,100,100)\nplt.plot(n1s, [nullcline_1.subs(n1,i) for i in n1s], label='$n_1$ nullcline')\nplt.plot(n1s, [nullcline_2.subs(n1,i) for i in n1s], label='$n_2$ nullcline')\n\nplt.xlabel('number of birds on island 1, $n_1$')\nplt.ylabel('number of birds on island 2, $n_2$')\nplt.xlim(0,100)\nplt.ylim(0,100)\nplt.legend()\nplt.show()\n</pre> <p></p> <p>Along the blue line \\(n_1\\) does not change (so we must move exactly up or down) and along the orange line \\(n_2\\) does not change (so we must move exactly left or right). Where the two lines intersect neither \\(n_1\\) or \\(n_2\\) changes, meaning we have an equilibrium. What we would next like to know is how we move when not on either of the lines.</p> <p>Notice that the nullclines divide the plane into sections (4 in the plot above). Because the change in \\(n_1\\) and \\(n_2\\) is only zero on their respective nullclines, if \\(n_i\\) increases (or decreases) somewhere in a section than it increases (or decreases) everywhere in that section. That means we can choose a convenient place within a section, like \\(n_1=n_2=0\\). Here the differential equations are</p> \\[ \\begin{align} \\left.\\frac{\\mathrm{d}n_1}{\\mathrm{d}t}\\right|_{n_1=n_2=0} &amp;= m_1\\\\ \\left.\\frac{\\mathrm{d}n_2}{\\mathrm{d}t}\\right|_{n_1=n_2=0} &amp;= m_2. \\end{align} \\] <p>In our example both \\(m_i\\) are positive, meaning that both \\(n_1\\) and \\(n_2\\) are increasing at \\(n_1=n_2=0\\). This means that in that entire section we are moving up and to the right. We could draw an arrow in that section pointing in that direction.</p> <p>Now we can cross a nullcline and go into another section. Let's move straight up, clockwise, by crossing the orange nullcline for \\(n_2\\). Because we crossed a nullcline for \\(n_2\\), where the rate of change in \\(n_2\\) became 0, we know that the sign of the rate of change in \\(n_2\\) flipped. This means that in that top left section we must be moving down and to the right. </p> <p>If we keep moving clockwise, we next cross a blue \\(n_1\\) nullcline, and we must therefore be going down and the to the left in the top right section. Finally, we cross another orange nullcline, meaning we must be moving up and the the left in the bottom right section. </p> <p>All together, we have developed an image of the dynamics across the entire plane for the given set of parameter values. This means we can choose an arbitrary initial condition and have a sense of where the dynamics will go. In this case it is towards the equilibrium from any starting point, suggesting stability.</p> <p>On a computer we can populate the plane with many arrows whose direction and size indicate the direction and speed of change in the variables from that location.</p> <pre>\ndef plot_vector_field(ax, dn1, dn2, xlims, ylims, n_steps=10):\n\n    # Set x and y ranges\n    xrange, yrange = np.linspace(xlims[0], xlims[1], n_steps), np.linspace(ylims[0], ylims[1], n_steps)\n\n    # Initialize 2D grid with x,y values and additional grids to track derivatives\n    X, Y = np.meshgrid(xrange, yrange)\n    U, V = np.zeros(X.shape), np.zeros(Y.shape)\n\n    # Compute the gradient at each x,y position\n    for i in range(len(xrange)):\n        for j in range(len(xrange)):\n            U[i,j] = lambdify((n1, n2), dn1)(X[i,j], Y[i,j]) #change in n1\n            V[i,j] = lambdify((n1, n2), dn2)(X[i,j], Y[i,j]) #change in n2\n\n    # Plot figure\n    ax.quiver(X,Y,U,V, linewidth=1) #plot arrow from X,Y that moves U in x and V in y\n</pre> <pre>\nfig, ax = plt.subplots()\n\n# plot nullclines\nn1s = np.linspace(0,100,100)\nax.plot(n1s, [nullcline_1.subs(n1,i) for i in n1s], label='$n_1$ nullcline')\nax.plot(n1s, [nullcline_2.subs(n1,i) for i in n1s], label='$n_2$ nullcline')\n\n# plot vector field\nplot_vector_field(ax, dn1dt, dn2dt, xlims=[0,100], ylims=[0,100])\n\nplt.xlabel('number of birds on island 1, $n_1$')\nplt.ylabel('number of birds on island 2, $n_2$')\nplt.xlim(0,100)\nplt.ylim(0,100)\nplt.legend()\nplt.show()\n</pre> <p></p>"},{"location":"labs/tutorial-04/#problem","title":"Problem","text":"<p>Competition between two species can be modelled with the two-species Lotka-Volterra model, which is an extension of logistic growth to two species,</p> \\[ \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} = n_1 r_1 \\left(1 - \\frac{n_1 + \\alpha_{12}n_2}{K_1}\\right) \\] \\[ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} = n_2 r_2 \\left(1 - \\frac{n_2 + \\alpha_{21}n_1}{K_2}\\right). \\] <p>Here \\(n_i\\) is the number of individuals of species \\(i\\), \\(r_i\\) is the intrinsic growth rate of species \\(i\\), \\(K_i\\) is the carrying capacity of species \\(i\\), and \\(\\alpha_{ij}\\) is the competitive effect an individual of species \\(j\\) has on an individual of species \\(i\\).</p> <p>Solve for the nullclines. </p> <p>Below is a phase plane with the nullclines plotted. Where are the equilibria? Determine the direction of movement in each section by starting in the top right, where \\(n_1\\) and \\(n_2\\) are much larger than \\(K_1\\) and \\(K_2\\). Which equilibrium appears to be stable?</p> <pre>\n# variables\nn1, n2 = symbols('n1, n2')\n\n# parameter values\nr1, r2 = 0.5, 0.5\nk1, k2 = 1000, 1000\na12, a21 = 0.5, 0.5\n\n# differential equations\ndn1 = r1 * n1 * (1 - (n1 + a12 * n2) / k1)\ndn2 = r2 * n2 * (1 - (n2 + a21 * n1) / k2)\n\n# find nullclines\nnullcline_1 = solve(Eq(dn1, 0),n1)\nnullcline_2 = solve(Eq(dn2, 0),n2)\n\nfig, ax = plt.subplots()\n\nnmax = 2000\nxrange, yrange = np.linspace(0, nmax, 100), np.linspace(0, nmax, 100)\nfor cline in nullcline_1:\n    ax.plot([cline.subs(n2,i) for i in yrange], yrange, color=plt.cm.tab10(0), label='$n_1$')\nfor cline in nullcline_2:\n    ax.plot(xrange, [cline.subs(n1,i) for i in xrange], color=plt.cm.tab10(1), label='$n_2$')\n\nplt.xlabel('$n_1$')\nplt.ylabel('$n_2$')\nplt.legend()\nplt.show()\n</pre> <p></p> <pre>\n\n</pre>"},{"location":"labs/tutorial-05/","title":"Tutorial 5","text":""},{"location":"labs/tutorial-05/#tutorial-5-long-term-dynamics-of-linear-multivariate-models","title":"Tutorial 5: long-term dynamics of linear multivariate models","text":"Run notes interactively?"},{"location":"labs/tutorial-05/#discrete-time","title":"Discrete time","text":"<p>We now know that the general solution of a system of linear recursion equations, \\(\\vec{x}(t+1) = \\mathbf{M}\\vec{x}(t) + \\vec{m}\\), can be written </p> \\[ \\vec{x}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}(\\vec{x}(0)-\\hat{\\vec{x}}) + \\hat{\\vec{x}}, \\] <p>where \\(\\hat{\\vec{x}}=-(\\mathbf{M} - \\mathbf{I})^{-1}\\vec{m}\\) is the equilibrium, \\(\\mathbf{A}\\) is a matrix with right eigenvectors as columns, and \\(\\mathbf{D}\\) is a diagonal matrix with eigenvalues along the diagonal.</p> <p>Note that as time proceeds, the eigenvalues in \\(\\mathbf{D}\\) get raised to higher and higher powers. As a result, the eigenvalue with the largest absolute value, which we call the leading eigenvalue, comes to dominate. To see this, let the leading eigenvalue be \\(\\lambda_1\\) and factor it out of \\(\\mathbf{D}^t\\),</p> \\[ \\mathbf{D}^t = \\lambda_1^t  \\begin{pmatrix}  1 &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; (\\lambda_2/\\lambda_1)^t &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\  0 &amp; 0 &amp; \\cdots &amp; (\\lambda_n/\\lambda_1)^t\\\\   \\end{pmatrix}. \\] <p>Since \\(|\\lambda_i/\\lambda_1|&lt;1\\) for all \\(i\\), for large \\(t\\) these all go to zero and we have</p> \\[ \\mathbf{D}^t \\approx \\tilde{\\mathbf{D}}^t  \\equiv \\lambda_1^t  \\begin{pmatrix}  1 &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; 0 &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\  0 &amp; 0 &amp; \\cdots &amp; 0\\\\   \\end{pmatrix}. \\] <p>Warning</p> <p>Note that this does not work if more than one eigenvalue share the largest absolute value.</p> <p>We can therefore approximate \\(\\vec{x}(t)\\) after a sufficient amount of time as</p> \\[ \\begin{aligned} \\tilde{\\vec{x}}(t) &amp;= \\mathbf{A}\\tilde{\\mathbf{D}}^t\\mathbf{A}^{-1}(\\vec{x}(0)-\\hat{\\vec{x}}) + \\hat{\\vec{x}}\\\\ &amp;= \\lambda_1^t \\vec{v}_1 \\vec{u}_1 (\\vec{x}(0)-\\hat{\\vec{x}}) + \\hat{\\vec{x}}, \\end{aligned} \\] <p>where \\(\\vec{v}_1\\) is the right eigenvector associated with the leading eigenvalue (the first column of \\(\\mathbf{A}\\)) and \\(\\vec{u}_1\\) is the left eigenvector associated with the leading eigenvalue, \\(\\lambda_1\\) (the first row of \\(\\mathbf{A}^{-1}\\)).</p> <p>Warning</p> <p>Finding the left eigenvectors via the inverse of \\(\\mathbf{A}\\) guarantees that the eigenvectors have been scaled such that \\(\\vec{u}_i\\vec{v}_i = 1\\). If you instead find the left eigenvectors by solving \\(\\vec{u}\\mathbf{M}=\\lambda\\vec{u}\\), make sure you scale \\(\\vec{u}_1\\) so that \\(\\vec{u}_1\\vec{v}_1 = 1\\), eg, divide your unscaled left eigenvector, \\(\\vec{u}_1'\\), by \\(\\vec{u}_1'\\vec{v}_1\\). Otherwise the long-term approximation will be off by a constant factor.</p> <p>This convenient approximation tells us two things:</p> <ul> <li>the system will eventually converge to the equilibrium \\(\\hat{\\vec{x}}\\) if and only if the leading eigenvalue has absolute value less than one, \\(|\\lambda_1|&lt;1\\)</li> <li>since \\(\\vec{u}_1 (\\vec{x}(0)-\\hat{\\vec{x}})\\) is a scalar (row vector times column vector), \\(\\vec{v}_1\\) tells us the relative deviation of each \\(x_i\\) from its equilibrium value in the long term, eg, if \\(\\vec{v}_1= \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\) then \\(x_1\\) is eventually twice as far from its equilibrium value than \\(x_1\\) is</li> </ul>"},{"location":"labs/tutorial-05/#continuous-time","title":"Continuous time","text":"<p>In continuous time we the general solution is</p> \\[ \\vec{x}(t) = \\mathbf{A}\\exp(\\mathbf{D}t)\\mathbf{A}^{-1}(\\vec{x}(0) - \\hat{\\vec{x}}) + \\hat{\\vec{x}}, \\] <p>with \\(\\hat{\\vec{x}}=-\\mathbf{M}^{-1}\\vec{m}\\).</p> <p>As \\(t\\) increases \\(\\exp(\\mathbf{D}t)\\) becomes dominated by the eigenvalue with the largest (ie, most positive) value, which we call the leading eigenvalue. The long term dynamics can then be approximated </p> \\[ \\vec{x}(t) \\approx \\exp(\\lambda_1 t) \\vec{v}_1 \\vec{u}_1 (\\vec{x}(0)-\\hat{\\vec{x}}) + \\hat{\\vec{x}}. \\] <p>Therefore</p> <ul> <li>the system will eventually converge to the equilibrium \\(\\hat{\\vec{x}}\\) if and only if the leading eigenvalue is negative, \\(\\lambda_1&lt;0\\)</li> <li>\\(\\vec{v}_1\\) tells us the relative deviation of each \\(x_i\\) from its equilibrium value in the long term</li> </ul>"},{"location":"labs/tutorial-05/#problem","title":"Problem","text":"<p>Many  eukaryotic  genes  have  both  exons  and  introns,  where  only  exons code  for  protein  sequence.  Messenger RNAs (mRNAs)  transcribed  from  such  genes  initially  include the  introns,  which  must  be  spliced  out  before  the  mRNAs  can  be  translated into proteins.  Let \\(n_1\\) be the number of unspliced \u201cpre-mRNAs,\u201d which are produced by the cell at rate \\(c\\) and let \\(n_2\\) be the number of spliced \u201cprocessed mRNAs\u201d.  If pre-mRNAs are  spliced at rate \\(a\\) and processed mRNAs degrade at rate \\(d\\), the numbers of pre-mRNAs and processed mRNAs change according to</p> \\[ \\begin{align} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &amp;= c - a n_1 \\\\  \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} &amp;= a n_1 - d n_2 . \\end{align} \\] <p>Writing this in matrix form, \\(\\mathrm{d}\\vec{n}/\\mathrm{d}t = \\mathbf{M}\\vec{n} + \\vec{m}\\), determine \\(\\mathbf{M}\\) and \\(\\vec{m}\\). Determine the eigenvalues (and associated eigenvectors if you have time). Is the equilibrium stable? </p>"},{"location":"labs/tutorial-06/","title":"Tutorial 6","text":""},{"location":"labs/tutorial-06/#tutorial-6-linear-multivariate-practice","title":"Tutorial 6: Linear multivariate practice","text":"Run notes interactively?"},{"location":"labs/tutorial-06/#model","title":"Model","text":"<p>Metastasis is a process by which cancer cells spread throughout the body. Sometimes cancer cells move via the bloodstream and become lodged in the capillaries of different organs. Some of these new cells then move across the capillary wall, where they initiate new tumours. We will construct a model for the dynamics of cancer cells lodged in the capillaries of an organ, \\(C\\), and the number of cancer cells that have actually invaded that organ, \\(I\\). Suppose that cells are lost from the capillaries by dislodgement or death at per capita rate \\(\\delta_1\\) and that they invade the organ from the capillaries at a per capita rate \\(\\beta\\). Once cells are in the organ they die at rate \\(\\delta_2\\), and the cancer cells replicate at a per capita rate \\(\\rho\\). This gives the differential equations</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}C}{\\mathrm{d}t} &amp;= -\\delta_1 C - \\beta C\\\\ \\frac{\\mathrm{d}I}{\\mathrm{d}t} &amp;= \\beta C -\\delta_2 I + \\rho I. \\end{aligned} \\]"},{"location":"labs/tutorial-06/#questions","title":"Questions","text":"<p>Write this in matrix form, \\(\\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} = \\mathbf{M}\\vec{n}\\), specifying what \\(\\mathbf{M}\\) and \\(\\vec{n}\\) are.</p> <p>What is the equilibrium of this model?</p> <p>Calculate the eigenvalues of \\(\\mathbf{M}\\). When is the equilibrium stable? (Based on the description of the model we can assume all parameters are positive.) Explain what this means biologically.</p> <p>Assuming the system is unstable, calculate the right eigenvector associated with the leading of \\(\\mathbf{M}\\). Where are most of the cells in the long term?</p>"},{"location":"labs/tutorial-07/","title":"Tutorial 7","text":""},{"location":"labs/tutorial-07/#tutorial-7-lotka-volterra-competition","title":"Tutorial 7: Lotka-Volterra competition","text":"Run notes interactively?"},{"location":"labs/tutorial-07/#model","title":"Model","text":"<p>As we saw in Tutorial 4, competition between two species can be modelled with the two-species Lotka-Volterra model, which is an extension of logistic growth to two species,</p> \\[ \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} = n_1 r_1 \\left(1 - \\frac{n_1 + \\alpha_{12}n_2}{K_1}\\right) \\] \\[ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} = n_2 r_2 \\left(1 - \\frac{n_2 + \\alpha_{21}n_1}{K_2}\\right). \\] <p>Here \\(n_i\\) is the number of individuals of species \\(i\\), \\(r_i\\) is the intrinsic growth rate of species \\(i\\), \\(K_i\\) is the carrying capacity of species \\(i\\), and \\(\\alpha_{ij}\\) is the competitive effect an individual of species \\(j\\) has on an individual of species \\(i\\). Assume all parameters are positive.</p> <p>In Tutorial 4 we solved for the nullclines, which are plotted below.</p> <pre>\nimport matplotlib.pyplot as plt\nfrom sympy import *\nimport numpy as np\n\n# variables\nn1, n2 = symbols('n1, n2')\n\n# parameter values\nr1, r2 = 0.5, 0.5\nk1, k2 = 1000, 1000\na12, a21 = 0.5, 0.5\n\n# differential equations\ndn1 = r1 * n1 * (1 - (n1 + a12 * n2) / k1)\ndn2 = r2 * n2 * (1 - (n2 + a21 * n1) / k2)\n\n# find nullclines\nnullcline_1 = solve(Eq(dn1, 0),n1)\nnullcline_2 = solve(Eq(dn2, 0),n2)\n\nfig, ax = plt.subplots()\n\nnmax = 2000\nxrange, yrange = np.linspace(0, nmax, 100), np.linspace(0, nmax, 100)\nfor cline in nullcline_1:\n    ax.plot([cline.subs(n2,i) for i in yrange], yrange, color=plt.cm.tab10(0), label='$n_1$')\nfor cline in nullcline_2:\n    ax.plot(xrange, [cline.subs(n1,i) for i in xrange], color=plt.cm.tab10(1), label='$n_2$')\n\nplt.xlabel('$n_1$')\nplt.ylabel('$n_2$')\nplt.legend()\nplt.show()\n</pre> <p></p>"},{"location":"labs/tutorial-07/#problem","title":"Problem","text":"<p>Solve for the four equilibria. When are they biologically valid?</p> <p>Derive the Jacobian.</p> <p>Determine the conditions for local stability for each of the three equilibria where at least one species is extinct.</p>"},{"location":"lectures/lecture-01/","title":"Lecture 1","text":""},{"location":"lectures/lecture-01/#lecture-1-why-and-how-to-build-a-model","title":"Lecture 1: Why and how to build a model","text":"Run notes interactively?"},{"location":"lectures/lecture-01/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Why to build a model</li> <li>How to build a model</li> <li>Summary</li> </ol>"},{"location":"lectures/lecture-01/#1-why-to-build-a-model","title":"1. Why to build a model","text":"<p>Mathematics permeates ecology and evolution, from simple back-of-the-envelope calculations to the development of sophisticated mathematical models. This is because mathematics is a unique tool that can take what we already know (or assume to be true) and rigorously lead us to the logical conclusions.</p> <p>To see this (while also introducing you to the kind of models I work with), let's look at an example. See sections 1.2-1.4 of the text for another motivating example (within-patient HIV dynamics).</p>"},{"location":"lectures/lecture-01/#motivating-model-evolution-during-extreme-events","title":"Motivating model: evolution during extreme events","text":"<p>This is a model that I helped Dr. Kelsey Lyberger (then a PhD student at UC Davis) with in our paper, Lyberger et al 2021.</p> <sup>Kelsey Lyberger, doing Daphnia fieldwork I suppose.</sup> <p>Kelsey was interested in how populations respond to extreme climatic events, like lizards to hurricanes. It has long been clear that such events can impact the size of a population, e.g., by causing extra mortality, and may in fact put populations at risk of extinction. More recently it has become apparent that extreme events can also impose strong natural selection, and that populations can quickly adapt to the new environment. Some examples include:</p> <ul> <li>Hurricanes select on lizard limbs and toe pads</li> <li>Ice-storms select on sparrow body size</li> <li>Droughts select on Darwin finch beaks</li> <li>Droughts select on flowering time in Brassica</li> </ul> <sup>Lizard being blown off perch by a leaf blower. Source: colindonihue.com</sup> <p>Now, how should such rapid adaptive evolution impact population size? This is the question Kelsey set out to answer with a mathematical model.</p> Details of Kelsey's model <p>Kelsey assumed each individual has a quantitative genetic trait, such as lizard limb length, that is determined by many alleles of small effect plus some environmental noise. Fitness is assumed to be a bell-shaped function of the difference between the optimum trait value, \\(\\theta\\), and an individual's trait value, \\(z\\), which we write as \\(W(\\theta - z)\\). </p> <p>The change in population size, \\(N\\), from one generation to rate the next is the current population size times mean fitness, \\(\\overline{W}(\\theta - \\bar{z})\\), the latter depending on the population mean trait value, \\(\\bar{z}\\). This gives</p> <p>\\(\\Delta N = N \\overline{W}(\\theta - \\bar{z}).\\)</p> <p>The change in the mean trait value from one generation to the next is roughly the product of genetic variance in the trait, \\(V_g\\), and the strength of selection. The strength of selection is defined as the derivative of the natural logarithm of population mean fitness with respect to the mean trait value, \\(\\mathrm{d}\\ln\\overline{W}(\\theta - \\bar{z})/\\mathrm{d}\\bar{z}\\). This gives</p> <p>\\(\\Delta \\bar{z} = V_g \\frac{\\mathrm{d}}{\\mathrm{d}\\bar{z}}\\ln\\overline{W}(\\theta - \\bar{z}).\\)</p> <p>Together, these coupled recursion equations, \\(\\Delta N\\) and \\(\\Delta \\bar{z}\\), can be used to describe how evolution affects population size under an extreme event, which is modeled as a sudden but temporary change in the optimum phenotype, \\(\\theta\\).</p> <p>Below we numerically iterate the two equations above (dashed lines) and compare with stochastic simulations (solid lines) to recreate Figure 1 from Lyberger et al. With an activated kernel, you can tinker and run the code below to adjust the plot (the simulations take a few seconds). </p> <pre>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# choose parameter values\nVo, Ve, event_duration, seed = 0.75, 0, 1, 0\ngenerations, K, w, lmbda, event_time, initial_theta_t, dtheta_t, event_duration = 110, 500, 1, 2, 100, 0, 2.5, 1\nnreps = 10\n\n# recursions\ndef recursions(t0,n0,z0,lmbda,w,K,event_time,event_duration,initial_theta_t,dtheta_t,Vo,Ve,tmax=np.inf):\n\n    # initialize\n    t,nt,zt=t0,n0,z0\n\n    Vs = w**2 + Ve\n    Vg = (2*Vo - Vs + np.sqrt(4*Vo**2 + 12*Vo*Vs + Vs**2))/4\n    Vt = Vg + Vs\n\n    # yield and update\n    while t np.random.uniform(0,1) else False for p in prob_survival])\n        if len(survived) == 0:\n            break\n        members = members[survived]\n\n        # random mating\n        offspring = []\n        for m in np.random.choice(members, len(members)):\n            if len(offspring) &gt; K: #if more than K offspring already then choose K and stop matings\n                offspring = np.random.choice(offspring, K)\n                break\n            else:\n                n_off = np.random.poisson(lmbda)\n                mate = np.random.choice(members)\n                offspring += [(m + mate)/2 for _ in range(n_off)] #give offspring mean parental value\n\n        # sample new trait values for offspring\n        offspring = np.array(offspring)\n        offspring = np.random.normal(offspring, Vo) #add segregation variance\n\n        # record statistics\n        population_size.append(len(offspring))\n        mean_breeding_value.append(np.mean(offspring))\n\n        # update for next generation\n        members = offspring\n\n    return (\n        np.arange(0, generations-event_time+1), #times\n        np.array(population_size[event_time-1:]), #population size\n        np.array(mean_breeding_value[event_time-1:]) #mean trait value\n    )\n\n# initialize plot\nfig, ax = plt.subplots(2, sharex=True)\nfig.set_size_inches(8,6)\n\n# for each segregation (Vo) and environment variance (Ve) parameter combination\nfor Vo, Ve, c, lab in [[0.75, 0, 'black', 'with evolution'], [0, 1, 'red', 'without evolution']]:\n\n    # plot recursions\n    tnz = recursions(0,K,initial_theta_t,lmbda,w,K,event_time,event_duration,initial_theta_t,dtheta_t,Vo/2,Ve,generations)\n    tnzs = np.array([vals for vals in tnz])\n    ax[0].plot(tnzs[event_time:,0]-event_time,tnzs[event_time:,1], color=c, linestyle='--')\n    ax[1].plot(tnzs[event_time:,0]-event_time,tnzs[event_time:,2], color=c, linestyle='--')\n\n    # plot stochastic simulations\n    simulations = np.array([lyberger_model(Vo, Ve, event_duration, s, generations, K, w, lmbda, event_time, initial_theta_t, dtheta_t) for s in range(nreps)])\n    ax[0].plot(simulations[:,0].T, simulations[:,1].T, color=c, alpha=0.3);\n    ax[1].plot(simulations[:,0].T, simulations[:,2].T, color=c, alpha=0.3);\n\n    # hack together only one instance of the legend\n    ax[0].plot([np.min(simulations[:,0].T)], [np.min(simulations[:,1].T)], alpha=1,\n                  label = lab, color=c)\n    ax[1].plot([np.min(simulations[:,0].T)], [np.min(simulations[:,2].T)], alpha=1,\n                  label = lab, color=c)\n\n# add environmental event duration\nax[0].fill_between([0,event_duration], y1=K, alpha=0.2)\nax[1].fill_between([0,event_duration], y1=-0.2, y2=2, alpha=0.2)\n\n# add labels\nax[0].set_ylabel('Population size', fontsize=12)\nax[1].set_ylabel('Mean trait value', fontsize=12)\nax[1].set_xlabel('Generation', fontsize=14)\n\n# add legend\nplt.legend(frameon=False)\nplt.show()\n\n\n<p></p>\n<p>The key result, that you can see in the plot above, is that when extreme events are short, adaptive evolution (black lines) can paradoxically reduce population size (relative to the red lines, where there is no evolution). The reason for this is that, while during the extreme event (shaded section) evolution is adaptive, once the extreme event ends the population finds itself maladapted to the original environment. Adaptive evolution can therefore hamper population persistence, and this is an important thing to keep in mind when documenting rapid adaptive evolution in response to extreme events -- it is not necessarily a good thing for the species (or our conservation goals).</p>\n<p></p>"},{"location":"lectures/lecture-01/#2-how-to-build-a-model","title":"2. How to build a model","text":"<p>Although many problems require specific formulations and assumptions, there are a few general principles to constructing a model.</p>"},{"location":"lectures/lecture-01/#i-formulate-the-question","title":"i. Formulate the question","text":"<ul>\n<li>What do you want to know?</li>\n<li>Describe the model in the form of a question.</li>\n<li>Simplify, Simplify!</li>\n<li>Start with the simplest, biologically reasonable description of the problem.</li>\n</ul>\n<p>For example, we might ask: how does immigration affect population size?</p>"},{"location":"lectures/lecture-01/#ii-determine-the-basic-ingredients","title":"ii. Determine the basic ingredients","text":"<ul>\n<li>Define the variables in the model.</li>\n<li>Describe any constraints on the variables.</li>\n<li>Describe any interactions between variables.</li>\n<li>Decide whether you will treat time as continuous or discrete.</li>\n<li>Choose a time scale (i.e., decide what a time step equals in discrete time and specify whether rates will be measured per second, minute, day, year, generation, etc.).</li>\n<li>Define the parameters in the model.</li>\n<li>Describe any constraints on the parameters.</li>\n</ul>\n<p>In our example, there is a single variable, \\(n(t)\\), denoting population size at time \\(t\\). This must be non-negative, \\(n(t)\\geq0\\), to be biologically valid. </p>\n<p>We'll look at both continuous and discrete time, with an arbitrary time scale (if we wanted to plug in some numerical parameter values we'd have to specify this, but here we won't).</p>\n<p>In continuous time our parameters will be immigration rate (\\(m\\)), per capita birth rate (\\(b\\)), and per capita death rate (\\(d\\)). These must all be non-negative under our interpretation. The units of \\(m\\) are individuals/time while the units of \\(b\\) and \\(d\\) are simply 1/time (e.g., for birth we have the number of individuals produced per individual per time, so that individuals cancels out and we are left with 1/time).</p>\n<p>In discrete time the parameters will be the average number of immigrants per time step (\\(M\\)), the average number of offspring per individual per time step (\\(B\\)), and the fraction of individuals that die each time step (\\(D\\)). These must all be non-negative and \\(D\\) must also be less than or equal to 1 as it is a fraction. </p>"},{"location":"lectures/lecture-01/#iii-qualitatively-describe-the-biological-system","title":"iii. Qualitatively describe the biological system","text":"<ul>\n<li>For continuous-time models, draw a flow diagram to describe changes to the variables over time.</li>\n<li>For discrete-time models, draw a life-cycle diagram with all the events that occur each time unit.</li>\n<li>For discrete time models with multiple variables and events, construct a table listing the outcome of every event.</li>\n</ul>\n<p>For our continuous-time example we could draw the following:</p>\n\n<pre><code>graph LR;\n    A((n)) --birth--&gt; A;\n    B[ ] --migration--&gt; A;\n    A --death--&gt; C[ ];\n    style B height:0px;\n    style C height:0px;</code></pre>\n\n\n<p>For our discrete-time example, if we assume migration, then birth, then death each time step, we could draw the following:</p>\n\n<pre><code>graph LR;\n    A((n)) --migration--&gt; B((n'));\n    B --birth--&gt; C((n''));\n    C --death--&gt; A;</code></pre>\n\n\n<p>We'll see an example of a table of events later in the course.</p>"},{"location":"lectures/lecture-01/#iv-quantitatively-describe-the-biological-system","title":"iv. Quantitatively describe the biological system","text":"<ul>\n<li>Using the diagrams and tables as a guide, write down the equations. Perform checks. Are the constraints on the variables still met as time passes? Make sure that the units of the right-hand side equal those on the left-hand side.</li>\n<li>For continuous-time models, add rates for arrows coming in to the flow diagram and subtract rates for arrows coming out of the flow diagram. </li>\n</ul>\n<p>For example, in the model shown above the rate of change in the number of individuals, \\(\\frac{\\mathrm{d} n}{\\mathrm{d} t}\\), is</p>\n\\[\n\\begin{aligned}\n\\frac{\\mathrm{d} n}{\\mathrm{d} t} &amp;= m + b n(t) - d n(t)\\\\\n&amp;= m + (b - d) n(t)\n\\end{aligned}\n\\]\n<p>This is a differential equation.</p>\n<p>In discrete time, you must take into account the order of events when constructing equations. To build an equation for the population size in the next generation, \\(n(t+1)\\), based on the life-cycle diagram above, first construct an equation for each event </p>\n\\[n' = n(t) + M\\]\n\\[n'' = n' + Bn'\\]\n\\[n(t+1) = n'' - Dn''\\]\n<p>Next, substitute \\(n''\\) and then \\(n'\\) into the equation for \\(n(t+1)\\) to write \\(n(t+1)\\) in terms of \\(n(t)\\)</p>\n\\[\n\\begin{aligned}\nn(t+1) &amp;= n'' \u2212 Dn'' \\\\\n&amp;= (n' + Bn') \u2212 D(n' + Bn') \\\\\n&amp;= n'(1 + B \u2212 D \u2212 DB) \\\\\n&amp;= (n(t) + M)(1 + B \u2212 D \u2212 DB) \\\\\n\\end{aligned}\n\\]\n<p>We now have a recursion equation that correctly takes into account the order of the life cycle (migration, birth, death) and the point at which the census is taken (immediately after death).</p>"},{"location":"lectures/lecture-01/#v-analyze-the-equations","title":"v. Analyze the equations","text":"<ul>\n<li>Start by using the equations to simulate and graph the changes to the system over time. </li>\n<li>Choose and perform appropriate analyses.</li>\n<li>Make sure that the analyses can address the problem.</li>\n</ul>\n<p>We'll save the mathematical analyses for later in the course, but let's try simulating the recursion equations here to get a sense of the dynamics (we'll learn more about how to do this in the next lecture).</p>\n<pre>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# define a genarator to efficiently run the recursion\ndef recursion(n0, b, d, m, tmax):\n    t,n=0,n0 #initial conditions\n    while t &lt; tmax:\n        yield t,n #output current state\n        t += 1 #update time\n        n = (n + m) * (1 + b - d - b*d) #update population size using the recursion equation we derived above\n\n# simulate and plot\nfig, ax = plt.subplots()\n\nn0, b, d, tmax, dt = 1, 0.05, 0.1, 100, 0.1 #parameter values\nfor m in [0,1]:\n    tn = recursion(n0, b, d, m, tmax) #define recursion\n    tns = np.array([vals for vals in tn]) #get output\n    ax.scatter(tns[:,0], tns[:,1], label='M=%s'%m) #plot population size at each time\n\nax.set_xlabel('time, $t$')\nax.set_ylabel('population size, $n(t)$')\nplt.legend()\nplt.show()\n</pre>\n\n<p></p>\n<p>How does migration affect population size?</p>"},{"location":"lectures/lecture-01/#vi-checks-and-balances","title":"vi. Checks and balances","text":"<ul>\n<li>Check the results against data or any known special cases.</li>\n<li>Determine how general the results are.</li>\n<li>Consider alternatives to the simplest model.</li>\n<li>Extend or simplify the model, as appropriate, and repeat steps 2-5.</li>\n</ul>\n<p>If \\(b&gt;d\\) (or \\(B&gt;D\\)) the population grows without bound (try this in the code above). But competition should prevent unbounded population growth. We could therefore extend the model to include competition.</p>"},{"location":"lectures/lecture-01/#vii-relate-the-results-back-to-the-question","title":"vii. Relate the results back to the question","text":"<ul>\n<li>Do the results answer the biological question?</li>\n<li>Are the results counter-intuitive? Why?</li>\n<li>Interpret the results verbally, and describe conceptually any new insights into the biological process.</li>\n<li>Describe potential experiments.</li>\n</ul>\n<p>Immigration appears to increase the population size (compare \\(M=0\\) and \\(M=1\\) in the plot above), though we'd need to do the mathematical analysis to be more confident in this statement. </p>\n<p>There is some counter-intuitiveness in the discrete model: try creating the recursion equation for a different order of events in the lifecycle. The recursion equation depends on the order and will therefore lead to different dynamics.</p>\n<p>Potential experiments incude manipulating immigration in lab populations (e.g., bacteria) or comparing population sizes on islands that are different distances from the mainland.</p>\n<p></p>"},{"location":"lectures/lecture-01/#3-summary","title":"3. Summary","text":"<ul>\n<li>Mathematical models take us from assumptions to conclusions</li>\n<li>There is a recipe to build them</li>\n</ul>\n<p>Practice questions from the textbook: 2.1 - 2.8</p>"},{"location":"lectures/lecture-02/","title":"Lecture 2","text":""},{"location":"lectures/lecture-02/#lecture-2-numerical-and-graphical-techniques-univariate","title":"Lecture 2: Numerical and graphical techniques (univariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-02/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Plots of variables over time</li> <li>Plots of variables as a function of themselves</li> <li>Summary</li> </ol> <p>Before we jump into more rigorous mathematical analyses, we\u2019re first going to learn how to get a feel for the dynamics of our models.</p> <p>To do so we\u2019re going to choose some particular numerical values for our parameters and then use our models to predict what happens over time.</p> <p>The downside of this approach is that we often won\u2019t know the parameter values to choose and, regardless, choosing particular values doesn\u2019t tell us about the dynamics of our model more generally.</p> <p>The upside is that this approach can highlight errors or reveal unexpected patterns that guide future mathematical analyses.</p> <p></p>"},{"location":"lectures/lecture-02/#1-plots-of-variables-over-time","title":"1. Plots of variables over time","text":"<p>The first plot is of the variables over time. We've already seen two examples of this in the previous lecture, for both evolution during extreme events and migration. In both cases we wrote recursive functions (actually, \"generator\"s in Python) to generate values of the variables at sequential time points.</p> <p>Here let's look at a simpler example to demonstrate the essence of the method by hand. Take the recursion for the migration example \\(n(t+1) = (n(t)+M)(1+B-D-BD)\\), ignore migration, \\(M=0\\), and create a new parameter, \\(R=1+B-D-BD\\). Our simplified recursion is then </p> <p>\\(n(t+1)=Rn(t)\\).</p> <p>This is called exponential growth (technically \"geometric\" growth in discrete time) and the only parameter of the model, \\(R\\), is referred to as the reproductive value.</p> <p>To plot a variable over time we</p> <ul> <li>choose an initial condition,</li> <li>choose parameter values, and</li> <li>numerically iterate the equation.</li> </ul> <p>Here let's choose initial condition \\(n(0)=100\\) and parameter value \\(R=2\\). Now we can iterate:</p> <p>\\(n(1) = Rn(0) = 2\\times100 = 200\\)</p> <p>\\(n(2) = Rn(1) = 2\\times200 = 400\\)</p> <p>\\(\\vdots\\)</p> <p>After doing this a few more times we can plot</p> <pre>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# recursion\ndef exp_growth(n0, R, tmax):\n    '''define generator for exponential growth'''\n\n    # set initial values\n    t,n=0,n0\n\n    # yield new values\n    while t &lt; tmax:\n        yield t,n\n\n        # update values\n        t += 1\n        n = n * R\n\n# evaluate \ntn = exp_growth(n0=100, R=2, tmax=10) #choose some parameter values and initial conditions\ntns = np.array([vals for vals in tn]) #get all the t, n(t) values\n\n# plot\nfig, ax = plt.subplots()\nax.plot(tns[:,0], tns[:,1], marker = '.', markersize = 10)\nax.set_xlabel('time step, $t$')\nax.set_ylabel('population size, $n(t)$')\nplt.show()\n</pre> <p></p> <p>We can get a feel for the model by adjusting initial conditions and parameter values. What happens to the population size when you set \\(0&lt;R&lt;1\\) in the code above?</p> <p></p>"},{"location":"lectures/lecture-02/#2-plots-of-variables-as-a-function-of-themselves","title":"2. Plots of variables as a function of themselves","text":"<p>OK, so now we\u2019ll move on to a plot that is easier to generate and is very useful for models with just one variable (which is what we\u2019ve been working with so far).</p> <p>Instead of plotting the variable as a function of time, we\u2019ll plot the variable as a function of the variable in the previous time, e.g., plotting \\(n(t+1)\\) as a function of \\(n(t)\\). We could do this for exponential growth but let's move on to something else for variety.</p> <p>Consider a population with two types of individuals, \\(n_A(t)\\) with allele \\(A\\) and \\(n_a(t)\\) with allele \\(a\\). The frequency of \\(A\\) in the population is \\(p(t) = \\frac{n_A(t)}{n_A(t) + n_a(t)}\\). This is the variable we wish to track.</p> <p>Let\u2019s assume that each individual with an \\(A\\) leaves \\(W_A\\) descendants in the next generation and each individual with an \\(a\\) leaves \\(W_a\\) descendants. These \\(W_i\\) are referred to as the absolute fitnesses as they determine the (absolute) numbers of individuals with an \\(A\\) and an \\(a\\) in the next generation, \\(n_i(t+1) = W_i n_i(t)\\), for \\(i=A\\) and \\(i=a\\). In other words, each allele grows exponentially with reproductive value \\(R=W_i\\). The frequency of \\(A\\) in the next generation is then</p> \\[ \\begin{aligned} p(t+1)  &amp;= \\frac{n_A(t+1)}{n_A(t+1) + n_a(t+1)} \\\\ &amp;= \\frac{W_A n_A(t)}{W_A n_A(t) + W_a n_a(t)} \\\\ &amp;= \\frac{W_A\\frac{n_A(t)}{n_A(t) + n_a(t)}}{W_A\\frac{n_A(t)}{n_A(t) + n_a(t)} + W_a\\frac{n_a(t)}{n_A(t) + n_a(t)}}\\\\ &amp;= \\frac{W_A p(t)}{W_A p(t) + W_a (1-p(t))}. \\end{aligned} \\] <p>This is the recursion equation we want to plot. It is a classic model called haploid selection. Below is some code that plots \\(p(t+1)\\) as a function of \\(p(t)\\).</p> <pre>\nimport sympy\n\n# Build cobweb plotting function\ndef cobweb_haploid(p0, WA, Wa, max=np.inf):\n    t, pnow, pnext = 0, p0, 0 #initial conditions\n    while t &lt;= max:\n        yield pnow, pnext #current value of p(t) and p_(t+1)\n        pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) #update p_(t+1)\n        yield pnow, pnext #current value of p(t) and p_(t+1)\n        pnow = pnext #update p(t)\n        t += 1 #update t\n\n# Build function for generating figure\ndef plot_haploid_selection(WA, Wa, p0=0.5, ax=None):\n    pt = sympy.symbols('pt') #define our variable p(t)\n\n    # Write out sympy equation\n    f = (WA * pt) / (WA * pt + Wa * (1 - pt)) #the recursion equation\n\n    # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation (turn it into a function)\n    t = np.linspace(0,1,100)\n    fy = sympy.lambdify(pt, f)(t)\n\n    # Build plot\n    if ax == None:\n        fig, ax = plt.subplots()\n    ax.plot(t, fy, color='black', label=f\"$W_A$ = {WA}, $W_a$ = {Wa}\") #plot p_(t+1) as function of p(t)\n    ax.plot(t, t, color='black', linestyle='--') #draw 1:1 line for reference\n\n    # Add cobweb\n    cobweb = np.array([p for p in cobweb_haploid(p0, WA, Wa, max=100)])\n    ax.plot(cobweb[:,0], cobweb[:,1])\n\n    # Annotate and label plot\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,1)\n    ax.set_xlabel(\"allele frequency at $t$, $p(t)$\")\n    ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\")\n    ax.legend(frameon=False)\n    return ax\n\n# Plot figure\nfig, ax = plt.subplots(1,2)\nfig.set_size_inches(12,4)\n\n# First cobweb with WA &gt; Wa\nplot_haploid_selection(WA = 1, Wa = 0.5, ax=ax[0])\n\n# Second cobweb with WA &lt; Wa\nplot_haploid_selection(WA = 0.5, Wa = 1, ax=ax[1])\n\nplt.show()\n</pre> <p></p> <p>There are three components to this plot. First, the solid curve gives the recursion itself (\\(p(t+1)\\) as a function of \\(p(t)\\)). Second, the dashed line shows where \\(p(t+1)=p(t)\\). And third, the blue lines show how the variable changes over multiple time steps. </p> <p>Foreshadowing what is to come, the dashed line is helpful for two reasons. First, it indicates where the variable does not change over time. So wherever the recursion (solid line) intersects with the dashed line is an equilibrium. Second, it reflects \\(p(t+1)\\) back onto \\(p(t)\\), updating the variable. For example, in the left panel above we start with an allele frequency of \\(p(t)=0.5\\), draw a blue vertical line to the recursion to find \\(p(t+1)\\), and then update \\(p(t)\\) to \\(p(t+1)\\) by drawing the horizontal blue line to the dashed line. Now we can ask what \\(p(t+1)\\) is given this updated value of \\(p(t)\\) by drawing another vertical blue line, and so on. Following the blue line we can therefore see where the system is heading, which tells us about the stability of the equilibria. </p> <p>What are the stable equilibria in the two panels above?</p> <p>We can do something very similar for difference and differential equations. Now we plot the rate of change in the variable as a function of the current value of the variable, e.g., plot \\(\\Delta n = n(t+1)-n(t)\\) or \\(dn/dt\\) as a function of \\(n(t)\\).</p> <p>Let's consider haploid selection in continuous time. To derive the differential equation let's first return to exponential growth and turn this into a difference equation,</p> \\[ \\begin{aligned} n(t+1) &amp;= R n(t)\\\\ n(t+1) - n(t) &amp;= R n(t) - n(t)\\\\ \\Delta n &amp;= (R-1)n(t). \\end{aligned} \\] <p>Now recall that \\(R=1+B-D-BD\\) so that \\(R-1=B-D-BD\\). And let's consider a small timestep \\(\\Delta t\\) during which there are \\(B\\Delta t\\) births and \\(D\\Delta t\\) deaths per individual. Then the difference equation over this timestep, \\(\\Delta n = n(t+\\Delta t)-n(t)\\), is</p> \\[ \\Delta n = (B\\Delta t - D\\Delta t - BD(\\Delta t)^2)n(t). \\] <p>We then divide both sides by \\(\\Delta t\\) and take the limit as \\(\\Delta t\\rightarrow 0\\) to get the differential equation</p> \\[ \\begin{aligned} \\frac{\\Delta n}{\\Delta t} &amp;= (B - D - BD\\Delta t)n(t)\\\\ \\lim_{\\Delta t\\rightarrow0}\\frac{\\Delta n}{\\Delta t} &amp;= (B - D)n(t)\\\\ \\frac{\\mathrm{d}n}{\\mathrm{d}t} &amp;= rn(t). \\end{aligned} \\] <p>This is exponential growth in continuous time where \\(r\\) is the per capita growth rate.</p> <p>Now, returning to haploid selection, consider that allele \\(A\\) has growth rate \\(r_A\\) and allele \\(a\\) has growth rate \\(r_a\\), this gives two differential equations for the respective population sizes</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}n_A}{\\mathrm{d}t} &amp;= r_An_A(t)\\\\ \\frac{\\mathrm{d}n_a}{\\mathrm{d}t} &amp;= r_an_a(t). \\end{aligned} \\] <p>We now summon up the qoutient rule from 1st year calculus to derive the differential equation for the frequency of \\(A\\) over time</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}p}{\\mathrm{d}t} &amp;= \\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(\\frac{n_A(t)}{n_A(t)+n_a(t)}\\right)\\\\ &amp;= \\frac{dn_A/dt(n_A(t)+n_a(t)) - n_A(t)(dn_A/dt+dn_a/dt)}{(n_A(t)+n_a(t))^2}\\\\ &amp;= \\frac{r_An_A(t)(n_A(t)+n_a(t)) - n_A(t)(r_An_A(t)+r_an_a(t))}{(n_A(t)+n_a(t))^2}\\\\ &amp;= r_Ap(t) - p(t)(r_Ap(t)+r_a(1-p(t)))\\\\ &amp;= p(t)(1 - p(t))(r_A - r_a)\\\\ &amp;= p(t)(1 - p(t))s. \\end{aligned} \\] <p>The new parameter \\(s=r_A-r_a\\) is called the selection coefficient of \\(A\\) relative to \\(a\\). The plot of \\(dp/dt\\) vs. \\(p\\) is below.</p> <pre>\n# Initialize sympy symbols\np0, s, t = sympy.symbols('p0, s, t')\np = sympy.Function('t')\n\n# Specify differential equation\ndiffeq = sympy.Eq(p(t).diff(t), s * p(t) * (1 - p(t)))\n\n# Convert differential equation RHS to pythonic function\ndp = sympy.lambdify((s, p(t)), diffeq.rhs)\n\n# Plot the curve\nfig, ax = plt.subplots()\n\nfor s_coeff in [0.01, -0.01]:\n    ax.plot(\n        np.linspace(0, 1, 100),\n        dp(s_coeff, np.linspace(0,1, 100)),\n        label=f\"s = {s_coeff}\"\n    )\n\nax.set_xlabel('allele frequency at $t, p$')\nax.set_ylabel('change in allele frequency, $\\mathrm{d}p/\\mathrm{d}t$')\nax.legend(frameon=False)\nplt.show()\n</pre> <p></p> <p>What does this tell us about how allele frequency will change when \\(s&gt;0\\) vs. \\(s&lt;0\\)? And what allele frequencies, \\(p\\), cause more rapid evolution?</p> <p>Now let's simplify the above plots and just indicate the direction (and magnitude) of change in \\(p(t)\\) with time. This is known as a phase-line diagram.</p> <pre>\ndef phase_line_haploid(p0, WA, Wa, max=np.inf):\n    'generator for p(t)'\n    t, pnow, pnext = 0, p0, 0 #initial conditions\n    while t &lt; max:\n        yield pnow #current value of p(t) and p_(t+1)\n        pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow))\n        pnow = pnext #update p(t)\n        t += 1 #update t\n\ndef plot_phase_line_haploid(WA, Wa, p0, max=20, ax=None):\n    'plot phase line'\n\n    # Set up figure\n    if ax==None:\n        fig, ax = plt.subplots()\n        fig.set_size_inches(8,0.25)\n    ax.axhline(0, color='black', linewidth=0.5)\n\n    # Plot phase-line\n    pts = [pt for pt in phase_line_haploid(p0, WA, Wa, max=max)] #pt values\n    ax.plot(\n        pts,\n        np.zeros(max) #dummy y values (0 for all x values) because we want to plot a 1d line\n    )\n\n    # Plot vector field\n    marker = '&gt;' if pts[2] &gt; pts[1] else '&lt;' #determine which direction to point based on first 2 time points\n    ax.scatter(\n        pts,\n        np.zeros(max),#dummy y again\n        marker=marker, s=150\n    )\n\n    # Remove background axes\n    ax.set_ylabel('$p$', rotation=0)\n    ax.set_xlabel(f\"$W_A$ = {WA}, $W_a$ = {Wa}, $p_0$ = {p0}\")\n    ax.get_yaxis().set_ticks([])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.set_xlim(0,1)\n    plt.show()\n\nplot_phase_line_haploid(WA=1, Wa=0.5, p0=0.01)\n\nplot_phase_line_haploid(WA=0.5, Wa=1, p0=0.99)\n</pre> <p></p> <p></p> <p>As above, we see the frequency of \\(A\\) approaches \\(p=1\\) when \\(W_A&gt;W_a\\) (i.e., \\(s&gt;0\\)) and \\(p=0\\) when \\(W_a&gt;W_A\\) (i.e., \\(s&lt;0\\)). We also notice, as above, the changes are fastest (fewer, longer arrows) at intermediate frequencies.</p> <p></p>"},{"location":"lectures/lecture-02/#3-summary","title":"3. Summary","text":"<p>To get a feel for a model it is helpful to plot some numerical examples:</p> <ul> <li>plot the variable as a function of time (\"simulate\")</li> <li>plot the variable (or change in variable) as a function of itself</li> </ul> <p>We've also just been introduced to two classic models in ecology and evolution, in both discrete and continuous time:</p> <ul> <li>exponential growth</li> <li>haploid selection</li> </ul> <p>Practice questions from the textbook: 4.4-4.6, 4.12</p>"},{"location":"lectures/lecture-03/","title":"Lecture 3","text":""},{"location":"lectures/lecture-03/#lecture-3-equilibria-univariate","title":"Lecture 3: Equilibria (univariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-03/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Equilibria</li> <li>Example: diploid selection</li> <li>Summary</li> </ol>"},{"location":"lectures/lecture-03/#1-equilibria","title":"1. Equilibria","text":"<p>An equilibrium is any state of a system which tends to persist unchanged over time.</p> <p>For discrete-time models, the equilibria are defined as those values of the variables where no changes occur from one time step to the next. </p> <p>For example, those values of allele frequency \\(p(t)\\) where</p> \\[ \\begin{aligned} \\Delta p &amp;= 0\\\\ p(t+1) - p(t) &amp;= 0\\\\ p(t+1) &amp;= p(t) \\end{aligned} \\] <p>Similarly, for continuous-time models, the equilibria are defined as those values of the variables for which the rate of change in the variables equals zero. </p> <p>For example, those values of allele frequency \\(p(t)\\) where</p> \\[ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = 0 \\] <p>What are the equilibria of the exponential population growth and haploid selection models discussed in the previous lecture?</p> <p></p>"},{"location":"lectures/lecture-03/#2-example-diploid-selection","title":"2. Example: diploid selection","text":""},{"location":"lectures/lecture-03/#the-model","title":"The model","text":"<p>Let's now consider selection on diploid individuals, where each individual is characterized by two alleles at a locus. This leads to three genotypes: \\(AA\\), \\(Aa\\), and \\(aa\\). The \\(Aa\\) genotype is called the heterozygote and the others are homozygotes.</p> <p>Let the number of individuals with each genotype be</p> <ul> <li>\\(n_{AA}(t) =\\) number of individuals with the \\(AA\\) genotype in generation \\(t\\)</li> <li>\\(n_{Aa}(t) =\\) number of individuals with the \\(Aa\\) genotype in generation \\(t\\)</li> <li>\\(n_{aa}(t) =\\) number of individuals wite the \\(aa\\) genotype in generation \\(t\\)</li> </ul> <p>The frequency of allele \\(A\\) is calculated by counting up all the \\(A\\) alleles in the population and dividing by the total number of alleles,</p> \\[ \\begin{aligned} p(t) &amp;= \\frac{2n_{AA}(t) + n_{Aa}(t)}{2(n_{AA}(t) + n_{Aa}(t) + n_{aa}(t))} \\\\ &amp;= \\frac{n_{AA}(t) + \\frac{1}{2}n_{Aa}(t)}{n_{AA}(t) + n_{Aa}(t) + n_{aa}(t)}. \\end{aligned} \\] <p>To determine the recursion equation, let's use the following life cycle diagram, where we census the population immediately after the diploid individuals are formed by union of the haploid gametes</p> <pre><code>    graph LR;\n    A((p'')) --gamete union--&gt; B((p));\n    B --selection--&gt; C((p'));\n    C --meiosis--&gt; A;</code></pre> <p>Now, let\u2019s assume that during selection each diploid individual has reproductive factor</p> <ul> <li>\\(W_{AA} =\\) reproductive factor of individuals with the \\(AA\\) genotype </li> <li>\\(W_{Aa} =\\) reproductive factor of individuals with the \\(Aa\\) genotype</li> <li>\\(W_{aa} =\\) reproductive factor of individuals with the \\(aa\\) genotype</li> </ul> <p>These reproductive factors are again referred to as the absolute fitnesses as they determine the (absolute) numbers of individuals after selection, \\(n_i' = W_i n_i(t)\\) for \\(i=AA\\), \\(i=Aa\\), and \\(i=aa\\).</p> <p>After selection these genotypes segregate into haploids via meiosis, go through the haploid phase of the life cycle, and then randomly pair to create diploids again. Random union and segregation shuffle alleles between genotypes but don't affect allele frequency. The frequency of \\(A\\) in the next generation is therefore</p> \\[ \\begin{aligned} p(t+1)  &amp;= \\frac{n_{AA}(t+1) + \\frac{1}{2}n_{Aa}(t+1)}{n_{AA}(t+1) + n_{Aa}(t+1) + n_{aa}(t+1)}\\\\ &amp;= \\frac{W_{AA}n_{AA}(t) + \\frac{1}{2}W_{Aa}n_{Aa}(t)}{W_{AA}n_{AA}(t) + W_{Aa}n_{Aa}(t) + W_{aa}n_{aa}(t)}. \\end{aligned} \\] <p>We want the recursion equation in terms of allele frequency, so we want to replace the \\(n_i\\)'s on the right hand side of this equation with \\(p\\)'s. To do this we note that given the random union of gametes the diploid offspring are in Hardy-Weinberg proportions, i.e.,</p> \\[ \\begin{aligned} n_{AA}(t) &amp;= p(t)^2 n(t) \\\\ n_{Aa}(t) &amp;= 2p(t) (1-p(t)) n(t) \\\\ n_{aa}(t) &amp;= (1-p(t))^2 n(t) \\end{aligned} \\] <p>where \\(n(t) = n_{AA}(t) + n_{Aa}(t) + n_{aa}(t)\\) is the total population size.</p> <p>Substituting these Hardy-Weinberg proportions in and simplifying, the total population size cancels out and we can rewrite the above equation in terms of allele frequency alone,</p> \\[ \\begin{aligned} p(t+1)  &amp;= \\frac{W_{AA}p(t)^2 n(t) + W_{Aa}p(t) (1-p(t)) n(t)}{W_{AA}p(t)^2 n(t) + 2W_{Aa}p(t) (1-p(t)) n(t) + W_{aa}(1-p(t))^2 n(t)}\\\\ &amp;= \\frac{W_{AA}p(t)^2 + W_{Aa}p(t) (1-p(t))}{W_{AA}p(t)^2 + 2W_{Aa}p(t)(1-p(t)) + W_{aa}(1-p(t))^2}. \\end{aligned} \\] <p>This is a recursion equation for allele frequency in our model of diploid selection. It's a bit long. It can be written quite a bit shorter using \\(q=1-p\\) for the frequency of allele \\(a\\) and noticing that the denominator is mean fitness in the population, \\(\\bar{W}(p(t))\\), </p> \\[ p(t+1)  = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t)q(t)}{\\bar{W}(p(t))}. \\]"},{"location":"lectures/lecture-03/#the-equilibria","title":"The equilibria","text":"<p>To find the equilibria we replace \\(p(t+1)\\) and \\(p(t)\\) with \\(\\hat p\\) and the \\(q(t)\\) with \\(\\hat{q}\\) and solve for these equilibrium values, \\(\\hat p\\) and \\(\\hat q\\),</p> \\[ \\begin{aligned} \\hat{p} &amp;= \\frac{\\hat{p}^2 W_{AA} + \\hat{p} \\hat{q} W_{Aa}}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}}\\\\ \\hat{p} &amp;= \\frac{\\hat{p}(\\hat p W_{AA} + \\hat{q} W_{Aa})}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}} \\end{aligned} \\] <p>We see that \\(\\hat{p}=0\\) is one equilibrium as then both sides of the equation are 0. If \\(\\hat p\\) is not 0 we can divide by \\(\\hat p\\) to get</p> \\[ \\begin{aligned} 1 &amp;= \\frac{\\hat p W_{AA} + \\hat{q} W_{Aa}}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}}\\\\ \\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa} &amp;= \\hat p W_{AA} + \\hat{q} W_{Aa}\\\\ 0 &amp;= (\\hat{p} - \\hat{p}^2) W_{AA} + (\\hat{q} - 2 \\hat{p} \\hat{q}) W_{Aa} - \\hat{q}^2 W_{aa}\\\\ 0 &amp;= \\hat{p}(1 - \\hat{p}) W_{AA} + \\hat{q}(1 - 2 \\hat{p}) W_{Aa} - \\hat{q}^2 W_{aa}\\\\ 0 &amp;= \\hat{q}(\\hat{p} W_{AA} + (1 - 2 \\hat{p}) W_{Aa} - \\hat{q} W_{aa}) \\end{aligned} \\] <p>And so \\(\\hat{q}=0\\implies\\hat{p}=1\\) is another equilibrium. Dividing by \\(\\hat{q}\\) and putting everything in terms of \\(p\\) we have</p> \\[ \\begin{aligned} 0 &amp;= \\hat{p} W_{AA} + (1 - 2 \\hat{p}) W_{Aa} - \\hat{q} W_{aa}\\\\ 0 &amp;= \\hat{p} W_{AA} + (1 - 2 \\hat{p}) W_{Aa} - (1 - \\hat{p}) W_{aa}\\\\ 0 &amp;= \\hat{p}(W_{AA} -2W_{Aa} + W_{aa}) + W_{Aa} - W_{aa}\\\\ W_{aa} - W_{Aa} &amp;= \\hat p(W_{AA} -2W_{Aa} + W_{aa})\\\\ \\frac{W_{Aa} - W_{aa}}{2W_{Aa} - W_{AA} - W_{aa}} &amp;= \\hat p\\\\ \\end{aligned} \\] <p>We therefore have three equilibria under diploid selection: </p> <ul> <li>\\(\\hat{p}=0\\)</li> <li>\\(\\hat p = \\frac{W_{Aa} - W_{aa}}{2W_{Aa} - W_{AA} - W_{aa}}\\)</li> <li>\\(\\hat p = 1\\)</li> </ul> <p>Since a frequency is bounded between 0 and 1, we must have \\(0 \\leq p \\leq 1\\). We therefore call \\(\\hat{p}=0\\) and \\(\\hat{p}=1\\) boundary equilibria. These bounds also imply the third equilibrium is only biologically valid when </p> \\[ 0 \\leq \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} \\leq 1. \\] <p>When \\(W_{Aa} = W_{aa}\\) this equilibrium reduces to \\(\\hat{p}=0\\) and when \\(W_{Aa} = W_{AA}\\) this reduces to \\(\\hat{p}=1\\) (check this for yourself). The third equilibrium will be an internal equilibrium, representing a population with both \\(A\\) and \\(a\\) alleles, when</p> \\[ 0 &lt; \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} &lt; 1. \\] <p>The equilibrium is positive when the numerator and denominator have the same sign (i.e., are both positive or both negative). Let's split this into two \"cases\". Case A will have a positive numerator, \\(W_{Aa} &gt; W_{aa}\\), and Case B will have a negative numerator, \\(W_{Aa} &lt; W_{aa}\\). So, in Case A, the equilibrium is positive when the denominator is positive, \\(2 W_{Aa} - W_{AA} - W_{aa} &gt; 0\\). Meanwhile in case B the equilibrium is positive when the denominator is negative, \\(2 W_{Aa} - W_{AA} - W_{aa} &lt; 0\\).</p> <p>Now we can rearrange the equilibrium to show that it is less than 1 when</p> \\[ \\begin{aligned} \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} &amp;&lt; 1\\\\ \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} - 1 &amp;&lt; 0\\\\ \\frac{W_{Aa} - W_{aa} - (2 W_{Aa} -W_{AA} - W_{aa})}{2 W_{Aa} -W_{AA} - W_{aa}} &amp;&lt; 0\\\\ \\frac{W_{AA} - W_{Aa}}{2 W_{Aa} -W_{AA} - W_{aa}} &amp;&lt; 0\\\\ \\frac{W_{Aa} - W_{AA}}{2 W_{Aa} -W_{AA} - W_{aa}} &amp;&gt; 0. \\end{aligned} \\] <p>Again, we need the numerator and denominator to have the same sign for this inequality to hold. In case A, where we've said that denominator is positive, this means we also need the numerator to be positive, \\(W_{Aa} &gt; W_{AA}\\). While in case B we said that the denominator is negative, so we also need the numerator to be negative, \\(W_{Aa} &lt; W_{AA}\\).</p> <p>Putting this all together, there is a biologically-relevant internal equilibrium when either</p> <ul> <li>Case A: \\(W_{Aa} &gt; W_{aa}\\) and \\(W_{Aa} &gt; W_{AA}\\) (which ensures \\(2 W_{Aa} - W_{AA} - W_{aa} &gt; 0\\); go ahead and check!)</li> <li>Case B: \\(W_{Aa} &lt; W_{aa}\\) and \\(W_{Aa} &lt; W_{AA}\\)  (which ensures \\(2 W_{Aa} - W_{AA} - W_{aa} &lt; 0\\))</li> </ul> <p>Case A therefore represents \"heterozygote advantage\", \\(W_{AA} &lt; W_{Aa} &gt; W_{aa}\\), while Case B represents \"heterozygote disadvantage\", \\(W_{AA} &gt; W_{Aa} &lt; W_{aa}\\). </p> <p></p>"},{"location":"lectures/lecture-03/#3-summary","title":"3. Summary","text":"<p>Equilibria are defined by the values of the variables that persist over time, i.e., where the change is zero.</p> <p>With diploid selection there are three equilibria, two external and one potentially internal -- in the next lecture we'll see which are stable.</p> <p>Practice questions from the textbook: 5.1-5.3, 5.4a-c, 5.5, 5.8a-b, 5.9a-b, 5.10a-b, 5.11a-b, 5.12a-b, 5.13a-b</p>"},{"location":"lectures/lecture-04/","title":"Lecture 4","text":""},{"location":"lectures/lecture-04/#lecture-4-stability-univariate","title":"Lecture 4: Stability (univariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-04/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Stability</li> <li>Example: diploid selection</li> <li>Summary</li> </ol>"},{"location":"lectures/lecture-04/#1-stability","title":"1. Stability","text":"<p>When a variable (\\(x\\)) is exactly at an equilibrium (\\(\\hat x\\)) its value will never change. But what happens when we are not exactly at, but just near an equilibrium?</p> <p>Starting near an equilibrium, if the system moves towards the equilibrium over time the equilibrium is said to be locally stable. In contrast, if the system moves away from the equilibrium over time the equilibrium is said to be unstable.</p>"},{"location":"lectures/lecture-04/#formal-analysis","title":"Formal analysis","text":"<p>To determine local stability mathematically, we focus on a small perturbation (\\(\\epsilon\\)) away from an equilibrium and determine whether this perturbation will grow or shrink.</p> <p>Let's do this in discrete time with recursion \\(x(t+1) = f(x(t))\\) where \\(f(x(t))\\) is the function that defines the recursion, e.g., \\(f(x(t))=Rx(t)\\) for exponential population growth.</p> <p>If at time \\(t\\) the variable is a small distance from equilibrium, \\(x(t) = \\hat{x} + \\epsilon(t)\\),  then at time \\(t+1\\) the variable will be at \\(x(t+1) = f(\\hat{x} + \\epsilon(t))\\).</p> <p>To work with this arbitrary function, \\(f\\), let's introduce a pretty remarkable mathematical fact, the Taylor Series.</p> <p>Taylor Series</p> <p>Any function \\(f(x)\\) can be written as an infinite series of derivatives evaluated at \\(x=a\\)</p> \\[ f(x) = \\sum_{k = 0}^{\\infty}\\frac{f^{(k)}(a)}{k!}(x-a)^k \\] <p>where \\(f^{(k)}(a)\\) is the \\(k^{\\mathrm{th}}\\) derivative of the function with respect to \\(x\\), evaluated at point \\(a\\). (See section P1.3 in the textbook for more information).</p> <p>Taking the Taylor Series of \\(f(\\hat{x} + \\epsilon(t))\\) around \\(\\epsilon(t)=0\\) </p> \\[ \\begin{aligned} x(t+1) &amp;= f(\\hat{x} + \\epsilon(t))\\\\ &amp;= f(\\hat{x}) + f^{(1)}(\\hat{x})(\\hat{x} + \\epsilon(t) - \\hat{x}) + \\frac{f^{(2)}(\\hat{x})}{2}(\\hat{x} + \\epsilon(t) - \\hat{x})^2 + \\cdots\\\\ &amp;= f(\\hat{x}) + f^{(1)}(\\hat{x})\\epsilon(t) + \\frac{f^{(2)}(\\hat{x})}{2}\\epsilon(t)^2 + \\cdots\\\\ \\end{aligned} \\] <p>Now, to work with this infinite series we will make an assumption, that we are very the equilibrium, meaning the deviation is small, \\(\\epsilon&lt;&lt;1\\). This means that \\(\\epsilon^2\\) is even smaller, and \\(\\epsilon^3\\) even smaller than that, and so on. By considering small \\(\\epsilon\\) we can therefore cut-off our infinite series by ignoring any term with \\(\\epsilon\\) to a power greater than 1. This is called a \"first order\" Taylor series approximation of \\(f\\) around \\(\\epsilon=0\\). This assumption is what limits us to determining only local stability. Global stability would require us to consider large deviations from the equilibrium as well, which is not possible for even mildly complicated functions, \\(f\\).</p> <p>Keeping the first order term only, we can solve for the new deviation from the equilibrium, \\(\\epsilon(t+1) = x(t+1) - \\hat x\\),</p> \\[ \\begin{aligned} x(t+1) &amp;= f(\\hat{x}) + f^{(1)}(\\hat{x})\\epsilon(t)\\\\ &amp;= \\hat{x} + f^{(1)}(\\hat{x})\\epsilon(t)\\\\ x(t+1) - \\hat x &amp;= f^{(1)}(\\hat{x})\\epsilon(t)\\\\ \\epsilon(t+1) &amp;= f^{(1)}(\\hat{x})\\epsilon(t) \\end{aligned} \\] <p>So the recursion for the deviation from equilibrium, \\(\\epsilon\\), is the recursion for exponential population growth with reproductive factor \\(\\lambda = f^{(1)}(\\hat{x})\\). </p> <p>Based on our knowledge of discrete-time exponential growth, we therefore know that the deviation from equilibrium will:</p> <ul> <li> <p>move from one side of the equilibrium to the other (i.e., oscillate) if \\(\\lambda\\) is negative</p> <ul> <li>grow if \\(\\lambda &lt; -1\\) \\(\\implies\\hat{x}\\) unstable</li> <li>shrink if \\(-1 &lt; \\lambda &lt; 0\\) \\(\\implies\\hat{x}\\) locally stable</li> </ul> </li> <li> <p>stay on the same side of the equilibrium (i.e., not oscillate) if \\(\\lambda\\) is positive</p> <ul> <li>shrink if \\(0&lt;\\lambda&lt;1\\) \\(\\implies\\hat{x}\\) locally stable</li> <li>grow if \\(1&lt;\\lambda\\) \\(\\implies\\hat{x}\\) unstable</li> </ul> </li> </ul> <p>Local stability in discrete time therefore requires the slope of the recursion to be between -1 and 1 at the equilibrium, </p> \\[ -1 &lt; f^{(1)}(\\hat{x})=\\left.\\frac{\\mathrm{d}x(t+1)}{\\mathrm{d}x(t)}\\right|_{x(t)=\\hat x} &lt; 1. \\] <p>A very similar analysis in continuous time, where the variable changes accorging to differential equation \\(df(x)/dt=f(x)\\), shows that an equilibrium \\(\\hat x\\) is stable if the slope of the differential equation is negative at the equilibrium, </p> \\[ f^{(1)}(\\hat{x}) = \\left.\\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(\\frac{\\mathrm{d}x}{\\mathrm{d}t}\\right)\\right|_{x=\\hat{x}} &lt; 0. \\] <p>In continuous time there are no oscilations in one dimension because you cannot go past an equilibrium without hitting it.</p> <p>Notice that these derivatives are the slopes of the curves at the equilibria in the plots of the variables as functions of themselves: \\(x(t+1)\\) and \\(dx/dt\\) as functions of \\(x(t)\\).</p> <p></p>"},{"location":"lectures/lecture-04/#2-example-diploid-selection","title":"2. Example: diploid selection","text":"<p>To try this out on a biological example, let's return to diploid selection, where the frequency of the \\(A\\) allele in the next generation is</p> \\[ p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t)(1-p(t))}{\\bar{W}(p(t))}, \\] <p>where \\(\\bar{W}(p(t)) = W_{AA}p(t)^2 + 2W_{Aa}p(t) (1-p(t)) + W_{aa}(1-p(t))^2\\) is the mean fitness in the population, which depends on \\(p(t)\\).</p> <p>In the previous lecture we found there were three equilibria:</p> <ul> <li>\\(\\hat p = 0\\)</li> <li>\\(\\hat p = \\frac{W_{Aa} - W_{aa}}{2W_{Aa} - W_{AA} - W_{aa}}\\)</li> <li>\\(\\hat p = 1\\)</li> </ul> <p>Now our job is to find when each of these is locally stable, which requires the slope of the recursion at that point be between -1 and 1, </p> \\[ -1 &lt; \\left.\\frac{\\mathrm{d}p(t+1)}{\\mathrm{d}p(t)}\\right|_{p(t)=\\hat p} &lt; 1. \\] <p>To see this visually first, we can draw cobweb plots for particular parameter values. Last lecture we learned that the internal equilibrium is only valid if the hterozygote is most fit \\(W_{AA} &lt; W_{Aa} &gt; W_{aa}\\) (overdominance) or least fit \\(W_{AA} &gt; W_{Aa} &lt; W_{aa}\\) (underdominance), so let's look at parameter values that fall within those two cases.</p> <pre>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# recursion\ndef f(p, WAA, WAa, Waa):\n    return (WAA * p**2 + WAa * p * (1 - p)) / (WAA * p**2 + WAa * 2 * p * (1 - p) + Waa * (1 - p)**2)\n\n# generator for cobweb\ndef pt(p0, WAA, WAa, Waa, max=np.inf):\n    t, pnow, pnext = 0, p0, 0 #initial conditions\n    while t &lt; max:\n        yield pnow, pnext #current value of p(t) and p(t+1), as both are needed for cob-webbing\n        pnext = (WAA * pnow**2 + WAa * pnow * (1 - pnow)) / (WAA * pnow**2 + WAa * 2 * pnow * (1 - pnow) + Waa * (1 - pnow)**2) #update p(t+1)\n        yield pnow, pnext #current value of p(t) and p(t+1)\n        pnow = pnext #update p(t)\n        t += 1 #update t\n# plot\nfig, axs = plt.subplots(1,2, figsize=(10,5))\n\nWAA, Waa = 1, 1\nWAas = [2,0.5]\np0= 0.25\n\nfor i,WAa in enumerate(WAas): \n\n    # evaluate\n    xs = np.linspace(0,1,100) #allele freqs at time t\n    ys = [f(x, WAA, WAa, Waa) for x in xs] #allele freqs at time t+1\n    pts = pt(p0, WAA, WAa, Waa, max=10) # initialize generator\n    ps = np.array([[x,y] for x,y in pts]) # compute x,y pairs by iterating through generator\n\n    # recursion\n    axs[i].plot(xs, ys, color='black') #p(t+1) as function of p(t)\n    axs[i].plot(xs, xs, color='black', linestyle='--') #1:1 line for reference\n\n    # cobwebs\n    axs[i].plot(ps[:,0], ps[:,1])\n\n    axs[i].set_xlim(0,1)\n    axs[i].set_ylim(0,1)\n    axs[i].set_xlabel(\"allele frequency at $t$, $p(t)$\")\n    axs[i].set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\")\n    axs[i].set_title(r'$W_{AA}=%s, W_{Aa}=%s, W_{aa}=%s$' %(WAA,WAa,Waa))\n\nplt.show()\n</pre> <p></p> <p>The equilibria are where the solid line intersects the dashed line. Focusing on the internal equilibrium for illustration, we see that the slope at the intersection is less than one (which is the slope of the dashed line) in the left plot and greater than one in the right plot. Correspondingly, we see that the internal equilibrium is stable in the left plot and unstable in the right plot. To more precisely determine the boundary between stability and instability we need to perform our local stability analysis.</p> <p>We start the stability analysis by taking the derivative of \\(p(t+1)\\) as a function of \\(p(t)\\), using the quotient rule</p> \\[ \\begin{aligned} &amp;\\frac{\\mathrm{d}}{\\mathrm{d}p(t)}\\left(\\frac{W_{AA}p(t)^2 + W_{Aa}p(t)(1-p(t))}{\\bar{W}(p(t))}\\right)\\\\ &amp;=\\frac{(2W_{AA}p(t) + W_{Aa}(1-2p(t)))\\bar{W}(p(t))}{\\bar{W}(p(t))^2}\\\\ &amp;-\\frac{(W_{AA}p(t)^2+W_{Aa}p(t)(1-p(t)))(2W_{AA}p(t)+2W_{Aa}(1-2p(t))-2(1-p(t))W_{aa})}{\\bar{W}(p(t))^2}. \\end{aligned} \\] <p>This is a complicated expression! Fortunately stability is determined by the value of this derivative evaluated at \\(p(t)=\\hat{p}\\), which can be considerably simpler. </p> <p>Let's start with \\(\\hat{p} = 0\\). In this case we have </p> \\[ \\begin{aligned} \\left.\\frac{\\mathrm{d}p(t+1)}{\\mathrm{d}p(t)}\\right|_{p(t)=0} &amp;= \\frac{W_{Aa}\\bar{W}(0)}{\\bar{W}(0)^2}\\\\ &amp;= \\frac{W_{Aa}}{\\bar{W}(0)}\\\\ &amp;= \\frac{W_{Aa}}{W_{aa}}. \\end{aligned} \\] <p>And similarly for \\(\\hat p = 1\\), we have</p> \\[ \\begin{aligned} \\left.\\frac{\\mathrm{d}p(t+1)}{\\mathrm{d}p(t)}\\right|_{p(t)=1} &amp;= \\frac{(2W_{AA} - W_{Aa})\\bar{W}(1) - W_{AA}(2W_{AA}-2W_{Aa})}{\\bar{W}(1)^2}\\\\ &amp;= \\frac{(2W_{AA} - W_{Aa})W_{AA} - W_{AA}(2W_{AA}-2W_{Aa})}{W_{AA}^2}\\\\ &amp;= \\frac{2W_{AA} - W_{Aa} - 2W_{AA}+2W_{Aa}}{W_{AA}}\\\\ &amp;= \\frac{W_{Aa}}{W_{AA}}. \\end{aligned} \\] <p>Since fitness is always greater than or equal to zero, \\(W_i\\geq0\\), so are these derivatives. We therefore know that they are greater than -1 and what is left for stability is to determine when they are less than 1. This will occur whenever the numerator is less than the denominator, meaning that \\(\\hat p = 0\\) is stable when \\(W_{Aa} &lt; W_{aa}\\) and \\(\\hat p = 1\\) is stable when \\(W_{Aa} &lt; W_{AA}\\). This makes good sense. Take \\(\\hat p = 0\\) for example. This means that the \\(a\\) allele is fixed. When we introduce just a few \\(A\\) alleles, \\(p(t) &lt;&lt; 1\\), they will occur in heterozygotes because it is very unlikely that they pair with each other under under random mating, \\(p(t)^2\\approx0\\). So the spread of the \\(A\\) allele and an increase in \\(p\\) is determined by the spread of the heterozygote, which is prevented by selection when \\(W_{Aa} &lt; W_{aa}\\). The same reasoning holds for \\(\\hat p = 1\\). Check that these stability criteria are consistent with the cobweb plots above.</p> <p>Now for the internal equilibrium. It's stability is determined the same way, but this time we skip the algebra for the sake of time</p> \\[ \\begin{aligned} \\left.\\frac{\\mathrm{d}p(t+1)}{\\mathrm{d}p(t)}\\right|_{p(t)=\\frac{W_{Aa} - W_{aa}}{2W_{Aa} - W_{AA} - W_{aa}}} &amp;= \\frac{W_{AA}W_{Aa}-2W_{AA}W_{aa}+W_{Aa}W_{aa}}{W_{Aa}^2-W_{AA}W_{aa}}. \\end{aligned} \\] <p>Not bad. Now let's consider the two scenarios under which this internal equilibrium is biologically valid.</p> <p>In case A we have overdominance, \\(W_{AA} &lt; W_{Aa} &gt; W_{aa}\\). This means that both numerator and denominator are positive. Since the derivative is positive it is always greater than -1 and all that is left for stability is for the derivative to be less than 1, ie the numerator is less than the denominator</p> \\[ \\begin{aligned} W_{AA}W_{Aa}-2W_{AA}W_{aa}+W_{Aa}W_{aa} &amp;&lt; W_{Aa}^2-W_{AA}W_{aa}\\\\ W_{AA}W_{Aa}-W_{AA}W_{aa}+W_{Aa}W_{aa} - W_{Aa}^2 &amp;&lt; 0, \\end{aligned} \\] <p>which is always true with \\(W_{AA} &lt; W_{Aa} &gt; W_{aa}\\), implying stability with overdominance.</p> <p>In case B we have underdominance, \\(W_{AA} &gt; W_{Aa} &lt; W_{aa}\\). This means that both numerator and denominator are negative. Since the derivative is positive it is always greater than -1 and all that is left for stability is for the derivative to be less than 1, which is the condition we just derived in the equation above. In case B however, the condition never holds, implying instability with underdominance.</p> <p>In sum, the internal equilibrium is biologically valid and locally stable only if there is overdominance, \\(W_{AA} &lt; W_{Aa} &gt; W_{aa}\\). This makes good intuitive sense since there are more heterozygotes at a more intermediate frequency (\\(2p(1-p)\\) has a max at \\(p=1/2\\)). It also implies that the internal equilibrium is stable when the boundary equilibria are unstable, and vice versa. When the internal equilibrium is unstable it therefore acts as a replellor between two locally stable equilibria, pushing allele frequency towards the boundary that is on the same side of the repellor. Check that this all makes sense with the plots above.</p> <p></p>"},{"location":"lectures/lecture-04/#3-summary","title":"3. Summary","text":"<p>Local stability analysis for discrete- and continuous-time models with one variable:</p> <ol> <li>take the derivative of the recursion or differential equation with respect to the variable, \\(f^{(1)}(x)\\)</li> <li>plug in the equilibrium value of the variable, \\(\\lambda=f^{(1)}(\\hat x)\\)</li> <li>local stability requires \\(-1 &lt; \\lambda &lt; 1\\) in discrete time and \\(\\lambda &lt; 0\\) in continuous time</li> </ol> <p>Practice questions from the textbook: 5.4, 5.6-5.13</p>"},{"location":"lectures/lecture-05/","title":"Lecture 5","text":""},{"location":"lectures/lecture-05/#lecture-5-general-solutions-univariate","title":"Lecture 5: General solutions (univariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-05/#lecture-overview","title":"Lecture overview","text":"<ol> <li>General solutions</li> <li>Example: haploid selection</li> <li>Summary</li> </ol>"},{"location":"lectures/lecture-05/#1-general-solutions","title":"1. General solutions","text":"<p>Equilibria and their stability describe the long-term dynamics of our models, i.e., what we expect after a long time has passed. Now we\u2019ll look at some  cases where we can describe the entire dynamics, including the short-term, by solving for the variable as a function of time, \\(x(t) = f(t)\\). This is called a general solution.</p>"},{"location":"lectures/lecture-05/#linear-models-in-discrete-time","title":"Linear models in discrete time","text":"<p>With a single variable, \\(x\\), a discrete time linear model can be written </p> \\[ x(t+1) = a x(t) + b. \\] <p>For example, this could be our previous model of population growth with immigration.</p> <p>The general solution can be found with the following steps.</p> <p>Step 1: Solve for the equilibrium,</p> \\[ \\begin{aligned} \\hat{x} &amp;= a \\hat{x} + b \\\\ \\hat{x} &amp;= \\frac{b}{1 - a}. \\end{aligned} \\] <p>Note</p> <p>Note that if \\(a=1\\) there is no equilibrium for \\(b\\neq0\\) and instead you can use brute force iteration (see below) to show that \\(x(t) = x_0 + b t\\).</p> <p>Step 2: Define \\(\\delta(t) = x(t) - \\hat{x}\\), the deviation of our variable from the equilibrium (this is our transformation).</p> <p>Step 3: Write the recursion equation for the transformed variable,</p> \\[ \\begin{aligned} \\delta(t+1) &amp;= x(t+1) - \\hat{x} \\\\ &amp;= a x(t) + b - \\hat{x} \\\\ &amp;= a(\\delta(t) + \\hat{x}) + b - \\hat{x}\\\\ &amp;= a \\left(\\delta(t) + \\frac{b}{1 - a}\\right) + b - \\frac{b}{1 - a}\\\\ &amp;= a \\delta(t). \\end{aligned} \\] <p>This is, once again, exponential growth.</p> <p>Step 4: From this we can use brute force iteration to get the general solution for the deviation from equilibrium,</p> \\[ \\begin{aligned} \\delta(t) &amp;= a \\delta(t-1)\\\\ &amp;= a^2 \\delta(t-2)\\\\ &amp;= a^3 \\delta(t-3)\\\\  &amp;\\vdots\\\\ &amp;= a^t \\delta(0). \\end{aligned} \\] <p>Step 5: Reverse transform back to \\(x(t)\\),</p> \\[ \\begin{aligned} x(t) &amp;= \\delta(t) + \\hat{x}\\\\ &amp;= a^t \\delta(0) + \\hat{x}\\\\ &amp;= a^t (x(0) - \\hat{x}) + \\hat{x}\\\\ &amp;= a^t x(0) + (1 - a^t)\\hat{x}. \\end{aligned} \\] <p>This says that our variable moves from \\(x(0)\\) towards/away from \\(\\hat{x}\\) by a factor \\(a\\) per time step. Note that if \\(b=0\\) then \\(\\hat{x}=0\\) and this reduces to what we derived above, \\(x(t)=a^t x(0)\\).</p> <p>Below we plot the general solution for a given value of \\(a\\) and \\(b\\) from a number of different intitial conditions. Try playing with the values of \\(a\\) and \\(b\\) and observe the different dynamics.</p> <pre>\na, b, x0 = 0.99, 1, 10 #define parameter values and initial condition\nts = range(1000) #time values\nxs = [a**t * x0 + (1-a**t)*b/(1-a) for t in ts] #variable values from general solution\nplt.scatter(ts, xs) #plot discretely\nplt.ylabel('$x(t)$')\nplt.xlabel('$t$')\nplt.show()\n</pre> <p></p>"},{"location":"lectures/lecture-05/#separation-of-variables-in-continuous-time","title":"Separation of variables in continuous time","text":"<p>Consider the generic continuous time model,</p> \\[ \\frac{\\mathrm{d}x}{\\mathrm{d}t} = f(x,t), \\] <p>where we've allowed the right-hand side to depend on time explicitly (eg, time lags, environmental change).</p> <p>We will try to get the general solution for \\(x(t)\\) using a method called separation of variables. This will only work if we can write the right hand side as \\(f(x)=g(x)h(t)\\), ie, if we can separate the variables, \\(x\\) and \\(t\\). If we can then</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}x}{\\mathrm{d}t} &amp;= g(x)h(t)\\\\ \\frac{\\mathrm{d}x}{g(x)} &amp;= h(t)\\mathrm{d}t\\\\ \\int\\frac{\\mathrm{d}x}{g(x)} &amp;= \\int h(t)\\mathrm{d}t. \\end{aligned} \\] <p>If we can solve these integrals then we get a general solution. In this class we will typically not have explicit time dependence, ie, \\(h(t)\\) is a constant. Then the right-hand integral is just the constant times \\(t\\) and a general solution is limited by our ability to integrate \\(1/g(x)\\).</p> <p></p>"},{"location":"lectures/lecture-05/#2-example-haploid-selection","title":"2. Example: haploid selection","text":"<p>To see an example of separation of variables, consider our model of haploid selection in continuous time. The frequency of the \\(A\\) allele changes at a rate that depends on its selection coefficient, \\(s\\), and the amount of genetic variance in the population, \\(p(1-p)\\),</p> \\[ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = sp(1-p). \\] <p>Grouping the \\(p\\) terms together, we can proceed with the above steps using \\(g(p)=p(1-p)\\) and \\(h(t)=s\\),</p> \\[ \\begin{aligned} \\int\\frac{\\mathrm{d}p}{g(p)} &amp;= \\int h(t)\\mathrm{d}t\\\\ \\int\\frac{\\mathrm{d}p}{p(1-p)} &amp;= \\int s\\mathrm{d}t\\\\ \\int \\left(\\frac{1}{p} + \\frac{1}{1-p}\\right) \\mathrm{d}p &amp;= \\int s\\mathrm{d}t \\;\\text{(method of partial fractions, rule A1.9 in the textbook)}\\\\ \\ln(p) - \\ln(1 - p) + c_1 &amp;= s t + c_2 \\;\\text{(rule A2.21 in the textbook)}\\\\ \\ln\\left(\\frac{p}{1-p}\\right) &amp;= s t + c \\; \\text{(where } c = c_2 - c_1\\text{)}\\\\ \\frac{p}{1-p} &amp;= \\exp(st + c)\\\\ \\frac{p(t)}{1-p(t)} &amp;= \\exp(st + c)\\; \\text{(just making the dependence on time explicit)}. \\end{aligned} \\] <p>Note that we were careful to include the essential integration constants, \\(c_1\\) and \\(c_2\\), which we combined into one unknown constant, \\(c\\). We can replace \\(c\\) with the value of \\(p\\) at some \\(t\\). We typically choose \\(t=0\\), rewriting \\(c\\) in terms of the initial condition \\(p(0)\\). Setting \\(t=0\\) we have,</p> \\[ \\begin{aligned} \\frac{p(t)}{1-p(t)} &amp;= \\exp(st + c) \\\\ \\frac{p(0)}{1-p(0)} &amp;= \\exp(c). \\end{aligned} \\] <p>Now using this to replace \\(c\\),</p> \\[ \\begin{aligned} \\frac{p(t)}{1-p(t)} &amp;= \\exp(st + c) \\\\ \\frac{p(t)}{1-p(t)} &amp;= \\exp(st)\\exp(c) \\\\ \\frac{p(t)}{1-p(t)} &amp;= \\exp(st)\\frac{p(0)}{1-p(0)}. \\end{aligned} \\] <p>And finally we solve for \\(p(t)\\),</p> \\[ \\begin{aligned} \\frac{p(t)}{1-p(t)} &amp;= \\exp(st)\\frac{p(0)}{1-p(0)} \\\\ p(t) &amp;= (1-p(t))\\exp(st)\\frac{p(0)}{1-p(0)} \\\\  p(t) &amp;= \\exp(st)\\frac{p(0)}{1-p(0)} - p(t)\\exp(st)\\frac{p(0)}{1-p(0)} \\\\ p(t) + p(t)\\exp(st)\\frac{p(0)}{1-p(0)} &amp;= \\exp(st)\\frac{p(0)}{1-p(0)} \\\\ p(t)\\left(1 + \\exp(st)\\frac{p(0)}{1-p(0)}\\right) &amp;= \\exp(st)\\frac{p(0)}{1-p(0)} \\\\ p(t) &amp;= \\frac{\\exp(st)\\frac{p(0)}{1-p(0)}}{1 + \\exp(st)\\frac{p(0)}{1-p(0)}} \\\\ p(t) &amp;= \\frac{1}{1 + \\exp(-st)\\frac{1-p(0)}{p(0)}}. \\end{aligned} \\] <p>This is a very classic result in population genetics, which we plot below for an initially rare and beneficial allele A that sweeps to fixation. </p> <pre>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ns, p0 = 0.1, 0.01 #define parameter values and initial condition\nts = range(100) #time values\nps = [1/(1 + np.exp(-s*t)*(1-p0)/p0) for t in ts] #variable values from general solution\n\nplt.plot(ts, ps) #plot continuously\nplt.ylabel('allele frequency, $p(t)$')\nplt.xlabel('generation, $t$')\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-05/#3-summary","title":"3. Summary","text":"<p>We can sometimes solve for the entire dynamics of our variables, the ultimate solution.</p> <ul> <li>for linear discrete-time models, we can always do this with a transformation and brute force integration </li> <li>for continuous-time models, we can sometimes do this with separation of variables</li> </ul> <p>Unfortunately most models we encounter in research are too complex to solve for exactly. We then often rely on equilibria, stability, and simulations. </p> <p>Practice questions from the textbook: 6.2-6.3, 6.4a-e, 6.5-6.6, 6.7d, 6.8</p>"},{"location":"lectures/lecture-06/","title":"Lecture 6","text":""},{"location":"lectures/lecture-06/#lecture-6-representing-linear-multivariate-models-with-vectors-and-matrices","title":"Lecture 6: Representing linear multivariate models with vectors and matrices","text":"Run notes interactively?"},{"location":"lectures/lecture-06/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Motivation</li> <li>Vectors</li> <li>Matrices</li> <li>Addition</li> <li>Multiplication</li> <li>Summary</li> </ol>"},{"location":"lectures/lecture-06/#1-motivation","title":"1. Motivation","text":"<p>Until now, we have been dealing with problems in a single variable changing over time.</p> <p>Often, dynamical systems involve more than one variable (ie, they are multivariate). For instance, we may be interested in how the numbers of two species change as they interact (e.g., compete) with one another.</p> <p>As an introductory example with more than one variable, consider a model tracking the number of birds on two islands. Let the number of birds on island 1 be \\(n_1\\) and let the number of birds on island 2 be \\(n_2\\). We assume the birds migrate from island \\(j\\) to island \\(i\\) at per capita rate \\(m_{ij}\\), the birds on each island give birth at per capita rates \\(b_1\\) and \\(b_2\\), the birds on each island die at per capita rates \\(d_1\\) and \\(d_2\\), and new birds arrive on each island at rates \\(m_1\\) and \\(m_2\\). This is captured in the following flow diagram</p> <pre><code>graph LR;\n    A1((n1)) --b1 n1--&gt; A1;\n    B1[ ] --m1--&gt; A1;\n    A1 --d1 n1--&gt; C1[ ];\n    A2((n2)) --b2 n2--&gt; A2;\n    B2[ ] --m2--&gt; A2;\n    A2 --d2 n2--&gt; C2[ ];\n    A1 --m21 n1--&gt; A2;\n    A2 --m12 n2--&gt; A1;\n    style B1 height:0px;\n    style C1 height:0px;\n    style B2 height:0px;\n    style C2 height:0px;</code></pre> <p>The rate of change in \\(n_1\\) and \\(n_2\\) are then described by the following system of differential equations</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &amp;= (b_1 - d_1 - m_{21})n_1 + m_{12} n_2 + m_1 \\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} &amp;= m_{21} n_1 + (b_2 - d_2 - m_{12})n_2 + m_2 \\end{aligned} \\] <p>These equations are linear functions of the variables (i.e., they contain only constant multiples of \\(n_1\\) and \\(n_2\\) and nothing more complicated such as \\(n_1^2\\) or \\(e^{n_2}\\)).</p> <p>Linear systems of equations like these can also be written in matrix form</p> \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} \\end{pmatrix}  &amp;= \\begin{pmatrix} b_1 - d_1 - m_{21} &amp; m_{12} \\\\ m_{21} &amp; b_2 - d_2 - m_{12} \\end{pmatrix} \\begin{pmatrix} n_1 \\\\ n_2 \\end{pmatrix}  + \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}\\\\ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} &amp;= \\mathbf{M}\\vec{n} + \\vec{m} \\end{aligned} \\] <p>Not only is this a nice compact expression, there are rules of linear algebra that can help us conveniently solve this (and any other) set of linear equations.</p> <p>Let's get to know these rules.</p> <p></p>"},{"location":"lectures/lecture-06/#2vectors","title":"2.Vectors","text":"<p>Vectors are lists of elements (elements being numbers, parameters, functions, or variables).</p> <p>A column vector has elements arranged from top to bottom</p> \\[ \\begin{equation*} \\begin{pmatrix}   5 \\\\   2 \\end{pmatrix}, \\begin{pmatrix}   1 \\\\   5 \\\\   9 \\\\   7 \\end{pmatrix}, \\begin{pmatrix}   x \\\\   y \\end{pmatrix}, \\begin{pmatrix}   x \\\\   y \\\\   z \\end{pmatrix}, \\begin{pmatrix}   x_1 \\\\   x_2 \\\\   \\vdots \\\\   x_n \\end{pmatrix} \\end{equation*} \\] <p>A row vector has elements arranged from left to right</p> \\[ \\begin{pmatrix}5 &amp; 2\\end{pmatrix}, \\begin{pmatrix} 1 &amp; 5 &amp; 9 &amp; 7\\end{pmatrix}, \\begin{pmatrix} x &amp; y \\end{pmatrix}, \\begin{pmatrix} x &amp; y &amp; z\\end{pmatrix}, \\begin{pmatrix} x_1 &amp; x_2 &amp; \\cdots &amp; x_n \\end{pmatrix} \\] <p>Transposing a vector switches it from a row vector to a column vector or vice-versa,</p> \\[ \\begin{equation*} \\begin{pmatrix}   a \\\\   b \\\\   c \\end{pmatrix}^\\intercal =  \\begin{pmatrix}   a &amp; b &amp; c \\end{pmatrix} \\end{equation*} \\] \\[ \\begin{equation*} \\begin{pmatrix}   a &amp; b &amp; c \\end{pmatrix} \\end{equation*}^\\intercal =  \\begin{pmatrix}   a \\\\   b \\\\   c \\end{pmatrix} \\] <p>We will indicate vectors by placing an arrow on top of the symbol</p> \\[ \\vec{x} = \\begin{pmatrix} x_1 &amp; x_2 &amp; \\cdots &amp; x_n \\end{pmatrix} \\] <p>The number of elements in the vector indicates its dimension, \\(n\\).</p> <p>For example, the row vector \\(\\begin{pmatrix}x &amp; y\\end{pmatrix}\\) has dimension \\(n=2\\).</p> <p>You can represent a vector of dimension \\(n\\) as an arrow in \\(n\\) dimensions, connecting the origin with a point whose coordinates are given by elements in the vector. For example, the vector \\(\\vec{v} = \\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) can be depicted as below.</p> <pre>\nimport matplotlib.pyplot as plt #import plotting library\n\nplt.arrow(0, 0, #starting x and y values of arrow\n          1, 2, #change in x and y \n          head_width=0.1, color='black', length_includes_head=True) #aesthetics\nplt.xlim(0,2.5) #set bounds on x axis\nplt.ylim(0,2.5) #set bounds on y axis\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-06/#3-matrices","title":"3. Matrices","text":"<p>An \\(m \\times n\\) matrix has \\(m\\) rows and \\(n\\) columns (a vector can be seen as a special case, where either \\(m\\) or \\(n\\) is 1)</p> \\[ \\begin{equation*} \\begin{pmatrix}   x_{11}  &amp; x_{12} &amp; \\cdots &amp; x_{1n}\\\\   x_{21}  &amp; x_{22} &amp; \\cdots &amp; x_{2n}\\\\   \\vdots &amp; \\vdots &amp;        &amp; \\vdots\\\\   x_{m1}  &amp; x_{m2} &amp; \\cdots &amp; x_{mn}\\\\ \\end{pmatrix}, \\begin{pmatrix}   a &amp; b \\\\   c &amp; d \\end{pmatrix}, \\begin{pmatrix}   75 &amp; 67 \\\\   66 &amp; 34 \\\\   12 &amp; 14 \\end{pmatrix}, \\begin{pmatrix}   1 &amp; 0 &amp; 0 \\\\   0 &amp; 1 &amp; 0 \\\\   0 &amp; 0 &amp; 1 \\end{pmatrix} \\end{equation*} \\] <p>We will indicate matrices by bolding the symbol (and using capital letters)</p> \\[ \\mathbf{X} = \\begin{pmatrix}   x_{11}  &amp; x_{12} &amp; \\cdots &amp; x_{1n}\\\\   x_{21}  &amp; x_{22} &amp; \\cdots &amp; x_{2n}\\\\   \\vdots &amp; \\vdots &amp;        &amp; \\vdots\\\\   x_{m1}  &amp; x_{m2} &amp; \\cdots &amp; x_{mn}\\\\ \\end{pmatrix} \\] <p>A matrix with an equal number of rows and columns, \\(m=n\\), is a square matrix</p> \\[ \\begin{pmatrix}   a &amp; b &amp; c \\\\   d &amp; e &amp; f \\\\   g &amp; h &amp; i \\end{pmatrix} \\] <p>A matrix with zeros everywhere except along the diagonal is called a diagonal matrix</p> \\[ \\begin{pmatrix}   a &amp; 0 &amp; 0 \\\\   0 &amp; b &amp; 0 \\\\   0 &amp; 0 &amp; c \\end{pmatrix} \\] <p>And a special case of this with 1s along the diagonal is called the identity matrix</p> \\[ \\begin{pmatrix}   1 &amp; 0 &amp; 0 \\\\   0 &amp; 1 &amp; 0 \\\\   0 &amp; 0 &amp; 1 \\end{pmatrix} \\] <p>A matrix with all zeros below the diagonal is called an upper triangular matrix</p> \\[ \\begin{pmatrix}   a &amp; b &amp; c \\\\   0 &amp; d &amp; e \\\\   0 &amp; 0 &amp; f \\end{pmatrix} \\] <p>A matrix with all zeros above the diagonal is called an lower triangular matrix</p> \\[ \\begin{pmatrix}   a &amp; 0 &amp; 0 \\\\   b &amp; d &amp; 0 \\\\   c &amp; e &amp; f \\end{pmatrix} \\] <p>It is sometimes useful to chop a matrix up into multiple blocks, creating a block matrix</p> \\[ \\begin{pmatrix}   a &amp; b &amp; c \\\\   d &amp; e &amp; f \\\\   g &amp; h &amp; i \\end{pmatrix} =  \\begin{pmatrix}   \\mathbf{A} &amp; \\mathbf{B} \\\\   \\mathbf{C} &amp; \\mathbf{D} \\end{pmatrix} \\] <p>where \\(\\mathbf{A}=\\begin{pmatrix} a &amp; b \\\\ d &amp; e\\end{pmatrix}\\), \\(\\mathbf{B}=\\begin{pmatrix} c\\\\ f\\end{pmatrix}\\), \\(\\mathbf{C}=\\begin{pmatrix} g &amp; h \\end{pmatrix}\\), and \\(\\mathbf{D}=\\begin{pmatrix} i\\end{pmatrix}\\). </p> <p>This is especially helpful when the block form has off-diagonal submatrices consisting of all zeros. For instance, when \\(\\mathbf{B}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\\) or \\(\\mathbf{C}=\\begin{pmatrix} 0 &amp; 0 \\end{pmatrix}\\), we have a block triangular matrix. And when \\(\\mathbf{B}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\\) and \\(\\mathbf{C}=\\begin{pmatrix} 0 &amp; 0 \\end{pmatrix}\\),  we have a block diagonal matrix.</p> <p>Finally, it is sometimes useful to transpose a matrix, which exchanges the rows and columns (an element in row \\(i\\) column \\(j\\) moves to row \\(j\\) column \\(i\\))</p> \\[ \\begin{pmatrix}   a_1 &amp; a_2 &amp; a_3 \\\\   b_1 &amp; b_2 &amp; b_3 \\end{pmatrix}^\\intercal =  \\begin{pmatrix}   a_1 &amp; b_1  \\\\   a_2 &amp; b_2  \\\\   a_3 &amp; b_3 \\end{pmatrix} \\] <p>Like vectors, matrices have a graphical/geometrical interpretation: they stretch and rotate vectors (as we will see shortly).</p> <p></p>"},{"location":"lectures/lecture-06/#4-addition","title":"4. Addition","text":"<p>Vector and matrix addition (and subtraction) is straightforward, entry-by-entry:</p> \\[ \\begin{equation*} \\begin{pmatrix}   a \\\\   b \\end{pmatrix} + \\begin{pmatrix}   c \\\\   d \\end{pmatrix} = \\begin{pmatrix}   a+c \\\\   b+d \\end{pmatrix} \\end{equation*} \\] \\[ \\begin{equation*} \\begin{pmatrix}   a &amp; b \\\\   c &amp; d \\end{pmatrix} + \\begin{pmatrix}   e &amp; f \\\\   g &amp; h \\end{pmatrix} = \\begin{pmatrix}   a+e &amp; b+f \\\\   c+g &amp; d+h \\end{pmatrix} \\end{equation*} \\] <p>Warning</p> <p>The vectors or matrices added together must have the same dimension!</p> <p>Geometrically, adding vectors is like placing the second vector at the end of the first. Below we add the black and red vectors together to get the blue vector.</p> <pre>\nimport matplotlib.pyplot as plt #import plotting library\n\nv1 = [1,2] #vector 1\nv2 = [1,0] #vector 2\nv12 = [i+j for i,j in zip(v1,v2)] #sum of the two vectors\n\n#first vector\nplt.arrow(0, 0, #starting x and y values of arrow\n          v1[0], v1[1], #change in x and y \n          head_width=0.1, color='black', length_includes_head=True) #aesthetics\n\n#second vector placed at the end of first vector\nplt.arrow(v1[0], v1[1], #starting x and y values of arrow\n          v2[0], v2[1], #change in x and y \n          head_width=0.1, color='red', length_includes_head=True) #aesthetics\n\n#sum of the vectors\nplt.arrow(0, 0, #starting x and y values of arrow\n          v12[0], v12[1], #change in x and y \n          head_width=0.1, color='blue', length_includes_head=True) #aesthetics\n\nplt.xlim(0,2.5) #set bounds on x axis\nplt.ylim(0,2.5) #set bounds on y axis\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-06/#5-multiplication","title":"5. Multiplication","text":"<p>Vector and matrix multiplication by a scalar (which may be a constant, a variable, or a function, but not a matrix or a vector) is also straightforward, we just multiply every element by the scalar:</p> \\[ \\begin{equation*} \\alpha  \\begin{pmatrix}   a \\\\   b \\end{pmatrix} = \\begin{pmatrix}   \\alpha a \\\\   \\alpha b \\end{pmatrix} \\end{equation*} \\] \\[ \\begin{equation*} \\alpha  \\begin{pmatrix}   a &amp; b\\\\   c &amp; d \\end{pmatrix} = \\begin{pmatrix}   \\alpha a &amp; \\alpha b\\\\   \\alpha c &amp; \\alpha d \\end{pmatrix} \\end{equation*} \\] <p>Geometrically, multiplying by a scalar stretches (if \\(\\alpha&gt;1\\)) or compresses (if \\(\\alpha&lt;1\\)) a vector. Below we multiply the black vector by \\(1/2\\) to get the red vector.</p> <pre>\nimport matplotlib.pyplot as plt #import plotting library\n\nv1 = [1,2] #vector 1\nalpha = 1/2 #scalar\nv2 = [i*alpha for i in v1] #multiplication by a scalar\n\n#original vector\nplt.arrow(0, 0, #starting x and y values of arrow\n          v1[0], v1[1], #change in x and y \n          head_width=0.1, color='black', length_includes_head=True) #aesthetics\n\n#stretched vector\nplt.arrow(0, 0, #starting x and y values of arrow\n          v2[0], v2[1], #change in x and y \n          head_width=0.1, color='red', length_includes_head=True) #aesthetics\n\nplt.xlim(0,2.5) #set bounds on x axis\nplt.ylim(0,2.5) #set bounds on y axis\nplt.show()\n</pre> <p></p> <p>Multiplying vectors and matrices together is a bit trickier, but is based on the fact that a row vector times a column vector is equal to the sum of the products of their respective entries</p> \\[ \\begin{equation*} \\begin{pmatrix} a &amp; b &amp; c \\end{pmatrix} \\begin{pmatrix}   x \\\\   y \\\\   z \\end{pmatrix} = ax + by + cz \\end{equation*} \\] <p>This is referred to as the dot product. (There are other types of products for vectors and matrices, which we won't cover in this class.)</p> <p>To multiply a matrix by a vector, this procedure is repeated: first for the first row of the matrix, then for the second row of the matrix, etc, and stacking the sums in the resulting vector,</p> \\[ \\begin{equation*} \\begin{pmatrix}   a &amp; b &amp; c \\\\   d &amp; e &amp; f \\\\   g &amp; h &amp; i \\end{pmatrix} \\begin{pmatrix}   x \\\\   y \\\\   z \\end{pmatrix} = \\begin{pmatrix}   ax + by + cz \\\\   dx + ey + fz \\\\   gx + hy + iz \\\\ \\end{pmatrix} \\end{equation*} \\] <p>Geometrically, multiplying a vector by a matrix stretches and rotates a vector. Below we multiply the black vector by a matrix to get the red vector.</p> <pre>\nimport matplotlib.pyplot as plt #import plotting library\nfrom sympy import *\n\nv = Matrix([[2],[1]]) #column vector\nM = Matrix([[1,-1],[1,1/4]]) #matrix\nu = M*v\n\n#original vector\nplt.arrow(0, 0, #starting x and y values of arrow\n          float(v[0]), float(v[1]), #change in x and y \n          head_width=0.1, color='black', length_includes_head=True) #aesthetics\n\n#stretched and rotated vector\nplt.arrow(0, 0, #starting x and y values of arrow\n          float(u[0]), float(u[1]), #change in x and y \n          head_width=0.1, color='red', length_includes_head=True) #aesthetics\n\nplt.xlim(0,2.5) #set bounds on x axis\nplt.ylim(0,2.5) #set bounds on y axis\nplt.show()\n</pre> <p></p> <p>To multiply a matrix by a matrix, the same procedure is then also repeated acros the columns of the second matrix: first for the first column of the second matrix, then for the second column of the second matrix, etc, and placing the sums in their respective rows and columns of the resulting matrix,</p> \\[ \\begin{equation*} \\begin{pmatrix}   a &amp; b \\\\   c &amp; d \\end{pmatrix} \\begin{pmatrix}   e &amp; f \\\\   g &amp; h \\end{pmatrix} = \\begin{pmatrix}   ae + bg &amp; af + bh \\\\   ce + dg &amp; cf + dh \\end{pmatrix} \\end{equation*} \\] <p>Warning</p> <p>An \\(m \\times n\\) matrix (or vector) \\(\\mathbf{A}\\) can be multiplied on the right by \\(\\mathbf{B}\\) only if \\(\\mathbf{B}\\) is an \\(n \\times p\\) matrix (or vector). The resulting matrix (or vector) will then be \\(m \\times p\\). </p> <p>As opposed to basic algebra, matrix multiplication is not commutative. That is, \\(\\mathbf{AB}\\) does not generally equal \\(\\mathbf{BA}\\).</p> <p>This means that if we want to multiply both sides of an equation, e.g., \\(\\mathbf{AB} = \\mathbf{C}\\), by \\(\\mathbf{D}\\), we need to do so on the same side, either multiplying by \\(\\mathbf{D}\\) on the right \\(\\mathbf{ABD} = \\mathbf{CD}\\) or on the left \\(\\mathbf{DAB} = \\mathbf{DC}\\). </p> <p>On the other hand, matrix multiplication does satisfy the other basic algebra rules:</p> <ul> <li>\\((\\mathbf{AB})\\mathbf{C} = \\mathbf{A}(\\mathbf{BC})\\) (associative law)</li> <li>\\(\\mathbf{A}(\\mathbf{B+C}) = \\mathbf{AB}+\\mathbf{AC}\\) (distributive law)</li> <li>\\((\\mathbf{A}+\\mathbf{B})\\mathbf{C} = \\mathbf{AC}+\\mathbf{BC}\\) (distributive law)</li> <li>\\(\\alpha(\\mathbf{AB}) = (\\alpha\\mathbf{A})\\mathbf{B} = \\mathbf{A}(\\alpha\\mathbf{B}) = (\\mathbf{A}\\mathbf{B})\\alpha\\) (commutative law for scalars)</li> </ul> <p>Multiplication between the identity matrix and any vector, \\(\\vec{v}\\), or square matrix, \\(\\mathbf{M}\\), has no effect (the identity matrix is like a \"1\" in basic algebra),</p> \\[ \\mathbf{I}\\vec{v}=\\vec{v} \\] \\[ \\mathbf{I}\\mathbf{M}=\\mathbf{M}\\mathbf{I}=\\mathbf{M} \\] <p></p>"},{"location":"lectures/lecture-06/#6-summary","title":"6. Summary","text":"<p>We can represent linear multivariate models in terms of matrices and vectors that are added and multiplied together.</p> <p>You can now check for yourself that the matrix version of the bird model</p> \\[ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} = \\mathbf{M}\\vec{n} + \\vec{m}, \\] <p>with </p> \\[ \\begin{aligned} \\vec{n} &amp;= \\begin{pmatrix} n_1 \\\\ n_2 \\end{pmatrix}\\\\ \\mathbf{M} &amp;= \\begin{pmatrix} b_1 - d_1 - m_{21} &amp; m_{12} \\\\ m_{21} &amp; b_2 - d_2 - m_{12} \\end{pmatrix}\\\\ \\vec{m} &amp;= \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}, \\end{aligned} \\] <p>is equivalent to the coupled differential equation version,</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &amp;= (b_1 - d_1 - m_{21})n_1 + m_{12} n_2 + m_1 \\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} &amp;= m_{21} n_1 + (b_2 - d_2 - m_{12})n_2 + m_2, \\end{aligned} \\] <p>by evaluating \\(\\mathbf{M}\\vec{n} + \\vec{m}\\).</p> <p>We would also like to be comfortable creating the matrices and vectors for a given set of linear differential or recursion equations. For example, given</p> \\[ \\begin{aligned} x_1(t+1) &amp;= b + a x_1(t) \\\\ x_2(t+1) &amp;= - d x_2(t) + c x_1(t) , \\end{aligned} \\] <p>what are \\(\\mathbf{A}\\) and \\(\\vec{v}\\) such that \\(\\vec{x}(t+1) = \\mathbf{A}\\vec{x}(t) + \\vec{v}\\)?</p> <p>Practice questions from the textbook: Exercises P2.1-P2.5.</p>"},{"location":"lectures/lecture-07/","title":"Lecture 7","text":""},{"location":"lectures/lecture-07/#lecture-7-finding-equilibria-in-linear-multivariate-models","title":"Lecture 7: Finding equilibria in linear multivariate models","text":"Run notes interactively?"},{"location":"lectures/lecture-07/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Motivation</li> <li>Matrix inversion</li> <li>Solving for equilibrium</li> <li>Solving for equilibrium when singular</li> <li>Summary</li> </ol>"},{"location":"lectures/lecture-07/#1-motivation","title":"1. Motivation","text":"<p>We now know that we can represent our linear multivariate model for the number of birds on two islands in matrix form,</p> \\[ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} = \\mathbf{M}\\vec{n} + \\vec{m}, \\] <p>where </p> \\[ \\begin{aligned} \\vec{n} &amp;= \\begin{pmatrix} n_1 \\\\ n_2 \\end{pmatrix}\\\\ \\mathbf{M} &amp;= \\begin{pmatrix} b_1 - d_1 - m_{21} &amp; m_{12} \\\\ m_{21} &amp; b_2 - d_2 - m_{12} \\end{pmatrix}\\\\ \\vec{m} &amp;= \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}, \\end{aligned} \\] <p>with \\(n_i\\) the number of birds on island \\(i\\) at time \\(t\\), \\(b_i\\) the birth rate on island \\(i\\), \\(d_i\\) the death rate on island \\(i\\), \\(m_{ij}\\) the rate at which birds on island \\(j\\) migrate to island \\(i\\), and \\(m_i\\) the rate at which birds arrive to island \\(i\\) from elsewhere.</p> <p>Next we would like to solve for the equilibria. We can start by setting the differential equation to 0 and subtracting \\(\\vec{m}\\) from both sides,</p> \\[ \\begin{align*} 0 &amp;= \\mathbf{M}\\hat{\\vec{n}} + \\vec{m}\\\\ -\\vec{m} &amp;= \\mathbf{M}\\hat{\\vec{n}}. \\end{align*} \\] <p>Now to isolate our variable, \\(\\hat{\\vec{n}}\\), we effectively need to move \\(\\mathbf{M}\\) to the other side of the equation. In normal algebra we would simply divide both sides by \\(\\mathbf{M}\\). But note that in the last lecture we discussed matrix addition and multiplication. We did not yet discuss division. This is because for matrices there is no such thing as division. The analogy is the inverse.</p> <p></p>"},{"location":"lectures/lecture-07/#2-matrix-inversion","title":"2. Matrix inversion","text":"<p>A square \\(m\\times m\\) matrix \\(\\mathbf{M}\\) is invertible if it may be multiplied by another matrix to get the identity matrix.  We call this second matrix, \\(\\mathbf{M}^{-1}\\), the inverse of the first,</p> \\[ \\mathbf{M}\\mathbf{M}^{-1} = \\mathbf{I} = \\mathbf{M}^{-1}\\mathbf{M}. \\] <p>Geometrically, the inverse reverses the stretching and rotating that the original matrix does to a vector,</p> \\[ \\mathbf{M}^{-1}(\\mathbf{M}\\vec{v}) = (\\mathbf{M}^{-1}\\mathbf{M})\\vec{v} = \\mathbf{I}\\vec{v} = \\vec{v}. \\] <p>There are rules to find the inverse of a matrix when it is invertible. To know if a matrix is invertible we can calculate the determinant.</p>"},{"location":"lectures/lecture-07/#determinant","title":"Determinant","text":"<p>The determinant of a \\(2 \\times 2\\) matrix is</p> \\[ \\begin{equation*} \\text{Det}\\left( \\begin{pmatrix}   a &amp; b \\\\   c &amp; d \\end{pmatrix}\\right) = \\begin{vmatrix}   a &amp; b\\\\   c &amp; d \\end{vmatrix} =ad-bc. \\end{equation*} \\] <p>In principle we can calculate the determinant of any square matrix. The determinant of an \\(n \\times n\\) matrix can be obtained by working along the first row, multiplying the first element of the first row by the determinant of the matrix created by deleting the first row and first column minus the second element of the first row times the determinant of the matrix created by deleting the first row and second column plus the third element... and so on. </p> <p>For example, </p> \\[ \\begin{equation*} \\begin{vmatrix}   a &amp; b &amp; c \\\\   d &amp; e &amp; f\\\\   g &amp; h &amp; i \\end{vmatrix} = a \\begin{vmatrix}   e &amp; f\\\\   h &amp; i \\end{vmatrix} - b \\begin{vmatrix}   d &amp; f\\\\   g &amp; i \\end{vmatrix} + c \\begin{vmatrix}   d &amp; e\\\\   g &amp; h \\end{vmatrix}. \\end{equation*} \\] <p>More generally we can perform this technique along any row or any column. The trick is remembering which terms are added and which are subtracted. One way to remember is: if both the row \\(i\\) and column \\(j\\) are even or both are odd then the term multiplied by the element at that position, \\(m_{ij}\\), gets a plus, otherwise it gets a minus.  </p> General formula for determinants <p>Moving along any row \\(i\\),</p> \\[ |\\mathbf{M}| = (-1)^{i+1}\\sum_{j=1}^{n}(-1)^{j+1}m_{ij}  |\\mathbf{M}_{ij}|. \\] <p>Moving along column \\(j\\),</p> \\[ |\\mathbf{M}| = (-1)^{j+1}\\sum_{i=1}^{n}(-1)^{i+1}m_{ij}  |\\mathbf{M}_{ij}|. \\] <p>A few useful rules emerge from this method:</p> <ul> <li>the determinant of a matrix is the same as the determinant of its transpose, \\(|\\mathbf{M}| = |\\mathbf{M}^\\intercal|\\),</li> <li>the determinant of a diagonal or triangular matrix is the product of the diagonal elements, \\(|\\mathbf{M}| = \\prod_{i=1}^n m_{ii} = m_{11}m_{22}\\cdots m_{nn}\\),</li> <li>the determinant of a block-diagonal or block-triangular matrix is the product of the determinants of the diagonal submatrices.</li> </ul> <p>It also suggests that rows or columns with lots of zeros are very helpful when calculating the determinant, for example,</p> \\[ \\begin{aligned} \\begin{vmatrix}   m_{11} &amp; m_{12} &amp; m{13} \\\\   m_{21} &amp; 0 &amp; 0 \\\\   m_{31} &amp; m_{32} &amp; m_{33} \\\\ \\end{vmatrix} =  - m_{21}  \\begin{vmatrix}   m_{12} &amp; m_{13} \\\\   m_{32} &amp; m_{33} \\\\ \\end{vmatrix}. \\end{aligned} \\] <p>Now, why does the determinant tell us anything about whether a matrix is invertible? Well, when the determinant is zero, \\(|\\mathbf{M}|=0\\), it means that the rows are not linearly independent, that is, some row \\(\\vec{r}_k\\) can be written as \\(a_1 \\vec{r}_1 + \\cdots + a_{k-1} \\vec{r}_{k-1} + a_{k+1} \\vec{r}_{k+1} + \\cdots + a_n \\vec{r}_n\\), where the \\(a_i\\) are scalars and the \\(\\vec{r}_i\\) are the rows of \\(\\mathbf{M}\\). As a result, when we multiply a vector by a matrix with a determinant of zero we lose some information and therefore cannot reverse the operation. This is analagous to mutliplying by 0 in normal algebra -- if we multiply a bunch of different numbers by zero we have no way of reversing the operation to know what the original numbers were. So, a matrix is invertible if and only if it has a nonzero determinant, \\(|\\mathbf{M}|\\neq0\\). Matrices that are not invertible are called singular.</p> <p>Geometrically, mutliplying multiple vectors by a matrix whose deteriminant is zero causes them to fall along a line. Below we multiply the two black vectors by a matrix whose determinant is zero to get the two red vectors, which fall along the same line. We have lost information and cannot undue the operation to recover the original vectors.</p> <pre>\nimport matplotlib.pyplot as plt #import plotting library\nfrom sympy import *\n\nv1 = Matrix([[2],[1]]) #column vector 1\nv2 = Matrix([[1],[1]]) #column vector 2\nM = Matrix([[1/2,1],[1,2]]) #matrix with determinant of zero\n\n#original vectors\nfor v in [v1,v2]:\n    plt.arrow(0, 0, #starting x and y values of arrow\n              float(v[0]), float(v[1]), #change in x and y \n              head_width=0.1, color='black', length_includes_head=True) #aesthetics\n\n#stretched and rotated vectors\nfor v in [M*v1,M*v2]:\n    plt.arrow(0, 0, #starting x and y values of arrow\n              float(v[0]), float(v[1]), #change in x and y \n              head_width=0.1, color='red', length_includes_head=True) #aesthetics\n\nplt.xlim(0,5) #set bounds on x axis\nplt.ylim(0,5) #set bounds on y axis\nplt.show()\n</pre> <p></p>"},{"location":"lectures/lecture-07/#inverting","title":"Inverting","text":"<p>Now back to how to find the inverse of a matrix.</p> <p>For an invertible 2x2 matrix we do the following</p> \\[ \\begin{align} \\mathbf{M}^{-1}  =&amp;\\begin{pmatrix}   a &amp; b \\\\   c &amp; d \\end{pmatrix}^{-1}\\\\ &amp;=\\frac{1}{|\\mathbf{M}|} \\begin{pmatrix}   d  &amp; -b \\\\   -c &amp; a \\end{pmatrix}\\\\ &amp;=\\frac{1}{ad-bc} \\begin{pmatrix}   d  &amp; -b \\\\   -c &amp; a \\end{pmatrix}\\\\ &amp;= \\begin{pmatrix}   \\frac{d}{ad-bc}  &amp; \\frac{-b}{ad-bc} \\\\   \\frac{-c}{ad-bc} &amp; \\frac{a}{ad-bc} \\end{pmatrix} \\end{align} \\] <p>Larger matrices are more difficult to invert by hand. One exception is if they are diagonal, in which case we simply invert each of the diagonal elements,</p> \\[ \\mathbf{M}^{-1} =  \\begin{pmatrix} 1/m_{11} &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; 1/m_{22} &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ 0 &amp; 0 &amp; \\cdots &amp; 1/m_{nn} \\end{pmatrix}. \\] <p></p>"},{"location":"lectures/lecture-07/#3-solving-for-equilibrium","title":"3. Solving for equilibrium","text":"<p>OK, now let's return to our model of birds on islands,  </p> \\[ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} = \\mathbf{M}\\vec{n} + \\vec{m}, \\] <p>and solve for the equilibria, \\(\\hat{\\vec{n}}\\). </p> <p>We do this by setting the rate of change to zero \\(\\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t}=0\\), subtracting \\(\\vec{m}\\) from both sides, and multiplying by the inverse matrix \\(\\mathbf{M}^{-1}\\) on the left:</p> \\[ \\begin{align*} 0 &amp;= \\mathbf{M}\\hat{\\vec{n}} + \\vec{m}\\\\ -\\vec{m} &amp;= \\mathbf{M}\\hat{\\vec{n}}\\\\ -\\mathbf{M}^{-1}\\vec{m} &amp;= \\mathbf{M}^{-1}\\mathbf{M}\\hat{\\vec{n}}\\\\ -\\mathbf{M}^{-1}\\vec{m} &amp;= \\mathbf{I}\\hat{\\vec{n}}\\\\ -\\mathbf{M}^{-1}\\vec{m} &amp;= \\hat{\\vec{n}}. \\end{align*} \\] <p>We can write the left hand side in terms of our parameters by calculating the inverse of this 2x2 matrix and multiplying by the vector</p> \\[ \\begin{align} \\hat{\\vec{n}}  &amp;=-\\mathbf{M}^{-1}\\vec{m}\\\\ &amp;=-\\frac{1}{|\\mathbf{M}|} \\begin{pmatrix} b_2 - d_2 - m_{12} &amp; -m_{12} \\\\ -m_{21} &amp; b_1 - d_1 - m_{21} \\end{pmatrix} \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}\\\\ &amp;= -\\frac{1}{(b_1 - d_1 - m_{21})(b_2 - d_2 - m_{12})-m_{21}m_{12}} \\begin{pmatrix} (b_2 - d_2 - m_{12})m_1 -m_{12}m_2 \\\\ -m_{21}m_1 + (b_1 - d_1 - m_{21})m_2 \\end{pmatrix}\\\\ &amp;= \\begin{pmatrix} -\\frac{(b_2 - d_2 - m_{12})m_1 -m_{12}m_2}{(b_1 - d_1 - m_{21})(b_2 - d_2 - m_{12})-m_{21}m_{12}} \\\\ -\\frac{-m_{21}m_1 + (b_1 - d_1 - m_{21})m_2}{(b_1 - d_1 - m_{21})(b_2 - d_2 - m_{12})-m_{21}m_{12}} \\end{pmatrix} \\end{align} \\] <p>Ta-da! Using linear algebra we solved for both equilibria, \\(\\hat{n}_1\\) and \\(\\hat{n}_2\\), with a single equation. </p> <p>We can visualize this equilibrium as the intersection of the nullclines, which are the values of the variables that make the change in each variable zero. In this case we can solve for the nullclines in terms of \\(n_2\\),</p> \\[ \\begin{align} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &amp;= 0\\\\ (b_1 - d_1 - m_{21})n_1 + m_{12} n_2 + m_1 &amp;= 0\\\\ m_{12} n_2 &amp;= -m_1 - (b_1 - d_1 - m_{21})n_1\\\\ n_2 &amp;= \\frac{-m_1 - (b_1 - d_1 - m_{21})n_1}{m_{12}} \\end{align} \\] <p>and</p> \\[ \\begin{align} \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} &amp;= 0\\\\ (b_2 - d_2 - m_{12})n_2 + m_{21} n_1 + m_2 &amp;= 0\\\\ (b_2 - d_2 - m_{12})n_2  &amp;= -m_{21} n_1 - m_2\\\\ n_2  &amp;= \\frac{-m_{21} n_1 - m_2}{b_2 - d_2 - m_{12}}, \\end{align} \\] <p>and plot them as functions of \\(n_1\\). Our predicted equilibrium correctly lands right on the intersection of the two nullclines.</p> <pre>\nimport matplotlib.pyplot as plt\nfrom sympy import *\nimport numpy as np\n\n# define the variables\nn1, n2 = symbols('n1, n2')\n\n# Choose the parameter values\nb1, b2 = 1, 1\nd1, d2 = 1.1, 1.1\nm12, m21 = 0.05, 0.05\nm1, m2 = 5, 5\n\n# define differential equations\ndn1dt = (b1 - d1 - m12) * n1 + m21 * n2 + m1\ndn2dt = m12 * n1 + (b2 - d2 - m21) * n2 + m2\n\n# get the nullclines\nnullcline_1 = solve(Eq(dn1dt, 0),n2)[0]\nnullcline_2 = solve(Eq(dn2dt, 0),n2)[0]\n\n# plot\nn1s = np.linspace(0,100,100)\nplt.plot(n1s, [nullcline_1.subs(n1,i) for i in n1s], label='$n_1$ nullcline')\nplt.plot(n1s, [nullcline_2.subs(n1,i) for i in n1s], label='$n_2$ nullcline')\n\n# add predicted equilibrium\nn1eq = -((b2-d2-m21)*m1-m21*m2)/((b1-d1-m12)*(b2-d2-m21)-m21*m12)\nn2eq = -(-m12*m1+(b1-d1-m12)*m2)/((b1-d1-m12)*(b2-d2-m21)-m21*m12)\nplt.scatter(n1eq,n2eq, color='k', zorder=2, s=100, label='equilibrium')\n\nplt.xlabel('number of birds on island 1, $n_1$')\nplt.ylabel('number of birds on island 2, $n_2$')\nplt.xlim(0,100)\nplt.ylim(0,100)\nplt.legend()\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-07/#4-solving-for-equilibrium-when-singular","title":"4. Solving for equilibrium when singular","text":"<p>Now, if our matrix \\(\\mathbf{M}\\) is singular we can still try to solve for equilibrium, we just can't use matrix algebra. Instead we try to simultaneously solve all equations. We've nearly done this above with the nullclines already. Setting the two nullclines equal to one another to find their intersection,</p> \\[ \\begin{align} \\frac{-m_1 - (b_1 - d_1 - m_{21})\\hat{n}_1}{m_{12}} &amp;= \\frac{-m_{21} \\hat{n}_1 - m_2}{b_2 - d_2 - m_{12}}\\\\ (-m_1 - (b_1 - d_1 - m_{21})\\hat{n}_1)(b_2 - d_2 - m_{12}) &amp;= (-m_{21} \\hat{n}_1 - m_2)m_{12}\\\\ (m_{21}m_{12}-(b_1 - d_1 - m_{21})(b_2 - d_2 - m_{12}))\\hat{n}_1 &amp;= - m_2m_{12} + m_1(b_2 - d_2 - m_{12})\\\\ -|\\mathbf{M}|\\hat{n}_1 &amp;= - m_2m_{12} + m_1(b_2 - d_2 - m_{12})\\\\ 0 &amp;= - m_2m_{12} + m_1(b_2 - d_2 - m_{12}). \\end{align} \\] <p>This is either true or not true, regardless of \\(n_1\\) and \\(n_2\\). This means that the two nullclines intersect everywhere or nowhere, which implies there are either infinite equilibria (along the shared nullcline) or no equilibria.</p> <p>Below is a scenario where there are no equilibria, in which case the nullclines never cross because they are parallel to one another.</p> <pre>\nimport matplotlib.pyplot as plt\nfrom sympy import *\nimport numpy as np\n\n# define the variables\nn1, n2 = symbols('n1, n2')\n\n# choose the parameter values\nb1, b2 = 1.1, 1\nd1, d2 = 1.01, 1.01\nm12 = 0.5\nm21 = (b1 - d1)*(b2 - d2 - m12)/(m12 + (b2 - d2 - m12)) #forces zero determinant\nm1 = 5\ndelta = 10 #0 for infinite equilibria, nonzero for no equilibria\nm2 = m1*(b2 - d2 - m12)/m12 + delta\n\n# define differential equations\ndn1dt = (b1 - d1 - m21) * n1 + m12 * n2 + m1\ndn2dt = m21 * n1 + (b2 - d2 - m12) * n2 + m2\n\n# get the nullclines\nnullcline_1 = solve(Eq(dn1dt, 0),n2)[0]\nnullcline_2 = solve(Eq(dn2dt, 0),n2)[0]\n\n# plot\nn1s = np.linspace(0,100,100)\nplt.plot(n1s, [nullcline_1.subs(n1,i) for i in n1s], label='$n_1$ nullcline')\nplt.plot(n1s, [nullcline_2.subs(n1,i) for i in n1s], label='$n_2$ nullcline')\n\n# # add predicted equilibrium\n# n1eq = -((b2-d2-m21)*m1-m21*m2)/((b1-d1-m12)*(b2-d2-m21)-m21*m12)\n# n2eq = -(-m12*m1+(b1-d1-m12)*m2)/((b1-d1-m12)*(b2-d2-m21)-m21*m12)\n# plt.scatter(n1eq,n2eq, color='k', zorder=2, s=100, label='equilibrium')\n\nplt.xlabel('number of birds on island 1, $n_1$')\nplt.ylabel('number of birds on island 2, $n_2$')\nplt.xlim(0,100)\nplt.ylim(0,100)\nplt.legend()\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-07/#5-summary","title":"5. Summary","text":"<p>We can solve multivariate linear equations using matrix inversion, giving us a way to find equilibria when the matrix is invertible (ie, the determinant is nonzero). These equilibria are where the change in all variables is zero, i.e., where the nullclines for all variables intersect.</p> <p>Practice questions from the textbook: P2.6-P2.11.</p>"},{"location":"lectures/lecture-08/","title":"Lecture 8","text":""},{"location":"lectures/lecture-08/#lecture-8-general-solutions-for-linear-multivariate-models","title":"Lecture 8: General solutions for linear multivariate models","text":"Run notes interactively?"},{"location":"lectures/lecture-08/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Motivation</li> <li>Finding eigenvalues</li> <li>Finding eigenvectors</li> <li>Motivation revisited</li> <li>Summary</li> </ol>"},{"location":"lectures/lecture-08/#1-motivation","title":"1. Motivation","text":"<p>Let's return to our model of the number of birds on two islands and switch to discrete time (assuming migration, then birth, then death). The number of birds on each island in the next time step is then</p> \\[ \\begin{aligned} n_1(t+1) &amp;= ((1-m_{21})n_1(t) + m_{12}n_2(t))(1+b_1)(1-d_1) + m_1\\\\ n_2(t+1) &amp;= (m_{21}n_1(t) + (1-m_{12})n_2(t))(1+b_2)(1-d_2) + m_2. \\end{aligned} \\] <p>Or in matrix form,</p> \\[ \\begin{aligned} \\vec{n}(t+1) &amp;= \\mathbf{M}\\vec{n}(t) + \\vec{m}, \\end{aligned} \\] <p>where</p> \\[ \\mathbf{M} =  \\begin{pmatrix}  (1-m_{21})(1+b_1)(1-d_1) &amp; m_{12}(1+b_1)(1-d_1) \\\\  m_{21}(1+b_2)(1-d_2) &amp; (1-m_{12})(1+b_2)(1-d_2) \\end{pmatrix} \\] <p>and </p> \\[ \\vec{m} =  \\begin{pmatrix}  m_1 \\\\ m_2. \\end{pmatrix}. \\] <p>The question we now want to answer is, how do the numbers of birds on the two islands change over time?</p> <p>We start by noting that the equilibrium is</p> \\[ \\begin{aligned} \\hat{\\vec{n}} &amp;= \\mathbf{M}\\hat{\\vec{n}} + \\vec{m}\\\\ -\\vec{m} &amp;= \\mathbf{M}\\hat{\\vec{n}} - \\hat{\\vec{n}}\\\\ -\\vec{m} &amp;= \\mathbf{M}\\hat{\\vec{n}} - \\mathbf{I}\\hat{\\vec{n}} \\\\ -\\vec{m} &amp; = (\\mathbf{M} - \\mathbf{I})\\hat{\\vec{n}}\\\\ -(\\mathbf{M} - \\mathbf{I})^{-1}\\vec{m} &amp; = \\hat{\\vec{n}} \\end{aligned} \\] <p>and the deviation from this equilibrium, \\(\\vec{\\delta}(t) = \\vec{n}(t) - \\hat{\\vec{n}}\\), obeys</p> \\[ \\begin{aligned} \\vec{\\delta}(t+1) &amp;= \\vec{n}(t+1) - \\hat{\\vec{n}}\\\\ &amp;= \\mathbf{M}\\vec{n}(t) + \\vec{m} - \\hat{\\vec{n}}\\\\ &amp;= \\mathbf{M}(\\vec{\\delta}(t) + \\hat{\\vec{n}}) + \\vec{m} - \\hat{\\vec{n}}\\\\ &amp;= \\mathbf{M}\\vec{\\delta}(t) + \\mathbf{M}\\hat{\\vec{n}} - \\mathbf{I}\\hat{\\vec{n}}  + \\vec{m}\\\\ &amp;= \\mathbf{M}\\vec{\\delta}(t) + (\\mathbf{M} - \\mathbf{I})\\hat{\\vec{n}}  + \\vec{m}\\\\ &amp;= \\mathbf{M}\\vec{\\delta}(t) - \\vec{m}  + \\vec{m}\\\\ &amp;= \\mathbf{M}\\vec{\\delta}(t). \\end{aligned} \\] <p>This is (multivariate) exponential growth, which can be solved by brute force iteration,</p> \\[ \\begin{aligned} \\vec{\\delta}(t) &amp;= \\mathbf{M}\\vec{\\delta}(t-1)\\\\ &amp;= \\mathbf{M}\\mathbf{M}\\vec{\\delta}(t-2)\\\\  &amp;= \\mathbf{M}^2\\vec{\\delta}(t-2)\\\\ &amp;\\vdots\\\\ &amp;= \\mathbf{M}^t\\vec{\\delta}(0). \\end{aligned} \\] <p>Great! We now have the general solution,</p> \\[ \\begin{aligned} \\vec{\\delta}(t) &amp;= \\mathbf{M}^t\\vec{\\delta}(0)\\\\ \\vec{n}(t) - \\hat{\\vec{n}} &amp;= \\mathbf{M}^t(\\vec{n}(0) - \\hat{\\vec{n}})\\\\ \\vec{n}(t) &amp;= \\mathbf{M}^t(\\vec{n}(0) - \\hat{\\vec{n}}) + \\hat{\\vec{n}}. \\end{aligned} \\] <p>The trouble is that it will generally be hard to compute \\(\\mathbf{M}^t\\) (if you don't believe me, try calculating \\(\\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}^3\\)).</p> <p>Now imagine that there existed an invertible matrix \\(\\mathbf{A}\\) and a diagonal matrix \\(\\mathbf{D}\\) that satisfied \\(\\mathbf{M} \\mathbf{A} = \\mathbf{A} \\mathbf{D}\\). Then</p> \\[ \\begin{aligned} \\mathbf{M} \\mathbf{A} &amp;= \\mathbf{A} \\mathbf{D}\\\\ \\mathbf{M} \\mathbf{A}\\mathbf{A}^{-1} &amp;= \\mathbf{A} \\mathbf{D}\\mathbf{A}^{-1}\\\\   \\mathbf{M} &amp;= \\mathbf{A} \\mathbf{D} \\mathbf{A}^{-1} \\end{aligned} \\] <p>and</p> \\[ \\begin{aligned}  \\vec{n}(t) &amp;= \\mathbf{M}^t(\\vec{n}(0) - \\hat{\\vec{n}}) + \\hat{\\vec{n}}\\\\ &amp;= (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1})^t(\\vec{n}(0) - \\hat{\\vec{n}}) + \\hat{\\vec{n}}\\\\ &amp;= (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) \\cdots (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) (\\vec{n}(0) - \\hat{\\vec{n}}) + \\hat{\\vec{n}}\\\\ &amp;= \\mathbf{A}\\mathbf{D}(\\mathbf{A}^{-1} \\mathbf{A})\\mathbf{D}(\\mathbf{A}^{-1}\\mathbf{A}) \\cdots (\\mathbf{A}^{-1}\\mathbf{A})\\mathbf{D}\\mathbf{A}^{-1} (\\vec{n}(0) - \\hat{\\vec{n}}) + \\hat{\\vec{n}}\\\\  &amp;= \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1} (\\vec{n}(0) - \\hat{\\vec{n}}) + \\hat{\\vec{n}}.  \\end{aligned} \\] <p>This is much easier to calculate because \\(\\mathbf{D}^t\\) is just \\(\\mathbf{D}\\) with each of the diagonal elements to the power \\(t\\). It also hints at a way to approximate longer-term dynamics and determine the stability of an equilibrium -- as we will see later.</p> <p>Continuous time</p> <p>A similar analysis can be done in continuous time. If we start with </p> \\[ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} = \\mathbf{M}\\vec{n} + \\vec{m}, \\] <p>then the general solution is</p> \\[ \\vec{n}(t) = \\mathbf{A}\\exp(\\mathbf{D}t)\\mathbf{A}^{-1}(\\vec{n}(0) - \\hat{\\vec{n}}) + \\hat{\\vec{n}}, \\] <p>where \\(\\hat{\\vec{n}}=-\\mathbf{M}^{-1}\\vec{m}\\) and </p> \\[ \\exp(\\mathbf{D}t) = \\begin{pmatrix} \\exp(\\lambda_1 t) &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; \\exp(\\lambda_2 t) &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ 0 &amp; \\cdots &amp; 0 &amp; \\exp(\\lambda_n t). \\end{pmatrix} \\] <p>Now how do we find \\(\\mathbf{A}\\) and \\(\\mathbf{D}\\)? We need \\(\\mathbf{M} \\mathbf{A} = \\mathbf{A} \\mathbf{D}\\). Writing \\(\\mathbf{A}\\) in terms of its column vectors, \\(\\mathbf{A} = \\begin{pmatrix} \\vec{v}_1 &amp; \\vec{v}_2 \\end{pmatrix}\\), and denoting the diagonal elements of \\(\\mathbf{D}\\) as \\(\\lambda_i\\), we can unpack the equation in terms of column vectors,</p> \\[ \\begin{aligned} \\mathbf{M} \\mathbf{A} &amp;= \\mathbf{A} \\mathbf{D}\\\\ \\mathbf{M} \\begin{pmatrix} \\vec{v}_1 &amp; \\vec{v}_2 \\end{pmatrix} &amp;= \\begin{pmatrix} \\vec{v}_1 &amp; \\vec{v}_2 \\end{pmatrix} \\begin{pmatrix} \\lambda_1 &amp; 0 \\\\ 0 &amp; \\lambda_2 \\end{pmatrix}\\\\   \\begin{pmatrix} \\mathbf{M}\\vec{v}_1 &amp; \\mathbf{M}\\vec{v}_2 \\end{pmatrix} &amp;= \\begin{pmatrix} \\lambda_1\\vec{v}_1 &amp; \\lambda_2\\vec{v}_2 \\end{pmatrix}.   \\end{aligned} \\] <p>So to find \\(\\mathbf{A}\\) and \\(\\mathbf{D}\\) we need to solve \\(\\mathbf{M}\\vec{v}_i = \\lambda_i\\vec{v}_i\\). The solutions to this equation are called eigenvalues (\\(\\lambda_i\\)) and their associated right eigenvectors (\\(\\vec{v}_i\\)). </p> <p></p>"},{"location":"lectures/lecture-08/#2-finding-eigenvalues","title":"2. Finding eigenvalues","text":"<p>To find the eigenvalues of a matrix, first notice that if we try to use linear algebra to solve for a right eigenvector, \\(\\vec{v}\\), we find</p> \\[ \\begin{aligned}  \\mathbf{M}\\vec{v} &amp;= \\lambda\\vec{v}\\\\ \\mathbf{M}\\vec{v} - \\lambda\\vec{v} &amp;= \\vec{0}\\\\ \\mathbf{M}\\vec{v} - \\lambda\\mathbf{I}\\vec{v} &amp;= \\vec{0} \\\\ (\\mathbf{M} - \\lambda\\mathbf{I})\\vec{v} &amp;= \\vec{0}\\\\ \\vec{v} &amp;= (\\mathbf{M} - \\lambda\\mathbf{I})^{-1}\\vec{0} \\\\ \\vec{v} &amp;= \\vec{0}. \\end{aligned} \\] <p>But if \\(\\vec{v} = \\vec{0}\\) then \\(\\mathbf{A}\\) is not invertible. This contradiction implies that we did something wrong in our calculations. The only place we made any assumptions was in our last step, where we assumed \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) was invertible. We then conclude that \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) is non-invertible and therefore must have a determinant of zero, \\(|\\mathbf{M} - \\lambda\\mathbf{I}|=0\\). </p> <p>Interestingly, this last equation, \\(|\\mathbf{M} - \\lambda\\mathbf{I}|=0\\), gives us a way to solve for the eigenvalues, \\(\\lambda\\), without knowing the eigenvectors, \\(\\vec{v}\\). The determinant of the \\(n\\times n\\) matrix \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) is an \\(n^{th}\\) degree polynomial in \\(\\lambda\\), which is called the characteristic polynomial of \\(\\mathbf{M}\\). Setting this polynomial equal to zero and solving for \\(\\lambda\\) gives the \\(n\\) eigenvalues of \\(\\mathbf{M}\\): \\(\\lambda_1,\\lambda_2,...,\\lambda_n\\).</p> <p>For example, in the \\(n=2\\) case we have</p> \\[ \\begin{aligned} \\mathbf{M} - \\lambda \\mathbf{I} &amp;= \\begin{pmatrix} m_{11} &amp; m_{12} \\\\ m_{21} &amp; m_{22} \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}\\\\ &amp;= \\begin{pmatrix} m_{11} &amp; m_{12} \\\\ m_{21} &amp; m_{22} \\end{pmatrix} -  \\begin{pmatrix} \\lambda &amp; 0 \\\\ 0 &amp; \\lambda \\end{pmatrix}\\\\ &amp;= \\begin{pmatrix} m_{11} - \\lambda &amp; m_{12} \\\\ m_{21} &amp; m_{22} - \\lambda \\end{pmatrix} \\end{aligned} \\] <p>so that the characteristic polynomial is</p> \\[ \\begin{aligned} |\\mathbf{M} - \\lambda \\mathbf{I} | =&amp; (m_{11}-\\lambda)(m_{22}-\\lambda)-m_{21} m_{12}\\\\ =&amp;\\lambda^2 - (m_{11}+m_{22})\\lambda + (m_{11}m_{22}-m_{21}m_{12})\\\\ =&amp;\\lambda^2 - \\mathrm{Tr}(\\mathbf{M})\\lambda + \\mathrm{Det}(\\mathbf{M}), \\end{aligned} \\] <p>where we've introduced the trace of a matrix, \\(\\mathrm{Tr}(\\mathbf{M})\\), which is the sum of the diagonal elements.</p> <p>Setting this polynomial equal to zero, the two solutions can be found using the quadratic formula</p> \\[ \\lambda = \\frac{\\mathrm{Tr}(\\mathbf{M}) \\pm \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4\\mathrm{Det}(\\mathbf{M})}}{2}. \\] <p>Finding the determinant of \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) becomes trickier for larger matrices, but we've already learned some helpful properties of determinants that come in handy.</p> <p>For instance, the eigenvalues of a diagonal or triangular matrix are simply the diagonal elements,</p> \\[ \\begin{aligned} |\\mathbf{M} - \\lambda \\mathbf{I}| &amp;= \\begin{vmatrix} m_{11} - \\lambda &amp; 0 &amp; 0 \\\\ m_{21} &amp; m_{22} - \\lambda &amp; 0 \\\\ m_{31} &amp; m_{32} &amp; m_{33} - \\lambda \\end{vmatrix}\\\\ &amp;= (m_{11} - \\lambda) (m_{22} - \\lambda) (m_{33} - \\lambda). \\end{aligned} \\] <p>Similarly, the eigenvalues of a block-diagonal or block-triangular matrix are the eigenvalues of the submatrices along the diagonal,</p> \\[ \\begin{aligned} |\\mathbf{M} - \\lambda \\mathbf{I}| &amp;= \\begin{vmatrix} \\begin{pmatrix} m_{11} - \\lambda &amp; 0 \\\\ m_{21} &amp; m_{22} - \\lambda \\end{pmatrix} &amp; \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\\\ \\begin{pmatrix} m_{31} &amp; m_{32} \\end{pmatrix} &amp; \\begin{pmatrix} m_{33} - \\lambda \\end{pmatrix} \\end{vmatrix}\\\\  &amp;= \\begin{vmatrix} m_{11} - \\lambda &amp; 0 \\\\ m_{21} &amp; m_{22} - \\lambda \\end{vmatrix} \\begin{vmatrix} m_{33} - \\lambda \\end{vmatrix}\\\\ &amp;= (m_{11} - \\lambda) (m_{22} - \\lambda) (m_{33} - \\lambda). \\end{aligned} \\] <p></p>"},{"location":"lectures/lecture-08/#3-finding-eigenvectors","title":"3. Finding eigenvectors","text":"<p>Now that we can find an eigenvalue, how do we find its associated eigenvectors?</p> <p>We know we can't solve \\(\\mathbf{M}\\vec{v} = \\lambda \\vec{v}\\) for \\(\\vec{v}\\) with linear algebra because \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) is singular. Instead we need to write out the system of equations represented by \\(\\mathbf{M}\\vec{v} = \\lambda \\vec{v}\\) and solve for one variable after another.</p> <p>For example, for a \\(2 \\times 2\\) matrix \\(\\mathbf{M}\\) with eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\) we know that a right eigenvector associated with \\(\\lambda_1\\), \\(\\vec{v}_1\\), must solve</p> \\[ \\begin{aligned} \\mathbf{M}\\vec{v}_1 &amp;= \\lambda_1 \\vec{v}_1\\\\ \\begin{pmatrix}   m_{11} &amp; m_{12} \\\\   m_{21} &amp; m_{22} \\end{pmatrix} \\begin{pmatrix}   v_1 \\\\   v_2 \\end{pmatrix} &amp;= \\lambda_1 \\begin{pmatrix}   v_1 \\\\   v_2 \\end{pmatrix}. \\end{aligned} \\] <p>Carrying out the matrix multiplication, we can write down a system of equations corresponding the the rows,</p> \\[ \\begin{aligned} m_{11} v_1 + m_{12} v_2 &amp;= \\lambda_1 v_1 \\\\ m_{21} v_1 + m_{22} v_2 &amp;= \\lambda_1 v_2. \\end{aligned} \\] <p>This system of equations determines the elements of the right eigenvector, \\(\\vec{v}_1\\), associated with \\(\\lambda_1\\).</p> <p>Note from \\(\\mathbf{M}\\vec{v} = \\lambda \\vec{v}\\) that we can multiply \\(\\vec{v}\\) by any constant and that will also be a solution. This means there are an infinite number of eigenvectors associated with an eigenvalue and we can set one of the elements to an arbitrary value. A typical choice is to set the first entry equal to one, \\(v_1 = 1\\).</p> <p>Now we have just one unknown, \\(v_2\\), so we can choose either of the equations above to solve for \\(v_2\\). We pick the first, giving</p> \\[ \\begin{aligned} m_{11} v_1 + m_{12} v_2 &amp;= \\lambda_1 v_1 \\\\ m_{11} 1 + m_{12} v_2 &amp;= \\lambda_1 1 \\\\ v_2 &amp;= (\\lambda_1 - m_{11}) / m_{12}. \\end{aligned} \\] <p>We therefore have right eigenvector \\(\\vec{v}_1 =  \\begin{pmatrix} 1 \\\\ (\\lambda_1 - m_{11})/m_{12} \\end{pmatrix}\\) associated with the eigenvalue \\(\\lambda_1\\). </p> <p>Because we've done this quite generally, we also now know that the right eigenvector associated with the second eigenvalue, \\(\\lambda_2\\), is \\(\\vec{v}_2 = \\begin{pmatrix} 1 \\\\ (\\lambda_2 - m_{11})/m_{12} \\end{pmatrix}\\).</p> <p>Dependence of the eigenvectors on \\(m_{21}\\) and \\(m_{22}\\) will come through the eigenvalues, \\(\\lambda_i\\).</p> <p></p>"},{"location":"lectures/lecture-08/#4-motivation-revisited","title":"4. Motivation revisited","text":"<p>Now let's return to our motivating example of birds on islands. And let's imagine we have good estimates of the parameter values (after years of tough fieldwork!): \\(m_{12}=m_{21}=0.1\\), \\(b_1=b_2=0.2\\), \\(d_1=d_2=0.3\\), \\(m_1=10\\), and \\(m_2=5\\). To derive the general solution, giving the number of birds on the two islands in year \\(t\\), we first derive the eigenvalues and eigenvectors of \\(\\mathbf{M}\\). Using the techniques above we find that the eigenvalues are \\(\\lambda_1=0.672\\) and \\(\\lambda_2=0.84\\). The associated right eigenvectors are \\(\\vec{v}_1=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) and \\(\\vec{v}_2=\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\). We therefore have </p> \\[ \\mathbf{D} = \\begin{pmatrix} 0.672 &amp; 0 \\\\ 0 &amp; 0.84 \\end{pmatrix} \\] <p>and </p> \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; -1 \\end{pmatrix}. \\] <p>This year's census of the islands tells us that there are currently 100 birds on island 1 and 50 on island 2. Taking this as the starting point, \\(\\vec{n}(0) = \\begin{pmatrix} 100 \\\\ 50 \\end{pmatrix}\\), we can use our general solution to predict the number of birds on the two islands over time. Below we plot the predicted number of birds on the two islands over the next 50 years.</p> <pre>\nm12, m21, b1, b2, d1, d2, m1, m2 = 0.1, 0.1, 0.2, 0.2, 0.3, 0.3, 10, 5 #parameter values\nt = symbols('t')\n\n# general solution\nfrom sympy import *\nM = Matrix([[(1-m21)*(1+b1)*(1-d1), m12*(1+b1)*(1-d1)], #matrix\n            [m21*(1+b2)*(1-d2), (1-m12)*(1+b2)*(1-d2)]])\nA, D = M.diagonalize() #quick way to get matrix of right eigenvectors (A) and eigenvalues (D)\nn0 = Matrix([100,50]) #note this is made into a column vector automatically\nm = Matrix([m1,m2])\nnhat = -(M - eye(2)).inv()*m\nnt = A*D**t*A.inv()*(n0-nhat) + nhat #general solution\n\n# plot\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nfor j in range(2): #for each island\n    ax.plot([nt.subs(t,i)[j] for i in range(50)], label='island %d'%(j+1), marker=\".\")\nax.legend()\nax.set_xlabel('years from now')\nax.set_ylabel('number of birds')\nax.set_ylim(0,None)\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-08/#5-summary","title":"5. Summary","text":"<p>To summarize,  </p> <ul> <li>for any system of linear recursion equations, \\(\\vec{x}(t+1) = \\mathbf{M}\\vec{x}(t) + \\vec{m}\\), we can write the general solution as \\(\\vec{x}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}(\\vec{x}(0)-\\hat{\\vec{x}}) + \\hat{\\vec{x}},\\) where \\(\\hat{\\vec{x}}=-(\\mathbf{M} - \\mathbf{I})^{-1}\\vec{m}\\) is the equilibrium, \\(\\mathbf{A}\\) is a matrix with right eigenvectors as columns, and \\(\\mathbf{D}\\) is a diagonal matrix with eigenvalues along the diagonal</li> <li>similarly, for any system of linear differential equations, \\(\\mathrm{d}\\vec{x}/\\mathrm{d}t = \\mathbf{M}\\vec{x} + \\vec{m}\\), we can write the general solution as \\(\\vec{x}(t) = \\mathbf{A}\\exp(\\mathbf{D}t)\\mathbf{A}^{-1}(\\vec{x}(0)-\\hat{\\vec{x}}) + \\hat{\\vec{x}},\\) where \\(\\hat{\\vec{x}}=-\\mathbf{M}^{-1}\\vec{m}\\) is the equilibrium, \\(\\mathbf{A}\\) is a matrix with right eigenvectors as columns, and \\(\\mathbf{D}\\) is a diagonal matrix with eigenvalues along the diagonal</li> <li>the eigenvalues, \\(\\lambda\\), are found by solving \\(|\\mathbf{M} - \\lambda\\mathbf{I}|=0\\)</li> <li>the right eigenvectors, \\(\\vec{v}\\), are found by solving the equations given by \\(\\mathbf{M}\\vec{v}=\\lambda\\vec{v}\\)</li> </ul> <p>Practice problems from the text book: P2.12-P2.14, 9.1-9.3.</p>"},{"location":"lectures/lecture-09/","title":"Lecture 9","text":""},{"location":"lectures/lecture-09/#lecture-9-complex-eigenvalues","title":"Lecture 9: Complex eigenvalues","text":"Run notes interactively?"},{"location":"lectures/lecture-09/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Complex eigenvalues</li> <li>Example</li> <li>Summary</li> </ol>"},{"location":"lectures/lecture-09/#1-complex-eigenvalues","title":"1. Complex eigenvalues","text":""},{"location":"lectures/lecture-09/#discrete-time","title":"Discrete time","text":"<p>We now know that the general solution of a system of linear recursion equations, \\(\\vec{x}(t+1) = \\mathbf{M}\\vec{x}(t) + \\vec{m}\\), can be written </p> \\[ \\vec{x}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}(\\vec{x}(0)-\\hat{\\vec{x}}) + \\hat{\\vec{x}}, \\] <p>where \\(\\hat{\\vec{x}}=-(\\mathbf{M} - \\mathbf{I})^{-1}\\vec{m}\\) is the equilibrium, \\(\\mathbf{A}\\) is a matrix with right eigenvectors as columns, and \\(\\mathbf{D}\\) is a diagonal matrix with eigenvalues along the diagonal.</p> <p>The dynamics over time therefore critically depend on \\(\\mathbf{D}^t\\),</p> \\[ \\mathbf{D}^t =   \\begin{pmatrix}  \\lambda_1^t &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; \\lambda_2^t &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\  0 &amp; 0 &amp; \\cdots &amp; \\lambda_n^t\\\\   \\end{pmatrix}, \\] <p>and therefore on the eigenvalues, \\(\\lambda\\), of \\(\\mathbf{M}\\). For example, for \\(\\vec{x}(t)\\) to converge to the equilibrium, \\(\\hat{\\vec{x}}\\), need all \\(|\\lambda|&lt;1\\).</p> <p>We also now know how to find the eigenvalues of \\(\\mathbf{M}\\), by solving \\(|\\mathbf{M}-\\lambda\\mathbf{I}|=0\\) for \\(\\lambda\\). For example, when \\(\\mathbf{M}\\) is a 2x2 matrix \\(|\\mathbf{M}-\\lambda\\mathbf{I}|\\) is a quadratic polynomial in \\(\\lambda\\) and the solutions can be written in terms of the trace and determinant of \\(\\mathbf{M}\\),</p> \\[ \\lambda = \\frac{\\mathrm{Tr}(\\mathbf{M}) \\pm \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4\\mathrm{Det}(\\mathbf{M})}}{2}. \\] <p>Now, notice that if \\(\\mathrm{Tr}(\\mathbf{M})^2 &lt; 4\\mathrm{Det}(\\mathbf{M})\\) we need to take the square of a negative number. This implies that the eigenvalues will be complex, i.e., that they will involve the imaginary number \\(i = \\sqrt{-1}\\). In particular, when \\(\\mathrm{Tr}(\\mathbf{M})^2 &lt; 4\\mathrm{Det}(\\mathbf{M})\\) we can write the square root as</p> \\[ \\begin{align} \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4\\mathrm{Det}(\\mathbf{M})} &amp;= \\sqrt{(4\\mathrm{Det}(\\mathbf{M})- \\mathrm{Tr}(\\mathbf{M})^2)(-1)}\\\\ &amp;= \\sqrt{4\\mathrm{Det}(\\mathbf{M})- \\mathrm{Tr}(\\mathbf{M})^2}\\sqrt{-1}\\\\ &amp;= \\sqrt{4\\mathrm{Det}(\\mathbf{M})- \\mathrm{Tr}(\\mathbf{M})^2}i, \\end{align} \\] <p>where \\(\\sqrt{4\\mathrm{Det}(\\mathbf{M})- \\mathrm{Tr}(\\mathbf{M})^2}\\) is a real number (i.e., not complex). Letting \\(\\mathrm{Tr}(\\mathbf{M})/2=A\\) and \\(\\sqrt{4\\mathrm{Det}(\\mathbf{M})- \\mathrm{Tr}(\\mathbf{M})^2}/2=B\\), the two eigenvalues can then be written as \\(\\lambda=A\\pm Bi\\). Two numbers that take this form are called complex conjugates.</p> <p>We can think of a complex number as a vector on the complex plane, a two-dimensional space with the real part, \\(A\\), on the x-axis and the imaginary part, \\(B\\), on the y-axis. The length of our vector is \\(R=\\sqrt{A^2+B^2}\\) and the angle of our vector (from the vector that points directly right along the x-axis) is \\(\\theta = \\arctan(B/A)\\). Using basic geometric rules we then have \\(A=R\\cos(\\theta)\\) and \\(B=R\\sin(\\theta)\\). We can therefore write our complex eigenvalue as \\(\\lambda = R(\\cos(\\theta) + \\sin(\\theta)i) = R\\exp(\\theta i)\\), where the last step we used a fascinating identity known as Euler's equation.</p> <pre>\nimport matplotlib.pyplot as plt\nimport math\nfrom matplotlib.patches import Arc\n\nA,B = 1,1 #real and imaginary parts\n\nfig, ax = plt.subplots()\n\nax.arrow(0,0,A,B, head_width=0.05, color='black', length_includes_head=True) #eigenvalue as vector in complex plane\n\ndx = 0.05\nax.plot([0-dx/2,A-dx/2],[0+dx,B+dx],marker='o',c='b')\nax.text(A/2,B/2+3*dx,r'$R = \\sqrt{A^2 + B^2}$',rotation=math.atan(B/A)*180/math.pi-8,c='b',fontsize=15,ha='center',va='center')\n\nax.plot([0,A],[0,0],marker='o',c='r')\nax.text(A/2,0+dx,r'$A=R \\cos(\\theta)$',c='r',fontsize=15,ha='center',va='center')\n\nax.plot([A,A],[0,B],marker='o',c='g')\nax.text(A+dx,B/2,r'$B=R \\sin(\\theta)$',c='g',fontsize=15,ha='center',va='center',rotation=90)\n\nax.set_xlabel('real part, $A$')\nax.set_ylabel('imaginary part, $B$')\nax.set_xlim(-dx,A+2*dx)\nax.set_ylim(-dx,B+2*dx)\n\ndx=A/4\nax.plot([0,dx],[0,0],c='orange')\nax.add_patch(Arc((0,0), width=2*dx, height=2*dx, theta1=0, theta2=math.atan(B/A)*180/math.pi, edgecolor='orange'))\nax.text(dx/2,dx/6,r'$\\theta$',fontsize=15,c='orange')\n\nplt.show()\n</pre> <p></p> <p>Now to see how a complex eigenvalue affects the dynamics of our system, consider what happens to that element of \\(\\mathbf{D}^t\\) as time proceeds, </p> \\[ \\begin{aligned} \\lambda^t &amp;= (A + Bi)^t\\\\ &amp;= (R(\\cos(\\theta) + \\sin(\\theta)i))^t\\\\ &amp;= (R\\exp(i \\theta))^t\\\\ &amp;= R^t\\exp(i \\theta t)\\\\ &amp;= R^t(\\cos(\\theta t) + \\sin(\\theta t)i). \\end{aligned} \\] <p>Two key implications emerge: </p> <ul> <li>\\(\\cos(\\theta t) + \\sin(\\theta t)i\\) implies oscillatory dynamics (cycles)</li> <li>\\(\\lambda^t\\) will shrink with time when \\(R&lt;1\\) and therefore stability in discrete time requires \\(A^2 + B^2&lt;1\\), i.e., the imaginary part, \\(B\\), influences stability</li> </ul>"},{"location":"lectures/lecture-09/#continuous-time","title":"Continuous time","text":"<p>In continuous time we need to consider \\(\\exp(\\lambda t)\\) rather than \\(\\lambda^t\\),</p> \\[ \\begin{aligned} \\exp(\\lambda t) &amp;= \\exp((A + Bi)t)\\\\ &amp;= \\exp(At + Bti)\\\\ &amp;= \\exp(At)\\exp(Bti)\\\\ &amp;= \\exp(At)(\\cos(Bt) + \\sin(Bt)i). \\end{aligned} \\] <p>Two key implications emerge: </p> <ul> <li>\\(\\cos(B t) + \\sin(B t)i\\) implies oscillatory dynamics (cycles)</li> <li>\\(\\exp(\\lambda t)\\) will shrink with time when \\(A&lt;0\\) and therefore stability in continuous time only depends on the sign of the real part, \\(A\\)</li> </ul> <p>In the 2x2 case there is a shortcut to determine stability in continuous time (a special case of the Routh-Hurwitz stability criteria), regardless of whether the eigenvalues are complex. It comes from the fact that the two eigenvalues (given by the quadratic equation above) sum to \\(\\mathrm{Tr}(\\mathbf{M})\\) and multiply to \\(\\mathrm{Det}(\\mathbf{M})\\) (give this a check if you want). So</p> <ul> <li>\\(\\mathrm{Tr}(\\mathbf{M})&lt;0\\) and \\(\\mathrm{Det}(\\mathbf{M})&gt;0\\) implies both eigenvalues are negative (stability)</li> <li>\\(\\mathrm{Tr}(\\mathbf{M})&gt;0\\) and \\(\\mathrm{Det}(\\mathbf{M})&gt;0\\) implies both eigenvalues are positive (unstable)</li> <li>\\(\\mathrm{Det}(\\mathbf{M})&lt;0\\) implies the eigenvalues have opposite sign (unstable)</li> </ul> <p>Combining this with the fact that we will get oscillations when \\(\\mathrm{Tr}(\\mathbf{M})^2 &lt; 4\\mathrm{Det}(\\mathbf{M})\\), we can summarize the dynamics in the following plot. If there are ocillations we call the equilibrium a focus, otherwise it is a node.</p> <pre>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nxs = np.linspace(-1,1,100)\nfig, ax = plt.subplots()\n\nax.plot(xs, [x**2/4 for x in xs])\n\nax.set_xlim(-1,1)\nax.set_ylim(-1/4,1/4)\n\nax.text(0,1/4,r'$\\mathrm{Det}(\\mathbf{M})$',ha='center',fontsize=14)\nax.text(1,0,r'$\\mathrm{Tr}(\\mathbf{M})$',va='center',fontsize=14)\nax.text(1,1/4,r'$\\mathrm{Det}(\\mathbf{M}) = \\mathrm{Tr}(\\mathbf{M})^2/4$',va='center',fontsize=12, color=plt.cm.tab10(0))\nax.text(1/2,-1/8,'unstable',ha='center',fontsize=12)\nax.text(-1/2,-1/8,'unstable',ha='center',fontsize=12)\nax.text(2/5,1/8,'unstable focus',ha='center',fontsize=12)\nax.text(-2/5,1/8,'stable focus',ha='center',fontsize=12)\nax.text(4/5,1/16,'unstable node',ha='center',fontsize=12)\nax.text(-4/5,1/16,'stable node',ha='center',fontsize=12)\n\n# set the x-spine (see below for more info on `set_position`)\nax.spines['left'].set_position('zero')\nax.set_xticks([])\n# turn off the right spine/ticks\nax.spines['right'].set_color('none')\nax.set_yticks([])\n# set the y-spine\nax.spines['bottom'].set_position('zero')\n# turn off the top spine/ticks\nax.spines['top'].set_color('none')\nax.xaxis.tick_bottom()\n\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-09/#2-example","title":"2. Example","text":"<p>To see more concretely how complex eigenvalues appear and affect the dynamics of our models, let's consider a model of sexual selection in continuous time. We will model the mean value of a male trait, \\(\\bar{z}\\), such as the length of a birds tail, and the mean value of female preference for that trait, \\(\\bar{p}\\) (if \\(\\bar{p}&gt;0\\) females tend to prefer larger male traits, if \\(\\bar{p}&lt;0\\) females tend to prefer smaller male traits). We assume the optimal male trait value in the absence of sexual selection is \\(\\theta\\), i.e., natural selection always pushes \\(\\bar{z}\\) towards \\(\\theta\\) (we'll take \\(\\theta=0\\), meaning \\(\\bar{z}\\) is measured relative to the optimum). We assume female choice is costly, i.e., natural selection always pushes \\(\\bar{p}\\) towards 0. Finally we will assume that male traits and female preference share some genetic basis, meaning that they will covary (e.g., there may be some alleles that increase the trait value when in males and increase the preference when in females, causing positive covariance). This covariance means that a change in the male trait will cause a change in female preference, and vice-versa.</p> <p>We can describe the dynamics of \\(\\bar{z}\\) and \\(\\bar{p}\\) with a system of linear differential equations,</p> \\[ \\begin{align} \\frac{\\mathrm{d}\\bar{z}}{\\mathrm{d}t} &amp;= G_z (a \\bar{p} - c \\bar{z}) - B b \\bar{p}\\\\ \\frac{\\mathrm{d}\\bar{p}}{\\mathrm{d}t} &amp;= B (a \\bar{p} - c \\bar{z}) - G_p b \\bar{p}, \\end{align} \\] <p>where \\(G_z\\) and \\(G_p\\) are the amounts of genetic variation in male traits and female preference (this is the \"fuel\" of evolution, so the rates of evolution are proportional to these variances), \\(B\\) is the genetic covariance between male traits and female preference, \\(a\\) is the strength of sexual selection, and \\(c\\) and \\(b\\) are the strengths of natural selection on male traits and female preference.</p> <p>This is a linear multivariate model whose dynamics are determined by the matrix </p> \\[ \\mathbf{M} = \\begin{pmatrix} -G_zc &amp; G_za - Bb \\\\ -Bc &amp; Ba - G_pb \\end{pmatrix}. \\] <p>The trace and determinant are</p> \\[ \\begin{aligned} \\mathrm{Tr}(\\mathbf{M})&amp;=-G_zc + Ba - G_pb\\\\ \\mathrm{Det}(\\mathbf{M})&amp;=-G_zc(Ba - G_pb)-(G_za - Bb)(-Bc)\\\\ &amp;=bc(G_pG_z-B^2). \\end{aligned} \\] <p>The Routh-Hurwitz criteria tell us that stability requires \\(\\mathrm{Tr}(\\mathbf{M})&lt;0\\) and \\(\\mathrm{Det}(\\mathbf{M})&gt;0\\). We are guaranteed the latter by the properties of variances and covariances. We have complex eigenvalues, and therefore cycling, whenever \\(\\mathrm{Tr}(\\mathbf{M})^2 &lt; 4\\mathrm{Det}(\\mathbf{M})\\).</p> <p>Below we plot the dynamics when the equilibrium is a stable focus, meaning the oscillations decay to the equilibrium over time.</p> <p>Biologically, this cycling occurs because initially the mean male trait is positive but there is no mean female preference. This implies that both natural and sexual selection favour smaller male traits, causing the mean to decline. But because of a correlated response, female preference also declines, favouring male traits less than 0. Eventually female preference becomes too costly and begins to increase back toward zero. This causes a correlated increase in the male trait, and so on. The cycling decays to the equilibrium when \\(\\mathrm{Tr}(\\mathbf{M})&lt;0\\). This condition, \\(G_zc + G_pb &gt; Ba\\), means that the strength of evolution by natural selection pushing the traits to 0 is greater than the strength of indirect evolution by sexual selection exaggerating female preference. When, on the other hand, we have \\(\\mathrm{Tr}(\\mathbf{M})&gt;0\\), the indirect response of female preference to sexual selection on males leads to continued exaggeration, causing what is called \"runaway\" selection.</p> <pre>\nGz, Gp, B, a, b, c, z0, p0, tmax = 0.15, 0.8, 0.32, 0.95, 0.3, 0.45, 1, 0, 1000 #parameter values\n\n# general solution\nfrom sympy import *\nM = Matrix([[-Gz*c, Gz*a-b*B],\n            [-B*c, -Gp*b+a*B]])\nA, D = M.diagonalize() #quick way to get matrix of right eigenvectors (A) and eigenvalues (D)\nn0 = Matrix([z0,p0]) #note this is made into a column vector automatically\nnt = A*exp(D*t)*A.inv()*n0 #general solution\n\n# plot\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.plot([re(nt.subs(t,i)[0]) for i in range(tmax)], label='male trait') #need to remove the imaginary part because of numerical error\nax.plot([re(nt.subs(t,i)[1]) for i in range(tmax)], label='female preference')\nax.legend()\nax.set_xlabel('time')\nax.set_ylabel('value')\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-09/#3-summary","title":"3. Summary","text":"<ul> <li>complex eigenvalues, \\(\\lambda = A + Bi\\), indicate oscillatory dynamics</li> <li>the imaginary part, \\(B\\), influences stability in discrete time but not continuous time</li> </ul> <p>Practice problems from the textbook: 7.1-11</p>"},{"location":"lectures/lecture-10/","title":"Lecture 10","text":""},{"location":"lectures/lecture-10/#lecture-10-demography","title":"Lecture 10: Demography","text":"Run notes interactively?"},{"location":"lectures/lecture-10/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Demography</li> <li>Stage-structure</li> <li>Age-structure</li> <li>Summary</li> </ol>"},{"location":"lectures/lecture-10/#1-demography","title":"1. Demography","text":"<p>We're now going to use what we've learned about linear multivariate models to describe the dynamics of a population that is composed of different types of individuals. This area of research is called demography. </p> <p>We'll just consider discrete-time linear models here and consider closed populations, so that the number of individuals of each type in the next timestep is</p> \\[ \\vec{n}(t+1) = \\mathbf{M}\\vec{n}(t), \\] <p>where \\(\\mathbf{M}\\) describes the transitions from one type to another.</p> <p>The general solution can be written in terms of the eigenvalues (\\(\\mathbf{D}\\)) and eigenvectors (\\(\\mathbf{A}\\)),</p> \\[ \\vec{n}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{n}(0). \\] <p>As the eigenvalues and eigenvectors are often unobtainable when there are many types of individuals (without specifying parameter values), we often rely on the long-term approximation,</p> \\[ \\vec{n}(t) \\approx \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0). \\] <p>Note</p> <p>The long-term approximation, \\(\\vec{n}(t) = \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0)\\), is valid as long as the leading eigenvalue, \\(\\lambda_1\\), is</p> <ul> <li>real (no cycles in long-term)</li> <li>positive (no oscillations to negative numbers!)</li> <li>larger than all other eigenvalues (so that we can ignore the other eigenvalues/vectors)</li> </ul> <p>Fortunately we are guaranteed all these conditions in our demographic models since all entries of \\(\\mathbf{M}\\) are non-negative and all entries of \\(\\mathbf{M}^t\\) are positive for some value of \\(t\\). (This follows from something called the Perron-Frobenius Theorem.)</p> <p>For this we just need to know the leading eigenvalue (\\(\\lambda_1\\)) and the corresponding right (\\(\\vec{v}_1\\)) and left (\\(\\vec{u}_1\\)) eigenvectors, respectively. (Remember that we have scaled the eigenvectors such that \\(\\vec{u}_1\\vec{v}_1=1\\).)</p> <p>These three components (\\(\\lambda_1\\), \\(\\vec{v}_1\\), \\(\\vec{u}_1\\)) are the key demographic quantities that we will investigate:</p> <ul> <li>\\(\\lambda_1\\) is the long-term population growth rate</li> <li>\\(\\vec{v}_1\\) describes the stable stage-distribution</li> <li>\\(\\vec{u}_1\\) describes the relative reproductive values of each stage</li> </ul> <p>The most general demographic model is called stage-structure: we consider some finite number of discrete stages that an individual can be in and we use an abitrary matrix of transitions between stages (a projection matrix), \\(\\mathbf{M}\\), to project how the population size and composition changes over time.</p> <p>A common special case is age-structure: here we define the stages as the number of time steps an individual has been alive for, which leads to a simpler projection matrix (called a Leslie matrix) because individuals either transition to the next stage or die.</p> <p></p>"},{"location":"lectures/lecture-10/#2-stage-structure","title":"2. Stage-structure","text":"<p>Consider an arbitrary stage-structured population. The long term approximation for its dynamics is \\(\\vec{n}(t) \\approx \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0).\\) </p> <p>Question</p> <p>If we wanted to increase the total population size in the future and we could add one individual to any stage, which stage should it be?</p> <p>We want to know what entry of \\(\\vec{n}(0)\\) to add one to to maximize \\(\\vec{n}(t) \\approx \\lambda^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0)\\). Adding individuals will not affect the long-term growth rate (\\(\\lambda_1\\)) or the stable-stage distribution (\\(\\vec{v}_1\\)). We can therefore only increase \\(\\vec{u}_1\\vec{n}(0) = u_1 n_1(0) + u_2 n_2(0) + ... + u_m n_m(0)\\). And so we add 1 to the stage with the largest reproductive value, \\(u_i\\).</p> <p>Question</p> <p>If we wanted to increase the long-term population growth rate and we could increase any parameter a little bit, which parameter should it be?</p> <p>We want to know how increasing a given parameter of the model, \\(z\\), changes the long-term growth rate, \\(\\mathrm{d}\\lambda_1/\\mathrm{d}z\\), which we call the sensitivity of \\(\\lambda_1\\) to \\(z\\). Unfortunately \\(\\lambda_1\\) is typically a relatively complicated expression, if attainable at all. Instead, we start with the definition of eigenvalues and eigenvectors to find</p> \\[ \\begin{aligned} \\mathbf{M} \\vec{v}_1 &amp;= \\lambda_1 \\vec{v}_1 \\\\ \\vec{u}_1 \\mathbf{M} \\vec{v}_1 &amp;= \\vec{u}_1 \\lambda_1 \\vec{v}_1 \\\\ \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left(\\vec{u}_1 \\mathbf{M} \\vec{v}_1 \\right)&amp;= \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left(\\vec{u}_1 \\lambda_1 \\vec{v}_1 \\right)\\\\ \\frac{\\mathrm{d}\\vec{u}_1}{\\mathrm{d}z} \\mathbf{M} \\vec{v}_1 + \\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1 + \\vec{u}_1 \\mathbf{M} \\frac{\\mathrm{d}\\vec{v}_1}{\\mathrm{d}z} &amp;= \\frac{\\mathrm{d}\\vec{u}_1}{\\mathrm{d}z} \\lambda_1 \\vec{v}_1 + \\vec{u}_1 \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\vec{v}_1 + \\vec{u}_1 \\lambda_1 \\frac{\\mathrm{d}\\vec{v}_1}{\\mathrm{d}z}\\\\ \\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1 &amp;= \\vec{u}_1 \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\vec{v}_1 \\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} &amp;= \\frac{\\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1}{\\vec{u}_1 \\vec{v}_1}. \\end{aligned}\\] <p>This helps because we only have to take the derivative of \\(\\mathbf{M}\\). Unfortunately the righthand side is still going to be too complicated to understand in general but we can quickly evaluate the eigenvectors and the derivative of \\(\\mathbf{M}\\) at some given parameter values.</p>"},{"location":"lectures/lecture-10/#example-north-atlantic-right-whale","title":"Example: North Atlantic right whale","text":"<p>Let's consider an example. North Atlantic right whales were hunted to near extinction in the 1800s and early 1900s, after which their population size is thought to have slowly recovered (there are less than 400 now, and unfortunately appear to be in decline again). Because of their long life span, over which survival and reproductive rates vary enormously, a stage-structured model is very appropriate. Below we draw a flow diagram representing all the transitions between calves, sexually immature individuals, sexually mature individuals, and actively reproducing individuals.</p> <pre><code>graph LR;\n    C((Calves)) --sIC nC--&gt; I((Immature));\n    I --sII nI--&gt; I;\n    I --sMI nI--&gt; M((Mature));\n    I --sRI nI--&gt; R((Reproductive));\n    M --sMM nM--&gt; M;\n    M --sRM nM--&gt; R;\n    R --sRR nR--&gt; R;\n    R --sMR nR--&gt; M;\n    R --b nR--&gt; C;</code></pre> <p>Converting this flow diagram into a system of recursion equations, \\(\\vec{n}(t+1) = \\mathbf{M}\\vec{n}(t)\\), the projection matrix is</p> \\[ \\mathbf{M} = \\begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; b \\\\ s_{IC} &amp; s_{II} &amp; 0 &amp; 0\\\\ 0 &amp; s_{MI} &amp; s_{MM} &amp; s_{MR} \\\\ 0 &amp; s_{RI} &amp; s_{RM} &amp; s_{RR} \\end{pmatrix}. \\] <p>If we now plug in some estimated parameter values from the literature (\\(b=0.3\\), \\(s_{IC}=0.92\\), \\(s_{II}=0.86\\), \\(s_{MI}=0.08\\), \\(s_{MM}=0.8\\), \\(s_{MR}=0.88\\), \\(s_{RI}=0.02\\), \\(s_{RM}=0.19\\), \\(s_{RR}=0\\)) we can numerically calculate the three key demographic quantities (see the code below),</p> \\[ \\begin{aligned} &amp;\\lambda_1 \\approx 1.003 \\\\ &amp;\\vec{v}_1 \\approx \\begin{pmatrix} 0.04 \\\\ 0.23 \\\\ 0.61 \\\\ 0.12\\end{pmatrix} \\\\ &amp;\\vec{u}_1 \\approx \\begin{pmatrix} 0.69 &amp; 0.76 &amp; 1.07 &amp; 1.15 \\end{pmatrix}. \\end{aligned} \\] <p>These quantities tells us, for example, that in the long-run the population is predicted to grow (\\(\\lambda_1&gt;1\\)), the majority of individuals are expected to be mature (the second last entry in \\(\\vec{v}_1\\) is the largest), and mature and reproductive individuals are expected to have much higher reproductive values than calves and immature individuals (the last two entries in \\(\\vec{u}_1\\) are much larger than the first two). </p> <p>Caveat</p> <p>The parameter values above were estimated before the current population decline -- changes in climate and human behaviour, eg boat traffic and fishing, have presumably altered the parameter values, making the predictions less accurate. For example, the increased incidence of entanglement in fishing nets has likely decreased survival rates to the point that the population is now expected to decline. See here for more info.</p> <p>Below we plot the general solution and long-term approximation. Note the quick convergence.</p> <pre>\nb,sic,sii,smi,smm,smr,sri,srm,srr = 0.3,0.92,0.86,0.08,0.8,0.88,0.02,0.19,0 #parameter values\n\nimport numpy as np\n\nM = np.array([[0,0,0,b], #projection matrix\n              [sic,sii,0,0],\n              [0,smi,smm,smr],\n              [0,sri,srm,srr]])\n\n# calculate\neigs = np.linalg.eig(M) #eigenvalues and right eigenvectors\nix = np.argmax(np.abs(eigs[0])) #which is leading eigenvalue\nl1 = eigs[0][ix] #leading eigenvalue\nv1 = eigs[1][:,ix] #leading right eigenvector\nv1 = v1/np.sum(v1) #normalized to sum to 1\nus = np.linalg.inv(eigs[1]) #left eigenvectors\nu1 = us[ix] #leading left eigenvalue\nu1 = u1/np.dot(u1,v1) #normalized so u1*v1=1\n\nn0 = np.array([50,50,50,50]) #initial state\n\ndef ntfull(t):\n    '''full projection'''\n    return np.dot(np.linalg.matrix_power(M,t), n0)\n\ndef ntapp(t):\n    '''long term approximation'''\n    return l1**t * v1 * np.dot(u1, n0)\n\n# plot\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nlabels = ['calves','immature','mature','reproductive']\ncolors = plt.get_cmap('tab10')\nfor i,j in enumerate(range(len(M))): #for each stage\n    ax.plot([ntfull(t)[j] for t in range(100)], color=colors(i), label=labels[i])\n    ax.plot([ntapp(t)[j] for t in range(100)], color=colors(i), linestyle='--') #long-term approx\n\nax.legend()\nax.set_xlabel('years')\nax.set_ylabel('number of whales')\nplt.show()\n</pre> <p></p> <p>Let's now return to our two questions. </p> <p>Question</p> <p>If we wanted to increase the total population size in the future and we could add one individual to any stage, which stage should it be?</p> <p>The reproductive values for these whales are \\(\\vec{u}_1 \\approx \\begin{pmatrix} 0.69 &amp; 0.76 &amp; 1.07 &amp; 1.15 \\end{pmatrix}\\). To maximize population size we should add an individual with the largest reproductive value: a reproductively active individual. This is perhaps not surprising since reproductively active individuals are the only individuals that produce offspring and if we introduced an individual in an earlier stage there is some chance that they would die before becoming reproductively active.</p> <p>Question</p> <p>If we wanted to increase the long-term population growth rate and we could increase any parameter a little bit, which parameter should it be?</p> <p>Below we differentiate \\(\\mathbf{M}\\) with respect to each parameter, \\(\\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z}\\), and then numerically calculate the sensitivities, \\(\\frac{\\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1}{\\vec{u}_1 \\vec{v}_1}\\), at the current parameter estimates. We see that increasing the fraction of mature individuals that survive to become reproductively active, \\(s_{RM}\\), has the largest effect. This makes good sense because increasing \\(s_{RM}\\) increases the rate at which the most populous stage (from \\(\\vec{v}_1\\)) transitions to the stage with the highest reproductive value (from \\(\\vec{u}_1\\)).</p> <pre>\nimport numpy as np\nfrom sympy import *\nimport matplotlib.pyplot as plt\n\n# differentiate M symbolically\nb,sic,sii,smi,smm,smr,sri,srm,srr = symbols('b,s_ic,s_ii,s_mi,s_mm,s_mr,s_ri,s_rm,s_rr')\nM = Matrix([[0,0,0,b], #projection matrix\n            [sic,sii,0,0],\n            [0,smi,smm,smr],\n            [0,sri,srm,srr]])\ndMdzs = [diff(M,z) for z in [b,sic,sii,smi,smm,smr,sri,srm,srr]] #derivatives of M with respect to parameters\n\n# numerically get eigenvectors\nparam_vals = {'b':0.3,'s_ic':0.92,'s_ii':0.86,'s_mi':0.08,'s_mm':0.8,'s_mr':0.88,'s_ri':0.02,'s_rm':0.19,'s_rr':0} #parameter values\nnumeric_M = np.array(M.subs(param_vals),dtype=float)\neigs = np.linalg.eig(numeric_M) #eigenvalues and right eigenvectors\nix = np.argmax(np.abs(eigs[0])) #which is leading eigenvalue\nv1 = eigs[1][:,ix] #leading right eigenvector\nv1 = v1/np.sum(v1) #normalized to sum to 1\nus = np.linalg.inv(eigs[1]) #left eigenvectors\nu1 = us[ix] #leading left eigenvalue\nu1 = u1/np.dot(u1,v1) #normalized so u1*v1=1\n\n# change in leading eigenvalue with increase in each parameter\ndl1dzs = [np.matmul(np.matmul(u1,dMdz),v1)/np.matmul(u1,v1) for dMdz in dMdzs] \n\n# plot\nplt.bar(['$b$','$S_{IC}$','$S_{II}$','$S_{MI}$','$S_{MM}$','$S_{MR}$','$S_{RI}$','$S_{RM}$','$S_{RR}$'],dl1dzs)\nplt.xlabel('parameter, $z$')\nplt.ylabel('sensitivity, $d\\lambda_1/dz$')\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-10/#3-age-structure","title":"3. Age-structure","text":"<p>Now let's look at the special case of age-structure. Here individuals in stage \\(i\\) at time \\(t\\) can only contribute to stage \\(i+1\\) (survival) and stage \\(1\\) (reproduction) at time \\(t+1\\).</p> <pre><code>graph LR;\n    1((1)) --p1 n1--&gt; 2((2));    \n    1 --m1 n1--&gt; 1;\n    2 --p2 n2--&gt; 3((3));\n    2 --m2 n2--&gt; 1;    \n    3 --p3 n3--&gt; 4((4));\n    3 --m3 n3--&gt; 1;\n    4 --m4 n4--&gt; 1;</code></pre> <p>Because of this, the projection matrix is simpler in the sense that it contains more zeros and has non-zero entries in very specific places,</p> \\[ \\mathbf{L} =  \\begin{pmatrix}   m_1 &amp; m_2 &amp; m_3 &amp; \\cdots &amp; m_d \\\\ p_1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; p_2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp;  &amp; \\vdots &amp;  &amp; \\vdots \\\\   0 &amp; \\cdots &amp; 0 &amp; p_{d-1} &amp; 0\\\\ \\end{pmatrix}. \\] <p>We call this a Leslie matrix and often denote it with \\(\\mathbf{L}\\). The first row contains the fecundities of each age group, \\(m_i\\), while the entries immediately below the diagonal give the fraction of individuals that survive each age group, \\(p_i\\).</p> <p>Because of the structure of the Leslie matrix, many expressions are now simpler. For example, the characteristic polynomial, \\(\\mathrm{Det}(\\mathbf{L}-\\mathbf{I}\\lambda)=0\\), which we use to get the eigenvalues, can be calculated using the first row. After rearranging we get what is known as the Euler-Lotka equation,</p> \\[ 1 = \\sum_{i=1}^{d} \\frac{l_i m_i}{\\lambda^i}, \\] <p>where \\(l_1 = 1\\), \\(l_i = p_1 p_2 \\cdots p_{i-1}\\) is the fraction of individuals that survive from birth to age \\(i\\) and \\(d\\) is the number of ages (ie, \\(\\mathbf{L}\\) is a \\(d\\times d\\) matrix). Given the \\(l_i\\) and \\(m_i\\) we can use this equation to find the long-term population growth rate, \\(\\lambda_1\\).</p>"},{"location":"lectures/lecture-10/#example-stickleback","title":"Example: stickleback","text":"<p>For example, let's look at a model of stickleback, a small fish. We assume stickleback do not live more than 4 years and estimate the Leslie matrix as</p> \\[ \\mathbf{L} =  \\begin{pmatrix} 2 &amp; 3 &amp; 4 &amp; 4\\\\ 0.6 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0.3 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0.1 &amp; 0 \\end{pmatrix}. \\] <p>The Euler-Lotka equation is then</p> \\[ \\begin{aligned} 1 &amp;= \\sum_{i=1}^{4} \\frac{l_i m_i}{\\lambda^i}\\\\ 1 &amp;= \\frac{l_1 m_1}{\\lambda} + \\frac{l_2 m_2}{\\lambda^2} + \\frac{l_3 m_3}{\\lambda^3} + \\frac{l_4 m_4}{\\lambda^4}\\\\ 1 &amp;= \\frac{2}{\\lambda} + \\frac{1.8}{\\lambda^2} + \\frac{0.72}{\\lambda^3} + \\frac{0.072}{\\lambda^4}.  \\end{aligned} \\] <p>This can be numerically solved to give our four eigenvalues: \\(\\lambda\\approx2.75, -0.3 \\pm 0.3i, -0.14\\). The long-term growth rate is the eigenvalue with the largest absolute value, \\(\\lambda_1=2.75\\).</p> <p>Below we plot the general solution and long-term approximation. As expected based on \\(\\lambda_1=2.75\\), the population grows very quickly (note the log scale on the y-axis).</p> <pre>\nm1,m2,m3,m4,p1,p2,p3 = 2,3,4,4,0.6,0.3,0.1 #parameter values\n\nimport numpy as np\n\nM = np.array([[m1,m2,m3,m4], #projection matrix\n              [p1,0,0,0],\n              [0,p2,0,0],\n              [0,0,p3,0]])\n\n# calculate\neigs = np.linalg.eig(M) #eigenvalues and right eigenvectors\nix = np.argmax(np.abs(eigs[0])) #which is leading eigenvalue\nl1 = eigs[0][ix] #leading eigenvalue\nv1 = eigs[1][:,ix] #leading right eigenvector\nv1 = v1/np.sum(v1) #normalized to sum to 1\nus = np.linalg.inv(eigs[1]) #left eigenvectors\nu1 = us[ix] #leading left eigenvalue\nu1 = u1/np.dot(u1,v1) #normalized so u1*v1=1\n\nn0 = np.array([25,25,25,25]) #initial state\n\ndef ntfull(t):\n    '''full projection'''\n    return np.dot(np.linalg.matrix_power(M,t), n0)\n\ndef ntapp(t):\n    '''long term approximation'''\n    return l1**t * v1 * np.dot(u1, n0)\n\n# plot\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nlabels = ['age 1','age 2','age 3','age 4']\ncolors = plt.get_cmap('tab10')\nfor i,j in enumerate(range(len(M))): #for each stage\n    ax.plot([ntfull(t)[j] for t in range(10)], color=colors(i), label=labels[i])\n    ax.plot([ntapp(t)[j] for t in range(10)], color=colors(i), linestyle='--') #long-term approx\n\nax.legend()\nax.set_xlabel('years')\nax.set_ylabel('number of stickleback')\nax.set_yscale('log')\nplt.show()\n</pre> <pre><code>/Users/mmosmond/.virtualenvs/eeb430/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:1298: ComplexWarning: Casting complex values to real discards the imaginary part\n  return np.asarray(x, float)\n</code></pre> <p></p> <p></p>"},{"location":"lectures/lecture-10/#4-summary","title":"4. Summary","text":"<ul> <li>linear multivariate models can describe the demography of a population composed of multiple types of individuals (stages or ages)</li> <li>in this case the terms in the long-term approximation are<ul> <li>\\(\\lambda_1\\):  long-term population growth rate</li> <li>\\(\\vec{v}_1\\): the stable stage-distribution</li> <li>\\(\\vec{u}_1\\): the relative reproductive values of each (st)age</li> </ul> </li> <li>adding individuals with larger reproductive value increases future population size more</li> <li>the effect of each parameter on the long-term growth rate (its sensitivity) can be calculated as \\(\\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} = \\frac{\\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1}{\\vec{u}_1 \\vec{v}_1}\\)</li> <li>with age-structure the projection matrix \\(\\mathbf{M}\\) takes a special form (Leslie matrix) allowing the eigenvalues to be found with the Euler-Lotka equation, \\(1 = \\sum_{i=1}^{d} \\frac{l_i m_i}{\\lambda^i}\\)</li> </ul>"},{"location":"lectures/lecture-11/","title":"Lecture 11","text":""},{"location":"lectures/lecture-11/#lecture-11-equilibria-nonlinear-multivariate","title":"Lecture 11: Equilibria (nonlinear multivariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-11/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Continuous time</li> <li>Discrete time</li> <li>Summary</li> </ol> <p>Now that we've covered linear multivariate models, we turn our attention to the more complex, and more common, nonlinear multivariate models.</p> <p>Let's start by learning how to find equilibria.</p> <p></p>"},{"location":"lectures/lecture-11/#1-continuous-time","title":"1. Continuous time","text":"<p>In general, if we have \\(n\\) variables, \\(x_1, x_2, ..., x_n\\), we can write any continuous time model like</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}x_1}{\\mathrm{d}t} &amp;=  f_1(x_1, x_2, ..., x_n)\\\\ \\frac{\\mathrm{d}x_2}{\\mathrm{d}t} &amp;=  f_2(x_1, x_2, ..., x_n)\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}x_n}{\\mathrm{d}t} &amp;=  f_n(x_1, x_2, ..., x_n).   \\end{aligned} \\] <p>If any of these variables interact with one another we do not have a linear system of equations. This prevents us from writing the system of equations in matrix form with a matrix composed only of parameters. And this generally prevents us from solving for equilibrium with linear algebra. To find the equilibria, \\(\\hat{x}_1,\\hat{x}_2, ..., \\hat{x}_n\\), we instead set all these equations to 0 and solve for one variable at a time. </p>"},{"location":"lectures/lecture-11/#example-epidemiology-the-spread-of-infectious-disease","title":"Example: epidemiology (the spread of infectious disease)","text":"<p>Consider a population composed of \\(S\\) susceptible individuals and \\(I\\) infected individuals. We assume new susceptible individuals arrive at rate \\(\\theta\\) via immigration and existing susceptibles die at per capita rate \\(d\\). We assume infected individuals die at an elevated per capita rate \\(d+v\\) and recover at per capita rate \\(\\gamma\\). So far this is a linear (affine) model. Finally, we assume susceptibles become infected at rate \\(\\beta S I\\). This is the non-linear part.</p> <p>We can describe this with the following flow diagram.</p> <pre><code>graph LR;\n    A[ ] --theta--&gt; S((S));\n    S --beta S I--&gt; I((I));\n    S --d S--&gt; B[ ];    \n    I --\"(d + v) I\"--&gt; C[ ];\n    I --gamma I--&gt; S;\n    style A height:0px;\n    style B height:0px;\n    style C height:0px;</code></pre> <p>The corresponding system of differential equations is</p> \\[\\begin{aligned} \\frac{\\mathrm{d}S}{\\mathrm{d}t} &amp;= \\theta - \\beta S I - d S + \\gamma I \\\\ \\frac{\\mathrm{d}I}{\\mathrm{d}t} &amp;= \\beta S I - (d + v) I - \\gamma I. \\end{aligned}\\] <p>At equilibrium both derivatives are equal to zero, </p> \\[\\begin{aligned} 0 &amp;= \\theta - \\beta \\hat{S} \\hat{I} - d \\hat{S} + \\gamma \\hat{I} \\\\ 0 &amp;= \\beta \\hat{S} \\hat{I} - (d + v) \\hat{I} - \\gamma \\hat{I} . \\end{aligned}\\] <p>To be systematic, we could start with the first equation and solve for the first variable, \\(\\hat{S}\\), in terms of the remaining variables, \\(\\hat{I}\\). We could then sub that expression for \\(\\hat{S}\\) into the second equation, which would then be an equation for \\(\\hat{I}\\) alone. After solving for \\(\\hat{I}\\) we could then sub that solution into \\(\\hat{S}\\) and be done. But through experience we notice that there is an easier approach. </p> <p>Because the second equation is proportional to \\(\\hat{I}\\) we immediately know \\(\\hat{I}=0\\) is one potential equilibrium point. For this to work we also need the first equation to be zero. Subbing in \\(\\hat{I}=0\\) to that first equation and solving for \\(\\hat{S}\\) gives \\(\\hat{S}=\\theta/d\\). One equilibrium is therefore</p> \\[\\begin{aligned} \\hat{S} &amp;= \\theta/d \\\\ \\hat{I} &amp;= 0, \\end{aligned}\\] <p>which we call the \"disease-free\" equilibrium. At this equilibrium there is no disease (\\(\\hat{I}=0\\)) and the number of susceptibles is determined by the balance between immigration and disease-independent death (\\(\\hat{S}=\\theta/d\\)). This is always biologically valid (given the parameters are positive, which we can assume given the description of the model above).</p> <p>Now, there may be more than one equilibrium because this is a non-linear model. Returning to the second equation, after factoring out \\(\\hat{I}\\) we are left with \\(0 = \\beta \\hat{S} - (d + v + \\gamma)\\), implying \\(\\hat{S} = (d + v + \\gamma)/\\beta\\). Plugging this into the first equation and solving for \\(\\hat{I}\\) we see that a second equilibrium is</p> \\[\\begin{aligned} \\hat{S} &amp;= (d + v + \\gamma)/\\beta \\\\ \\hat{I} &amp;= \\frac{\\theta - d(d + v + \\gamma)/\\beta}{d+v}, \\end{aligned}\\] <p>which we call the \"endemic equilibrium\". This equilibrium is only biologically valid when the numerator of \\(\\hat{I}\\) is non-negative which can be rearranged as \\(\\beta\\theta/d \\geq d + v + \\gamma\\). When this is an equality the endemic equilbrium reduces to the disease-free equilibrium. </p> <p>We can visualize these equilibria by plotting the nullclines on the phase plane. The equilibria are where the nullclines of the two variables intersect.</p> <p>The nullcline for \\(S\\) is</p> \\[\\begin{aligned} 0 &amp;= \\theta - \\beta S I - d S + \\gamma I \\\\ \\beta S I - \\gamma I &amp;= \\theta - d S \\\\ (\\beta S - \\gamma) I &amp;= \\theta - d S \\\\ I &amp;= \\frac{\\theta - d S}{(\\beta S - \\gamma)}, \\end{aligned}\\] <p>which is undefined at \\(S=\\gamma/\\beta\\) (in the plot below the nullcline takes on negative \\(S\\) values when \\(S\\) is less than this, so we'll plot only it for \\(S\\) values above this).</p> <p>The nullclines for \\(I\\) are </p> \\[\\begin{aligned} 0 &amp;= \\beta S I - (d + v) I - \\gamma I \\\\ \\text{implying} \\; I &amp;= 0 \\;\\text{and} \\\\ 0 &amp;= \\beta S - (d + v) - \\gamma \\\\ S &amp;= \\frac{d + v + \\gamma}{\\beta}.  \\end{aligned}\\] <pre>\nimport matplotlib.pyplot as plt\nfrom sympy import *\nimport numpy as np\n\n# define the variables\nS, I = symbols('S, I')\n\n# Choose the parameter values\nd, v = 0.1, 0.7\ngamma = 0.1\nbeta = 0.005\ntheta = 50\n\n# plot range\nSmin = gamma/beta + 1\nSmax = 550\nImax = 60\nSs = np.linspace(Smin,Smax,100)\nIs = np.linspace(0,Imax,100)\n\n# plot nullclines\nSnull = (theta-d*S)/(beta*S-gamma)\nInull0 = 0\nInull1 = (d+v+gamma)/beta\nplt.plot(Ss, [Snull.subs(S,i) for i in Ss], label='$S$ nullclines')\nplt.plot(Ss, [Inull0 for i in Ss], label='$I$ nullclines')\nplt.plot([Inull1 for i in Is], Is, color=plt.cm.tab10(1))\n\n# plot equilibria\neqS = [theta/d,(d+v+gamma)/beta]\neqI = [0,(theta - d*(d+v+gamma)/beta)/(d+v)]\nplt.scatter(eqS,eqI,c='k',zorder=5, label='equilibria')\n\nplt.xlabel('number of susceptibles, $S$')\nplt.ylabel('number of infecteds, $I$')\nplt.xlim(Smin,Smax)\nplt.ylim(-1,Imax)\nplt.legend()\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-11/#2-discrete-time","title":"2. Discrete time","text":"<p>In discrete time our system of equations is</p> \\[ \\begin{aligned} x_1(t+1) &amp;=  f_1(x_1(t), x_2(t), ..., x_n(t))\\\\ x_2(t+1) &amp;=  f_2(x_1(t), x_2(t), ..., x_n(t))\\\\ &amp;\\vdots\\\\ x_n(t+1) &amp;=  f_n(x_1(t), x_2(t), ..., x_n(t)). \\end{aligned} \\] <p>Now the equilibria are found by setting all \\(x_i(t+1) = x_i(t) = \\hat{x}_i\\) and solving for the \\(\\hat{x}_i\\) one at a time.</p>"},{"location":"lectures/lecture-11/#example-density-dependent-natural-selection","title":"Example: Density-dependent natural selection","text":"<p>We previously derived and analyzed an equation for allele frequency change under haploid selection, where the fitness of the two alleles, \\(A\\) and \\(a\\), where constants. Here we explore what happens when the fitness of each allele depends on the number of individuals. We refer to this scenario as density-dependent natural selection. </p> <p>Let \\(p\\) be the frequency of allele \\(A\\) and \\(N=N_A+N_a\\) be the total number of individuals in the population. Let the absolute fitness of allele \\(i\\) be \\(W_i(N)\\), a function of population size.  Then the allele frequency and population size in the next generation are</p> \\[\\begin{aligned} p(t+1) &amp;= \\frac{W_A(N(t))}{\\bar{W}(N(t),p(t))}p(t)\\\\ N(t+1) &amp;= W_A(N(t))N_A(t) + W_a(N(t))N_a(t) = \\bar{W}(N(t),p(t))N(t), \\end{aligned}\\] <p>where \\(\\bar{W}(N(t),p(t)) = W_A(N(t))p(t) + W_a(N(t))(1-p(t))\\) is the population mean fitness. </p> <p>To be concrete about density-dependence, let the fitness of each type decline exponentially from \\(1+r\\) with population size at rate \\(\\alpha_i\\), \\(W_i(N(t)) = (1+r)\\exp(-\\alpha_i N(t))\\).</p> <p>Let's start our search for equilibria with the allele frequency equation,</p> \\[\\begin{aligned} \\hat{p} &amp;= \\frac{W_A(\\hat{N})}{\\bar{W}(\\hat{N},\\hat{p})}\\hat{p}. \\end{aligned}\\] <p>We see that \\(\\hat{p}=0\\) satsifies this. Otherwise we can divide both sides by \\(\\hat{p}\\),</p> \\[\\begin{aligned} 1 &amp;= \\frac{W_A(\\hat{N})}{\\bar{W}(\\hat{N},\\hat{p})} \\\\ \\bar{W}(\\hat{N},\\hat{p}) &amp;= W_A(\\hat{N}) \\\\ W_A(\\hat{N})\\hat{p} + W_a(\\hat{N})(1-\\hat{p}) &amp;= W_A(\\hat{N}) \\\\ W_a(\\hat{N})(1-\\hat{p}) &amp;= W_A(\\hat{N})(1-\\hat{p}). \\end{aligned}\\] <p>We see that \\(\\hat{p}=1\\) satisfies this. Otherwise we can divide both sides by \\(1-\\hat{p}\\),</p> \\[\\begin{aligned} W_a(\\hat{N}) &amp;= W_A(\\hat{N}) \\\\ (1+r)\\exp(-\\alpha_a \\hat{N}) &amp;= (1+r)\\exp(-\\alpha_A \\hat{N}) \\\\ \\alpha_a \\hat{N} &amp;= \\alpha_A \\hat{N}. \\end{aligned}\\] <p>Unless the two alleles have equal fitness at any population size, \\(\\alpha_A=\\alpha_a\\), (which we'll ignore) this is only satisfied if the population is extinct, \\(\\hat{N}=0\\), in which case we don't care about the allele frequency.</p> <p>OK, so if the population is not extinct we have either \\(\\hat{p}=0\\) or \\(\\hat{p}=1\\), aligning with out density-independent analysis earlier in the course. But now what do these equilibrium allele frequencies mean for equilibrium population size? </p> <p>When \\(\\hat{p}=0\\) we have  </p> \\[\\begin{aligned} \\hat{N} &amp;= W_a(\\hat{N}) \\hat{N} \\\\ 1 &amp;= W_a(\\hat{N}) \\\\ 1 &amp;= (1+r)\\exp(-\\alpha_a \\hat{N}) \\\\ \\exp(\\alpha_a \\hat{N}) &amp;= (1+r)\\\\ \\alpha_a \\hat{N} &amp;= \\ln(1+r)\\\\ \\hat{N} &amp;= \\ln(1+r)/\\alpha_a. \\end{aligned}\\] <p>Similarly, when \\(\\hat{p}=1\\) we have \\(\\hat{N} = \\ln(1+r)/\\alpha_A\\).</p> <p>To summarize, there are three equilibria in this model of density-dependent haploid selection:</p> <ul> <li>extinction, \\(\\hat{N}=0\\) (and any allele frequency, which is irrelevant)</li> <li>fixation of \\(a\\), \\(\\hat{p}=0\\), and the associated equilibrium population size, \\(\\hat{N}=\\ln(1+r)/\\alpha_a\\)</li> <li>fixation of \\(A\\), \\(\\hat{p}=1\\), and the associated equilibrium population size, \\(\\hat{N}=\\ln(1+r)/\\alpha_A\\) </li> </ul> <p></p>"},{"location":"lectures/lecture-11/#3-summary","title":"3. Summary","text":"<p>We find equilibria of nonlinear multivariate models by either 1) setting the differential equations to zero or 2) the variables in the next time step equal to the variables in the current time step and then solving for the variables one by one.</p> <p>Two important things to note about nonlinear models (multivariate or not):</p> <ul> <li>there can be more than one equilibrium</li> <li>we can't always solve for all (or any) equilibria</li> </ul> <p>Practice questions from the textbook: 8.2, 8.3, 8.4a-b, 8.5a-b, 8.6a, 8.7a, 8.8a, 8.10a, 8.11a, 8.12a </p>"},{"location":"lectures/lecture-12/","title":"Lecture 12","text":""},{"location":"lectures/lecture-12/#lecture-12-stability-nonlinear-multivariate","title":"Lecture 12: Stability (nonlinear multivariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-12/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Continuous time</li> <li>Discrete time</li> <li>Summary</li> </ol> <p>Now that we know how to find equilibria in nonlinear multivariate models, let's see how to determine if they're stable.</p> <p></p>"},{"location":"lectures/lecture-12/#1-continuous-time","title":"1. Continuous time","text":"<p>In general, if we have \\(n\\) interacting variables, \\(x_1, x_2, ..., x_n\\), we can write any continuous time model like</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}x_1}{\\mathrm{d}t} &amp;=  f_1(x_1, x_2, ..., x_n)\\\\ \\frac{\\mathrm{d}x_2}{\\mathrm{d}t} &amp;=  f_2(x_1, x_2, ..., x_n)\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}x_n}{\\mathrm{d}t} &amp;=  f_n(x_1, x_2, ..., x_n). \\end{aligned} \\] <p>If we then want to find the equilibria, \\(\\hat{x}_1,\\hat{x}_2, ..., \\hat{x}_n\\), we set all these equations to 0 and solve for one variable at a time. Let's assume we've already done this and now want to know when an equilibrium is stable.</p> <p>In the linear multivariate case we determined stability with the eigenvalues of a matrix \\(\\textbf{M}\\) of parameters. Unfortunately we can not generally write a system of nonlinear equations in matrix form with a matrix composed only of parameters. In order to use what we've learned about eigenvalues and eigenvectors we're first going to have to linearize the system so that the corresponding matrices do not contain variables.</p> <p>As we saw in nonlinear univariate models, one useful way to linearize a system is to measure the system relative to equilibrium, \\(\\epsilon = x - \\hat{x}\\). Then assuming that the deviation from equilibrium, \\(\\epsilon\\), is small, we used a Taylor series expansion to approximate the nonlinear system with a linear system. To do that with multivariate models we'll need to take a Taylor series expansion of a multivariate function.</p> Taylor series expansion of a multivariate function <p>Taking the series of \\(f\\) around \\(x_1=a_1\\), \\(x_2=a_2\\), ..., \\(x_n=a_n\\) gives</p> \\[ \\begin{aligned} f(x_1, x_2, ..., x_n) &amp;= f(a_1, a_2, ..., a_n)\\\\  &amp;+ \\sum_{i=1}^{n} \\left( \\frac{\\partial f}{\\partial x_i} \\bigg|_{x_1=a_1, x_2=a_2, ..., x_n=a_n} \\right) (x_i - a_i)\\\\ &amp;+ \\sum_{i=1}^{n}\\sum_{j=1}^n \\left( \\frac{\\partial f}{\\partial x_i \\partial x_j} \\bigg|_{x_1=a_1, x_2=a_2, ..., x_n=a_n} \\right) (x_i - a_i)(x_j - a_j)\\\\ &amp;+ \\cdots \\end{aligned} \\] <p>where \\(\\frac{\\partial f}{\\partial x_i}\\) is the \"partial derivative\" of \\(f\\) with respect to \\(x_i\\), meaning that we treat all the other variables as constants when taking the derivative. Considering only the first two terms gives a linear approximation.</p> <p>So let \\(\\epsilon_i = x_i - \\hat{x}_i\\) be the deviation of variable \\(x_i\\) from its equilibrium value, \\(\\hat{x}_i\\), and write a system of equations describing the change in the deviations for all of our variables,</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &amp;=  f_1(x_1, x_2, ..., x_n)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &amp;=  f_2(x_1, x_2, ..., x_n)\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &amp;=  f_n(x_1, x_2, ..., x_n).   \\end{aligned} \\] <p>We can take a Taylor series around \\(x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n\\) to get a linear approximation of our system near the equilibrium,</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &amp;\\approx  f_1(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &amp;\\approx  f_2(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &amp;\\approx  f_n(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i). \\end{aligned} \\] <p>And then note that all \\(f_i(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n)=0\\) by definition of a equilibrium, leaving</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &amp;\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &amp;\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &amp;\\approx  \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i). \\end{aligned} \\] <p>Each of the partial derivatives \\(\\frac{\\partial f_i}{\\partial x_j}\\) is evaluated at the equilibrium, so these are constants. And \\(x_i - \\hat{x}_i = \\epsilon_i\\). So we now have a linear system,</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &amp;\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &amp;\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &amp;\\approx  \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i, \\end{aligned} \\] <p>which we can write in matrix form</p> \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} \\\\ \\vdots \\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} &amp; \\frac{\\partial f_n}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}. \\end{aligned} \\] <p>The matrix of first derivatives is called the Jacobian,</p> \\[ \\mathbf{J} =  \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} &amp; \\frac{\\partial f_n}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix}. \\] <p>If any of the equations, \\(f_i\\), are nonlinear then \\(\\textbf{J}\\) depends on the variables. To determine local stability we find the eigenvalues of the Jacobian evaluated at an equilibrium.</p>"},{"location":"lectures/lecture-12/#example-epidemiology","title":"Example: epidemiology","text":"<p>Let's return to our model of disease spread from the previous lecture. The rate of change in the number of susceptibles and infecteds is</p> \\[\\begin{aligned} \\frac{\\mathrm{d}S}{\\mathrm{d}t} &amp;= \\theta - \\beta S I - d S + \\gamma I \\\\ \\frac{\\mathrm{d}I}{\\mathrm{d}t} &amp;= \\beta S I - (d + v) I - \\gamma I.  \\end{aligned}\\] <p>We found two equilibria, the diease-free equilibrium,</p> \\[\\begin{aligned} \\hat{S} &amp;= \\theta/d \\\\ \\hat{I} &amp;= 0, \\end{aligned}\\] <p>which is always valid, and the endemic equilirbium,</p> \\[\\begin{aligned} \\hat{S} &amp;= (d + v + \\gamma)/\\beta \\\\ \\hat{I} &amp;= \\frac{\\theta - d(d + v + \\gamma)/\\beta}{d+v}, \\end{aligned}\\] <p>which is valid when \\(\\beta\\theta/d - (d + v + \\gamma)&gt;0\\).</p> <p>Now when are these stable? </p> <p>We first calculate the Jacobian,</p> \\[\\begin{aligned} \\mathbf{J}  &amp;=  \\begin{pmatrix} \\frac{\\partial}{\\partial S}\\left(\\frac{\\mathrm{d}S}{\\mathrm{d}t}\\right) &amp; \\frac{\\partial}{\\partial I}\\left(\\frac{\\mathrm{d}S}{\\mathrm{d}t}\\right) \\\\ \\frac{\\partial}{\\partial S}\\left(\\frac{\\mathrm{d}I}{\\mathrm{d}t}\\right) &amp; \\frac{\\partial}{\\partial I}\\left(\\frac{\\mathrm{d}I}{\\mathrm{d}t}\\right) \\end{pmatrix}\\\\ &amp;= \\begin{pmatrix} -d-\\beta I &amp; -\\beta S+\\gamma \\\\ \\beta I &amp; \\beta S-(d+v+\\gamma) \\end{pmatrix}. \\end{aligned}\\] <p>We can now determine the local stability of an equilibrium by evaluating the Jacobian at that equilibrium and calculating the eigenvalues. </p> <p>Let's do that first for the simpler disease-free equilibrium. Plugging \\(\\hat{S}\\) and \\(\\hat{I}\\) into the Jacobian gives </p> \\[\\begin{aligned} \\mathbf{J}_\\mathrm{disease-free}  &amp;=  \\begin{pmatrix} -d &amp; -\\beta \\theta/d+\\gamma \\\\ 0 &amp; \\beta \\theta/d-(d+v+\\gamma) \\end{pmatrix}. \\end{aligned}\\] <p>This is an upper triangular matrix, so the eigenvalues are just the diagonal elements, \\(\\lambda = -d, \\beta\\theta/d-(d+v+\\gamma)\\). Because all the parameters are rates they are all non-negative, and therefore the only eigenvalue that can have a positive real part (and therefore cause instability) is \\(\\lambda=\\beta\\theta/d-(d+v+\\gamma)\\). The equilibrium is unstable when this is positive, \\(\\beta\\theta/d-(d+v+\\gamma)&gt;0\\). Because this equilibrium has no infected individuals, instability in this case means the infected individuals will increase in number from rare -- ie, the disease can invade. </p> <p>We can rearrange the instability condition to get a little more intuition. The disease can invade whenever</p> \\[\\begin{aligned} \\beta\\theta/d - (d+v+\\gamma)&amp; &gt; 0 \\\\ \\beta\\theta/d &amp;&gt; d+v+\\gamma \\\\ \\frac{\\beta\\theta/d}{d+v+\\gamma} &amp;&gt; 1. \\end{aligned}\\] <p>The numerator is \\(\\beta\\) times the number of susceptibles at the disease-free equilibrium, \\(\\hat{S}=\\theta/d\\). This is the rate that a rare disease infects new individuals. The denominator is the rate at which the disease is removed from the population. Therefore a rare disease that infects faster than it is removed can spread. This ratio, in our case \\(\\frac{\\beta\\theta/d}{d+v+\\gamma}\\), turns out to be the expected number of new infections per infection when the disease is rare. This is termed \\(R_0\\), which is a very key epidemiological quantity (you may remember estimates of \\(R_0\\) in the news from a certain recent virus...).</p> <p>Now for the endemic equilibrium. Plugging these values into the Jacobian and simplifying gives</p> \\[\\begin{aligned} \\mathbf{J}_\\mathrm{endemic}  &amp;=  \\begin{pmatrix} -\\frac{\\beta \\theta - d \\gamma}{d+v} &amp; -(d+v) \\\\ \\frac{\\beta \\theta - d (d+v+\\gamma)}{d+v} &amp; 0 \\end{pmatrix} \\end{aligned}\\] <p>Here, instead of calculating the eigenvalues explicitly, we can use the Routh-Hurwitz stability criteria for a 2x2 matrix, a negative trace and positive determinant. The determinant is \\(\\beta \\theta - d (d+v+\\gamma)\\). For this to be positive we need \\(\\beta \\theta/d &gt; (d+v+\\gamma)\\), which was the instability condition on the disease-free equilibrium (\\(R_0&gt;1\\)). The trace is \\(-\\frac{\\beta \\theta - d \\gamma}{d+v}\\). For this to be negative we need \\(\\beta \\theta/d &gt; \\gamma\\), which is guaranteed if the determinant is positive. So in conclusion, the endemic equilibrium is valid and stable whenever the disease can invade, \\(R_0&gt;1\\).</p> <p></p>"},{"location":"lectures/lecture-12/#2-discrete-time","title":"2. Discrete time","text":"<p>Note</p> <p>The short version of this section is that we can do the same thing in discrete time -- local stability is determined by the eigenvalues of the Jacobian, where the functions in that Jacobian are now our recursions, \\(x_i(t+1) = f_i(x_1(t), x_2(t), ..., x_n(t))\\).</p> <p>We can do something very similar for nonlinear multivariate models in discrete time</p> \\[ \\begin{aligned} x_1(t+1) &amp;=  f_1(x_1(t), x_2(t), ..., x_n(t))\\\\ x_2(t+1) &amp;=  f_2(x_1(t), x_2(t), ..., x_n(t))\\\\ &amp;\\vdots\\\\ x_n(t+1) &amp;=  f_n(x_1(t), x_2(t), ..., x_n(t)). \\end{aligned} \\] <p>Now the equilibria are found by setting all \\(x_i(t+1) = x_i(t) = \\hat{x}_i\\) and solving for the \\(\\hat{x}_i\\) one at a time. Let's assume we've done this.</p> <p>To linearize the system around an equilibrium we again measure the system in terms of deviation from the equilibrium, \\(\\epsilon_i(t) = x_i(t) - \\hat{x}_i\\), giving</p> \\[ \\begin{aligned} \\epsilon_1(t+1) &amp;=  f_1(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ \\epsilon_2(t+1) &amp;=  f_2(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ &amp;\\vdots\\\\ \\epsilon_n(t+1) &amp;=  f_n(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1. \\end{aligned} \\] <p>Then taking the Taylor series of each \\(f_i\\) around \\(x_1(t) = \\hat{x}_1, ..., x_n(t) = \\hat{x}_n\\) we can approximate our system near the equilibrium as</p> \\[ \\begin{aligned}   \\epsilon_1(t+1) &amp;=  f_1(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n)  + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_1\\\\   \\epsilon_2(t+1) &amp;=  f_2(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_2\\\\   &amp;\\vdots\\\\   \\epsilon_n(t+1) &amp;=  f_n(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_n.   \\end{aligned} \\] <p>Noting that \\(f_i(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) = \\hat{x}_i\\) and \\(x_i(t) - \\hat{x}_i = \\epsilon_i(t)\\) we have a linear system,</p> \\[ \\begin{aligned}   \\epsilon_1(t+1) &amp;= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\   \\epsilon_2(t+1) &amp;= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\   &amp;\\vdots\\\\   \\epsilon_n(t+1) &amp;= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t),   \\end{aligned} \\] <p>which can be written in matrix form</p> \\[ \\begin{aligned} \\begin{pmatrix} \\epsilon_1(t+1) \\\\ \\epsilon_2(t+1) \\\\ \\vdots \\\\ \\epsilon_n(t+1) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} &amp; \\frac{\\partial f_n}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\begin{pmatrix} \\epsilon_1(t) \\\\ \\epsilon_2(t) \\\\ \\vdots \\\\ \\epsilon_n(t) \\end{pmatrix}. \\end{aligned} \\] <p>As in continuous time, the dynamics near an equilibrium are described by the Jacobian matrix,</p> \\[ \\mathbf{J} =  \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} &amp; \\frac{\\partial f_n}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n}. \\end{pmatrix} \\] <p>We assess the local stability of an equilibrium by evaluating the Jacobian at that equilibrium and finding the eigenvalues.</p> <p>to-do</p> <p>Give density-dependent natural selection example. For now see pages 320-322 in the textbook.</p> <p></p>"},{"location":"lectures/lecture-12/#3-summary","title":"3. Summary","text":"<p>We can determine the stability of nonlinear multivariate models with the eigenvalues of the Jacobian evaluated at an equilibrium. The recipe is</p> <ul> <li>Find the equilibrium of interest, \\(\\hat{x}_1, \\hat{x}_2, ... \\hat{x}_n\\)</li> <li>Calculate the Jacobian, \\(\\mathbf{J}\\)</li> <li>Evaluate the Jacobian at the equilibrium of interest, \\(\\mathbf{J}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n}\\)</li> <li>Calculate the characteristic polynomial \\(|\\mathbf{J}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} - \\lambda\\mathbf{I}|\\) </li> <li>Set the characteristic polynomial equal to 0 and solve for the \\(n\\) eigenvalues, \\(\\lambda\\)</li> </ul> <p>Stability reminder</p> <p>Continuous time</p> <ul> <li>if all eigenvalues have a negative real part the equilibrium is stable</li> <li>if any eigenvalue has a non-zero imaginary part there will be cycling</li> </ul> <p>Discrete time</p> <ul> <li>if all eigenvalues have an absolute value less than one the equilibrium is stable</li> <li>if any eigenvalue has a non-zero imaginary part there will be cycling</li> </ul> <p>Practice questions from the textbook: 8.1, 8.4, 8.5, 8.6, 8.7, 8.8, 8.12.</p>"},{"location":"lectures/lecture-13/","title":"Lecture 13","text":""},{"location":"lectures/lecture-13/#lecture-13-general-solutions-linear-multivariate","title":"Lecture 13: General solutions (linear multivariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-13/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Discrete time</li> <li>Continuous time</li> </ol>"},{"location":"lectures/lecture-13/#1-discrete-time","title":"1. Discrete time","text":""},{"location":"lectures/lecture-13/#motivating-example","title":"Motivating example","text":"<p>Let's return to our model of the number of birds on two islands from Lecture 10, which was the motivation for learning linear algebra in the first place. Let's adjust the model slightly, removing foreign immigration and switching to discrete time (assuming migration, then birth, then death). </p> <pre><code>graph LR;\n    A1((n1)) --b1 n1--&gt; A1;\n    A1 --d1 n1--&gt; C1[ ];\n\n    A2((n2)) --b2 n2--&gt; A2;\n    A2 --d2 n2--&gt; C2[ ];\n\n    A1 --m12 n1--&gt; A2;\n    A2 --m21 n2--&gt; A1;\n\n    style C1 height:0px;\n    style C2 height:0px;</code></pre> <p>The number of birds on the two islands in the next time step are then</p> \\[ \\begin{aligned} n_1(t+1) &amp;= (n_1(t)(1-m_{12}) + n_2(t)m_{21})(1+b_1)(1-d_1) \\\\ n_2(t+1) &amp;= (n_1(t)m_{12} + n_2(t)(1-m_{21}))(1+b_2)(1-d_2) \\\\ \\end{aligned} \\] <p>As we noted in Lecture 10, we can write a system of linear equations (in this case recursion equations) in matrix form</p> \\[ \\begin{aligned} \\vec{n}(t+1) &amp;= \\mathbf{M}\\vec{n}(t) \\end{aligned} \\] <p>where in this example</p> \\[ \\mathbf{M} =  \\begin{pmatrix}  (1-m_{12})(1+b_1)(1-d_1) &amp; m_{21}(1+b_1)(1-d_1) \\\\  m_{12}(1+b_2)(1-d_2) &amp; (1-m_{21})(1+b_2)(1-d_2) \\end{pmatrix} \\] <p>The question we now want to answer is, how do the numbers of birds on the two islands change over time?</p>"},{"location":"lectures/lecture-13/#general-formulation","title":"General formulation","text":"<p>Instead of analyzing this specific model, let's investigate the dynamics of any system of linear equations in discrete time. We will then return to our motivating example.</p> <p>If there are \\(n\\) variables to keep track of, \\(x_1\\), \\(x_2\\), ..., \\(x_n\\), then there will be \\(n\\) recursion equations</p> \\[ \\begin{aligned} x_1(t+1) &amp;= m_{11} x_1(t) + m_{12} x_2(t) + \\cdots + m_{1n} x_n(t) \\\\ x_2(t+1) &amp;= m_{21} x_1(t) + m_{22} x_2(t) + \\cdots + m_{2n} x_n(t) \\\\    &amp;\\vdots \\\\ x_n(t+1) &amp;= m_{n1} x_1(t) + m_{n2} x_2(t) + \\cdots + m_{nn} x_n(t) \\end{aligned} \\] <p>e.g., in the motivating example above we have \\(n=2\\).</p> <p>These equations can be written in matrix form</p> \\[ \\begin{aligned} \\begin{pmatrix} x_1(t+1) \\\\ x_2(t+1) \\\\ \\vdots \\\\ x_n(t+1) \\end{pmatrix} &amp;=  \\begin{pmatrix}          m_{11} &amp; m_{12} &amp; \\cdots &amp; m_{1n} \\\\           m_{21} &amp; m_{22} &amp; \\cdots &amp; m_{2n}\\\\          \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\          m_{n1} &amp; m_{n2} &amp; \\cdots &amp; m_{nn}  \\end{pmatrix} \\begin{pmatrix}           x_1(t) \\\\ x_2(t) \\\\ \\vdots \\\\ x_n(t)  \\end{pmatrix}\\\\ \\vec{x}(t+1) &amp;= \\mathbf{M} \\vec{x}(t) \\end{aligned} \\] <p>How does \\(\\vec{x}\\) change over time?</p>"},{"location":"lectures/lecture-13/#general-solution","title":"General solution","text":"<p>Since this is just a multivariate version of exponential growth, we can derive the general solution by brute force iteration</p> \\[ \\begin{aligned}  \\vec{x}(t) &amp;= \\mathbf{M}\\vec{x}(t-1)\\\\  &amp;= \\mathbf{M}^2\\vec{x}(t-2)\\\\ &amp; \\vdots \\\\ &amp;= \\mathbf{M}^t\\vec{x}(0)  \\end{aligned} \\] <p>However, in most cases it will be hard to compute \\(\\mathbf{M}^t\\) (if you don't believe me, try calculating even just \\(\\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}^3\\)). Fortunately there is a trick involving eigenvalues and eigenvectors.</p> <p>Recall the equation for the eigenvalues, \\(\\lambda\\), and right eigenvectors, \\(\\vec{v}\\),</p> \\[ \\mathbf{M} \\vec{v} = \\lambda \\vec{v} \\] <p>For our \\(n\\)-dimensional model, there will often be \\(n\\) eigenvalues and eigenvectors that satisfy this equation (sometimes the \\(n\\) eigenvalues are not all distinct).</p> <p>We can actually write all \\(n\\) of these equations in matrix form</p> \\[ \\begin{aligned} \\mathbf{M} \\begin{pmatrix} \\vec{v}_1 &amp; \\vec{v}_2 &amp; \\cdots &amp; \\vec{v}_n \\end{pmatrix}  &amp;= \\begin{pmatrix} \\lambda_1 \\vec{v}_1 &amp; \\lambda_2 \\vec{v}_2 &amp; \\cdots &amp; \\lambda_n \\vec{v}_n \\end{pmatrix}\\\\ \\mathbf{M} \\mathbf{A} &amp;= \\mathbf{A} \\mathbf{D} \\end{aligned} \\] <p>where the columns of \\(\\mathbf{A}\\) are the right eigenvectors and \\(\\mathbf{D}\\) is a diagonal matrix of the eigenvalues</p> \\[ \\mathbf{D} =  \\begin{pmatrix}  \\lambda_1 &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; \\lambda_2 &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\  0 &amp; 0 &amp; \\cdots &amp; \\lambda_n\\\\   \\end{pmatrix} \\] <p>Now here is the trick: multiply both sides of \\(\\mathbf{M} \\mathbf{A} = \\mathbf{A} \\mathbf{D}\\) by \\(\\mathbf{A}^{-1}\\) on the right</p> \\[ \\begin{aligned} \\mathbf{M} \\mathbf{A} &amp;= \\mathbf{A} \\mathbf{D}\\\\ \\mathbf{M} \\mathbf{A}\\mathbf{A}^{-1} &amp;= \\mathbf{A} \\mathbf{D}\\mathbf{A}^{-1}\\\\   \\mathbf{M} &amp;= \\mathbf{A} \\mathbf{D} \\mathbf{A}^{-1} \\end{aligned} \\] <p>Note</p> <p>We can only take the inverse of our \\(n\\times n\\) matrix of right eigenvectors, \\(\\mathbf{A}\\), when \\(\\mathbf{A}\\) is invertible, which requires the determinant be non-zero or, equivalently, the rows (or columns) of \\(\\mathbf{A}\\) to be linearly independent. Fortunately, in many cases the columns (the right eigenvectors) will be linearly independent. However, sometimes the right eigenvectors will not be linearly independent, which only occurs when there are less than \\(n\\) distinct eigenvalues. When the right eigenvectors of \\(\\mathbf{M}\\) are not linearly independent we call \\(\\mathbf{M}\\) defective. In that case we need to derive the general solution in another way, but we won't deal with that in this class.</p> <p>Subbing this alternate version of \\(\\mathbf{M}\\) into our general solution above, we see that most of the \\(\\mathbf{A}\\) and \\(\\mathbf{A}^{-1}\\) matrices cancel, leaving us with</p> \\[ \\begin{aligned}  \\vec{x}(t) &amp;= \\mathbf{M}^t\\vec{x}(0)\\\\ &amp;= (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1})^t\\vec{x}(0)\\\\ &amp;= (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) \\cdots (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) \\vec{x}(0)\\\\ &amp;= \\mathbf{A}\\mathbf{D}(\\mathbf{A}^{-1} \\mathbf{A})\\mathbf{D}(\\mathbf{A}^{-1}\\mathbf{A}) \\cdots (\\mathbf{A}^{-1}\\mathbf{A})\\mathbf{D}\\mathbf{A}^{-1} \\vec{x}(0)\\\\  &amp;= \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1} \\vec{x}(0)  \\end{aligned} \\] <p>And this is great because \\(\\mathbf{D}\\) is a diagonal matrix, meaning \\(\\mathbf{D}^t\\) is easy to calculate</p> \\[ \\mathbf{D}^t =   \\begin{pmatrix}     \\lambda_1^t &amp; 0 &amp; \\cdots &amp; 0 \\\\      0 &amp; \\lambda_2^t &amp; \\cdots &amp; 0\\\\     \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\     0 &amp; 0 &amp; \\cdots &amp; \\lambda_n^t \\end{pmatrix} \\] <p>It would not have been so easy to find \\(\\mathbf{M}^t\\)!</p> <p>Note that \\(\\mathbf{A}^{-1}\\) is a matrix whose rows are the left eigenvectors of \\(\\textbf{M}\\),  which can be seen by deriving \\(\\mathbf{M} = \\mathbf{A} \\mathbf{D}\\mathbf{A}^{-1}\\) starting  with the equation for the left eigenvectors \\(\\vec{u}\\mathbf{M}=\\lambda\\vec{u}\\).</p> <p>Note</p> <p>Another way to arrive at this general solition, \\(\\vec{x}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{x}(0)\\), is to consider a transformation from our current coordinate system, \\(\\vec{x}\\), to another defined by \\(\\vec{y} = \\mathbf{A}^{-1}\\vec{x}\\). </p> <p>If we then attempt to derive a recursion in our new coordinate system we find</p> \\[ \\begin{aligned} \\vec{y}(t+1) &amp;= \\mathbf{A}^{-1}\\vec{x}(t+1)\\\\ &amp;= \\mathbf{A}^{-1}\\mathbf{M}\\vec{x}(t)\\\\ &amp;= \\mathbf{A}^{-1}\\mathbf{M}\\mathbf{A}\\vec{y}(t)\\\\ &amp;= \\mathbf{A}^{-1}(\\mathbf{A} \\mathbf{D} \\mathbf{A}^{-1})\\mathbf{A}\\vec{y}(t)\\\\ &amp;= \\mathbf{D}\\vec{y}(t) \\end{aligned} \\] <p>Voila! In the new coordinate system we just move along each axis by a factor \\(\\lambda_i\\), just as we saw in Lecture 12. In this way the eigenvectors form a new, more convenient, coordinate system.</p> <p>To convert back to our original coordinate system we multiply both sides of the equation by \\(\\mathbf{A}\\) on the left and then use \\(\\vec{y} = \\mathbf{A}^{-1}\\vec{x}\\)</p> \\[ \\begin{aligned} \\mathbf{A}\\vec{y}(t+1) &amp;= \\mathbf{A}\\mathbf{D}\\vec{y}(t)\\\\ \\vec{x}(t+1) &amp;= \\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}\\vec{x}(t)\\\\ \\end{aligned} \\]"},{"location":"lectures/lecture-13/#long-term-dynamics","title":"Long-term dynamics","text":"<p>A major implication from the general solution is that only if all the eigenvalues have an absolute value less than one, \\(|\\lambda_i|&lt;1\\), will all entries of \\(\\mathbf{D}^t\\) decay to zero and hence \\(\\vec{x}(t)\\) decay to \\(\\vec{0}\\) (the equilibrium) as time increases.</p> <p>Further, as time increases \\(\\mathbf{D}^t\\) will be dominated by the eigenvalue with the largest absolute value, which we will call the leading eigenvalue.</p> <p>Note</p> <p>To see that \\(\\mathbf{D}^t\\) will be dominated by the eigenvalue with the largest absolute value as time increases, let the leading eigenvalue be \\(\\lambda_1\\) and factor it out of \\(\\mathbf{D}^t\\)</p> \\[ \\mathbf{D}^t = \\lambda_1^t  \\begin{pmatrix}  1 &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; (\\lambda_2/\\lambda_1)^t &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\  0 &amp; 0 &amp; \\cdots &amp; (\\lambda_n/\\lambda_1)^t\\\\   \\end{pmatrix} \\] <p>Since \\(|\\lambda_i/\\lambda_1|&lt;1\\) for all \\(i\\), for large \\(t\\) these all go to zero and we have</p> \\[ \\mathbf{D}^t \\approx \\tilde{\\mathbf{D}}^t  \\equiv \\lambda_1^t  \\begin{pmatrix}  1 &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; 0 &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\  0 &amp; 0 &amp; \\cdots &amp; 0\\\\   \\end{pmatrix} \\] <p>We can therefore approximate \\(\\vec{x}(t)\\) after a sufficient amount of time as </p> \\[ \\begin{aligned} \\tilde{\\vec{x}}(t) &amp;= \\mathbf{A}\\tilde{\\mathbf{D}}^t\\mathbf{A}^{-1}\\vec{x}(0)\\\\ &amp;= \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{x}(0) \\end{aligned} \\] <p>where \\(\\vec{v}_1\\) and \\(\\vec{u}_1\\) are the right and left eigenvectors associated with the leading eigenvalue, \\(\\lambda_1\\).</p> <p>Warning</p> <p>Finding the left eigenvectors via the inverse of \\(\\mathbf{A}\\) guarantees that the eigenvectors have been scaled such that \\(\\vec{u}_1\\vec{v}_1 = 1\\). If the eigenvectors have been derived in another way, make sure you scale them so that this is true, eg, make the left eigenvalue equal to \\(\\vec{u}_1/(\\vec{u}_1 \\vec{v}_1)\\). Otherwise the long-term approximation will be off by a constant factor.</p> <p>This means that, in the long-term,</p> <ul> <li>\\(\\vec{x}(t)\\) will grow like \\(\\lambda_1^t\\), where \\(\\lambda_1\\) is the leading eigenvalue</li> <li>each variable in \\(\\vec{x}(t)\\) will oscillate around the equilibrium if \\(\\lambda_1&lt;0\\)</li> <li>\\(\\vec{x}(t)\\) will approach the right eigenvector associated with the leading eigenvalue, \\(\\vec{v}_1\\)</li> <li>\\(\\vec{x}(t)\\) is weighted by a constant, \\(\\vec{u}_1 \\vec{x}(0)\\), describing the \"initial size\" of the system</li> </ul>"},{"location":"lectures/lecture-13/#complex-eigenvalues","title":"Complex eigenvalues","text":"<p>The final complication to discuss is complex eigenvalues. Specifically, what is \\(\\lambda^t\\) when \\(\\lambda = A + B i\\)?</p> <p>To do this, we can first use some simple geometry on the complex plane (a two-dimensional space with the real part, \\(A\\), on the x-axis and the imaginary part, \\(B\\), on the y-axis) to show that any complex number can be written</p> \\[ A + Bi = R (\\cos(\\theta) + i \\sin(\\theta)) \\] <p>where \\(R = \\sqrt{A^2 + B^2}\\) is the absolute value of \\(\\lambda\\) and \\(\\theta = \\arctan(B/A)\\) is the angle between \\(\\lambda\\) and the x-axis.</p> <pre>\nimport matplotlib.pyplot as plt\nimport math\n\nA,B = 1,1 #real and imaginary parts\n\nfig, ax = plt.subplots()\n\nax.arrow(0,0,A,B, head_width=0.05, color='black', length_includes_head=True) #eigenvalue as vector in complex plane\n\ndx = 0.05\nax.plot([0-dx/2,A-dx/2],[0+dx,B+dx],marker='o',c='b')\nax.text(A/2,B/2+3*dx,r'$R$',rotation=math.atan(B/A)*180/math.pi,c='b',fontsize=15,ha='center',va='center')\n\nax.plot([0,A],[B,B],marker='o',c='r')\nax.text(A/2,B+dx,r'$A=R \\cos(\\theta)$',c='r',fontsize=15,ha='center',va='center')\n\nax.plot([A,A],[0,B],marker='o',c='g')\nax.text(A+dx,B/2,r'$B=R \\sin(\\theta)$',c='g',fontsize=15,ha='center',va='center',rotation=90)\n\nax.set_xlabel('real part, $A$')\nax.set_ylabel('imaginary part, $B$')\nax.set_xlim(-dx,A+2*dx)\nax.set_ylim(-dx,B+2*dx)\n\ndx=A/4\nax.plot([0,dx],[0,0],c='orange')\nax.add_patch(Arc((0,0), width=2*dx, height=2*dx, theta1=0, theta2=math.atan(B/A)*180/math.pi, edgecolor='orange'))\nax.text(dx/2,dx/6,r'$\\theta$',fontsize=15,c='orange')\n\nplt.show()\n</pre> <p></p> <p>We then use Euler's equation, \\(\\cos(\\theta) + i \\sin(\\theta) = e^{i \\theta}\\), to write </p> \\[ A + Bi = R e^{i \\theta} \\] <p>And we can now take powers of \\(\\lambda\\) </p> \\[ \\lambda^t = R^t e^{i \\theta t} \\]"},{"location":"lectures/lecture-13/#summary","title":"Summary","text":"<p>To summarize, for any system of linear recursion equations, \\(\\vec{x}(t+1)\\), we can </p> <ul> <li>write the general solution in terms of the eigenvalues and eigenvectors, \\(\\vec{x}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{x}(0)\\)</li> <li>define the leading eigenvalue as the one with the largest absolute value</li> <li>say that the equilibrium is stable if the leading eigenvalue is less than 1</li> <li>approximate the long-term dynamics in terms of the leading eigenvalue and its associated eigenvectors, \\(\\vec{x}(t) = \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{x}(0)\\)</li> </ul>"},{"location":"lectures/lecture-13/#motivating-example-revisited","title":"Motivating example revisited","text":"<p>Now let's return to our motivating example of birds on islands. And let's imagine we have good estimates of the parameter values (after years of tough fieldwork!): \\(m_{12}=m_{21}=0.1\\), \\(b_1=b_2=0.2\\), \\(d_1=0.1\\), \\(d_2=0.2\\). To derive the general solution, giving the number of birds on the two islands in year \\(t\\), we first derive the eigenvalues and eigenvectors of \\(\\mathbf{M}\\). Using the techniques in Lecture 12 we find that the eigenvalues are \\(\\lambda_1\\approx1.03\\) and \\(\\lambda_2\\approx0.8\\). The associated right eigenvectors are \\(\\vec{v}_1\\approx\\begin{pmatrix} 1 \\\\ 0.57 \\end{pmatrix}\\) and \\(\\vec{v}_2\\approx\\begin{pmatrix} 1 \\\\ -1.57 \\end{pmatrix}\\). We therefore have </p> \\[ \\mathbf{D} = \\begin{pmatrix} 1.03 &amp; 0 \\\\ 0 &amp; 0.8 \\end{pmatrix} \\] <p>and </p> \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 1 \\\\ 0.57 &amp; -1.57 \\end{pmatrix} \\] <p>This year's census of the islands tells us that there are currently 100 birds on island 1 and 50 on island 2. Taking this as the starting point, \\(\\vec{n}(0) = \\begin{pmatrix} 100 \\\\ 50 \\end{pmatrix}\\), we can use our general solution to predict the number of birds on the two islands over time. Below we plot the predicted number of birds on the two islands over the next 100 years.</p> <pre>\nm12, m21, b1, b2, d1, d2 = 0.1, 0.1, 0.2, 0.2, 0.1, 0.2 #parameter values\n\n# general solution\nfrom sympy import *\nM = Matrix([[(1-m12)*(1+b1)*(1-d1), m21*(1+b1)*(1-d1)], #matrix\n            [m12*(1+b2)*(1-d2), (1-m21)*(1+b2)*(1-d2)]])\nA, D = M.diagonalize() #quick way to get matrix of right eigenvectors (A) and eigenvalues (D)\nn0 = Matrix([100,50]) #note this is made into a column vector automatically\nnt = A*D**t*A.inv()*n0 #general solution\n\n# plot\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nfor j in range(2): #for each island\n    ax.plot([nt.subs(t,i)[j] for i in range(100)], label='island %d'%(j+1), marker=\".\")\nax.legend()\nax.set_xlabel('years from now')\nax.set_ylabel('number of birds')\nplt.show()\n</pre> <p></p> <p>We see that the population grows, which we should have expected given that the leading eigenvalue, \\(\\lambda_1\\approx1.03\\), has an absolute value greater than 1. </p> <p>We also see that there are about 0.57 birds on island 2 for every 1 bird on island 1, as predicted by the right eigenvector associated with the leading eigenvector, \\(\\vec{v}_1 \\approx \\begin{pmatrix} 1 \\\\ 0.57 \\end{pmatrix}\\).</p> <p>More generally, our long-term prediction is \\(\\vec{n}(t) \\approx \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0)\\). Overlaying this approximation as black curves on the above plot shows that this works very well.</p> <pre>\n# long-term approximation\nu1 = Matrix(1,2,A.inv()[0:2]) #left eigenvector is first row in A inverse\nntapp = l1**t * v1[0] * u1 * n0\n\n# plot\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nfor j in range(2): #for each island\n    ax.plot([nt.subs(t,i)[j] for i in range(100)], label='island %d'%(j+1), marker=\".\") #general solution\n    ax.plot([ntapp.subs(t,i)[j] for i in range(100)], c='k') #long-term approx\nax.legend()\nax.set_xlabel('years from now')\nax.set_ylabel('number of birds')\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-13/#2-continuous-time","title":"2. Continuous time","text":"<p>Now let's consider a system of linear equations in continuous time, which we can write in matrix form as</p> \\[ \\frac{\\mathrm{d}\\vec{x}}{\\mathrm{d}t} = \\mathbf{M} \\vec{x} \\]"},{"location":"lectures/lecture-13/#general-solution_1","title":"General solution","text":"<p>Just as in the univariate case of exponential growth, the general solution is simply </p> \\[ \\vec{x}(t) = e^{\\mathbf{M}t}\\vec{x}(0) \\] <p>But now we have \\(e\\) to the power of a matrix, and \\(e^{\\mathbf{M}t} = \\sum_{i=0}^{\\infty} \\mathbf{M}^i t^i / i!\\) is complicated!</p> <p>Fortunately we can use the same transform as in the discrete time case, \\(\\vec{y}=\\mathbf{A}^{-1}\\vec{x}\\), to write the general solution as </p> \\[ \\vec{x}(t) = \\mathbf{A} e^{\\mathbf{D}t} \\mathbf{A}^{-1}\\vec{x}(0) \\] <p>This is much simpler because \\(e^{\\mathbf{D}t}\\) is just</p> \\[ e^{\\mathbf{D}t} = \\begin{pmatrix} e^{\\lambda_1 t} &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; e^{\\lambda_2 t} &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ 0 &amp; \\cdots &amp; 0 &amp; e^{\\lambda_n t} \\end{pmatrix} \\]"},{"location":"lectures/lecture-13/#long-term-dynamics_1","title":"Long-term dynamics","text":"<p>From this general solution we see that the system will approach equilibrium, \\(\\hat{\\vec{x}} = \\vec{0}\\), only if all the entries of \\(e^{\\mathbf{D}t}\\) approach zero as time increases, which requires that all the eigenvalues are negative.</p> <p>Taking \\(\\lambda_1\\) to be the eigenvalue with the largest value, which we will call the leading eigenvalue, we also see that after sufficient time \\(e^{\\mathbf{D}t}\\) becomes dominated by this entry</p> \\[ e^{\\mathbf{D}t} \\approx  \\begin{pmatrix} e^{\\lambda_1 t} &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; 0 &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ 0 &amp; \\cdots &amp; 0 &amp; 0 \\end{pmatrix} \\] <p>implying that the long-term dynamics can be approximated by </p> \\[ \\vec{x}(t) \\approx e^{\\lambda_1 t} \\vec{v}_1 \\vec{u}_1 \\vec{x}(0) \\] <p>where \\(\\vec{v}_1\\) and \\(\\vec{u}_1\\) are the right and left eigenvectors associated with the leading eigenvalue, \\(\\lambda_1\\).</p>"},{"location":"lectures/lecture-13/#complex-eigenvalues_1","title":"Complex eigenvalues","text":"<p>When we have complex eigenvalues we can again use Euler's equation to write</p> \\[ \\begin{aligned} e^{\\lambda t} &amp;= e^{(A + Bi) t}\\\\ &amp;= e^{At}e^{Bti}\\\\ &amp;= e^{At}(\\cos(Bt) + i\\sin(Bt)) \\end{aligned} \\] <p>Because both \\(\\cos(Bt)\\) and \\(\\sin(Bt)\\) are bounded (between -1 and 1), whether \\(e^{\\lambda t}\\) grows or shrinks in long term depends only on the real part, \\(A\\). So, in contrast to discrete time, when determining the leading eigenvalue in continuous time we only have to consider the real parts of the eigenvalues.</p>"},{"location":"lectures/lecture-13/#summary_1","title":"Summary","text":"<p>In summary, for any system of linear differential equations, \\(\\frac{\\mathrm{d}\\vec{x}}{\\mathrm{d}t}\\), we can </p> <ul> <li>write the general solution in terms of the eigenvalues and eigenvectors, \\(\\vec{x}(t) = \\mathbf{A} e^{\\mathbf{D}t} \\mathbf{A}^{-1}\\vec{x}(0)\\)</li> <li>define the leading eigenvalue as the eigenvalue with the largest real part</li> <li>say that the equilibrium is stable if the real part of the leading eigenvalue is negative</li> <li>approximate the long-term dynamics in terms of the leading eigenvalue and its associated eigenvectors, \\(\\vec{x}(t) \\approx e^{\\lambda_1 t} \\vec{v}_1 \\vec{u}_1 \\vec{x}(0)\\)</li> </ul>"},{"location":"lectures/lecture-13/#example","title":"Example","text":"<p>Let's consider a different example here in continuous time. Instead of thinking about birds on islands, let's think about a model of sexual selection. We will model the mean value of a male trait, \\(\\bar{z}\\), such as the length of a birds tail, and the mean value of female preference for that trait, \\(\\bar{p}\\) (if \\(\\bar{p}&gt;0\\) females tend to prefer larger male traits, if \\(\\bar{p}&lt;0\\) females tend to prefer smaller male traits). We assume the optimal male trait value in the absence of sexual selection is \\(\\theta\\), i.e., natural selection always pushes \\(\\bar{z}\\) towards \\(\\theta\\) (we'll take \\(\\theta=0\\), meaning \\(\\bar{z}\\) is measured relative to the optimum). We assume female choice is costly, i.e., natural selection always pushes \\(\\bar{p}\\) towards 0. Finally we will assume that male traits and female preference share some genetic basis, meaning that they will covary (e.g., there may be some alleles that increase the trait value when in males and increase the preference when in females, causing positive covariance). This covariance means that a change in male trait will cause a change in female preference, and vice-versa.</p> <p>We can describe the dynamics of \\(\\bar{z}\\) and \\(\\bar{p}\\) with a system of linear differential equations</p> \\[ \\begin{align} \\frac{\\mathrm{d}\\bar{z}}{\\mathrm{d}t} = G_z (a \\bar{p} - c \\bar{z}) - B b \\bar{p}\\\\ \\frac{\\mathrm{d}\\bar{p}}{\\mathrm{d}t} = B (a \\bar{p} - c \\bar{z}) - G_p b \\bar{p} \\\\ \\end{align} \\] <p>where \\(G_z\\) and \\(G_p\\) are the amounts of genetic variation in male traits and female preference (this is the \"fuel\" of evolution, so the rates of evolution are proportional to these variances), \\(B\\) is the covariance between male traits and female preference, \\(a\\) is the strength of sexual selection, and \\(c\\) and \\(b\\) are the strengths of natural selection on male traits and female preference.</p> <p>Choosing some parameter values and plotting the general solution, we see the mean male trait and mean female preference cycle over time, decaying towards zero.</p> <pre>\nGz, Gp, B, a, b, c, z0, p0, tmax = 0.15, 0.8, 0.32, 0.95, 0.3, 0.45, 1, 0, 1000 #parameter values\n\n# general solution\nfrom sympy import *\nM = Matrix([[-Gz*c, Gz*a-b*B],\n            [-B*c, -Gp*b+a*B]])\nA, D = M.diagonalize() #quick way to get matrix of right eigenvectors (A) and eigenvalues (D)\nn0 = Matrix([z0,p0]) #note this is made into a column vector automatically\nnt = A*exp(D*t)*A.inv()*n0 #general solution\n\n# plot\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.plot([re(nt.subs(t,i)[0]) for i in range(tmax)], label='male trait') #need to remove the imaginary part because of numerical error\nax.plot([re(nt.subs(t,i)[1]) for i in range(tmax)], label='female preference')\nax.legend()\nax.set_xlabel('time')\nax.set_ylabel('value')\nplt.show()\n</pre> <p></p> <p>This cycling occurs because initially the mean male trait is positive but there is no mean female preference. This implies that both natural and sexual selection favour smaller male traits, causing the mean to decline. But because of a correlated response, female preference also declines, favouring male traits less than 0. Eventually female preference becomes too costly and begins to increase back toward zero. This causes a correlated increase in the male trait, and so on. </p> <p>With these parameter values the eigenvalues are \\(\\lambda \\approx -0.002 \\pm 0.05 i\\).</p> <p>We could therefore have predicted this cycling based on these eigenvalues, as they are complex.</p> <p>We could also have predicted the eventual decay to zero (the equilibrium), as the real parts of both eigenvalues are negative.</p> <p>In this case the two eigenvalues have the same real part and therefore there is no one leading eigenvalue, meaning that we cannot use our long-term approximation, \\(\\vec{x}(t) \\approx e^{\\lambda_1 t} \\vec{v}_1 \\vec{u}_1 \\vec{x}(0)\\).</p>"},{"location":"lectures/lecture-14/","title":"Lecture 14","text":""},{"location":"lectures/lecture-14/#lecture-14-demography","title":"Lecture 14: Demography","text":"Run notes interactively?"},{"location":"lectures/lecture-14/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Demography</li> <li>Stage-structure</li> <li>Age-structure</li> </ol>"},{"location":"lectures/lecture-14/#1-demography","title":"1. Demography","text":"<p>We're now going to use what we've learned about linear multivariate models to describe the dynamics of a single population that is composed of different types of individuals. This area of research is called demography. We'll just consider discrete-time models here.</p> <p>In the last lecture we saw that for any discrete-time linear multivariate model</p> \\[ \\vec{n}(t+1) = \\mathbf{M}\\vec{n}(t) \\] <p>we can compute the general solution</p> \\[ \\vec{n}(t) = \\mathbf{M}^t\\vec{n}(0) \\] <p>or, more conveniently, in terms of the eigenvalues (\\(\\mathbf{D}\\)) and eigenvectors (\\(\\mathbf{A}\\))</p> \\[ \\vec{n}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{n}(0 ) \\] <p>Despite this progress, the eigenvalues and eigenvectors are often unobtainable (without specifying parameter values), leaving us to rely on the long-term approximation</p> \\[ \\vec{n}(t) = \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0) \\] <p>For this we just need to know the leading eigenvalue (\\(\\lambda_1\\)) and the corresponding right (\\(\\vec{v}_1\\)) and left (\\(\\vec{u}_1\\)) eigenvectors, respectively. (Remember that we have scaled the eigenvectors such that \\(\\vec{u}_1\\vec{v}_1=1\\).)</p> <p>These three components (\\(\\lambda_1\\), \\(\\vec{v}_1\\), \\(\\vec{u}_1\\)) are the key demographic quantities that we will investigate</p> <ul> <li>\\(\\lambda_1\\) is the long-term population growth rate</li> <li>\\(\\vec{v}_1\\) describes the stable stage-distribution</li> <li>\\(\\vec{u}_1\\) describes the relative reproductive values of each stage</li> </ul> <p>Note</p> <p>The long-term approximation, \\(\\vec{n}(t) = \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0)\\), is valid as long as the leading eigenvalue, \\(\\lambda_1\\), is</p> <ul> <li>real (no cycles in long-term)</li> <li>positive (no oscillations to negative numbers!)</li> <li>larger than all other eigenvalues (so that we can ignore the other eigenvalues/vectors)</li> </ul> <p>Fortunately we are guaranteed all these conditions in our demographic models since all entries of \\(\\mathbf{M}\\) are non-negative and all entries of \\(\\mathbf{M}^t\\) are positive for some value of \\(t\\). (This follows from something called the Perron-Frobenius Theorem.)</p> <p>The most general demographic model is called stage-structure: we consider some finite number of discrete stages that an individual can be in and we use an abitrary matrix of transition rates between stages (a projection matrix), \\(\\mathbf{M}\\), to project how the population size and composition changes over time.</p> <p>A common special case is age-structure: here we define the stages as the number of time steps an individual has been alive for, which leads to a simpler projection matrix (called a Leslie matrix) because individuals always transition to the next stage (or die).</p> <p></p>"},{"location":"lectures/lecture-14/#2-stage-structure","title":"2. Stage-structure","text":""},{"location":"lectures/lecture-14/#example-north-atlantic-right-whale","title":"Example: North Atlantic right whale","text":"<p>Let's consider an example (from 10.2 in the text). North Atlantic right whales were hunted to near extinction in the 1800s and early 1900s, after which their population size is thought to have slowly recovered (there are less than 400 now, and unfortunately appear to be in decline again). Because of their long life span, over which survival and reproductive rates vary enormously, a stage-structured model is very appropriate. Below we draw a flow diagram representing all the transitions between calves, sexually immature individuals, sexually mature individuals, and actively reproducing individuals.</p> <pre><code>graph LR;\n    C((Calves)) --sIC nC--&gt; I((Immature));\n\n    I --sII nI--&gt; I;\n    I --sMI nI--&gt; M((Mature));\n    I --sRI nI--&gt; R((Reproductive));\n\n    M --sMM nM--&gt; M;\n    M --sRM nM--&gt; R;\n\n    R --sRR nR--&gt; R;\n    R --sMR nR--&gt; M;\n    R --b nR--&gt; C;</code></pre> <p>Converting this flow diagram into a system of recursion equations, \\(\\vec{n}(t+1) = \\mathbf{M}\\vec{n}(t)\\), the projection matrix is</p> \\[ \\mathbf{M} = \\begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; b \\\\ s_{IC} &amp; s_{II} &amp; 0 &amp; 0\\\\ 0 &amp; s_{MI} &amp; s_{MM} &amp; s_{MR} \\\\ 0 &amp; s_{RI} &amp; s_{RM} &amp; s_{RR} \\end{pmatrix} \\] <p>If we now plug in some estimated parameter values from the literature (\\(b=0.3\\), \\(s_{IC}=0.92\\), \\(s_{II}=0.86\\), \\(s_{MI}=0.08\\), \\(s_{MM}=0.8\\), \\(s_{MR}=0.88\\), \\(s_{RI}=0.02\\), \\(s_{RM}=0.19\\), \\(s_{RR}=0\\)) we can numerically calculate the three key demographic quantities</p> \\[ \\begin{aligned} &amp;\\lambda_1 \\approx 1.003 \\\\ &amp;\\vec{v}_1 \\approx \\begin{pmatrix} 0.04 \\\\ 0.23 \\\\ 0.61 \\\\ 0.12\\end{pmatrix} \\\\ &amp;\\vec{u}_1 \\approx \\begin{pmatrix} 0.69 &amp; 0.76 &amp; 1.07 &amp; 1.15 \\end{pmatrix} \\end{aligned} \\] <p>These quantitives tells us, for example, that in the long-run the population is predicted to grow (\\(\\lambda_1&gt;1\\)), the majority of individuals are expected to be mature (the second last entry in \\(\\vec{v}_1\\) is the largest), and mature and reproductive individuals are expected to have much higher reproductive values than calves and immature individuals (the last two entries in \\(\\vec{u}_1\\) are much larger than the first two). </p> <p>Caveat</p> <p>The parameter values above were estimated before the current population decline -- changes in climate and human behaviour, eg boat traffic and fishing, have presumably altered the parameter values, making the predictions less accurate. For example, the increased incidence of entanglement in fishing nets has likely decreased survival rates to the point that the population is now expected to decline. See here for more info.</p> <p>We can now answer an important conservation question: </p> <p>Question</p> <p>If we wanted to increase the total population size in the future and we could add one individual to any stage, which stage should it be?</p> <p>We want to know what entry of \\(\\vec{n}(0)\\) to add 1 to to maximize \\(\\vec{n}(t) \\approx \\lambda^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0)\\). This will not affect the long-term growth rate (\\(\\lambda_1\\)) or the stable-stage distribution (\\(\\vec{v}_1\\)). We can therefore only increase \\(\\vec{u}_1\\vec{n}(0) = u_1 n_1(0) + u_2 n_2(0) + ... + u_m n_m(0)\\). And so we add 1 to the stage with the largest reproductive value, \\(u_i\\).</p> <p>So for these whales, where the reproductive values are \\(\\vec{u}_1 \\approx \\begin{pmatrix} 0.69 &amp; 0.76 &amp; 1.07 &amp; 1.15 \\end{pmatrix}\\), we should add a reproductively active individual. This is perhaps not surprising since those are the only individuals that produce offspring and if we introduced an individual in an earlier stage there is some chance that they would die before becoming reproductively active.</p> <p>One other question we might now consider is:</p> <p>Question</p> <p>If we wanted to increase the long-term population growth rate and we could increase any parameter a little bit, which parameter should it be?</p> <p>One way to answer this is to numerically calculate the long-term population growth rate, \\(\\lambda_1\\), for a range of values of one parameter, \\(z\\) (e.g., we might take \\(z=b\\), the fecundity of reproductive individuals).</p> <p>Or, we could try to analytically compute the rate of change in the leading eigenvalue with respect to \\(z\\), \\(\\mathrm{d}\\lambda_1/\\mathrm{d}z\\), while considering \\(\\mathbf{M}\\), \\(\\lambda_1\\), \\(\\vec{v}_1\\), and \\(\\vec{u}_1\\) to be functions of \\(z\\). </p> <p>Since \\(\\mathbf{M}\\vec{v}_1 = \\lambda_1 \\vec{v}_1\\) and \\(\\vec{u}_1\\mathbf{M} = \\vec{u}_1\\lambda_1\\) we have</p> \\[ \\begin{aligned} \\mathbf{M} \\vec{v}_1 &amp;= \\lambda_1 \\vec{v}_1 \\\\ \\vec{u}_1 \\mathbf{M} \\vec{v}_1 &amp;= \\vec{u}_1 \\lambda_1 \\vec{v}_1 \\\\ \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left(\\vec{u}_1 \\mathbf{M} \\vec{v}_1 \\right)&amp;= \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left(\\vec{u}_1 \\lambda_1 \\vec{v}_1 \\right)\\\\ \\frac{\\mathrm{d}\\vec{u}_1}{\\mathrm{d}z} \\mathbf{M} \\vec{v}_1 + \\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1 + \\vec{u}_1 \\mathbf{M} \\frac{\\mathrm{d}\\vec{v}_1}{\\mathrm{d}z} &amp;= \\frac{\\mathrm{d}\\vec{u}_1}{\\mathrm{d}z} \\lambda_1 \\vec{v}_1 + \\vec{u}_1 \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\vec{v}_1 + \\vec{u}_1 \\lambda_1 \\frac{\\mathrm{d}\\vec{v}_1}{\\mathrm{d}z}\\\\ \\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1 &amp;= \\vec{u}_1 \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\vec{v}_1 \\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} &amp;= \\frac{\\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1}{\\vec{u}_1 \\vec{v}_1} \\end{aligned}\\] <p>We call \\(\\mathrm{d}\\lambda_1/\\mathrm{d}z\\) the sensitivity of \\(\\lambda_1\\) to \\(z\\).</p> <p>This expression will be too complicated to understand in general in most cases, but we can evaluate at some particular value \\(z^*\\) (e.g., at the current estimate) to see how the growth rate changes as \\(z\\) increases from that value</p> \\[ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\bigg|_{z=z^*} = \\frac{\\vec{u}_1 \\left(\\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\big|_{z=z^*} \\right) \\vec{v}_1}{\\vec{u}_1 \\vec{v}_1} \\] <p>Returning to our question, we want to know which parameter, \\(z\\), gives the largest value of \\(\\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\big|_{z=z^*}\\).</p> <p>For these whales we can calculate \\(\\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\big|_{z=z^*}\\) for each parameter \\(z\\) (ie, for each entry in \\(\\mathbf{M}\\)) evaluated at it's current value \\(z^*\\). Doing that calculation we see that increasing the fraction of mature individuals that survive to become reproductively active, \\(s_{RM}\\), has the largest effect. This makes good sense because increasing \\(s_{RM}\\) increases the rate at which the most populous stage (from \\(\\vec{v}_1\\)) transitions to the stage with the highest reproductive value (from \\(\\vec{u}_1\\)).</p> <p>And finally, we can ask</p> <p>Question</p> <p>What are the predicted numbers of individuals in each stage over time?</p> <p>Here we simply plot the general solution (solid) and compare with the long-term approximation (dashed) out of interest. We see that the full solution very quickly converges to the long-term solution in this case. </p> <pre>\nb,sic,sii,smi,smm,smr,sri,srm,srr = 0.3,0.92,0.86,0.08,0.8,0.88,0.02,0.19,0 #parameter values\n\nimport numpy as np\n\nM = np.array([[0,0,0,b], #projection matrix\n              [sic,sii,0,0],\n              [0,smi,smm,smr],\n              [0,sri,srm,srr]])\n\n# calculate\neigs = np.linalg.eig(M) #eigenvalues and right eigenvectors\nix = np.argmax(np.abs(eigs[0])) #which is leading eigenvalue\nl1 = eigs[0][ix] #leading eigenvalue\nv1 = eigs[1][:,ix] #leading right eigenvector\nv1 = v1/np.sum(v1) #normalized to sum to 1\nus = np.linalg.inv(eigs[1]) #left eigenvectors\nu1 = us[ix] #leading left eigenvalue\nu1 = u1/np.dot(u1,v1) #normalized so u1*v1=1\n\nn0 = np.array([50,50,50,50]) #initial state\n\ndef ntfull(t):\n    '''full projection'''\n    return np.dot(np.linalg.matrix_power(M,t), n0)\n\ndef ntapp(t):\n    '''long term approximation'''\n    return l1**t * v1 * np.dot(u1, n0)\n\n# plot\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nlabels = ['calves','immature','mature','reproductive']\ncolors = plt.get_cmap('tab10')\nfor i,j in enumerate(range(len(M))): #for each stage\n    ax.plot([ntfull(t)[j] for t in range(100)], color=colors(i), label=labels[i])\n    ax.plot([ntapp(t)[j] for t in range(100)], color=colors(i), linestyle='--') #long-term approx\n\nax.legend()\nax.set_xlabel('years')\nax.set_ylabel('number of whales')\nplt.show()\n</pre> <p></p> <p></p>"},{"location":"lectures/lecture-14/#3-age-structure","title":"3. Age-structure","text":"<p>Now let's look at the special case of age-structure.</p> <p>Here individuals in stage \\(i\\) at time \\(t\\) can only contribute to stage \\(i+1\\) (survival) and stage \\(1\\) (reproduction) at time \\(t+1\\).</p> <pre><code>graph LR;\n    1((1)) --p1 n1--&gt; 2((2));    \n    1 --m1 n1--&gt; 1;\n    2 --p2 n2--&gt; 3((3));\n    2 --m2 n2--&gt; 1;    \n    3 --p3 n4--&gt; 4((4));\n    3 --m3 n3--&gt; 1;\n    4 --m4 n4--&gt; 1;</code></pre> <p>Because of this, the projection matrix is simpler in the sense that it contains more zeros and has non-zero entries in very specific places</p> \\[ \\mathbf{L} =  \\begin{pmatrix}   m_1 &amp; m_2 &amp; m_3 &amp; \\cdots &amp; m_d \\\\ p_1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; p_2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp;  &amp; \\vdots &amp;  &amp; \\vdots \\\\   0 &amp; \\cdots &amp; 0 &amp; p_{d-1} &amp; 0\\\\ \\end{pmatrix} \\] <p>We call it a Leslie matrix and often denote it with \\(\\mathbf{L}\\). The first row contains the fecundities of each age group, \\(m_i\\), while the entries immediately below the diagonal give the fraction of individuals that survive each age group, \\(p_i\\).</p> <p>Because of the structure of the Leslie matrix, many expressions are now simpler.</p> <p>For example, the characteristic polynomial, \\(\\mathrm{Det}(\\mathbf{L}-\\mathbf{I}\\lambda)=0\\), can be calculated using the first row. After rearranging we get what is known as the Euler-Lotka equation</p> \\[ 1 = \\sum_{i=1}^{d} \\frac{l_i m_i}{\\lambda^i} \\] <p>where \\(l_1 = 1\\), \\(l_i = p_1 p_2 \\cdots p_{i-1}\\) is the fraction of individuals that survive from birth to age \\(i\\) and \\(d\\) is the number of ages (ie, \\(\\mathbf{L}\\) is a \\(d\\times d\\) matrix).</p> <p>Given the \\(l_i\\) and \\(m_i\\) we can use this equation to find the long-term population growth rate, \\(\\lambda_1\\).</p>"},{"location":"lectures/lecture-14/#example-stickleback","title":"Example: stickleback","text":"<p>For example, let's look at a model of stickleback, a small fish (see section 10.6 of the text).</p> <p>We assume stickleback do not live more than 4 years and estimate the Leslie matrix as</p> \\[ \\mathbf{L} =  \\begin{pmatrix} 2 &amp; 3 &amp; 4 &amp; 4\\\\ 0.6 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0.3 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0.1 &amp; 0 \\end{pmatrix} \\] <p>The Euler-Lotka equation is then</p> \\[ \\begin{aligned} 1 &amp;= \\sum_{i=1}^{4} \\frac{l_i m_i}{\\lambda^i}\\\\ 1 &amp;= \\frac{l_1 m_1}{\\lambda} + \\frac{l_2 m_2}{\\lambda^2} + \\frac{l_3 m_3}{\\lambda^3} + \\frac{l_4 m_4}{\\lambda^4}\\\\ 1 &amp;= \\frac{2}{\\lambda} + \\frac{1.8}{\\lambda^2} + \\frac{0.72}{\\lambda^3} + \\frac{0.072}{\\lambda^4}\\\\     \\end{aligned} \\] <p>This can be numerically solved to give our four eigenvalues: \\(\\lambda\\approx2.75, -0.3 \\pm 0.3i, -0.14\\).</p> <p>The long-term growth rate is the eigenvalue with the largest absolute value, \\(\\lambda_1=2.75\\).</p> <p>Note</p> <p>We won't spend the time on it in class, but with age-structure we can also write the stable age distribution, reproductive values, and sensitivities as functions of survival, fecundity, and rate of population growth.</p> <p>The proportion of individuals that are age \\(x\\) (in the long-run) is</p> \\[ v_x = \\frac{l_x \\lambda_1^{-(x-1)}}{\\sum_{i=1}^{d}l_i \\lambda_1^{-(i-1)}} \\] <p>The reproductive value of individuals that are age \\(x\\), relative to age \\(1\\), is</p> \\[ \\frac{u_x}{u_1} = \\frac{\\lambda_1^{x-1}}{l_x}\\sum_{i=x}^{d}\\frac{l_i m_i}{\\lambda_1^{i}} \\] <p>The sensitivities of the long-term growth rate to survival and reproduction at age \\(x\\) are</p> \\[ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_x} = \\frac{v_x u_{x+1}}{\\vec{u}_1 \\vec{v}_1} \\] \\[ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_x} = \\frac{v_x u_1}{\\vec{u}_1 \\vec{v}_1} \\] <p>For example, in our stickleback model the proportion of the population that is age \\(x=2\\), in the long-run, is</p> \\[ \\begin{aligned} v_x &amp;= \\frac{l_x \\lambda_1^{-(x-1)}}{\\sum_{i=1}^{d}l_i \\lambda_1^{-(i-1)}}\\\\ v_2 &amp;= \\frac{l_2 \\lambda_1^{-1}}{\\sum_{i=1}^{4}l_i \\lambda_1^{-(i-1)}}\\\\ &amp;\\approx \\frac{0.6 (2.75)^{-1}}{1 (2.75)^{0} + 0.6 (2.75)^{-1} + 0.18 (2.75)^{-2} + 0.018 (2.75)^{-3}}\\\\ &amp;\\approx 0.18 \\end{aligned} \\] <p>and the relative reproductive value of age \\(x=2\\) individuals is </p> \\[ \\begin{aligned} \\frac{v_x}{v_1} &amp;= \\frac{\\lambda_1^{x-1}}{l_x}\\sum_{i=x}^{d}\\frac{l_i m_i}{\\lambda_1^{i}}\\\\ \\frac{v_2}{v_1} &amp;= \\frac{\\lambda_1^{2-1}}{l_2}\\sum_{i=2}^{4}\\frac{l_i m_i}{\\lambda_1^{i}}\\\\ &amp;= \\frac{\\lambda_1}{l_2} \\left(\\frac{l_2 m_2}{\\lambda_1^{2}} + \\frac{l_3 m_3}{\\lambda_1^{3}} + \\frac{l_4 m_4}{\\lambda_1^{i}} \\right)\\\\ &amp;\\approx \\frac{2.75}{0.6} \\left(\\frac{(0.6) (3)}{2.75^{2}} + \\frac{(0.18) (4)}{2.75^{3}} + \\frac{(0.018) (4)}{2.75^{4}} \\right)\\\\ &amp;\\approx 1.25 \\end{aligned} \\] <p>Repeating these for the other ages we get the stable-age distribution</p> \\[ \\vec{v}_1 \\approx \\begin{pmatrix} 0.80 \\\\ 0.18 \\\\ 0.02 \\\\ 0.0007 \\end{pmatrix} \\] <p>and the reproductive values</p> \\[ \\vec{u}_1 \\approx \\begin{pmatrix} 1 &amp; 1.25 &amp; 1.51 &amp; 1.45 \\end{pmatrix} \\] <p>From these we see that the majority of the population is expected to be age 1 (from \\(\\vec{v}_1\\)) and age 3 has the highest reproductive value (from \\(\\vec{u}_1\\)).</p> <p>We can now use these vectors to calculate the sensitivities of population growth rate to survival</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_x} &amp;= \\frac{v_x u_{x+1}}{\\vec{u}_1 \\vec{v}_1}\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_1} &amp;\\approx 0.95\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_2} &amp;\\approx 0.25\\\\  \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_3} &amp;\\approx 0.03 \\end{aligned} \\] <p>and to fecundity</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_x} &amp;= \\frac{v_x u_1}{\\vec{u}_1 \\vec{v}_1}\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_1} &amp;\\approx 0.76\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_2} &amp;\\approx 0.17\\\\  \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_3} &amp;\\approx 0.02\\\\  \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_4} &amp;\\approx 0.0007 \\end{aligned} \\] <p>And so we see that, because such a large proportion of the population is expected to be age 1, increasing the survival and fecundity of that age has the largest effect.</p> <p>Finally, we can plot the predicted dynamics as we did for the whales above. In this case the population grows very quickly (clearly there is a need to also model density-dependence via competition, otherwise our model predicts that the universe will soon be stuffed-full with sticklback!), so we plot the number of fish of each age on a log scale. Again, we compare the full general solution (solid) to the long-term approximation (dashed) and see quick convergence.</p> <pre>\nm1,m2,m3,m4,p1,p2,p3 = 2,3,4,4,0.6,0.3,0.1 #parameter values\n\nimport numpy as np\n\nM = np.array([[m1,m2,m3,m4], #projection matrix\n              [p1,0,0,0],\n              [0,p2,0,0],\n              [0,0,p3,0]])\n\n# calculate\neigs = np.linalg.eig(M) #eigenvalues and right eigenvectors\nix = np.argmax(np.abs(eigs[0])) #which is leading eigenvalue\nl1 = eigs[0][ix] #leading eigenvalue\nv1 = eigs[1][:,ix] #leading right eigenvector\nv1 = v1/np.sum(v1) #normalized to sum to 1\nus = np.linalg.inv(eigs[1]) #left eigenvectors\nu1 = us[ix] #leading left eigenvalue\nu1 = u1/np.dot(u1,v1) #normalized so u1*v1=1\n\nn0 = np.array([25,25,25,25]) #initial state\n\ndef ntfull(t):\n    '''full projection'''\n    return np.dot(np.linalg.matrix_power(M,t), n0)\n\ndef ntapp(t):\n    '''long term approximation'''\n    return l1**t * v1 * np.dot(u1, n0)\n\n# plot\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nlabels = ['age 1','age 2','age 3','age 4']\ncolors = plt.get_cmap('tab10')\nfor i,j in enumerate(range(len(M))): #for each stage\n    ax.plot([ntfull(t)[j] for t in range(10)], color=colors(i), label=labels[i])\n    ax.plot([ntapp(t)[j] for t in range(10)], color=colors(i), linestyle='--') #long-term approx\n\nax.legend()\nax.set_xlabel('years')\nax.set_ylabel('number of stickleback')\nax.set_yscale('log')\nplt.show()\n</pre> <p></p>"},{"location":"lectures/lecture-15/","title":"Lecture 15","text":""},{"location":"lectures/lecture-15/#lecture-15-equilibria-and-stability-nonlinear-multivariate","title":"Lecture 15: Equilibria and stability (nonlinear multivariate)","text":"Run notes interactively?"},{"location":"lectures/lecture-15/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Continuous time</li> <li>Discrete time</li> <li>Summary</li> </ol> <p>Now that we've covered linear multivariate models, we turn our attention to the most common type of model: one with multiple interacting variables.</p> <p>For example, a model of the number of susceptible, \\(S\\), and infected, \\(I\\), individuals often includes the interaction between these two variables, \\(SI\\), describing the rate at which these two classes of individuals meet one another (and potentially spread disease). </p> <p>Similarly, models of predator, \\(P\\), and prey, \\(N\\), abundance often include terms like \\(NP\\) describing the rate at which predators encounter prey (and potentially eat them).</p> <p>Let's first see how to deal with these models in general and then apply those techniques to specific circumstances like those mentioned above (in the next two lectures).</p> <p></p>"},{"location":"lectures/lecture-15/#1-continuous-time","title":"1. Continuous time","text":"<p>In general, if we have \\(n\\) interacting variables, \\(x_1, x_2, ..., x_n\\), we can write any continuous time model like</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}x_1}{\\mathrm{d}t} &amp;=  f_1(x_1, x_2, ..., x_n)\\\\ \\frac{\\mathrm{d}x_2}{\\mathrm{d}t} &amp;=  f_2(x_1, x_2, ..., x_n)\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}x_n}{\\mathrm{d}t} &amp;=  f_n(x_1, x_2, ..., x_n)   \\end{aligned} \\] <p>If we then want to find the equilibria, \\(\\hat{x}_1,\\hat{x}_2, ..., \\hat{x}_n\\), we set all these equations to 0 and solve for one variable at a time (note that solving for the equilibrium is not always possible in nonlinear models!).</p> <p>Now note that we can no longer write this system of equations in matrix form with a matrix composed only of parameters.</p> <p>In order to use what we've learned about eigenvalues and eigenvectors we're first going to have to linearize the system so that the corresponding matrices do not contain variables.</p> <p>As we saw in nonlinear univariate models, one useful way to linearize a system is to measure the system relative to equilibrium, \\(\\epsilon = n - \\hat{n}\\).</p> <p>Then assuming that the deviation from equilibrium, \\(\\epsilon\\), is small, we used a Taylor series expansion to approximate the nonlinear system with a linear system.</p> <p>To do that with multivariate models we'll need to know how to take a Taylor series expansion of multivariate functions</p> <p>Taylor series expansion of a multivariate function</p> <p>Taking the series of \\(f\\) around \\(x_1=a_1\\), \\(x_2=a_2\\), ..., \\(x_n=a_n\\) gives</p> \\[ \\begin{aligned} f(x_1, x_2, ..., x_n) &amp;= f(a_1, a_2, ..., a_n)\\\\  &amp;+ \\sum_{i=1}^{n} \\left( \\frac{\\partial f}{\\partial x_i} \\bigg|_{x_1=a_1, x_2=a_2, ..., x_n=a_n} \\right) (x_i - a_i)\\\\ &amp;+ \\sum_{i=1}^{n}\\sum_{j=1}^n \\left( \\frac{\\partial f}{\\partial x_i \\partial x_j} \\bigg|_{x_1=a_1, x_2=a_2, ..., x_n=a_n} \\right) (x_i - a_i)(x_j - a_j)\\\\ &amp;+ \\cdots \\end{aligned} \\] <p>where \\(\\frac{\\partial f}{\\partial x_i}\\) is the \"partial derivative\" of \\(f\\) with respect to \\(x_i\\), meaning that we treat all the other variables as constants when taking the derivative. </p> <p>Then when the difference between each variable and its value, \\(x_i-a_i\\), is small enough we can ignore all the terms with a \\((x_i-a_i)(x_j-a_j)\\), and we are left with a linear approximation of \\(f\\).</p> <p>So let \\(\\epsilon_i = x_i - \\hat{x}_i\\) be the deviation of variable \\(x_i\\) from its equilibrium value, \\(\\hat{x}_i\\).</p> <p>Then we can write a system of equations describing the change in the deviations for all of our variables</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &amp;=  f_1(x_1, x_2, ..., x_n)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &amp;=  f_2(x_1, x_2, ..., x_n)\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &amp;=  f_n(x_1, x_2, ..., x_n)   \\end{aligned} \\] <p>And then we can take a Taylor series around \\(x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n\\) to get a linear approximation of our system near the equilibrium</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &amp;\\approx  f_1(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &amp;\\approx  f_2(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &amp;\\approx  f_n(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i) \\end{aligned} \\] <p>And then note that all \\(f_i(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n)=0\\) by definition of a equilibrium, leaving</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &amp;\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &amp;\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &amp;\\approx  \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i) \\end{aligned} \\] <p>Each of the partial derivatives \\(\\frac{\\partial f_i}{\\partial x_j}\\) is evaluated at the equilibrium, so these are constants. And \\(x_i - \\hat{x}_i = \\epsilon_i\\). So we now have a linear system</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &amp;\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &amp;\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i\\\\ &amp;\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &amp;\\approx  \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i \\end{aligned} \\] <p>We can now write our approximate system around the equilibrium in matrix form</p> \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} \\\\ \\vdots \\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} &amp; \\frac{\\partial f_n}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix} \\end{aligned} \\] <p>This is a special matrix called the Jacobian</p> \\[ \\mathbf{J} =  \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} &amp; \\frac{\\partial f_n}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix} \\] <p>And now that we have a linear system around an equilibrium, we can assess its local stability just as we did with linear multivariate models (see Summary).</p> <p></p>"},{"location":"lectures/lecture-15/#2-discrete-time","title":"2. Discrete time","text":"<p>Note</p> <p>The short version of this section is that we can do the same thing in discrete time -- local stability is determined by the eigenvalues of the Jacobian, where the functions in that Jacobian are now our recursions, \\(x_i(t+1) = f_i(x_1(t), x_2(t), ..., x_n(t))\\).</p> <p>We can do something very similar for nonlinear multivariate models in discrete time</p> \\[ \\begin{aligned} x_1(t+1) &amp;=  f_1(x_1(t), x_2(t), ..., x_n(t))\\\\ x_2(t+1) &amp;=  f_2(x_1(t), x_2(t), ..., x_n(t))\\\\ &amp;\\vdots\\\\ x_n(t+1) &amp;=  f_n(x_1(t), x_2(t), ..., x_n(t))\\\\ \\end{aligned} \\] <p>Now the equilibria are found by setting all \\(x_i(t+1) = x_i(t) = \\hat{x}_i\\) and solving for the \\(\\hat{x}_i\\) one at a time.</p> <p>To linearize the system around an equilibrium we again measure the system in terms of deviation from the equilibrium, \\(\\epsilon_i(t) = x_i(t) - \\hat{x}_i\\), giving</p> \\[ \\begin{aligned} \\epsilon_1(t+1) &amp;=  f_1(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ \\epsilon_2(t+1) &amp;=  f_2(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ &amp;\\vdots\\\\ \\epsilon_n(t+1) &amp;=  f_n(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ \\end{aligned} \\] <p>Then taking the Taylor series of each \\(f_i\\) around \\(x_1(t) = \\hat{x}_1, ..., x_n(t) = \\hat{x}_n\\) we can approximate our system near the equilibrium as</p> \\[ \\begin{aligned}   \\epsilon_1(t+1) &amp;=  f_1(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n)  + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_1\\\\   \\epsilon_2(t+1) &amp;=  f_2(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_2\\\\   &amp;\\vdots\\\\   \\epsilon_n(t+1) &amp;=  f_n(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_n\\\\   \\end{aligned} \\] <p>And noting that \\(f_i(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) = \\hat{x}_i\\) and \\(x_i(t) - \\hat{x}_i = \\epsilon_i(t)\\) we have</p> \\[ \\begin{aligned}   \\epsilon_1(t+1) &amp;= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\   \\epsilon_2(t+1) &amp;= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\   &amp;\\vdots\\\\   \\epsilon_n(t+1) &amp;= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\   \\end{aligned} \\] <p>We can therefore write our approximation around the equilibrium in matrix form</p> \\[ \\begin{aligned} \\begin{pmatrix} \\epsilon_1(t+1) \\\\ \\epsilon_2(t+1) \\\\ \\vdots \\\\ \\epsilon_n(t+1) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} &amp; \\frac{\\partial f_n}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\begin{pmatrix} \\epsilon_1(t) \\\\ \\epsilon_2(t) \\\\ \\vdots \\\\ \\epsilon_n(t) \\end{pmatrix} \\end{aligned} \\] <p>As in continuous time, the dynamics are described by the Jacobian matrix</p> \\[ \\mathbf{J} =  \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} &amp; \\frac{\\partial f_n}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix} \\] <p>We therefore assess the local stability of an equilibrium by evaluating the Jacobian at that equilibrium and finding the eigenvalues (see Summary).</p> <p></p>"},{"location":"lectures/lecture-15/#3-summary","title":"3. Summary","text":"<p>We can determine the stability of nonlinear multivariate models with the eigenvalues of the Jacobian evaluated at an equilibrium. The recipe is</p> <ul> <li>Find the equilibrium of interest, \\(\\hat{x}_1, \\hat{x}_2, ... \\hat{x}_n\\)</li> <li>Calculate the Jacobian, \\(\\mathbf{J}\\)</li> <li>Evaluate the Jacobian at the equilibrium of interest, \\(\\mathbf{J}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n}\\)</li> <li>Calculate the characteristic polynomial \\(|\\mathbf{J}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} - \\lambda\\mathbf{I}|\\) </li> <li>Set the characteristic polynomial equal to 0 and solve for the \\(n\\) eigenvalues, \\(\\lambda\\)</li> </ul> <p>Stability reminder</p> <p>Continuous time</p> <ul> <li>the leading eigenvalue is the one with the largest real part</li> <li>if the leading eigenvalue has a negative real part the equilibrium is stable</li> <li>if any eigenvalue has a non-zero complex part there will be cycling</li> </ul> <p>Discrete time</p> <ul> <li>the leading eigenvalue is the one with the largest absolute value</li> <li>if the leading eigenvalue has an absolute value less than one the equilibrium is stable</li> <li>if any eigenvalue has a non-zero complex part there will be cycling</li> </ul>"},{"location":"lectures/lecture-16/","title":"Lecture 16","text":""},{"location":"lectures/lecture-16/#lecture-16-epidemiology","title":"Lecture 16: Epidemiology","text":"Run notes interactively?"},{"location":"lectures/lecture-16/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Epidemiology</li> </ol>"},{"location":"lectures/lecture-16/#1-epidemiology","title":"1. Epidemiology","text":"<p>In Lecture 15 we learned how to find equilibria and determine their stability in nonlinear multivariate models. To make this more concrete, let's consider a biological example (see Section 8.2 in the text). </p> <p>Consider a population composed of \\(S\\) susceptible individuals and \\(I\\) infected individuals. We assume new susceptible individuals arrive at rate \\(\\theta\\) via immigration and existing susceptibles die at per capita rate \\(d\\). We assume infected individuals die at an elevated per capita rate \\(d+v\\) and recover at per capita rate \\(\\gamma\\). So far this is a linear (affine) model. Finally, we assume susceptibles become infected at rate \\(\\beta S I\\). This is the non-linear part.</p> <p>We can describe this with the following flow diagram</p> <pre><code>graph LR;\n    A[ ] --theta--&gt; S((S));\n    S --beta S I--&gt; I((I));\n    S --d S--&gt; B[ ];    \n    I --\"(d + v) I\"--&gt; C[ ];\n    I --gamma I--&gt; S;\n\n    style A height:0px;\n    style B height:0px;\n    style C height:0px;</code></pre> <p>The corresponding system of differential equations is</p> \\[\\begin{aligned} \\frac{\\mathrm{d}S}{\\mathrm{d}t} &amp;= \\theta - \\beta S I - d S + \\gamma I \\\\ \\frac{\\mathrm{d}I}{\\mathrm{d}t} &amp;= \\beta S I - (d + v) I - \\gamma I  \\end{aligned}\\] <p>At equilibrium both derivatives are equal to zero </p> \\[\\begin{aligned} 0 &amp;= \\theta - \\beta \\hat{S} \\hat{I} - d \\hat{S} + \\gamma \\hat{I} \\\\ 0 &amp;= \\beta \\hat{S} \\hat{I} - (d + v) \\hat{I} - \\gamma \\hat{I}  \\end{aligned}\\] <p>To be systematic we could start with the first equation and solve for the first variable, \\(\\hat{S}\\), in terms of the remaining variables, \\(\\hat{I}\\). We could then sub that expression for \\(\\hat{S}\\) into the second equation, which would then be an equation for \\(\\hat{I}\\) alone. After solving for \\(\\hat{I}\\) we could then sub that solution into \\(\\hat{S}\\) and be done. But through experience we notice that there is an easier approach. </p> <p>Because the second equation is proportional to \\(\\hat{I}\\) we immediately know \\(\\hat{I}=0\\) is one potential equilibrium point. For this to work we also need the first equation to be zero. Subbing in \\(\\hat{I}=0\\) to that first equation and solving for \\(\\hat{S}\\) gives \\(\\hat{S}=\\theta/d\\). One equilibrium is therefore</p> \\[\\begin{aligned} \\hat{S} &amp;= \\theta/d \\\\ \\hat{I} &amp;= 0 \\end{aligned}\\] <p>which we call the \"disease-free\" equilibrium.</p> <p>Returning to the second equation, after factoring out \\(\\hat{I}\\) we are left with \\(0 = \\beta \\hat{S} - (d + v + \\gamma)\\), implying \\(\\hat{S} = (d + v + \\gamma)/\\beta\\). Plugging this into the first equation and solving for \\(\\hat{I}\\) we see that a second equilibrium is</p> \\[\\begin{aligned} \\hat{S} &amp;= (d + v + \\gamma)/\\beta \\\\ \\hat{I} &amp;= \\frac{\\theta - d(d + v + \\gamma)/\\beta}{d+v} \\end{aligned}\\] <p>which we call the \"endemic equilibrium\" because there is some non-zero amount of disease. Note that this equilibrium is only biologically valid when the numerator of \\(\\hat{I}\\) is positive which can be rearranged as \\(\\beta\\theta/d &gt; d + v + \\gamma\\).</p> <p>Now that we have the equilibria, the next step is to calculate the Jacobian. Letting \\(x_1=S\\) and \\(x_2=I\\) we have \\(f_1(x_1,x_2)=\\mathrm{d}S/\\mathrm{d}t\\) and \\(f_2(x_1,x_2)=\\mathrm{d}I/\\mathrm{d}t\\). The Jacobian is therefore</p> \\[\\begin{aligned} \\mathbf{J}  &amp;=  \\begin{pmatrix} \\frac{\\partial}{\\partial S}\\left(\\frac{\\mathrm{d}S}{\\mathrm{d}t}\\right) &amp; \\frac{\\partial}{\\partial I}\\left(\\frac{\\mathrm{d}S}{\\mathrm{d}t}\\right) \\\\ \\frac{\\partial}{\\partial S}\\left(\\frac{\\mathrm{d}I}{\\mathrm{d}t}\\right) &amp; \\frac{\\partial}{\\partial I}\\left(\\frac{\\mathrm{d}I}{\\mathrm{d}t}\\right) \\end{pmatrix}\\\\ &amp;= \\begin{pmatrix} -d-\\beta I &amp; -\\beta S+\\gamma \\\\ \\beta I &amp; \\beta S-(d+v+\\gamma) \\end{pmatrix} \\end{aligned}\\] <p>We can now determine the local stability of an equilibrium by evaluating the Jacobian at that equilibrium and calculating the eigenvalues. </p> <p>Let's do that first for the simpler disease-free equilibrium, where there are no infected individuals, \\(\\hat{I}=0\\), and the number of susceptibles is a balance of immigration and death, \\(\\hat{S} = \\theta/d\\). Plugging these into the Jacobian gives </p> \\[\\begin{aligned} \\mathbf{J}_\\mathrm{disease-free}  &amp;=  \\begin{pmatrix} -d &amp; -\\beta \\theta/d+\\gamma \\\\ 0 &amp; \\beta \\theta/d-(d+v+\\gamma) \\end{pmatrix} \\end{aligned}\\] <p>This is an upper triangular matrix, so the eigenvalues are just the diagonal elements, \\(\\lambda = -d, \\beta\\theta/d-(d+v+\\gamma)\\). Because all the parameters are rates they are all non-negative, and therefore the only eigenvalue that can have a positive real part (and therefore cause instability) is \\(\\lambda=\\beta\\theta/d-(d+v+\\gamma)\\). The equilibrium is unstable when this is positive, \\(\\beta\\theta/d-(d+v+\\gamma)&gt;0\\). Because this equilibrium has no infected individuals, instability in this case means the infected individuals will increase in number from rare -- ie, the disease can spread when rare. </p> <p>We can rearrange the instability condition to get a little more intuition. The disease will spread when rare whenever</p> \\[\\begin{aligned} \\beta\\theta/d - (d+v+\\gamma)&amp; &gt; 0 \\\\ \\beta\\theta/d &amp;&gt; d+v+\\gamma \\\\ \\frac{\\beta\\theta/d}{d+v+\\gamma} &amp;&gt; 1 \\end{aligned}\\] <p>The numerator is \\(\\beta\\) times the number of susceptibles at the disease-free equilibrium, \\(\\hat{S}=\\theta/d\\). This is the rate that a rare disease infects new individuals. The denominator is the rate at which the disease is removed from the population. Therefore a rare disease that infects faster than it is removed can spread. This ratio, in our case \\(\\frac{\\beta\\theta/d}{d+v+\\gamma}\\), is termed \\(R_0\\) and is a very key epidemiological quantity (you may remember estimates of \\(R_0\\) in the news from a certain recent virus...).</p> <p>Now for the endemic equilibrium. Plugging these values into the Jacobian and simplifying gives</p> \\[\\begin{aligned} \\mathbf{J}_\\mathrm{endemic}  &amp;=  \\begin{pmatrix} -\\frac{\\beta \\theta - d \\gamma}{d+v} &amp; -(d+v) \\\\ \\frac{\\beta \\theta - d (d+v+\\gamma)}{d+v} &amp; 0 \\end{pmatrix} \\end{aligned}\\] <p>Here, instead of calculating the eigenvalues explicitly, we will use the Routh-Hurwitz stability criteria for a 2x2 matrix.</p> <p>Routh-Hurwitz stability criteria for a 2x2 matrix</p> <p>When working with 2x2 matrices, there is a simple way to determine if both the eigenvalues have negative real parts (ie, if the equilibrium is stable) without having to calculate the eigenvalues themselves. These are called the Routh-Hurwitz stability criteria (and extend to larger matrices but we won't cover that here).</p> <p>Recall that for a 2x2 matrix, \\(\\mathbf{M}\\), the eigenvalues can be written </p> \\[\\lambda = \\frac{\\mathrm{Tr}(\\mathbf{M}) \\pm \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4\\mathrm{Det}(\\mathbf{M})}}{2}\\] <p>First notice that the product of the two eigenvalues is \\(\\mathrm{Det}(\\mathbf{M})\\) (you may want to check that for yourself). This means that the two eigenvalues have the same sign if and only if \\(\\mathrm{Det}(\\mathbf{M})&gt;0\\).</p> <p>Second, notice that the sum of the two eigenvalues is \\(\\mathrm{Tr}(\\mathbf{M})\\). </p> <p>We therefore know that the real parts of both eigenvalues will be negative (ie, the equilibrium will be stable) if and only if \\(\\mathrm{Det}(\\mathbf{M})&gt;0\\) and \\(\\mathrm{Tr}(\\mathbf{M})&lt;0\\).</p> <p>The determinant is \\(\\beta \\theta - d (d+v+\\gamma)\\), so for this to be positive we need \\(\\beta \\theta/d &gt; (d+v+\\gamma)\\), which was our validity condition (above) and also the instability condition on the disease-free equilibrium (\\(R_0&gt;1\\)). The trace is \\(-\\frac{\\beta \\theta - d \\gamma}{d+v}\\), so for this to be negative we need \\(\\beta \\theta/d &gt; \\gamma\\), which is guaranteed if the determinant is positive. So in conclusion, the endemic equilibrium is valid and stable whenever the disease can invade, \\(R_0&gt;1\\).</p>"},{"location":"lectures/lecture-17/","title":"Lecture 17","text":""},{"location":"lectures/lecture-17/#lecture-17-multi-locus-population-genetics","title":"Lecture 17: Multi-locus population genetics","text":"Run notes interactively?"},{"location":"lectures/lecture-17/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Multi-locus population genetics</li> </ol>"},{"location":"lectures/lecture-17/#1-multi-locus-population-genetics","title":"1. Multi-locus population genetics","text":"<p>In Lecture 15 we learned how to find equilibria and determine their stability in nonlinear multivariate models. In Lecture 16 we looked at a continuous-time example from epidemiology. Here, we'll extend the discrete-time dipliod selection model of Lecture 4 to multiple loci (see Section 8.3 in the text). </p> <p>Genomes contain many loci -- what new dynamics arise when we model more than one locus? Here we look at the simplest multi-locus model, with two loci each with two alleles. Let's denote the loci with letters, \\(A\\) and \\(B\\), and the alleles at each with numbers, \\(1\\) and \\(2\\). This gives a total of \\(2^2=4\\) haploid genotypes, which we'll give frequencies \\(x_1\\) to \\(x_4\\) like so</p> genotype frequency \\(A_1B_1\\) \\(x_1\\) \\(A_1B_2\\) \\(x_2\\) \\(A_2B_1\\) \\(x_3\\) \\(A_2B_2\\) \\(x_4\\) <p>with the constraint that the total frequency sums to one, \\(x_1+x_2+x_3+x_4 = 1\\).</p> <p>To determine how these frequencies change from one generation to the next, let's first determine the order of events in a life-cycle diagram</p> <pre><code>graph LR;\n    A((census)) --&gt; B((gamete union));\n    B --&gt; C((selection));\n    C --&gt; D((meiosis));    \n    D --&gt; A;</code></pre> <p>In words, we'll census the population in the gamete (haploid) phase while selection happens in the diploid phase.</p> <p>Next, to consider how the frequencies change through this life cyle, let's construct a life-cycle (mating) table. To do this, we need to consider what happens during meiosis when there are multiple loci. </p> <p>In the 1-locus models we analyzed earlier, meiosis meant Mendelian segregation: each allele is present in 1/2 of the gametes. Here, with 2 loci, things are slightly more complicated and we need to consider recombination. Every meiosis there is a chance, \\(r\\), of an odd number of crossover events between the two loci (\\(r\\) will increase with the distance between the loci). When this happens the pairing of the alleles at loci A and B get swapped. This only has an effect in diploid individuals that are heterozygous at both loci (\"double heterozygotes\"), here \\(A_1B_1\\) x \\(A_2B_2\\) and \\(A_1B_2\\) x \\(A_2B_1\\). Every time these individuals go through meiosis the original pairings are kept with probability \\(1-r\\) and the alternative pairings are created with probability \\(r\\). </p> <p>We can now fill in the following table</p> union frequency frequency after selection gamete frequency after meiosis (\\(A_1B_1\\), \\(A_1B_2\\), \\(A_2B_1\\), \\(A_2B_2\\)) \\(A_1B_1\\) x \\(A_1B_1\\) \\(x_1^2\\) \\(x_1^2 w_{11}/\\bar{w}\\) 1, 0, 0, 0 \\(A_1B_1\\) x \\(A_1B_2\\) \\(2x_1x_2\\) \\(2x_1x_2 w_{12}/\\bar{w}\\) 1/2, 1/2, 0, 0 \\(A_1B_1\\) x \\(A_2B_1\\) \\(2x_1x_3\\) \\(2x_1x_3 w_{13}/\\bar{w}\\) 1/2, 0, 1/2, 0 \\(A_1B_1\\) x \\(A_2B_2\\) \\(2x_1x_4\\) \\(2x_1x_4 w_{14}/\\bar{w}\\) \\((1-r)/2\\), \\(r/2\\), \\(r/2\\), \\((1-r)/2\\) \\(A_1B_2\\) x \\(A_1B_2\\) \\(x_2^2\\) \\(x_2^2 w_{22}/\\bar{w}\\) 0, 1, 0, 0 \\(A_1B_2\\) x \\(A_2B_1\\) \\(2x_2x_3\\) \\(2x_2x_3 w_{23}/\\bar{w}\\) \\(r/2\\), \\((1-r)/2\\), \\((1-r)/2\\), \\(r/2\\) \\(A_1B_2\\) x \\(A_2B_2\\) \\(2x_2x_4\\) \\(2x_2x_4 w_{24}/\\bar{w}\\) 0, 1/2, 0, 1/2 \\(A_2B_1\\) x \\(A_2B_1\\) \\(x_3^2\\) \\(x_3^2 w_{33}/\\bar{w}\\) 0, 0, 1, 0 \\(A_2B_1\\) x \\(A_2B_2\\) \\(2x_3x_4\\) \\(2x_3x_4 w_{34}/\\bar{w}\\) 0, 0, 1/2, 1/2 \\(A_2B_2\\) x \\(A_2B_2\\) \\(x_4^2\\) \\(x_4^2 w_{44}/\\bar{w}\\) 0, 0, 0, 1 <p>where \\(w_{ij}=w_{ji}\\) is the fitness of the diploid that is composed of haploid genotypes \\(i\\) and \\(j\\) and \\(\\bar{w}\\) is the population mean fitness, which is the sum of the frequencies after selection.</p> <p>We can build the recursion equations from this table by multiplying the frequency after selection by the gamete frequency after meiosis. For example, the frequency of \\(A_1B_1\\) in the next generation is found by multiplying the first entry in the final column by the frequency after selection and summing this up over rows, giving</p> \\[\\begin{align} x_1(t+1)  &amp;= x_1(t)^2 w_{11}/\\bar{w} + x_1(t)x_2(t) w_{12}/\\bar{w} + x_1(t)x_3(t) w_{13}/\\bar{w} + (1-r)x_1(t)x_4(t) w_{14}/\\bar{w} + r x_2(t)x_3(t) w_{23}/\\bar{w}\\\\ &amp;= x_1(t) (x_1(t) w_{11}/\\bar{w} + x_2(t) w_{12}/\\bar{w} + x_3(t) w_{13}/\\bar{w} + x_4(t) w_{14}/\\bar{w}) - r(x_1(t)x_4(t) w_{14}/\\bar{w} - x_2(t)x_3(t) w_{23}/\\bar{w})\\\\ &amp;= x_1(t) \\sum_{i=1}^{4} x_i(t) w_{1i}/\\bar{w} - r D^* \\end{align}\\] <p>where \\(D^*=x_1(t)x_4(t) w_{14}/\\bar{w} - x_2(t)x_3(t) w_{23}/\\bar{w}\\) is called linkage disequilibrium (the asterisk differentiates it from the same quantity measured prior to selection, \\(D=x_1(t)x_4(t) - x_2(t)x_3(t)\\)). Linkage disequilibrium is an important term in population genetics that measures the deviation of the association of alleles at two loci from that expected by chance under random assortment. For example, when \\(A_1\\) pairs with \\(B_1\\) more often than expected by chance then \\(x_1(t)x_4(t) &gt; x_2(t)x_3(t)\\) and \\(D&gt;0\\). </p> <p>The remaining equations are created in the same way, giving</p> \\[\\begin{align} x_2(t+1) &amp;= x_2(t) \\sum_{i=1}^{4} x_i(t) w_{2i}/\\bar{w} + r D^* \\\\ x_3(t+1) &amp;= x_3(t) \\sum_{i=1}^{4} x_i(t) w_{3i}/\\bar{w} + r D^* \\\\ x_4(t+1) &amp;= x_4(t) \\sum_{i=1}^{4} x_i(t) w_{4i}/\\bar{w} - r D^* \\\\ \\end{align}\\] <p>Now we have a system of recursion equations to work with. This system is nonlinear and four dimensional, which makes things relatively complex. For instance, it is impossible to find all the equilibria analytically. Here we'll not worry about that and just deal with a particularly simple equilibrium where \\(A_1B_1\\) is fixed, \\(\\hat{x}_1=1\\) and \\(\\hat{x}_2=\\hat{x}_3=\\hat{x}_4=0\\). This implies \\(\\bar{w}=w_{11}\\).</p> <p>To determine the stability of this equilbrium we need to calculate the Jacobian</p> \\[ \\mathbf{J} =  \\begin{pmatrix} \\frac{\\partial x_1(t+1)}{\\partial x_1(t)} &amp; \\frac{\\partial x_1(t+1)}{\\partial x_2(t)} &amp; \\frac{\\partial x_1(t+1)}{\\partial x_3(t)} &amp; \\frac{\\partial x_1(t+1)}{\\partial x_4(t)}\\\\ \\frac{\\partial x_2(t+1)}{\\partial x_1(t)} &amp; \\frac{\\partial x_2(t+1)}{\\partial x_2(t)} &amp; \\frac{\\partial x_2(t+1)}{\\partial x_3(t)} &amp; \\frac{\\partial x_2(t+1)}{\\partial x_4(t)}\\\\ \\frac{\\partial x_3(t+1)}{\\partial x_1(t)} &amp; \\frac{\\partial x_3(t+1)}{\\partial x_2(t)} &amp; \\frac{\\partial x_3(t+1)}{\\partial x_3(t)} &amp; \\frac{\\partial x_3(t+1)}{\\partial x_4(t)}\\\\ \\frac{\\partial x_4(t+1)}{\\partial x_1(t)} &amp; \\frac{\\partial x_4(t+1)}{\\partial x_2(t)} &amp; \\frac{\\partial x_4(t+1)}{\\partial x_3(t)} &amp; \\frac{\\partial x_4(t+1)}{\\partial x_4(t)}\\\\ \\end{pmatrix} \\] <p>(we omit writing out the derivatives for brevity) and then evaluate it at the focal equilibrium, \\(\\mathbf{J}_{x_1=1,x_2=x_3=x_4=0}\\). Local stability of the focal equilibrium is determined by the eigenvalues of this matrix, which are</p> \\[\\begin{aligned} \\lambda_1 &amp;= 0 \\\\ \\lambda_2 &amp;= w_{12}/w_{11} \\\\ \\lambda_3 &amp;= w_{13}/w_{11} \\\\ \\lambda_4 &amp;= (1-r)w_{14}/w_{11}  \\end{aligned}\\] <p>The first eigenvalue, \\(\\lambda_1 = 0\\), indicates that there is an axis along which or system does not change. This is due to the fact that the frequencies always sum to one, \\(x_1+x_2+x_3+x_4=1\\). We therefore effectively have a three dimensional model, e.g., we could just track \\(x_1\\), \\(x_2\\), and \\(x_3\\) because we know that \\(x_4 = 1 - x_1-x_2-x_3\\).</p> <p>The second and third eigenvalues, \\(\\lambda_2 = w_{12}/w_{11}\\) and \\(\\lambda_3 = w_{13}/w_{11}\\), are analogous to what we found in the 1 locus (univariate) case. Remembering that stability in discrete time requires that the eigenvalues are less than 1 in absolute value, we can interpret these eigenvalues as saying that the \\(B_2\\) allele can invade (instability) when it has higher fitness than the \\(B_1\\) allele (\\(w_{12}&gt;w_{11}\\)) and the \\(A_2\\) can invade when it has higher fitness than the \\(A_1\\) allele (\\(w_{13}&gt;w_{11}\\)). </p> <p>The fourth eigenvalue, \\(\\lambda_4 = (1-r)w_{14}/w_{11}\\), is the new part, which depends on recombination. Interestingly, here, even if \\(A_2B_2\\) has higher fitness than \\(A_1B_1\\), meaning \\(w_{14}&gt;w_{11}\\), it is possible that the \\(A_2B_2\\) genotype cannot invade. This is because, for a rare \\(A_2B_2\\) genotype, every generation it pairs with the common \\(A_1B_1\\) genotype and therefore gets broken apart into \\(A_1B_2\\) and \\(A_2B_1\\) by recombination with probability \\(r\\). In other words, recombination can hinder the spread of an adaptive combination of alleles. This is epitiomized by the scenario where having a single \"2\" allele is deleterious \\(w_{12}&lt;w_{11}\\) and \\(w_{13}&lt;w_{11}\\) (making \\(\\lambda_2&lt;1\\) and \\(\\lambda_3&lt;1\\)) but having two \"2\" alleles is beneficial, \\(w_{14}&gt;w_{11}\\). Such a scenario is called a fitness valley because of the plot below</p> <pre>\nimport matplotlib.pyplot as plt\n\nw11=1\nw12=0.9\nw13=0.8\nw14=1.1\n\nfig,ax=plt.subplots()\n\nax.plot([0,1,2],[w11,w12,w14],marker='o')\nax.plot([0,1,2],[w11,w13,w14],marker='o')\nax.text(0,w11,r'$A_1B_1$',va='bottom')\nax.text(1,w12,r'$A_1B_2$',va='top')\nax.text(1,w13,r'$A_2B_1$',va='top')\nax.text(2,w14,r'$A_2B_2$',va='bottom',ha='right')\n\nax.set_ylabel('fitness')\nax.set_xlabel('number of \"2\" alleles')\nax.set_xticks([0,1,2])\nplt.show()\n</pre> <p></p> <p>Here we've seen how recombination can slow the spread of the optimal genotype, \\(A_2B_2\\), potentially preventing fitness-valley crossing. There is also, however, a constructive aspect of recombination, not explored in this simple model: when the deleterious genotypes \\(A_1B_2\\) and \\(A_2B_1\\) are both present, there is a chance that they pair and recombine, giving rise to the optimal genotype \\(A_2B_2\\). The role of recombination in fitness-valley crossing is therefore a relatively interesting and complex problem.</p>"},{"location":"lectures/lecture-18/","title":"Lecture 18","text":""},{"location":"lectures/lecture-18/#lecture-18-evolutionary-invasion-analysis","title":"Lecture 18: Evolutionary invasion analysis","text":"Run notes interactively?"},{"location":"lectures/lecture-18/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Invasion fitness</li> <li>Evolutionarily singular strategies</li> <li>Evolutionarily stable strategies</li> <li>Evolutionary convergence</li> <li>Pairwise invasibility plots</li> </ol> <p>In the models we've discussed so far we've taken the parameters to be fixed. In reality, many of these parameters can evolve. For example, in our model of exponential growth in discrete time, \\(n(t+1)=n(t) R\\), we took \\(R\\) to be the same for all individuals for all time. But any mutation causing a larger \\(R\\) would increase in frequency, causing the value of \\(R\\) to increase over time. In this lecture we'll explore how to determine the direction of evolution and the stability of evolutionary endpoints for more complex models using a technique called evolutionary invasion analysis (also known as adaptive dynamics).</p> <p>The idea:</p> <ul> <li>determine which parameter(s) of our model an evolving trait affects</li> <li>take the population to be fixed for some \"resident\" trait value</li> <li>determine the equilibria and stability of the system with only the resident trait</li> <li>derive the growth rate of a new individual with a \"mutant\" trait value</li> <li>ask when the mutant trait value will invade</li> <li>look for potential evolutionary endpoints</li> <li>determine the stability of those endpoints</li> </ul> <p>The evolution of dispersal</p> <p>To motivate evolutionary invasion analysis, let's consider the evolution of dispersal. </p> <p>Imagine there are \\(S\\) sites, with a most one individual reproducing at each. We census the population at the time of reproduction. A reproducing individual has a large number \\(B\\) offspring and then dies. A fraction \\(d\\) of those offspring disperse and a fraction \\(1-c\\) of those survive. The survivors then equally divided among all sites. One individual in each site is then chosen at random to reproduce, which begins the life-cycle anew.</p> <p>The question is, how should dispersal, \\(d\\), evolve? There is a cost, \\(c\\), which selects against dispersal but dispersal also allows offspring to avoid competiting with their kin, which could select for more dispersal. We'll use evolutionary invasion analysis to sort this out.</p> <p></p>"},{"location":"lectures/lecture-18/#1-invasion-fitness","title":"1. Invasion fitness","text":"<p>Let's think about this analysis very generally (in discrete time).</p> <p>Let the number of individuals with the resident trait value be \\(n\\) and the number of individuals with the mutant trait value \\(n_m\\). (And we'll assume asexual reproduction for simplicity, so that residents produce residents and mutant produce mutants.) Let the potentially nonlinear dynamics of these two groups of individuals depend on their respective trait values, \\(z\\) and \\(z_m\\),</p> \\[ \\begin{aligned} n(t+1) &amp;= n(t) R(n(t), n_m(t), z, z_m)\\\\ n_m(t+1) &amp;= n_m(t) R_m(n(t), n_m(t), z, z_m) \\end{aligned} \\] <p>The Jacobian of this system is</p> \\[ \\begin{aligned} \\mathbf{J} &amp;=  \\begin{pmatrix} \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)} &amp; \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n_m(t)} \\\\ \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n(t)} &amp; \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)}  \\end{pmatrix} \\end{aligned} \\] <p>Now consider some non-zero resident equilibrium, \\(\\hat{n}&gt;0\\), without the mutant, \\(\\hat{n}_m=0\\). Assuming that the resident does not produce mutants, \\(\\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n(t)}\\big|_{n_m=0}=0\\), the Jacobian evaluated at this equilibrium simplifies to</p> \\[ \\begin{aligned} \\mathbf{J}\\big|_{n_m=0,n=\\hat{n}} &amp;=  \\begin{pmatrix} \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)} &amp; \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n_m(t)} \\\\ 0 &amp; \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)}  \\end{pmatrix}_{n_m=0,n=\\hat{n}} \\end{aligned} \\] <p>We can immediately see that the two eigenvalues of this upper triangular matrix are</p> \\[ \\begin{aligned} \\lambda_1 &amp;= \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)}\\bigg|_{n_m=0,n=\\hat{n}}\\\\ \\lambda_2 &amp;= \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)}\\bigg|_{n_m=0,n=\\hat{n}} \\end{aligned} \\] <p>The first, \\(\\lambda_1\\), determines whether the resident equilibrium, \\(\\hat{n}&gt;0\\), is stable in the absence of mutants. We'll take \\(0 &lt; \\lambda_1 &lt; 1\\) as a given (we're only interested in stable resident equilibria, we can disregard the others).</p> <p>The second, \\(\\lambda_2\\), determines whether the resident equilibrium is stable in the presence of a small number of mutants. We call \\(\\lambda_2\\) the invasion fitness, \\(\\lambda(z_m,z)\\). The mutant will invade whenever \\(\\lambda(z_m,z) &gt; 1\\).</p> <p>In some simple cases we might be able to use the invasion criterium, \\(\\lambda(z_m,z)&gt;1\\), to determine what values of \\(z_m\\) (relative to \\(z\\)) can invade. In most cases, however, \\(\\lambda(z_m,z)\\) will be complex enough that this will not be possible, so we rely on a simple approximation.</p> <p>The evolution of dispersal</p> <p>Let a fraction \\(d\\) of the resident offspring disperse and a fraction \\(d_m\\) of the mutant offspring disperse. And let there be \\(n(t)\\) residents, \\(n_m(t)\\) mutants, and \\(S-n(t)-n_m(t)\\geq0\\) empty sites. Then the probability a resident offspring replaces a resident is the number of resident offspring in a resident patch divided by the total number of offspring in that patch,</p> \\[ \\begin{aligned} p_{rr} &amp;= \\frac{B(1-d) + (n(t)-1)Bd(1-c)/S}{B(1-d) + (n(t)-1)Bd(1-c)/S + n_m(t)Bd_m(1-c)/S}\\\\ &amp;= \\frac{S(1-d) + (n(t)-1)d(1-c)}{S(1-d) + (n(t)-1)d(1-c) + n_m(t)d_m(1-c)} \\end{aligned} \\] <p>Here \\(B(1-d)\\) is the number of non-dispersing offspring produced by the resident in that patch, \\((n(t)-1)Bd(1-c)/S\\) is the number of resident offspring dispersing to the patch from elsewhere, and \\(n_m(t)Bd_m(1-c)/S\\) is the number of mutant offspring dispersing to the patch. The probability that a mutant offspring replaces a mutant \\(p_{mm}\\) is the same expression with \\(d\\) and \\(d_m\\) and \\(n\\) and \\(n_m\\) exchanged. The probability that a resident offspring wins an empty patch is </p> \\[ \\begin{aligned} p_{re} &amp;= \\frac{n(t)Bd(1-c)}{n(t)Bd(1-c) + n_m(t)Bd_m(1-c)}\\\\ &amp;= \\frac{n(t)d}{n(t)d + n_m(t)d_m} \\end{aligned} \\] <p>Then the number of resident and mutant individuals in the next generation are</p> \\[ \\begin{aligned} n(t+1) &amp;= n(t) p_{rr} + n_m(t) (1 - p_{mm}) + (S - n(t) - n_m(t)) p_{re}\\\\ n_m(t+1) &amp;= n(t) (1-p_{rr}) + n_m(t) p_{mm} + (S - n(t) - n_m(t)) (1-p_{re}) \\end{aligned} \\] <p>Now consider an equilibrium with no mutants, \\(\\hat{n}_m=0\\). Setting \\(n(t+1) = n(t) = \\hat{n}\\) gives \\(\\hat{n}=S\\).</p> <p>We determine the stability of this equilibrium with the Jacobian. From the general analysis above we know the two eigenvalues are</p> \\[ \\begin{aligned} \\lambda_1 &amp;= \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)}\\bigg|_{n_m=0,n=S} = 0\\\\ \\lambda_2 &amp;= \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)}\\bigg|_{n_m=0,n=S} = \\frac{1-d_m}{1-d_m + d(1-c)} + \\frac{d_m(1-c)}{1-d + d(1-c)} \\end{aligned} \\] <p>The first eigenvalue is less than 1 in absolute value, meaning the resident equilibrium is stable. The second eigenvalue is our invasion fitness</p> \\[ \\lambda(d_m,d) = \\frac{1-d_m}{1-d_m + d(1-c)} + \\frac{d_m(1-c)}{1-d + d(1-c)} \\] <p>In this relatively simple example, we can determine exactly what mutant trait values can invade by looking at where</p> \\[ \\lambda(d_m,d) - 1 = \\frac{(1-c)(d_m-d)(1-cd-d_m)}{(1-cd)(1-d_m+d(1-c))} \\] <p>is positive. The sign of \\(\\lambda(d_m,d) - 1\\) is the sign of \\((d_m-d)(1-cd-d_m)\\). So the mutant will invade if both terms are positive \\(d&lt;d_m&lt;1-cd\\) or both terms are negative \\(1-cd&lt;d_m&lt;d\\). In either case, the mutant will invade when it has a trait value between \\(1-cd\\) and \\(d\\).</p> <p></p>"},{"location":"lectures/lecture-18/#2-evolutionarily-singular-strategies","title":"2. Evolutionarily singular strategies","text":"<p>When the mutant trait value is very close to the resident trait value, we can approximate invasion fitness with a Taylor series around \\(z_m = z\\)</p> \\[ \\begin{aligned} \\lambda(z_m,z) &amp;\\approx \\lambda(z,z) + \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=z} (z_m-z)\\\\ &amp;= 1 + \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=z} (z_m-z) \\end{aligned} \\] <p>This allows us to determine which direction evolution will proceed from the current resident value:</p> <ul> <li>if \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}&gt;0\\) then invasion when \\(z_m&gt;z\\)</li> <li>if \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}&lt;0\\) then invasion when \\(z_m&lt;z\\)</li> </ul> <p>The direction of evolution by small steps is given by \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}\\), which we call the selection gradient.</p> <p>Potential evolutionary endpoints, also called evolutionarily singular strategies, are the resident trait values \\(z=\\hat{z}\\) where there is no directional selection</p> \\[ \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=\\hat{z}, z=\\hat{z}} = 0 \\] <p>The evolution of dispersal</p> <p>The selection gradient is</p> \\[ \\frac{\\partial \\lambda}{\\partial d_m}\\Big|_{d_m=d} = \\frac{(1-d-dc))(1-c)}{(1-cd)^2} \\] <p>The sign of this is the sign of 1-d-dc, meaning the dispersal rate will increase whenever d&lt;1/(1+c) and decrease whenever 1/(1+c)&lt;d. Setting the selection gradient to zero and solving for the evolutionarily singular strategy gives \\(\\hat{d} = 1/(1+c)\\).</p> <p></p>"},{"location":"lectures/lecture-18/#3-evolutionarily-stable-strategies","title":"3. Evolutionarily stable strategies","text":"<p>An evolutionarily singular strategy, \\(\\hat{z}\\), will only be an evolutionarily stable strategy (ESS), \\(z^*\\), if it cannot be invaded.</p> <p>Global evolutionary stability will be impossible to prove for most models and so we often focus on local stability, which requires that \\(\\lambda(z_m,z)|_{z=\\hat{z}}\\) is concave at \\(z_m=\\hat{z}\\) (i.e., \\(\\hat{z}\\) is a local fitness maximum),</p> \\[ \\frac{\\partial^2 \\lambda}{\\partial z_m^2}\\bigg|_{z_m=\\hat{z}, z=\\hat{z}} &lt; 0 \\] <p>The evolution of dispersal</p> <p>The second derivative of invasion fitness with respect to the mutant trait value evalulated at the singular strategy is</p> \\[ \\frac{\\partial^2 \\lambda}{\\partial d_m^2}\\Big|_{d_m=d=\\hat{d}} = -2(1-c)(1+c)^2 \\] <p>Because \\(0&lt;c&lt;1\\) this is always negative, which means the singular strategy is always evolutionarily stable.</p> <p></p>"},{"location":"lectures/lecture-18/#4-evolutionary-convergence","title":"4. Evolutionary convergence","text":"<p>There is one more characteristic of evolutionarily singular strategies that we care about, and that is whether evolution actually leads to that strategy or not. For evolution to move the trait value towards a singular strategy, \\(\\hat{z}\\), we need evolution to increase the trait value when it is less than \\(\\hat{z}\\) and decrease the trait value when it is greater than \\(\\hat{z}\\). In other words, we need the selection gradient \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}\\) to decrease as we move through \\(z=\\hat{z}\\)</p> \\[ \\frac{\\mathrm{d}}{\\mathrm{d} z}\\left( \\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z} \\right)_{z=\\hat{z}} &lt; 0 \\] <p>Singular strategies that satisfy this criteria are said to be convergence stable. </p> <p>Interestingly, not all evolutionarily stable strategies are convergence stable and not all convergence stable singular strategies are evolutionarily stable! Evolutionarily stable strategies that are not convergence stable are called Garden of Eden strategies. Singular strategies that are convergence stable but not evolutionarily stable are called evolutionary branching points. The latter are of particular interest because the system evolves towards a state where multiple strategies can invade and coexist, leading to diversification.</p> <p>The evolution of dispersal</p> <p>The derivative of the selection gradient evaluated at the singular strategy is</p> \\[ \\frac{\\mathrm{d}}{\\mathrm{d} d}\\left( \\frac{\\partial \\lambda}{\\partial d_m}\\Big|_{d_m=d} \\right)_{d=\\hat{d}} = -(1-c)(1+c)^3 \\] <p>Because \\(0&lt;c&lt;1\\) this is always negative, which means the singular strategy is always convergence stable.</p> <p></p>"},{"location":"lectures/lecture-18/#5-pairwise-invasibility-plots","title":"5. Pairwise invasibility plots","text":"<p>A helpful way to visualize the two types of stability of an evolutionarily singular strategy is called a pairwise invasibility plot (PIP). In this plot we have the resident trait value \\(z\\) on the x-axis, the mutant trait value \\(z_m\\) on the y-axis, and we color in the regions where the mutant can invade, \\(\\lambda(z_m,z)&gt;1\\).</p> <p>The four types of evolutionarily singular strategies \\(\\hat{z}\\) are then represented by the following PIPs</p> <pre>\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# dummy invasion fitness\ndef inv(x, y, slope, z=1):\n    if y &lt; x:\n      if y &gt; slope * x:\n        return z\n    if y &gt; x:\n      if y &lt; slope * x:\n        return z\n    return 1-z\n\n# evaluate\ndef compute_pip(slope=-2,z=1,xmin=-2,xmax=2,steps=100):\n    xs = np.linspace(xmin,xmax,steps)\n    X,Y = np.meshgrid(xs,xs) # X and Y values\n    # store the invasion success in a matrix\n    PIP = []\n    for y in xs:\n      row = []\n      for x in xs:\n        row.append(inv(x,y,slope,z))\n      PIP.append(row)\n    return X,Y,PIP\n\n# plot\ndef plotfun(X,Y,Z,slope=1,ax=None):\n    if ax==None:\n      fig, ax=plt.subplots(1,1,figsize=(5,5))\n    ax.contourf(X,Y,Z, colors=['white','black','blue','black'])\n    ax.plot(X[0],X[0],'k',lw=5)\n    ax.plot(X[0],X[0]*slope,'k',lw=5)\n    ax.set_xlim(min(X[0]),max(X[0]))\n    ax.set_ylim(min(X[0]),max(X[0]))\n    ax.set_xlabel('resident trait value $z$')\n    ax.set_ylabel('mutant trait value $z_m$')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig, axs = plt.subplots(2,2,figsize=(10,10))\nX,Y,Z = compute_pip(slope=-4,z=1)\nplotfun(X,Y,Z,slope=-4,ax=axs[0][0])\naxs[0][0].set_title('evolutionarily stable',fontsize=14)\naxs[0][1].text(2.1,0,'convergence stable',fontsize=14,rotation=270,verticalalignment='center')\nX,Y,Z = compute_pip(slope=4,z=0)\nplotfun(X,Y,Z,slope=4,ax=axs[0][1])\naxs[0][1].set_title('evolutionarily unstable',fontsize=14)\nX,Y,Z = compute_pip(slope=4,z=1)\nplotfun(X,Y,Z,slope=4,ax=axs[1][0])\naxs[1][1].text(2.1,0,'convergence unstable',fontsize=14,rotation=270,verticalalignment='center')\nX,Y,Z = compute_pip(slope=-4,z=0)\nplotfun(X,Y,Z,slope=-4,ax=axs[1][1])\n</pre> <p></p> <p>We can read a PIP by choosing a resident trait value (a point on the x-axis) and looking to see what mutant trait values can invade it (blue regions in that vertical slice). Choose one of the possible invading trait values and set this to be the new resident trait value. Continue indefinitely. </p> <p>When we assume mutants have trait values close to the resident, we restrict ourselves to moving along the 1:1 line. Then, we move to the right when there is blue directly above the 1:1 line but not below and we move to the left when there is blue directly below the 1:1 line but not above. Where there is blue directly above and below the 1:1 we are at a singular strategy that is a fitness minimum (it can be invaded in both directions). Where there is white directly above and below the 1:1 we are at a singular strategy that is a fitness maximum (it can't be invaded in either direction).</p> <p>Try reading each of the plots above. Prove to yourself that the top left has a convergence stable evolutionarily stable strategy, the top right has a branching point, the bottom left has a has a Garden of Eden, and the bottom right has an invasible repellor (a fitness minimum that is not convergence stable).</p> <p>The evolution of dispersal</p> <p>In our model of the evolution of dispersal, the singular strategy is always evolutionarily and convgence stable, and the PIP is below.</p> <pre>\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# invasion fitness\ndef inv_fun(dm,d,c):\n    invasionfitness = (1-dm)/(1-dm + d*(1-c)) + (dm*(1-c))/(1-d + d*(1-c))\n    if invasionfitness&gt;1:\n        return 1 # return 1 if mutant invades\n    return 0 # return 0 if mutant does not invade\n\n# evaluate\ndef compute_pip(c=0.5,dmin=0.01,dmax=0.99,steps=100):\n    ds = np.linspace(dmin,dmax,steps)\n    X,Y = np.meshgrid(ds,ds) # X and Y values\n    # store the invasion success in a matrix\n    PIP = []\n    for dm in ds:\n      row = []\n      for d in ds:\n        row.append(inv_fun(dm,d,c))\n      PIP.append(row)\n    return X,Y,PIP\n\n# plot\ndef plotfun(X,Y,Z):\n    fig, ax = plt.subplots(1,1,figsize=(5,5))\n    ax.set_xlabel('$d$')\n    ax.set_ylabel('$d_m$')\n    ax.contourf(X,Y,Z, colors=['white','black','blue','black'])\n    ax.plot(X[0],X[0],'k',lw=5)\n\nX,Y,Z=compute_pip()\nplotfun(X,Y,Z)\n</pre> <p></p>"},{"location":"lectures/lecture-19/","title":"Lecture 19","text":""},{"location":"lectures/lecture-19/#lecture-19-the-evolution-of-dominance","title":"Lecture 19: The evolution of dominance","text":"Run notes interactively?"},{"location":"lectures/lecture-19/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Model</li> <li>Resident equilibrium</li> <li>Mutant invasion</li> </ol> <p>So far we've considered invasion into an unstructured population, i.e., there is just one type of resident and one type of mutant. We can extend this analysis to consider structured populations, age- or spatial-structure for example. The general approach is the same: find a stable resident equilibria and evaluate the Jacobian at that equilibrium to determine the mutant's invasion fitness. The only difference is that our Jacobian is no longer a 2x2 matrix. </p> <p>Below we will demonstrate the general method with a specific example: the evolution of dominance. The motivating observation is that many deleterious alleles are recessive, meaning the wild-type allele partially shields a heterozygote individual from selection. There are many examples in humans, such as sickle cell anemia and cystic fibrosis. In these examples the fitness of heterozygous individuals is indistinguishable from those without any deleterious mutations. Could the dominance of a wild-type allele over a deleterious allele be the result of adaptive evolution? To ask this question we perform an evolutionary invasion analysis on a population with genetic structure. This type of model is often referred to as a modifier model within the field of population genetics.</p> <p></p>"},{"location":"lectures/lecture-19/#1-model","title":"1. Model","text":"<p>The model is an extension of the 2-locus model we developed in lecture 17 to include mutation. We consider two loci each with two alleles. At one locus we have alleles \\(A_1\\) and \\(A_2\\). We'll treat \\(A_1\\) as the wild-type allele and \\(A_2\\) as the deleterious allele and refer to this \\(A\\) locus as the selected locus. At the other locus we have alleles \\(B_1\\) and \\(B_2\\). We'll treat \\(B_1\\) as the resident allele and \\(B_2\\) as the mutant (modifier) and refer to this \\(B\\) locus as the modifier locus. The analysis will determine when \\(B_2\\) can invade. </p> <p>We consider diploid selection. Let the relative fitness of any individual with \\(A_1 A_1\\) be 1 and the relative fitness of any individual with \\(A_2 A_2\\) be \\(1-s\\), with \\(0&lt;s&lt;1\\). The relative fitnesses of the \\(A_1 A_2\\) heterozygotes are affected by their genotype at the \\(B\\) locus:</p> diploid genotype relative fitness \\(A_1A_2\\) \\(B_1B_1\\) \\(1 - h_{11}s\\) \\(A_1A_2\\) \\(B_1B_2\\) \\(1 - h_{12}s\\) \\(A_1A_2\\) \\(B_2B_2\\) \\(1 - h_{22}s\\) <p>We census the population in the haploid phase and keep track of the 4 haploid genotype frequencies:</p> genotype frequency \\(A_1B_1\\) \\(x_1\\) \\(A_1B_2\\) \\(x_2\\) \\(A_2B_1\\) \\(x_3\\) \\(A_2B_2\\) \\(x_4\\) <p>with the constraint that the total frequency sums to one, \\(x_1+x_2+x_3+x_4 = 1\\).</p> <p>We treat time as discrete and assume the following life-cycle:</p> <p> <pre><code>graph LR;\n    A((census)) --&gt; B((gamete union));\n    B --&gt; C((selection));\n    C --&gt; D((meiosis/recombination));    \n    D --&gt; E((mutation));\n    E --&gt; A</code></pre> </p> <p>After selection but before mutation the frequencies are as they were in lecture 17,</p> \\[\\begin{align} x_1' &amp;= x_1(t) \\sum_{i=1}^{4} x_i(t) w_{1i}/\\bar{w} - r D^* \\\\ x_2' &amp;= x_2(t) \\sum_{i=1}^{4} x_i(t) w_{2i}/\\bar{w} + r D^* \\\\ x_3' &amp;= x_3(t) \\sum_{i=1}^{4} x_i(t) w_{3i}/\\bar{w} + r D^* \\\\ x_4' &amp;= x_4(t) \\sum_{i=1}^{4} x_i(t) w_{4i}/\\bar{w} - r D^*, \\end{align}\\] <p>where \\(w_{ij}=w_{ji}\\) is the fitness of the diploid that is composed of haploid genotypes \\(i\\) and \\(j\\), \\(\\bar{w}\\) is the population mean fitness, and \\(D^*=x_1(t)x_4(t) w_{14}/\\bar{w} - x_2(t)x_3(t) w_{23}/\\bar{w}\\) is linkage disequilibrium after selection.</p> <p>We assume \\(A_1\\) mutates to \\(A_2\\) with probability \\(\\mu\\) and ignore back mutations (a relatively safe assumption given that the deleterious \\(A_2\\) allele will be rare and therefore will create few mutants). The recursion equations are then</p> \\[\\begin{align} x_1(t+1) &amp;= (1-\\mu)x_1'\\\\ x_2(t+1) &amp;= (1-\\mu)x_2'\\\\ x_3(t+1) &amp;= x3' + \\mu x_1'\\\\ x_4(t+1) &amp;= x4' + \\mu x_2'. \\end{align}\\] <p></p>"},{"location":"lectures/lecture-19/#2-resident-equilibrium","title":"2. Resident equilibrium","text":"<p>We cannot find all equilibria of this system of nonlinear equations, but we can find some. Here we are most interested in the \"resident\" equilibrium where there are no \\(B_2\\) alleles, \\(\\hat{x}_2=\\hat{x}_4=0\\), which reduces us back to a one locus model. We'll also take \\(h_{11}=1/2\\) for simplicity, meaning that the resident \\(B_1\\) allele makes the \\(A\\) locus additive. Then there is a relatively simple resident equilibrium, </p> \\[ \\begin{align} \\hat{x}_1 = 1 - \\frac{\\mu}{(1+\\mu)s/2} \\\\ \\hat{x}_3 = \\frac{\\mu}{(1+\\mu)s/2}, \\end{align} \\] <p>which says that the deleterious \\(A_2\\) allele is maintained at a balance between its removal by selection and its generation by mutation. This equilibrium is valid as long as \\(\\mu&lt;(1+\\mu)s/2\\), which means mutation is weak relative to selection.</p> <p></p>"},{"location":"lectures/lecture-19/#3-mutant-invasion","title":"3. Mutant invasion","text":"<p>To evaluate the stability of this equilibrium we use the Jacobian evaluated at the resident equilibrium. Because we have 4 equations this is a 4x4 matrix. But if we arrange the equations in the following order, \\(x_1(t+1), x_3(t+1), x_2(t+1), x_4(t+1)\\), then the Jacobian at the resident equilibrium can be written as an upper triangular block matrix, </p> \\[ \\mathbf{J}|_{x_1=\\hat{x}_1,x_3=\\hat{x}_3,x_3=x_4=0} = \\begin{bmatrix} \\mathbf{J}_\\mathrm{res} &amp; \\mathbf{V} \\\\ \\mathbf{0} &amp; \\mathbf{J}_\\mathrm{mut}\\end{bmatrix}. \\] <p>Here \\(\\mathbf{J}_\\mathrm{res}\\) describes the stability of the resident equilibrium in the absence of any \\(B_2\\) alleles and \\(\\mathbf{J}_\\mathrm{mut}\\) describes the stability of the resident equilibrium in the face of rare \\(B_2\\) alleles. One can show that the eigenvalues of \\(\\mathbf{J}_\\mathrm{res}\\) are always less than 1 when the resident equilibrium is valid, guaranteeing stability. </p> <p>We want to know whether \\(B_2\\) alleles can invade the resident equilibrium. This is determined by the leading eigenvalue of \\(\\mathbf{J}_\\mathrm{mut}\\), which we call the invasion fitness. Unfortunately the eigenvalues are a little complicated, but we can make some progress with a little trick. The equation for the eigenvalues of \\(\\mathbf{J}_\\mathrm{mut}\\) is </p> \\[ \\lambda^2 - \\mathrm{Tr}(\\mathbf{J}_\\mathrm{mut}) \\lambda + \\mathrm{Det}(\\mathbf{J}_\\mathrm{mut}) = 0. \\] <p>Now, given that \\(\\lambda\\) is the invasion fitness of a mutant with dominance coefficient \\(h_{12}\\) in a resident population with dominance coefficient \\(h_{11}=1/2\\), the selection gradient is \\(\\frac{\\partial\\lambda}{\\partial h_{12}}|_{h_{12}=1/2}\\). The trick is that we can get the selection gradient directly from the equation above without solving for invasion fitness. To make the dependence on \\(h_{12}\\) clear and simplify the notation, write \\(\\lambda=\\lambda(h_{12})\\), \\(-\\mathrm{Tr}(\\mathbf{J}_\\mathrm{mut})=a(h_{12})\\), and \\(\\mathrm{Det}(\\mathbf{J}_\\mathrm{mut})=b(h_{12})\\). We can then differentiate the equation above with respect to \\(h_{12}\\) and rearrange for the selection gradient (this is called implicit differentiation),</p> \\[ \\begin{align} \\frac{\\partial \\lambda^2}{\\partial h_{12}} + \\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) + a(h_{12}) \\frac{\\partial \\lambda}{\\partial h_{12}} + \\frac{\\partial b}{\\partial h_{12}} &amp;= 0 \\\\ 2\\lambda(h_{12})\\frac{\\partial \\lambda}{\\partial h_{12}} + \\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) + a(h_{12}) \\frac{\\partial \\lambda}{\\partial h_{12}} + \\frac{\\partial b}{\\partial h_{12}} &amp;= 0 \\\\ \\frac{\\partial \\lambda}{\\partial h_{12}}(2\\lambda(h_{12}) + a(h_{12})) &amp;= - \\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) - \\frac{\\partial b}{\\partial h_{12}} \\\\ \\frac{\\partial \\lambda}{\\partial h_{12}} &amp;= - \\frac{\\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) + \\frac{\\partial b}{\\partial h_{12}}}{2\\lambda(h_{12}) + a(h_{12})} \\end{align} \\] <p>We then evaluate at \\(h_{12}=1/2\\), which is where the mutant is equivalent to the resident, i.e., \\(\\lambda(1/2)=1\\),</p> \\[ \\begin{align} \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &amp;= - \\left(\\frac{\\frac{\\partial a}{\\partial h_{12}}\\lambda(h_{12}) + \\frac{\\partial b}{\\partial h_{12}}}{2\\lambda(h_{12})+a(h_{12})}\\right)_{h_{12}=1/2} \\\\ \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &amp;= - \\left(\\frac{\\left.\\frac{\\partial a}{\\partial h_{12}}\\right|_{h_{12}=1/2}\\lambda(1/2) + \\left.\\frac{\\partial b}{\\partial h_{12}}\\right|_{h_{12}=1/2}}{2\\lambda(1/2)+a(1/2)}\\right) \\\\ \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &amp;= - \\left(\\frac{\\left.\\frac{\\partial a}{\\partial h_{12}}\\right|_{h_{12}=1/2} + \\left.\\frac{\\partial b}{\\partial h_{12}}\\right|_{h_{12}=1/2}}{2+a(1/2)}\\right) \\\\ \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &amp;= - \\frac{2(4r + s(1-2r))(s(1+\\mu)-2\\mu)\\mu}{s(s(1+\\mu) - 2\\mu + r(2-s)(1-\\mu^2))}. \\end{align} \\] <p>Given biological validity, \\(\\mu&lt;(1+\\mu)s/2\\), this is always negative (recall that \\(r\\leq 1/2\\)). Therefore there is always selection to reduce the dominance coefficient, \\(h\\). This means that selection could be responsible for making deleterious mutations recesive! However, there is a massive caveat. While the selection gradient is negative it is also proportional to \\(\\mu\\), which is tiny. This means that selection for recessive deleterious mutations is exceptionaly weak and so is easily overwhelmed by other forces (eg, genetic drift, migration, etc). The biological reason for such weak selection is that selection only acts on the variation that is present, and deleterious mutations are very rare at mutation-selection balance, \\(\\mu/((1+\\mu)s/2)\\).  </p>"},{"location":"lectures/lecture-20/","title":"Lecture 20","text":""},{"location":"lectures/lecture-20/#lecture-20-probability-i-genetic-drift","title":"Lecture 20: Probability I (genetic drift)","text":"Run notes interactively?"},{"location":"lectures/lecture-20/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Discrete random variables</li> <li>Expectation of a discrete random variable</li> <li>Variance of a discrete random variable</li> <li>Independence</li> <li>Binomial random variable</li> <li>Genetic drift</li> </ol> <p>credits</p> <p>This lecture was created by PhD student Puneeth Deraje as part of a course development TA position -- thanks Puneeth! If you are following along with the text, this lecture does not follow along as closely.</p> <p>Until now we have been dealing with deterministic models, i.e., given the value of the variables at time \\(t\\), we know exactly what the values will be at the next time. However, life is a little bit more random than that. Stochasticity (i.e., chance) is inherent in nature and can significantly alter the outcomes. In order to capture the effect of this stochasticity, we need some tools from probability theory.</p> <p></p>"},{"location":"lectures/lecture-20/#1-discrete-random-variables","title":"1. Discrete random variables","text":""},{"location":"lectures/lecture-20/#example-1-a-fair-coin-toss","title":"Example 1: a fair coin toss","text":"<p>Let's toss a toonie. If you get a heads, you use that toonie to buy a chocolate, otherwise you put the toonie in a piggy bank for grad school :P. Let \\(X\\) denote the number of chocolates you have after the coin toss. Then \\(X\\) can be equal to 0 or 1.  </p> <p>In this case, tossing the coin is called an event and \\(X\\) is the random variable that records its outcome. The state space is the set of all outcomes, {0,1}. We call \\(X\\) a discrete random variable because there are a finite number of outcomes in the state space.</p> <p>We don't know the exact value of \\(X\\) until you actually toss the coin (i.e., until the event happens) -- we can not predict the outcome of the coin toss in that way. However, what we do know is that with a fair coin there is an equal chance of getting either a heads or tails. In other words, if we were to toss the coin a large number of times, say a million times, roughly half a million times we will see tails and the other half a million times we will see heads. </p> <p>Therefore, we say that \\(X\\) takes the value 0 with probability \\(\\frac{500000}{1000000}=0.5\\) and 1 with probability \\(\\frac{500000}{1000000}=0.5\\). We can write this as </p> \\[\\begin{aligned} \\Pr(X=0) &amp;= 0.5\\\\ \\Pr(X=1) &amp;= 0.5  \\end{aligned}\\] <p>Note that the sum of the probabilities of a random variable taking a value, over all values in the state space, is 1. Here, \\(\\Pr(X=0) + \\Pr(X=1) = 1\\).</p> <p>Note</p> <p>This way of thinking about the probability of a random variable \\(X\\) taking a value \\(x\\), as the fraction (frequency) of events where \\(X=x\\), is the frequentist interpretation of probability. In order for the frequency to match the probability you need to repeat the event a large number of times (ideally infinitely). </p>"},{"location":"lectures/lecture-20/#example-2-a-biased-coin-toss","title":"Example 2: a biased coin toss","text":"<p>Consider the same situation as above but with a biased toonie, i.e., one that shows heads in not half but in a fraction \\(p\\) of a very large number of events. Then</p> \\[\\begin{aligned} \\Pr(X=0) &amp;= 1-p \\\\ \\Pr(X=1) &amp;= p \\\\ \\end{aligned}\\] <p>This random variable \\(X\\) is called a Bernoulli random variable with parameter \\(p\\) and is denoted as \\(X\\sim\\mathrm{Ber}(p)\\). Above, with a fair coin, \\(X\\sim\\mathrm{Ber}(1/2)\\).</p>"},{"location":"lectures/lecture-20/#example-3-a-fair-die-roll","title":"Example 3: a fair die roll","text":"<p>Now suppose you roll a fair 6-sided die to decide how many chocolates you will buy (this is nice because you never get 0!). Then, if \\(X\\) denotes the number of chocolates, we have </p> \\[\\begin{aligned} \\Pr(X=1) &amp;= \\frac{1}{6}\\\\ \\Pr(X=2) &amp;= \\frac{1}{6} \\\\ \\Pr(X=3) &amp;= \\frac{1}{6}  \\\\ \\Pr(X=4) &amp;= \\frac{1}{6} \\\\ \\Pr(X=5) &amp;= \\frac{1}{6}  \\\\ \\Pr(X=6) &amp;= \\frac{1}{6}   \\end{aligned}\\] <p>The state space here is {1,2,3,4,5,6} and \\(\\sum_{x=1}^6 \\Pr(X=x) = 1\\).</p> <p></p>"},{"location":"lectures/lecture-20/#2-expectation-of-a-discrete-random-variable","title":"2. Expectation of a discrete random variable","text":"<p>Given this way of thinking about stochasticity, one might be interested in questions like \"What is the value of \\(X\\), on average?\" and \"How certain can I be about the outcome of \\(X\\)?\". </p> <p>In order to answer these questions we need to define two important properties of a random variable, its expectation and variance.</p> <p>The expectation of a random variable is the average value of outcomes if the event is repeated infinitely many times. </p>"},{"location":"lectures/lecture-20/#example-1-a-fair-coin-toss_1","title":"Example 1: a fair coin toss","text":"<p>Suppose we toss the fair coin a million times. By the frequentist definition of probability we should see tails (\\(X=0\\)) half a million times and heads (\\(X=1\\)) the other half a million times. The expected value of \\(X\\) is then</p> \\[\\begin{aligned} \\mathbb{E}(X) &amp;= \\frac{\\overbrace{0+0+...+0}^{\\text{0.5 million times}}+\\overbrace{1+1+...+1}^{\\text{0.5 million times}}}{\\text{1 million}}\\\\ &amp;= \\frac{ 0 \\times \\text{0.5 million} + 1 \\times \\text{0.5 million} }{\\text{1 million}}\\\\ &amp;= 0 \\times \\frac{\\text{0.5 million}}{\\text{1 million}} + 1 \\times \\frac{\\text{0.5 million}}{\\text{1 million}}\\\\ &amp;= 0 \\times (1/2) + 1 \\times (1/2) \\\\ &amp;= 0.5 \\end{aligned}\\]"},{"location":"lectures/lecture-20/#example-2-a-biased-coin-toss_1","title":"Example 2 : a biased coin toss","text":"<p>Suppose we repeat the whole procedure for a biased coin, i.e., \\(X\\sim\\mathrm{Ber}(p)\\). Then</p> \\[\\begin{aligned} \\mathbb{E}(X) &amp;= \\frac{\\overbrace{0+0+...+0}^{(1-p) \\text{ million times}}+\\overbrace{1+1+...+1}^{p \\text{ million times}}}{\\text{1 million}} \\\\ &amp;= 0 \\times (1-p) + 1 \\times p \\\\ &amp;= p \\end{aligned}\\]"},{"location":"lectures/lecture-20/#example-3-a-fair-die-roll_1","title":"Example 3: a fair die roll","text":"<p>If we do the same for a fair 6-sided die,</p> \\[\\begin{aligned} \\mathbb{E}(X)  &amp;= \\frac{\\overbrace{1+1+...+1}^{(1/6) \\text{ million times}} + \\overbrace{2+2+...+2}^{(1/6) \\text{ million times}} + \\overbrace{3+3+...+3}^{(1/6) \\text{ million times}} + \\overbrace{4+4+...+4}^{(1/6) \\text{ million times}} + \\overbrace{5+5+...+5}^{(1/6) \\text{ million times}} + \\overbrace{6+6+...+6}^{(1/6) \\text{ million times}}}{\\text{1 million}} \\\\ &amp;= 1\\times\\frac{1}{6} + 2\\times\\frac{1}{6} + 3\\times\\frac{1}{6} + 4\\times\\frac{1}{6} + 5\\times\\frac{1}{6} + 6\\times\\frac{1}{6}\\\\ &amp;= 3.5 \\end{aligned}\\] <p>Expectation</p> <p>In general, the expectation of a discrete random variable is \\(\\mathbb{E}(X) = \\sum_x x \\Pr(X=x)\\), where the sum is over the entire state space.</p>"},{"location":"lectures/lecture-20/#properties-of-expectation","title":"Properties of expectation","text":"<ol> <li>If \\(c\\) is a constant then \\(\\mathbb{E}(c)=c\\) and \\(\\mathbb{E}(cX)=c\\mathbb{E}(X)\\)</li> <li>If \\(X\\) and \\(Y\\) are two random variables then \\(\\mathbb{E}(X+Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\\)</li> <li>Suppose \\(X\\) is a random variable and \\(f\\) is any function then \\(\\mathbb{E}(f(X)) = \\sum_x f(x)P(X=x)\\). For example, let \\(f(x) = x^2\\) and \\(X\\sim\\mathrm{Ber}(p)\\) (e.g., a biased coin toss), then \\(\\mathbb{E}(f(X)) = \\sum_{x=0}^{1}x^2P(X=x)= 0^2(1-p)+1^2p = p\\).</li> </ol>"},{"location":"lectures/lecture-20/#3-variance-of-a-discrete-random-variable","title":"3. Variance of a discrete random variable","text":"<p>Suppose we have an extremely biased coin with \\(p=1\\). That is, the coin always shows heads. Can we, before tossing, say something about the outcome in this case? Yes, we know exactly what the outcome will be, heads. Similarly, if \\(p=0\\) we would know the exact outcome, tails. We say there is no uncertainity in either of these cases.</p> <p>Now, consider a slightly less biased coin with \\(p = 0.99\\). Although we can not say exactly what the outcome will be, it will very likely be heads. There is some small amount of uncertainity.</p> <p>Finally, suppose we toss a fair coin (i.e., \\(p = 0.5\\)). Then we have no idea at all what the outcome will be. There is very high uncertainity in this case.</p> <p>Variance</p> <p>The variance of a random variable \\(X\\), denoted \\(\\mathrm{Var}(X)\\), quantifies the uncertainity associated with the random variable and is given by</p> \\[\\begin{aligned} \\mathrm{Var}(X)  &amp;= \\mathbb{E}((X - \\mathbb{E}(X))^2 )\\\\ &amp;= \\mathbb{E}(X^2 - 2X\\mathbb{E}(X) +  \\mathbb{E}(X)^2)\\\\ &amp;= \\mathbb{E}(X^2) - \\mathbb{E}(2X\\mathbb{E}(X)) +  \\mathbb{E}(\\mathbb{E}(X)^2)\\\\ &amp;= \\mathbb{E}(X^2) - 2\\mathbb{E}(X)\\mathbb{E}(X) +  \\mathbb{E}(X)^2\\\\ &amp;= \\mathbb{E}(X^2) - \\mathbb{E}(X)^2 \\end{aligned}\\]"},{"location":"lectures/lecture-20/#example-a-biased-coin-toss","title":"Example: a biased coin toss","text":"<p>If \\(X\\sim\\mathrm{Ber}(p)\\) then</p> \\[\\begin{aligned}  \\mathrm{Var}(X)  &amp;= \\mathbb{E}(X^2) - \\mathbb{E}(X)^2 \\\\ &amp;= p - p^2 \\\\ &amp;= p(1-p) \\end{aligned}\\] <p>Below we plot the variance of \\(X\\) as a function of \\(p\\).</p> <pre>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\np_vals = np.arange(0,1,0.01)\nVar_values = [ p*(1-p) for p in p_vals ]\nplt.plot(p_vals, Var_values)\n\nplt.axvline(x=0.5,linestyle='dashed')\n\nplt.xlabel('$p$ = probability of getting a heads')\nplt.ylabel('Var($X$)')\nplt.show()\n</pre> <p></p> <p>Note that for \\(p=0\\) or \\(p=1\\) we have Var(\\(X\\)) = 0, i.e., there is no uncertainity. Further note that Var(\\(X\\)) = \\(p(1-p)\\) peaks at \\(p=0.5\\), i.e., the highest uncertainity is associated with the toss of a fair coin.</p>"},{"location":"lectures/lecture-20/#properties-of-variance","title":"Properties of variance","text":"<ol> <li>If \\(c\\) is a constant then Var\\((c)\\) = 0 </li> <li>If \\(a\\) and \\(b\\) are constants and \\(X\\) is a random variable then Var\\((aX + b)\\) = \\(a^2\\)Var\\((X)\\)</li> </ol>"},{"location":"lectures/lecture-20/#4-independence","title":"4. Independence","text":"<p>Let's now look at two random variables simultaneously.</p> <p>Let's play the same game as before. Toss a coin, if it's heads use the coin to buy a chocolate. If it's tails put the coin in the piggy bank. As before, let \\(X\\) be the number of chocolates after the toss, \\(X\\sim\\)Ber(\\(p\\)). This time we'll also keep track of how many coins we put in the piggy bank, \\(Y\\).</p> <p>Now if \\(X=0\\) then \\(Y=1\\) and if \\(X=1\\) then \\(Y=0\\). The value of \\(Y\\) depends on \\(X\\), and vice versa, so \\(X\\) and \\(Y\\) are not independent random variables.</p> <p>On the other hand, suppose you have two coins. You toss each coin and decide whether to use it to buy a chocolate. Let \\(X_1\\) be the outcome of tossing the first coin. Let \\(X_2\\) be the outcome of tossing the second coin. Then \\(X_1\\) does not depend on \\(X_2\\), and vice-versa, so \\(X_1\\) and \\(X_2\\) are independent random variables. </p> <p>Independence</p> <p>Two random variables \\(X\\) and \\(Y\\) are said to be independent if for every \\(x\\) and \\(y\\)</p> \\[\\Pr(X=x \\text{ and } Y=y) = \\Pr(X=x)\\Pr(Y=y)\\]"},{"location":"lectures/lecture-20/#properties-of-independent-random-variables","title":"Properties of independent random variables","text":"<p>If \\(X\\) and \\(Y\\) are independent random variables then</p> <ol> <li>\\(\\mathbb{E}(XY)=\\mathbb{E}(X)\\mathbb{E}(Y)\\)</li> <li>Var(\\(X+Y\\)) = Var(\\(X\\)) + Var(\\(Y\\))</li> </ol> <p></p>"},{"location":"lectures/lecture-20/#5-binomial-random-variable","title":"5. Binomial random variable","text":"<p>Let \\(X_1\\) and \\(X_2\\) be the outcome of two independent tosses of the same coin. We want know how many chocolates we have after two tosses, \\(X = X_1 + X_2\\). Recall that \\(X_1\\) and \\(X_2\\) are both Ber(\\(p\\)). Therefore, </p> \\[\\begin{aligned}  \\mathbb{E}(X) &amp;= \\mathbb{E}(X_1) + \\mathbb{E}(X_2)\\\\ &amp;= p + p \\\\ &amp;= 2p \\end{aligned}\\] <p>Since \\(X_1\\) and \\(X_2\\) are independent we also have</p> \\[\\begin{aligned}  \\mathrm{Var}(X)  &amp;= \\mathrm{Var}(X_1) +\\mathrm{Var}(X_2)\\\\ &amp;= p(1-p) + p(1-p)\\\\ &amp;= 2p(1-p) \\end{aligned}\\] <p>Now let's look at three special cases to see what this means. </p> <p>If \\(p=0\\) then \\(\\mathbb{E}(X) = 0\\) and \\(\\mathrm{Var}(X)=0\\). If the coin always shows tails then we know with certainity that we will have no chocolates after two tosses.</p> <p>Similarly, if \\(p=1\\) then \\(\\mathbb{E}(X) = 2\\) and \\(\\mathrm{Var}(X)=0\\). If the coin always shows heads then we know with certainity that we will have two chocolates after two tosses. </p> <p>Finally, if \\(p=0.5\\) then \\(\\mathbb{E}(X) = 1\\) and \\(\\mathrm{Var}(X)=1\\). With a fair coin we expect to have 1 chocolate but there is some uncertainity since we could end up with 0, 1, or 2.</p> <p>Binomial random variable</p> <p>In general we could have \\(n\\) coins. Let \\(X_i\\) denote the outcome of the \\(i^{\\mathrm{th}}\\) toss, which is Ber(\\(p\\)) and independent of all other tosses. Then the sum over all \\(n\\) Bernoulli random variables, \\(X = X_1 + X_2 + ... + X_n\\), is called a binomial random variable with parameters \\(n\\) and \\(p\\), and is denoted by Bin(\\(n,p\\)).</p> <p>The expectation of a binomial random variable is</p> \\[\\mathbb{E}(X) = np\\] <p>and its variance is</p> \\[\\mathrm{Var}(X) = np(1-p)\\] <p>Moreover, we can compute the probability of a binomial random variable \\(X\\sim\\)Bin(\\(n,p\\)) taking value \\(k\\), as </p> \\[P(X = k) = {n \\choose k} p^k (1-p)^{n-k}\\] <p>where \\({n \\choose k}\\) is read \"\\(n\\) choose \\(k\\)\" and is the number of ways of choosing \\(k\\) items from \\(n\\) options. Mathematically, </p> \\[\\begin{aligned} {n \\choose k}  &amp;= \\frac{n}{k} {n-1 \\choose k-1}\\\\ &amp;= \\frac{n}{k} \\frac{n-1}{k-1} {n-2 \\choose k-2}\\\\ &amp;= \\vdots\\\\ &amp;= \\frac{n (n-1) (n-2)\\cdots (n-k+1)}{k(k-1)(k-2)\\cdots 1}  \\end{aligned}\\] <p></p>"},{"location":"lectures/lecture-20/#6-genetic-drift","title":"6. Genetic drift","text":"<p>Recall our model of one-locus haploid selection from Lecture 4. If \\(p_t\\) is the frequency of allele \\(A\\) in the population at time \\(t\\) then we said that the frequency of \\(A\\) in the next generation is</p> \\[  p_{t+1} = \\frac{p_t W_A }{p_t W_A + (1-p_t)W_a}  \\] <p>where \\(W_A\\) is the fitness of \\(A\\) and \\(W_a\\) is the fitness of \\(a\\).</p> <p>One unspoken assumption here is that the population size is infinitely large (or at least sufficiently big). In reality, populations are not infinitely large and may even be quite small. This finiteness introduces stochasticity, which in evolution we call genetic drift. One well-known model that incorporates genetic drift is the Wright-Fisher model. This model assumes that there are \\(N\\) diploid individuals in generation \\(t\\), each of which produce an infinite number of haploid gametes, of which \\(2N\\) are randomly sampled to form the diploid individuals of generation \\(t+1\\). Here, with haploid selection, we assume the frequency of gametes changes due to selection before sampling for the next generation.</p> <pre><code>graph TD\n    A[N diploids at t] --&gt; B[Infinite haploids before selection];\n    B --&gt; C[Infinite haploids after selection];\n    C --&gt; D[N diploids at t+1];</code></pre> <p>So, let \\(p_t\\) be the frequency of allele \\(A\\) in generation \\(t\\). Each diploid individual contributes an infinite number of gametes, so the frequency of \\(A\\) in the gamete pool is also \\(p_t\\). Now selection acts on the gamete pool, altering the frequencies as in our original model, so that the frequency of \\(A\\) in the gamete pool is now \\(p_t' = \\frac{p_tW_A }{p_t W_A + (1-p_t)W_a}\\). We then sample \\(2N\\) gametes to form the next diploid generation. For each gamete we pick there is a probability \\(p'_t\\) that it has allele \\(A\\) and a probability \\(1-p'_t\\) that it has allele \\(a\\). Let \\(X\\) be the number of \\(A\\) alleles in a randomly chosen gamete. Then \\(X\\sim\\)Ber\\((p'_t)\\) and the total number of \\(A\\) alleles after \\(2N\\) samples, \\(n_A(t+1)\\), is the sum of \\(2N\\) Bernoulli random variables, implying \\(n_A(t+1)\\sim\\)Bin\\((2N,p'_t)\\). The frequency of allele \\(A\\) in the next generation is then</p> \\[\\begin{aligned}  p_{t+1}  &amp;= \\frac{n_A(t+1)}{2N}\\\\  &amp;= \\frac{\\text{Bin}(2N,p_t')}{2N} \\end{aligned}\\] <p>With this model we can no longer predict exactly what the frequency of \\(A\\) will be in the next generation. However, we can compute the expectation</p> \\[\\begin{aligned}  \\mathbb{E}(p_{t+1}) &amp;= \\mathbb{E}\\left(\\frac{\\text{Bin}(2N,p_t')}{2N}\\right)\\\\ &amp;= \\frac{1}{2N}\\mathbb{E}\\left(\\text{Bin}(2N,p_t')\\right)\\\\ &amp;= \\frac{2N p_t'}{2N}\\\\ &amp;= p_t'  \\end{aligned}\\] <p>and the variance</p> \\[\\begin{aligned}  \\text{Var}(p_{t+1})  &amp;= \\text{Var}\\left(\\frac{\\text{Bin}(2N,p_t')}{2N}\\right) \\\\ &amp;= \\frac{1}{4N^2}\\text{Var}(\\text{Bin}(2N,p_t')) \\\\ &amp;= \\frac{2Np_t'(1-p_t')}{4N^2}\\\\ &amp;= \\frac{p_t'(1-p_t')}{2N} \\end{aligned}\\] <p>The expectation is as before -- the population size is irrelevant. The variance, however, is largest when the population size is small and the expected frequency is near 0.5 (just as we saw for a general binomial random variable above).  </p> <p>Note that variance goes to 0 as \\(N\\) goes to infinity. Therefore, in an infinitely large population, the frequency of \\(A\\) in the next generation is \\(p_{t+1} = p_t'\\) with no uncertainity. This recovers the one-locus haploid selection model from earlier in the course. </p> <p>In the next lab we'll learn how to simulate this stochastic model of evolution.</p>"},{"location":"lectures/lecture-21/","title":"Lecture 21","text":""},{"location":"lectures/lecture-21/#lecture-21-probability-ii-demographic-stochasticity","title":"Lecture 21: Probability II (demographic stochasticity)","text":"Run notes interactively?"},{"location":"lectures/lecture-21/#lecture-overview","title":"Lecture overview","text":"<ol> <li>Poisson random variable</li> <li>Demographic stochasticity</li> <li>Extinction</li> <li>Establishment</li> </ol> <p>credits</p> <p>This lecture was created by PhD student Puneeth Deraje as part of a course development TA position -- thanks Puneeth! If you are following along with the text, this lecture does not follow along as closely.</p> <p>In Lecture 18 we learned how to model stochasticity in population genetics (genetic drift). In this lecture we'll learn how to model stochasticity in population dynamics (demographic stochasticity). </p> <p>To do so we'll work with the simplest model of population dynamics possible, exponential growth (see Lecture 3). In discrete time this is</p> \\[n_{t+1} = R n_t\\] <p>where \\(n_t\\) is the number of individuals at time \\(t\\) and \\(R\\) is the reproductive factor. In this deterministic model every individual had a reproductive factor of exactly \\(R\\) at every time step. In reality there will be stochasticity in \\(R\\) across individuals and time. We can account for that by replacing \\(R\\) with a random variable. </p> <p></p>"},{"location":"lectures/lecture-21/#1-poisson-random-variable","title":"1. Poisson random variable","text":"<p>Recall the binomial random variable from the previous lecture, \\(X \\sim \\mathrm{Bin}(n,p)\\). The probability this random variable takes on value \\(k\\) is then </p> \\[\\Pr(X = k) = {n \\choose k} p^k (1-p)^{n-k}\\] <p>We could model the reproductive factor as a binomial random variable. For example, perhaps each individual at time \\(t\\) produces \\(n\\) offspring before dying, and each offspring survives to become an adult with probability \\(p\\). Using this model requires estimates of two parameters, \\(n\\) and \\(p\\). But there is a simpler way.</p> <p>Let the mean number of offspring produced be \\(\\lambda=np\\). Rearranging this in terms of \\(p\\), we can write the binomial as \\(X\\sim \\mathrm{Bin}(n,\\lambda/n)\\). Note that the mean is always \\(\\lambda\\), regardless of \\(n\\). Now imagine that individuals in the population we are modelling tend to have a very large number of offspring, very few of which survive. We can approximate this in the extreme by taking \\(n\\rightarrow \\infty\\). Let \\(Y\\) be this random variable, \\(Y = \\lim_{n \\rightarrow \\infty} \\mathrm{Bin}(n, \\lambda/n))\\). The distribution of \\(Y\\) is then </p> \\[ \\begin{aligned} \\Pr(Y=k) &amp;= \\lim_{n \\rightarrow \\infty} {n \\choose k} \\left(\\frac{\\lambda}{n}\\right)^k \\left(1-\\frac{\\lambda}{n}\\right)^{n-k} \\\\ &amp;= \\lim_{n \\rightarrow \\infty} \\frac{n(n-1)(n-2)...(n-k+1)}{k!} \\frac{\\lambda^k}{n^k} \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &amp;= \\frac{\\lambda^k}{k!}\\lim_{n \\rightarrow \\infty} \\frac{n(n-1)(n-2)...(n-k+1)}{n^k} \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &amp;= \\frac{\\lambda^k}{k!}\\lim_{n \\rightarrow \\infty} 1\\left(1-\\frac{1}{n}\\right)\\left(1-\\frac{2}{n}\\right)...\\left(1-\\frac{k-1}{n}\\right) \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &amp;= \\frac{\\lambda^k}{k!}\\lim_{n \\rightarrow \\infty} \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &amp;= \\frac{\\lambda^k e^{-\\lambda}}{k!} \\end{aligned} \\] <p>We call \\(Y\\) a Poisson random variable with mean \\(\\lambda\\) and denoted it by \\(Y\\sim\\mathrm{Poi}(\\lambda)\\). We can now model the reproductive factor as a random variable with only one parameter, \\(\\lambda\\). </p> <p>Note that the simpler Poisson distribution is a good approximation of the binomial distribution even for fairly moderate values of \\(n\\), as seen in the plot below.</p> <pre>\nimport sympy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef binomial(n,p,k):\n    return sympy.binomial(n,k) * p**k * (1-p)**(n-k)\n\ndef poisson(lam,k):\n    return lam**k * np.exp(-lam)/sympy.factorial(k)\n\nn = 100\np = 0.1\nlam = n*p\nks = range(n+1)\n\nfig, ax = plt.subplots()\n\nax.plot(ks, [binomial(n,p,k) for k in ks], label='binomial')\nax.plot(ks, [poisson(lam,k) for k in ks], label='Poisson')\n\nax.set_xlabel('number of successes')\nax.set_ylabel('probability')\nax.legend()\nplt.show()\n</pre> <p></p> <p>Two key properties of a Poisson random variable are</p> <p>1) the variance is equal to the mean</p> \\[  \\begin{aligned}  \\mathrm{Var}(Y) &amp;= \\lim_{n \\rightarrow \\infty} \\mathrm{Var}\\left(\\mathrm{Bin}\\left(n,\\frac{\\lambda}{n}\\right)\\right) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} n \\frac{\\lambda}{n} \\left(1-\\frac{\\lambda}{n}\\right) \\\\ &amp;= \\lambda  \\lim_{n \\rightarrow \\infty} \\left(1-\\frac{\\lambda}{n}\\right) \\\\ &amp;= \\lambda \\end{aligned} \\] <p>2) if \\(Y_1\\) and \\(Y_2\\) are two independent Poisson random variables with means \\(\\lambda_1\\) and \\(\\lambda_2\\), then their sum is distributed as a Poisson with mean \\(\\lambda_1+\\lambda_2\\)</p> \\[Y_1 + Y_2 \\sim \\mathrm{Poi}(\\lambda_1 + \\lambda_2)\\] <p></p>"},{"location":"lectures/lecture-21/#2-demographic-stochasticity","title":"2. Demographic stochasticity","text":"<p>We can now model demographic stochasticity with the Poisson distribution. </p> <p>Assume each individual in the population produces a Poisson number of surviving offspring with mean \\(\\lambda\\), independent of all other individuals in the populations, and then dies. Write the number of offspring for individual \\(i\\) as \\(X_i\\sim\\mathrm{Poi}(\\lambda)\\). Let the current number of individuals be \\(n_t\\). Then the population size in the next generation, \\(n_{t+1}\\), is distributed like the sum of \\(n_t\\) independent and identical Poisson's</p> \\[\\begin{aligned} n_{t+1} &amp;\\sim \\sum_{i=1}^{n_t}\\mathrm{Poi}(\\lambda)\\\\ &amp;= \\mathrm{Poi}\\left(\\sum_{i=1}^{n_t} \\lambda\\right) \\\\ &amp;= \\mathrm{Poi}(\\lambda n_t) \\\\ \\end{aligned}\\] <p>This implies that the expected population size in the next generation is \\(\\lambda n_t\\), as is the variance. In Lab 11 we'll simulate this to get a better sense of the resulting dynamics.</p> <p></p>"},{"location":"lectures/lecture-21/#3-extinction","title":"3. Extinction","text":"<p>Let us now look at one property of this model -- the probability of extinction.</p> <p>Consider a given individual at time \\(t\\). Let \\(\\eta\\) be the probability this individual does not have descendants in the long-term, i.e., that its lineage goes extinct.</p> <p>To find \\(\\eta\\) we note that the probability that this lineage goes extinct, \\(\\eta\\), is the probability that this individual has \\(k\\) surviving offspring (for all values of \\(k\\)) and all of those offspring lineages go extinct (with probability \\(\\eta^k\\)). Given the probability of having \\(k\\) surviving offspring is \\(\\lambda^k e^{-\\lambda}/k!\\), this implies</p> \\[ \\begin{aligned} \\eta  &amp;= \\sum_{k=0}^{\\infty} \\frac{e^{-\\lambda}\\lambda^k}{k!} \\eta^k \\\\ &amp;= e^{-\\lambda} \\sum_{k=0}^{\\infty} \\frac{(\\eta \\lambda)^k}{k!} \\\\ &amp;= e^{-\\lambda}e^{\\lambda \\eta} \\\\ &amp;= e^{-\\lambda (1-\\eta)} \\end{aligned} \\] <p>One solution is \\(\\eta=1\\), certain extinction. But there can be a second biologically valid solution (between 0 and 1), meaning that extinction is not certain. This occurs when the mean number of surviving offspring is greater than one, \\(\\lambda&gt;1\\), as shown in plot below (the solutions are where the curve intersects the 1:1 line).</p> <pre>\nxs = np.linspace(0,1,100)\nfig,ax=plt.subplots()\n\nlam = 2\n\nax.plot(xs, xs)\nax.plot(xs, [np.exp(-lam*(1-x)) for x in xs])\n\nax.set_xlabel(r'$\\eta$')\nax.set_ylabel(r'$e^{-\\lambda (1-\\eta)}$')\n\nplt.show()\n</pre> <p></p> <p>Above we calculated the probability a single lineage goes extinct, \\(\\eta\\). From this, the probability that the entire population goes extinct is the probability that all \\(n_t\\) lineages go extinct, \\(\\eta^{n_t}\\). </p> <p>The two major conclusions from this are</p> <p>1) when the mean number of surviving offspring per parent is \\(\\lambda=1\\) the deterministic model predicts a constant population size but the stochastic model says that extinction is certain.</p> <p>2) when the mean number of surviving offspring per parent is \\(\\lambda&gt;1\\) the deterministic model predicts exponential growth but the stochastic model says there is still some non-zero probability of extinction.</p> <p></p>"},{"location":"lectures/lecture-21/#4-establishment","title":"4. Establishment","text":"<p>Before moving on, the above result about extinction is mathematically identical to a classic result in population genetics concerning the establishment of a beneficial allele.</p> <p>Consider a population at equilibrium such that the mean number of offspring per parent is 1. And now consider a beneficial allele that tends to have more offspring, \\(\\lambda&gt;1\\). Then we know from above that there is some non-zero probability this lineage does not go extinct, \\(\\eta&lt;1\\). Writing the above equation about extinction in terms of the establishment probability, \\(p=1-\\eta\\), we have</p> \\[\\begin{aligned} \\eta &amp;= e^{-\\lambda (1-\\eta)}\\\\ 1 - p &amp;= e^{-\\lambda p}\\\\ p &amp;= 1 - e^{-\\lambda p} \\end{aligned}\\] <p>Now assume that the beneficial allele increases the number of offspring only slightly, so that \\(\\lambda=1+s\\) with \\(s\\) small. We can then solve for \\(p\\) explicitly using a Taylor series expansion of \\(e^{-\\lambda p}=e^{-(1+s) p}\\) around \\(s=0\\)</p> \\[  \\begin{aligned} p &amp;= 1 - e^{-(1+s)p} \\\\ p &amp;\\approx 1 - \\left(1 - (1+s)p + \\frac{(1+s)^2p^2}{2}\\right) \\\\ p &amp;\\approx (1+s)p - \\frac{(1+s)^2p^2}{2} \\\\ 1 &amp;\\approx 1 + s - \\frac{(1+s)^2p}{2} \\\\ 0 &amp;\\approx s - \\frac{(1+s)^2p}{2} \\\\ p &amp;\\approx \\frac{2s}{(1+s)^2}\\\\ p &amp;\\approx 2s \\end{aligned} \\] <p>This says that the probability a weakly beneficial allele establishes (i.e., is not lost by genetic drift) is roughly twice its selective advantage.</p>"},{"location":"lectures/lecture-22/","title":"Lecture 22","text":""},{"location":"lectures/lecture-22/#lecture-22-probability-iii-the-coalescent","title":"Lecture 22: Probability III (the coalescent)","text":"Run notes interactively?"},{"location":"lectures/lecture-22/#lecture-overview","title":"Lecture overview","text":"<ol> <li>The coalescent</li> </ol> <p>All the models we have studied so far have looked forward in time, into the future. But this doesn't have to always be the case, we can also look back in time, modeling the past. Here we look at one particularly powerful example from population genetics.</p> <p></p>"},{"location":"lectures/lecture-22/#1-the-coalescent","title":"1. The coalescent","text":"<p>Let's consider a population composed of \\(N\\) diploid individuals, i.e., with \\(2N\\) alleles at a given locus. We want to model the history of these alleles -- from which alleles in the previous generations do they descend? </p> <p>One of the simplest ways to model this is to treat all of the alleles as equivalent (e.g., no fitness differences) so that a given allele in the current generation picks its \"parent\" allele at random from amongst the \\(2N\\) alleles in the previous generation.</p> <p>We can simulate this and plot the result, arranging the \\(2N\\) alleles horizontally, stacking previous generations on top, and drawing lines to connect \"children\" and \"parent\" alleles.</p> <pre>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef simulate(N,tmax):\n\n    p = np.arange(2*N) #initial configuration\n    t = 0\n    ps = np.empty((tmax,2*N), dtype='int')\n    while t &lt; tmax:\n        ps[t] = p\n        p = np.random.randint(0,2*N,2*N) #parents of current generation\n        t += 1\n\n    return ps\n</pre> <pre>\ndef plot_lineages(ps,ax,alpha):\n\n    # plot lineages\n    for t,p in enumerate(ps[1:]): #loop over generations\n        ax.plot([ps[0],ps[t+1]], [t,t+1], marker='o', color='k', alpha=alpha) #connect children with parents\n    ax.scatter(ps[0], [t+1 for _ in ps[0]], marker='o', color='k', alpha=alpha) #plot all alleles of last gen\n\n    # simplify presentation\n    # ax.axis('off')\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.xaxis.set_ticks([])\n    ax.yaxis.set_ticks(range(len(ps)))\n    ax.set_xlabel('alleles')\n    ax.set_ylabel('generations ago')\n</pre> <pre>\nfig, ax = plt.subplots()\n\nps = simulate(N=5,tmax=4)\nplot_lineages(ps,ax,alpha=0.5)\n\nplt.show()\n</pre> <p></p> <p>One of the most important aspects of this model is that we can choose to think about the history of just some subset of the alleles in the current generation. This is helpful because 1) we do not need to model the entire population and 2) this has a close connection to data (since we almost never sample every individual in a population).</p> <p>To see this visually, we can choose a few \"sample\" alleles from the current generation and highlight their \"lineages\".</p> <pre>\nfig, ax = plt.subplots()\n\nplot_lineages(ps,ax,alpha=0.1) #plot full population in background\n\n# plot sample lineages\nfor i in [0,5,9]: #samples\n    path = [i] #lineage of sample i\n    for p in ps[1:]: #loop over generations\n        path.append(p[i]) #add parent \n        i = p[i] #make parent the child\n    ax.plot(path,range(len(path)), marker='o')\n\nplt.show()\n</pre> <p></p>"},{"location":"lectures/lecture-22/#time-for-two-lineages-to-coalesce","title":"Time for two lineages to coalesce","text":"<p>When two sample lineages meet at a most recent common ancestor we say they coalesce (\"come together\"). This is where the name of the model comes from, the coalescent. (Note that some reserve that name for the continuous-time limit of this model, but we won't be so strict.) </p> <p>The time it takes for two lineages to coalesce determines how similar those alleles are. The more distant their most recent common ancestor the more mutations that are expected to have accumulated. Thus the time to coalescence influences the amount of genetic diversity we expect to see.  </p> <p>The first question we will ask with this model is: how many generations does it take for two lineages to coalesce? Let's call this random variable \\(T_2\\). We want to know how this random variable is distributed.</p> <p>Consider one generation at a time. We choose the parent of one of the alleles at random. We do the same for the second allele. The probability that the two lineages coalesce in the previous generation is the probability that the parent of the second allele is the same as the parent of the first allele, \\(p_2=1/(2N)\\). </p> <p>Let \\(X=1\\) if the two lineages coalesce in the previous generation, \\(X=0\\) if they don't coalesce. Then \\(X\\) is a Bernoulii random variable, \\(X\\sim\\text{Ber}(p_2)\\). We can now rephrase our question mathematically: how many Bernoulli trials do we have to perform to get one success?</p> <p>Geometric random variable</p> <p>Let \\(X\\) be the number of Bernoulli trials (with success probability \\(p\\)) that it takes to get 1 success. Then the probability we need to perform \\(X=k\\) trials is the probability of having \\(k-1\\) failures (which happens with probability \\((1-p)^{k-1}\\)) followed by a success (which happens with probability \\(p\\))</p> \\[\\Pr(X=k) = (1-p)^{k-1}p\\] <p>This random variable \\(X\\) is called a \"geometric random variable\" with parameter \\(p\\) and denoted \\(X\\sim\\text{Geo}(p)\\). After evaluating a few infinite sums it can be shown to have expectation</p> \\[\\mathbb{E}(X) = \\frac{1}{p}\\] <p>and variance</p> \\[\\text{Var}(X) = \\frac{1-p}{p^2}\\] <p>Returning to the coalescent, the time for two sample lineages to coalesce is therefore geometrically distributed, \\(T_2\\sim\\text{Geo}(p_2)\\). We then can expect to wait </p> \\[\\begin{aligned} \\mathbb{E}(T_2)  &amp;= 1/p_2\\\\ &amp;=2N \\end{aligned}\\] <p>generations until coalescence of the two lineages. However, the variance around this expectation is </p> \\[\\begin{aligned} \\text{Var}(T_2)  &amp;= \\frac{1-p_2}{p^2}\\\\ &amp;=2N(2N-1) \\end{aligned}\\] <p>which is roughly \\((2N)^2\\) in a large population, \\(2N&gt;&gt;1\\). Since \\(N^2\\) can be very large relative to \\(N\\), this means that there is a lot of noise around the expectation.</p> <p>We can see how noisey the coalescence time is by sampling from a geometric distribution for a given value of \\(N\\). For example, below we sample the coalescence time 1000 times with \\(N=1000\\) and plot those times in a histogram. We often see coalescence times ranging from very near 1 to well over 10000 (run the code a few times if you'd like).</p> <pre>\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nN = 1000 #diploid population size\np = 1/(2*N) #probability 2 lineages coalesce in the previous generation\nts = np.random.geometric(p,1000) #sample from a geometric with param p 1000 times\nax.hist(ts, bins=100) #histogram of coalescence times\n\nax.set_xlabel('number of generations until 2 lineages coalesce')\nax.set_ylabel('number of realizations')\nplt.show()\n</pre> <p></p>"},{"location":"lectures/lecture-22/#time-for-n-lineages-to-coalesce","title":"Time for \\(n\\) lineages to coalesce","text":"<p>Considering only two samples is a special case that gives us some intuition about the model. But in general we want to know, how long until \\(n\\) samples all share a common ancestor? Let this time be random variable \\(M_n\\). We want to know something about \\(M_n\\). </p> <p>We start by assuming that only one pair of lineages can coalesce each generation, which is valid when the population size is large, \\(N&gt;&gt;1\\), and the sample is relatively small \\(n&lt;&lt;N\\). Then we can write \\(M_n\\) as the time it takes to go from \\(n\\) to \\(n-1\\) lineages, \\(T_n\\), plus the time it takes to go from \\(n-1\\) to \\(n-2\\) lineages, \\(T_{n-1}\\), and so on down to the time it takes to go from 2 to 1 lineages, \\(T_2\\)</p> \\[M_n = T_n + T_{n-1} + ... + T_2\\] <p>The next step is to find out something about the \\(T_i\\). We do this by noting that when there are \\(i\\) lineages the probability of no coalescence in the previous generation is</p> \\[1-p_i = \\left(1-\\frac{1}{2N}\\right)\\left(1-\\frac{2}{2N}\\right)...\\left(1-\\frac{i-1}{2N}\\right)\\] <p>In words, we choose any parent for the first lineage, the second lineage does not have the same parent with probability \\(1-\\frac{1}{2N}\\), the third lineage does not have the same parent as either of the first two lineages with probability \\(1-\\frac{2}{2N}\\), and so on to the \\(i^\\text{th}\\) lineage.</p> <p>This is a complicated expression but we can simplify by taking a Taylor series around \\(1/N=0\\) and approximating to first order, which gives</p> \\[\\begin{aligned} 1-p_i  &amp;\\approx 1 - \\frac{1}{2N}\\sum_{j=1}^{i-1}j\\\\ &amp;= 1 - \\frac{1}{2N}\\frac{i(i-1)}{2}\\\\ &amp;= 1 - \\frac{{i \\choose 2}}{2N}\\\\ \\end{aligned}\\] <p>so that the probability that there is coalescence is</p> \\[p_i \\approx \\frac{{i \\choose 2}}{2N}\\] <p>This makes good sense. When \\(N\\) is large there can be at most 1 coalescent event per generation among the sample lineages. Since there are \\({i \\choose 2}\\) ways to choose a pair of lineages from \\(i\\) lineages, each of which coalesce with probability \\(1/(2N)\\), the probability of coalescence is \\({i \\choose 2}/(2N)\\).</p> <p>We can now treat the \\(T_i\\) as geometric random variables with parameter \\(p_i\\), \\(T_i\\sim\\text{Geo}(p_i)\\).</p> <p>This means that \\(M_n\\), the time for \\(n\\) samples to coalesce into 1, is the sum of \\(n-1\\) independent geometric random variables. Unfortunately this doesn't produce a nice distribution (in contrast to the sum of Poisson random variables we saw in the last lecture). However, we can calculate the expectation since we know the expectation of a sum is the sum of the expectations</p> \\[\\begin{aligned} \\mathbb{E}(M_n)  &amp;= \\mathbb{E}(T_n + T_{n-1} + ... + T_2)\\\\ &amp;= \\mathbb{E}(T_n) + \\mathbb{E}(T_{n-1}) + ... + \\mathbb{E}(T_2)\\\\ &amp;= 1/p_n + 1/p_{n-1} + ... + 1/p_2\\\\ &amp;\\approx \\frac{2N}{{n \\choose 2}} + \\frac{2N}{{n-1 \\choose 2}} + ... + \\frac{2N}{{2 \\choose 2}}\\\\ &amp;= 2N \\sum_{i=2}^n \\frac{1}{{i \\choose 2}} \\\\ &amp;= 4N \\frac{n-1}{n} \\\\ \\end{aligned}\\] <p>Since \\((n-1)/n&lt;1\\) we see that all \\(n\\) sample lineages are expected to coalesce in less than \\(4N\\) generations. This is pretty remarkable given we expect to wait more than half that time just for 2 sample lineages to coalesce! The reason for this is that coalescence happens faster when there are more lineages (since there are more pairs to choose from). So if we take a large sample then most of the lineages will coalesce very quickly and most of the time spent waiting for the most recent common ancestor will be once few lineages remain. Below we take advantage of a great Python package, <code>msprime</code>, to quickly simulate the coalescent with \\(n\\) samples and plot the history of those samples (a coalescent tree).</p> <pre>\nimport msprime\nn = 10\nts = msprime.sim_ancestry(n) #simulate coalescent with n samples\nts.first().draw_svg(node_labels={}, size=(500,300)) #plot tree without node labels\n</pre> <p></p>"},{"location":"lectures/schedule/","title":"Schedule","text":"<p>Under construction for Fall 2025 edition. Lectures will be updated as we go.</p> Lecture Topic Background reading 1 Why and how to build a model Preface (2p.), Chapter 1 (14p.), Chapter 2 (34p.) 2 Numerical and graphical techniques (univariate) 4.1-4.3 (23p.) 3 Equilibria (univariate) 5.1-5.2 (12p.) 4 Stability (univariate) 5.3 (13p.) 5 General solutions (univariate) 6.1-6.3, 6.5-6.7 (16p.) - Test 6 Representing linear multivariate models with vectors and matrices P2.1-P2.4 (14p.) 7 Finding equilibria in linear multivariate models P2.5-P2.7 (9p.) 8 General solutions for linear multivariate models P2.8-P2.9 (13p.), 9.1-9.2 (18p.) 9 Complex eigenvalues 7.3.3 (11p.), Box 8.2, Box 9.1, Box 9.2 10 Demography Chapter 10 (37p.) - Test 11 Equilibria (nonlinear multivariate) Chapter 8 (37p.) 12 Stability (nonlinear multivariate) Chapter 8 (37p.) 13 14 - Test 16 Evolutionary invasion analysis I 12.1-12.4 (30p.) 17 Evolutionary invasion analysis II 12.1-12.4 (30p.) 18 The evolution of dominance 12.5 (17p.) 19 Evolutionary invasion analysis III 12.1-12.4 (30p.) 20 Example - Test"},{"location":"syllabus/assignments/","title":"Assignments","text":""},{"location":"syllabus/assignments/#homeworks","title":"Homeworks","text":"<p>A list of homework problems will be released after class on Wednesday, with the solutions due by the start of class on Monday.</p>"},{"location":"syllabus/assignments/#labs","title":"Labs","text":"<p>Each computer lab contains a series of problems and the solutions are due by the end of the day (ideally by the end of the lab).</p>"},{"location":"syllabus/course_structure/","title":"Course structure","text":""},{"location":"syllabus/course_structure/#learning-objectives","title":"Learning objectives","text":"<p>Mathematics is central to science because it provides a rigorous way to go from a set of assumptions (what we take to be true) to their logical consequences (what we want to know). In ecology &amp; evolution this might be how we think SARS-CoV-2 may spread and evolve given a set of vaccination rates and travel restrictions, how caribou population sizes are predicted to respond to forecasted rates of climate change, or something much more abstract like the expected amount of genetic diversity in a randomly mating population. In this course we'll learn how to build, analyze, and interpret mathematical models of increasing complexity through readings, lectures, computer labs, and a final project. By the end of the course you will be able to:</p> <ul> <li> build a model: go from a verbal description of a biological system to a set of equations</li> <li> analyze a model: manipulate a set of equations into a mathematical expression of interest</li> <li> interpret a model: translate mathematical expressions back into biological meaning</li> </ul>"},{"location":"syllabus/course_structure/#weekly-tasks","title":"Weekly tasks","text":"<ul> <li> read text</li> <li> attend two lectures</li> <li> attend one lab</li> </ul>"},{"location":"syllabus/course_structure/#grading-scheme","title":"Grading scheme","text":"<ul> <li>in-class tests: 4 x 20%</li> <li>final project: 20%</li> </ul>"},{"location":"syllabus/final_project/","title":"Final project","text":""},{"location":"syllabus/final_project/#construct-your-own-model","title":"Construct your own model","text":"<p>In this project you will use the tools you've learned in class and apply them to a model that you develop. The model can be about any phenomenon in ecology and evolution, as long as you make up the model.</p> <p>You'll do the final project in two parts.</p> <ul> <li> <p> Part 1</p> <ul> <li>Describe your biological question and why this interests you </li> <li>Describe your model in words (ie, the main assumptions) and explain the main structure with a diagram (eg, flow or life cycle diagram) </li> <li>Write down the equations that you will analyze</li> <li>Describe what your analysis might reveal (ie, your hypothesis) </li> <li>Max 500 words (not counting diagrams and equations) </li> <li>Example</li> </ul> </li> <li> <p> Part 2 </p> <ul> <li>Re-iterate your biological question and why this interests you</li> <li>Describe your model assumptions in detail, defining all parameters and variables </li> <li>Write down the equations for your model </li> <li>Analyze your model </li> <li>Explain how the results address your original question</li> <li>Suggest how the model could be improved or extended</li> <li>Max 2000 words (not counting diagrams, equations, and supplementary Jupyter notebooks)</li> <li>Example</li> </ul> </li> </ul> <p>Tip</p> <p>If you are having trouble coming up with a new model, take one of the models that we've analysed in the course and change one or more of its underlying assumptions to get a new set of equations. Then analyse these equations. Discuss the differences between the assumptions used and also  between the results obtained.</p>"},{"location":"syllabus/general_info/","title":"General info","text":""},{"location":"syllabus/general_info/#land-acknowledgement","title":"Land acknowledgement","text":"<p>I wish to acknowledge this land on which the University of Toronto operates. For thousands of years it has been the traditional land of the Huron-Wendat, the Seneca, and the Mississaugas of the Credit. Today, this meeting place is still the home to many Indigenous people from across Turtle Island and I am grateful to have the opportunity to work on this land. For more information see University of Toronto's land acknowledgement.</p>"},{"location":"syllabus/general_info/#group-norms","title":"Group norms","text":"<p>The University of Toronto is committed to equity, human rights, and respect for diversity. All members of the learning environment in this course should strive to create an atmosphere of mutual respect where all members of our community can express themselves, engage with each other, and respect one another\u2019s differences. U of T does not condone discrimination or harassment against any persons or communities. Please contact me  if you have any concerns. For more information see the Code of Student Conduct.</p>"},{"location":"syllabus/general_info/#accessibility","title":"Accessibility","text":"<p>The University provides academic accommodations for students with disabilities in accordance with the terms of the Ontario Human Rights Code. This occurs through a collaborative process that acknowledges a collective obligation to develop an accessible learning environment that both meets the needs of students and preserves the essential academic requirements of the University\u2019s courses and programs. Students with diverse learning styles and needs are welcome in this course. If you have a disability that may require accommodations, please feel free to get in touch with me and/or the Accessibility Services office.</p>"},{"location":"syllabus/general_info/#religious-observances","title":"Religious observances","text":"<p>The University provides reasonable accommodation of the needs of students who observe religious holy days other than those already accommodated by ordinary scheduling and statutory holidays. Students have a responsibility to alert members of the teaching staff in a timely fashion to upcoming religious observances and anticipated absences and I will make every reasonable effort to avoid scheduling tests, examinations or other compulsory activities at these times. Please reach out to me as early as possible to communicate any anticipated absences related to religious observances, and to discuss any possible related implications for course work.</p>"},{"location":"syllabus/general_info/#family-care-responsibilities","title":"Family care responsibilities","text":"<p>The University of Toronto strives to provide a family-friendly environment. You may wish to inform me if you are a student with family responsibilities. If you are a student parent or have family responsibilities, you also may wish to visit the Family Care Office website.</p>"},{"location":"syllabus/instructors/","title":"Instructors","text":""},{"location":"syllabus/instructors/#professor","title":"Professor","text":"<p>Matthew Osmond (he/him)</p> <ul> <li>email: mm.osmond@utoronto.ca</li> <li>office: Earth Sciences Centre (ESC) 3041</li> <li>website: osmond-lab.github.io</li> </ul>"},{"location":"syllabus/instructors/#teaching-assistant","title":"Teaching assistant","text":"<p>Erik Curtis (he/him)</p> <ul> <li>email: erik.curtis@mail.utoronto.ca</li> </ul>"},{"location":"syllabus/previous_tests/","title":"Previous tests","text":""},{"location":"syllabus/previous_tests/#test-1","title":"Test 1","text":"<p>Covers lectures 1-5 and tutorials 1-3.</p> <ul> <li>2025, solutions</li> </ul>"},{"location":"syllabus/previous_tests/#previous-midterms","title":"Previous midterms","text":"<p>Roughly covers univariate lectures and labs.</p> <ul> <li>2024, solutions </li> <li>2022, solutions </li> <li>2021, solutions </li> </ul>"},{"location":"syllabus/previous_tests/#previous-finals","title":"Previous finals","text":"<p>Covers all material, with a focus on multivariate (and any stochastic) lectures and labs.</p> <ul> <li>2024, solutions </li> <li>2022, solutions </li> <li>2021, solutions </li> </ul>"},{"location":"syllabus/resources/","title":"Resources","text":"<p>There are many resources available at the University of Toronto to help you succeed in this course. Below are a few:</p> <ul> <li>Writing Center</li> <li>Academic integrity</li> <li>More on academic integrity</li> <li>CTSI list of supports</li> <li>Academic success module</li> <li>Get help with Quercus</li> </ul>"},{"location":"syllabus/textbook/","title":"Textbook","text":"<p>Otto &amp; Day 2007. A biologist's guide to mathematical modeling in ecology and evolution.</p> <ul> <li>UofT library e-copies</li> <li>UofT library physical-copies</li> <li>buy your own copy</li> </ul>"},{"location":"syllabus/when_and_where/","title":"When and where","text":""},{"location":"syllabus/when_and_where/#lectures","title":"Lectures","text":"<ul> <li>Monday 10:10 - 11:00 AM, Koffler House (KP) 113</li> <li>Wednesday 10:10 - 11:00 AM, Sidney Smith (SS) 1084</li> </ul>"},{"location":"syllabus/when_and_where/#labs","title":"Labs","text":"<ul> <li>Wednesday, 3:10 - 5:00 PM, Sidney Smith (SS), room 561</li> </ul>"},{"location":"syllabus/final_project/partII_example/","title":"partII example","text":""},{"location":"syllabus/final_project/partII_example/#final-project-part-ii-example","title":"Final project part II - example","text":"Run notes interactively?      <p>Note</p> <pre><code>This is a made-up example inspired by the research of Sydney Ackermann, an MSc student in the Osmond lab, who was in turn inspired by the models of [Roze &amp; Michod](https://doi.org/10.1086/323590) and [Pichugin et al.](https://doi.org/10.1371/journal.pcbi.1005860).\n</code></pre>"},{"location":"syllabus/final_project/partII_example/#biological-question","title":"Biological question","text":"<p>Note</p> <p>In this case, this is just repeated from part I, so that part II tells the whole story from question to answer. Update and refine as needed.</p> <p>I am interested in the origin of multicellularity and the reason why particular life-cycles predominate. For instance, many bacterial species exist as single cells which reproduce via binary fission while many multicellular organisms grow to large sizes and reproduce via single-celled offspring. However, there are many more options and it is not clear if or why these strategies are evolutionarily optimal or how multicellularity evolved from a binary fission ancestor. Here I ask the question, which multicellular life-cycles can invade a unicellular population producing via binary fission?</p>"},{"location":"syllabus/final_project/partII_example/#model","title":"Model","text":"<p>Note</p> <p>This is an improved, expanded version of the model description from part I, with more detail and now including the equations to be analyzed.</p>"},{"location":"syllabus/final_project/partII_example/#unicellular-resident","title":"Unicellular resident","text":"<p>Here I first model a unicellular population where individuals divide, die, and compete in continuous time. The variable in this model is the number of individuals, \\(n\\). The parameters are the birth rate (\\(b_1\\)), death rate (\\(d_1\\)), and amount of competition. To describe competition I will assume there are a total of \\(k\\) places for individuals to exist. When a cell divides, each of the two resulting offspring randomly choose one of these \\(k\\) locations. If an offspring lands on an empty spot it survives. If an offspring lands on an occupied spot it \"wins\" that spot with probability 1/2, killing the resident, and otherwise dies. The amount of competition is therefore controlled by parameter \\(k\\).</p> <p>This model can be described by the following flow diagram</p> <pre><code>graph LR;\n    A((n)) --\"b1 n (1 - n/k)\" --&gt; A;\n    A --d1 n--&gt; B[ ];\n\n    style B height:0px;</code></pre> <p>The corresponding differential equation is </p> \\[ \\frac{\\mathrm{d}n}{\\mathrm{d}t} = b_1 n \\left(1-\\frac{n}{k}\\right) - d_1 n \\] <p>which can be solved for equilibrium, \\(\\hat{n}\\).</p>"},{"location":"syllabus/final_project/partII_example/#multicellular-invader","title":"Multicellular invader","text":"<p>Next, imagine an invading multicellular population. </p>"},{"location":"syllabus/final_project/partII_example/#111","title":"1+1+1","text":"<p>For instance, imagine a life-cycle where offspring are unicellular, they divide to form an individual with two cells, and then divide again to produce three single-celled offspring. We call this strategy 1+1+1 because those are the sizes of the offspring produced.</p> <p>In this case we need to track the number of individuals with one, \\(n_1\\), and two, \\(n_2\\), cells (i.e., this is a structured population). I assume that each cell in an individual with \\(i\\) cells divides at rate \\(b_i\\) and that individuals with \\(i\\) cells die at rate \\(d_i\\). I also assume that this invading population is so rare (and \\(k\\) so large) that offspring produced by this life-history strategy never land on other individuals with this same strategy. The offspring may, however, land on an individual from the unicellular population, which will reduce the growth rate of the invader.</p> <p>The dynamics of this invading multicellular population can be described by the following flow diagram</p> <pre><code>graph LR;\n    A((n1)) --b1 n1--&gt; B((n2));\n    A --d1 n1--&gt; C1[ ];\n    B --d2 n2--&gt; C2[ ];\n    B --\"6b2 n2 (1-n/(2k))\"--&gt; A;\n\n    style C1 height:0px;\n    style C2 height:0px;</code></pre> <p>To determine whether this population can invade a unicellular resident at equilibrium, I will calculate the leading eigenvalue from the system of (linear in \\(n_i\\)) differential equations</p> \\[ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} = \\mathbf{M}\\vec{n} \\] <p>where \\(\\vec{n}=\\begin{pmatrix}n_1 \\\\ n_2\\end{pmatrix}\\) and </p> \\[ \\mathbf{M}= \\begin{pmatrix}  -b_1-d_1 &amp; 6b_2(1-\\hat{n}/(2k)) \\\\  b_1 &amp; -2b_2-d_2  \\end{pmatrix} \\]"},{"location":"syllabus/final_project/partII_example/#1-2","title":"1 + 2","text":"<p>The other multicellular strategy that has offspring sizes summing to 3 is 1+2. In this case the transition matrix is</p> \\[ \\mathbf{M}= \\begin{pmatrix}  -b_1-d_1 &amp; 2b_2(1-\\hat{n}/(2k)) \\\\  b_1 &amp; -2b_2-d_2+2b_2(1-\\hat{n}/(2k))  \\end{pmatrix} \\]"},{"location":"syllabus/final_project/partII_example/#1-1-1-1","title":"1 + 1 + 1 + 1","text":"<p>We can also consider larger multicellular strategies, like 1+1+1+1. In this case we need to keep track of the number of groups of size 1, 2, and 3. This gives a 3x3 projection matrix</p> \\[ \\mathbf{M}= \\begin{pmatrix}  -b_1-d_1 &amp; 0 &amp; 12b_3(1-\\hat{n}/(2k)) \\\\  b_1 &amp; -2b_2-d_2 &amp; 0 \\\\ 0 &amp; 2b_2 &amp; -3b_3-d_3 \\end{pmatrix} \\]"},{"location":"syllabus/final_project/partII_example/#1-1-2","title":"1 + 1 + 2","text":"<p>For the 1+1+2 strategy we have </p> \\[ \\mathbf{M}= \\begin{pmatrix}  -b_1-d_1 &amp; 0 &amp; 6b_3(1-\\hat{n}/(2k)) \\\\  b_1 &amp; -2b_2-d_2 &amp; 3b_3(1-\\hat{n}/(2k)) \\\\ 0 &amp; 2b_2 &amp; -3b_3-d_3 \\end{pmatrix} \\]"},{"location":"syllabus/final_project/partII_example/#1-3","title":"1 + 3","text":"<p>For the 1+3 strategy we have </p> \\[ \\mathbf{M}= \\begin{pmatrix}  -b_1-d_1 &amp; 0 &amp; 3b_3(1-\\hat{n}/(2k)) \\\\  b_1 &amp; -2b_2-d_2 &amp; 0 \\\\ 0 &amp; 2b_2 &amp; -3b_3-d_3+3b_3(1-\\hat{n}/(2k)) \\end{pmatrix} \\]"},{"location":"syllabus/final_project/partII_example/#2-2","title":"2 + 2","text":"<p>For the 2+2 strategy we have </p> \\[ \\mathbf{M}= \\begin{pmatrix}  -b_1-d_1 &amp; 0 &amp; 0 \\\\  b_1 &amp; -2b_2-d_2 &amp; 6b_3(1-\\hat{n}/(2k)) \\\\ 0 &amp; 2b_2 &amp; -3b_3-d_3 \\end{pmatrix} \\]"},{"location":"syllabus/final_project/partII_example/#results","title":"Results","text":""},{"location":"syllabus/final_project/partII_example/#unicellular-resident_1","title":"Unicellular resident","text":"<p>I first solve for the unicellular resident equilibrium</p> \\[ \\begin{aligned} \\frac{\\mathrm{d}n}{\\mathrm{d}t} &amp;= b_1 n \\left(1-\\frac{n}{k}\\right) - d_1 n \\\\ 0 &amp;= b_1 \\hat{n} \\left(1-\\frac{\\hat{n}}{k}\\right) - d_1 \\hat{n} \\\\ 0 &amp;= \\hat{n} \\left(b_1 \\left(1-\\frac{\\hat{n}}{k}\\right) - d_1\\right) \\\\ \\end{aligned} \\] <p>This implies that \\(\\hat{n}=0\\) or</p> \\[ \\begin{aligned} 0 &amp;= b_1 \\left(1-\\frac{\\hat{n}}{k}\\right) - d_1 \\\\ d_1/b_1 &amp;= 1-\\frac{\\hat{n}}{k} \\\\ 1 - d_1/b_1 &amp;= \\frac{\\hat{n}}{k} \\\\ k(1 - d_1/b_1) &amp;= \\hat{n} \\\\ \\end{aligned} \\] <p>This non-zero equilibrium is biologically valid whenever \\(0\\leq\\hat{n}\\implies d_1\\leq b_1\\). This non-zero equilibrium is stable when </p> \\[ \\begin{aligned} \\frac{\\mathrm{d}}{\\mathrm{d}n}\\left(\\frac{\\mathrm{d}n}{\\mathrm{d}t}\\right)_{n=\\hat{n}} &amp;&lt; 0\\\\ b_1 \\left(1-\\frac{\\hat{n}}{k}\\right) - b_1 \\hat{n}/k - d_1 &amp;&lt; 0\\\\ - b_1 (1-d_1/b_1) &amp;&lt; 0\\\\ - (b_1 - d_1) &amp;&lt; 0\\\\ b_1 &amp;&gt; d_1\\\\ \\end{aligned} \\] <p>We will assume \\(b_1&gt;d_1\\) such that this non-zero equilibrium is both biologically valid and stable.</p> <pre>\n# check our calculations\n\nfrom sympy import *\nvar('b1,d1,b2,d2,b3,d3,k,n')\n\ndndt = n*b1*(1-n/k) - d1*n #equation\neq = solve(dndt,n) #equilibrium\nprint(eq)\n\ndiff(dndt,n).subs(n,eq[1]).simplify() #stability condition\n</pre>"},{"location":"syllabus/final_project/partII_example/#multicellular-invader_1","title":"Multicellular invader","text":""},{"location":"syllabus/final_project/partII_example/#1-1-1","title":"1 + 1 + 1","text":"<p>Let's now look at the growth rate of a rare 1+1+1 strategy, by calculating the leading eigenvalue of \\(\\mathbf{M}\\). Because this is a 2x2 matrix we know that the eigenvalues solve</p> \\[ \\lambda^2 - \\mathrm{Tr}(\\mathbf{M})\\lambda + |\\mathbf{M}| = 0 \\] <p>giving </p> \\[ \\lambda = \\frac{\\mathrm{Tr}(\\mathbf{M}) \\pm \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4|\\mathbf{M}|}}{2} \\] <p>For stability (ie, the multicellular strategy does not invade) we need \\(|\\mathbf{M}|&gt;0\\) and \\(\\mathrm{Tr}(\\mathbf{M})&lt;0\\) (these are the Routh-Hurwitz conditions). The first requires</p> \\[ \\begin{aligned} |\\mathbf{M}| &amp;&gt; 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 6b_2(1-\\hat{n}/(2k))b_1 &amp;&gt; 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 6b_2(1-(1 - d_1/b_1)/2)b_1 &amp;&gt; 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 3b_2(1+d_1/b_1)b_1 &amp;&gt; 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 3b_2(b_1+d_1) &amp;&gt; 0\\\\ (b_1+d_1)(d_2 - b_2) &amp;&gt; 0\\\\ \\end{aligned} \\] <p>Since all \\(b_i\\) and \\(d_i\\) are non-negative (since they are rates) this implies stability when \\(d_2&gt;b_2\\).</p> <p>The second condition requires </p> \\[ \\begin{aligned} \\mathrm{Tr}(\\mathbf{M}) &amp;&lt; 0 \\\\ (-b_1-d_1-2b_2-d_2) &amp;&lt; 0 \\\\ b_1+d_1+2b_2+d_2 &amp;&gt; 0 \\\\ \\end{aligned} \\] <p>which is true as long as at least one of these rates is non-zero, which we already assumed to be the case for resident stability (\\(b_1&gt;d_1\\)). </p> <p>To conclude, this 1+1+1 strategy will invade the unicellular strategy whenever \\(b_2&gt; d_2\\).</p> <pre>\n# check our calculations\n\nM = Matrix([\n    [-b1 - d1, 6*b2*(1-n/(2*k))],\n    [b1, -2*b2 - d2]])\n\ndet = M.det().subs(n,eq[1]).simplify()\ntr =  M.trace().subs(n,eq[1]).simplify()\nprint(det)\nprint(tr)\n\n(det - (b1+d1)*(d2-b2)).simplify() #check our simplified expression of the determinant\n</pre>"},{"location":"syllabus/final_project/partII_example/#1-2_1","title":"1 + 2","text":"<p>We can take the same approach for a rare 1+2 invader. In this case the determinant condition reduces to</p> \\[ \\begin{aligned} |\\mathbf{M}| &amp;&gt; 0\\\\ (-b_1-d_1)(-2b_2-d_2+2b_2(1-\\hat{n}/(2k))) - 2b_2(1-\\hat{n}/(2k))b_1 &amp;&gt; 0\\\\ (b_1 + d_1)(d_2 - b_2 d_1/b_1) &amp;&gt; 0\\\\ \\end{aligned} \\] <p>and the trace determinant condition reduces to </p> \\[ \\begin{aligned} \\mathrm{Tr}(\\mathbf{M}) &amp;&lt; 0 \\\\ (-b_1-d_1-d_2+2b_2(1-\\hat{n}/(2k))) &amp;&lt; 0 \\\\ -b_1-d_1-d_2-b_2(1-d_1/b_1) &amp;&lt; 0 \\\\ \\end{aligned} \\] <p>The latter is always true, so the critical condition for invasion comes from the former, which can be rearranged as \\(b_2/b_1 &gt; d_2/d_1\\).</p> <pre>\n# check our calculations\n\nM = Matrix([\n    [-b1 - d1, 2*b2*(1-n/(2*k))],\n    [b1, -2*b2 - d2 + 2*b2*(1-n/(2*k))]])\n\ndet = M.det().subs(n,eq[1]).simplify()\ntr =  M.trace().subs(n,eq[1]).simplify()\nprint(det)\nprint(tr)\n\n(det - ((b1 + d1)*(d2 - b2*d1/b1))).simplify() #check our simplified expression of the determinant\n</pre>"},{"location":"syllabus/final_project/partII_example/#larger-multicellular-strategies","title":"Larger multicellular strategies","text":"<p>In the remaining cases we are dealing with 3x3 matrices, and so the analytical expressions for the eigenvalues are not easy to interpret. Instead we plot the real part of the leading eigenvalue (ie, the invasion growth rate) as a function of the cell division rate in groups of size 2, \\(b_2\\), for a specific set of paramter values (see figure caption). We also plot the invasion growth rates of two smaller multicellular strategies (analysed above) for comparison.  </p> <pre>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_invasion_rate(M, nhat, pvals, b2s, ax, label=''):\n    '''plot the invasion growth rate as a function of b2'''\n    evs = M.eigenvals(multiple=True) #eigenvalues\n    evseq = [ev.subs(n,nhat) for ev in evs] #eigenvalues at resident equilibrium\n    evseqp = [ev.subs(pvals) for ev in evseq] #evaluate at chosen parameter vaues\n    rs = [max([re(ev.subs(b2,i)).n() for ev in evseqp]) for i in b2s] #find the max of the real parts of each eigenvalue for each value of b2 \n    ax.plot(b2s, rs, label=label) #plot\n</pre> <pre>\n# projection matrices for each multicellular strategy\n\nM111 = Matrix([\n    [-b1 - d1, 6*b2*(1-n/(2*k))],\n    [b1, -2*b2 - d2]])\n\nM12 = Matrix([\n    [-b1 - d1, 2*b2*(1-n/(2*k))],\n    [b1, -2*b2 - d2 + 2*b2*(1-n/(2*k))]])\n\nM1111 = Matrix([\n    [-b1 - d1, 0, 12*b3*(1-n/(2*k))],\n    [b1, -2*b2 - d2, 0],\n    [0, 2*b2, -3*b3-d3]])\n\nM112 = Matrix([\n    [-b1 - d1, 0, 6*b3*(1-n/(2*k))],\n    [b1, -2*b2 - d2, 3*b3*(1-n/(2*k))],\n    [0, 2*b2, -3*b3-d3]])\n\nM13 = Matrix([\n    [-b1 - d1, 0, 3*b3*(1-n/(2*k))],\n    [b1, -2*b2 - d2, 0],\n    [0, 2*b2, -3*b3-d3+3*b3*(1-n/(2*k))]])\n\nM22 = Matrix([\n    [-b1 - d1, 0, 0],\n    [b1, -2*b2 - d2, 6*b3*(1-n/(2*k))],\n    [0, 2*b2, -3*b3-d3]])\n</pre> <pre>\n# chose parameter values\npvals = {'b1':1.0,'d1':0.1,'d2':0.1,'d3':0.1,'k':100,'b3':1.0}\nb2s = np.linspace(0,2,100) #range to plot over\nnhat = eq[1]\n\n#plot\nfig, ax = plt.subplots()\n\nplot_invasion_rate(M=M111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1')\nplot_invasion_rate(M=M12, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+2')\nplot_invasion_rate(M=M1111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1+1')\nplot_invasion_rate(M=M112, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+2')\nplot_invasion_rate(M=M13, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+3')\nplot_invasion_rate(M=M22, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='2+2')\n\nax.plot(b2s, [0 for _ in b2s], c='k') #0 line for reference (invade if above this line)\n\nplt.xlabel(r'$b_2$')\nplt.ylabel('invasion growth rate')\nplt.legend()\nplt.show()\n</pre> <p></p> <p>Figure 1. The invasion growth rate (real part of leading eigenvalue) versus the rate of cell division in groups of size 2, \\(b_2\\), for 6 multicellular strategies (see legend) invading a unicellular strategy, 1+1. Parameter values: \\(b_1=b_3=1\\), \\(d_i=0.1\\), and \\(k=100\\).</p>"},{"location":"syllabus/final_project/partII_example/#discussion","title":"Discussion","text":""},{"location":"syllabus/final_project/partII_example/#answering-the-question","title":"Answering the question","text":"<p>Above we have shown that any of the 6 chosen multicellular strategies can invade a unicellular strategy given certain conditions. For the smaller multicellular strategies (1+1+1 and 1+2) we were able to show these conditions analytically, in general. For the larger multicellular strategies (1+1+1+1, 1+1+2, 1+3, and 2+2), where the eigenvalues are more complicated, we turned to a numerical example. </p> <p>For 1+1+1, invasion requires cell division to be faster than death in groups of size 2, \\(b_2&gt;d_2\\). Essentially, the benefit a cell receives from being in a group (\\(b_2\\)) needs to outweigh the costs (\\(d_2\\)). It is interesting that this does not depend on the cell division and death rates in groups of size 1, \\(b_1\\) and \\(d_1\\). This is likely because all offspring of both the unicellular and multicellular strategy are size 1, so both strategies are affected equally by changes in \\(b_1\\) and \\(d_1\\).</p> <p>For 1+2, invasion requires the cell division rate in groups of size 2 relative to that in groups of size 1 to be greater than the death rate in groups of size 2 relative to that in groups of size 1, \\(b_2/b_1&gt;d_2/d_1\\). This may be the case if groups of cells cooperate by sharing resources or protecting each other from the environment, which would increase \\(b_2/b_1\\) or decrease \\(d_2/d_1\\).</p> <p>For larger multicellular strategies, our numerical example indicates that the invasion rate increases with the number of offspring.  Those strategies with 2 offspring (1+3 and 2+2) behave much like 1+2 while the strategy with 3 offspring behaves much like 1+1+1. It is the strategy with 4 offspring (1+1+1+1) that has the highest invasion growth rate and thus appears most like to invade a unicellular ancestor under this model. As shown in the figure below, this is despite the fact that the 1+1+1+1 strategy does not have the highest growth rate in the absence of competitors. Instead, it appears that having more (and hence smaller) offspring at the time of fragmenting makes a strategy more competitive, presumably because this increases the chances of winning a spot. </p> <pre>\n# chose parameter values\npvals = {'b1':1.0,'d1':0.1,'d2':0.1,'d3':0.1,'k':100,'b3':1.0}\nb2s = np.linspace(0,2,100) #range to plot over\nnhat = 0 #no unicellular individuals\n\n#plot\nfig, ax = plt.subplots()\n\nplot_invasion_rate(M=M111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1')\nplot_invasion_rate(M=M12, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+2')\nplot_invasion_rate(M=M1111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1+1')\nplot_invasion_rate(M=M112, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+2')\nplot_invasion_rate(M=M13, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+3')\nplot_invasion_rate(M=M22, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='2+2')\n\nax.plot(b2s, [0 for _ in b2s], c='k') #0 line for reference (invade if above this line)\n\nplt.xlabel(r'$b_2$')\nplt.ylabel('invasion growth rate')\nplt.legend()\nplt.show()\n</pre> <p></p> <p>Figure 2. The growth rate (real part of leading eigenvalue) versus the rate of cell division in groups of size 2, \\(b_2\\), for 6 multicellular strategies (see legend) growing from rare in the absence of competition. Parameter values: \\(b_1=b_3=1\\), \\(d_i=0.1\\), and \\(k=100\\).</p>"},{"location":"syllabus/final_project/partII_example/#limitations-and-extensions","title":"Limitations and extensions","text":"<p>There are a number of ways this analysis and model could be improved. For one, I have only looked at the invasion of a multicellular strategy into the unicellular resident. It therefore remains unclear if an invading multicellular strategy will completely displace the unicellular strategy or the both will coexist. Looking at the reverse situation, where the unicellular strategy invades a multicellular strategy would help answer this. Further, it is unclear how the multicellular strategies compete with each other. Exploring this would help us understand how and why complex multicellular organisms, like ourselves, have evolved. Another interesting direction is to allow competition to depend on the size of a group, for instance by making larger groups more likely to win spots resided by smaller groups. And finally, it would be interesting to include different cell types, like cooperators and cheaters, as multicellularity opens the door to within-group conflict, which may affect which strategies invade best.</p>"},{"location":"syllabus/final_project/partI_example/","title":"Final project part I - example","text":"<p>Note</p> <p>This is a made-up example inspired by the research of Sydney Ackermann, an MSc student in the Osmond lab, who was in turn inspired by the models of Roze &amp; Michod and Pichugin et al..</p>"},{"location":"syllabus/final_project/partI_example/#biological-question","title":"Biological question","text":"<p>I am interested in the origin of multicellularity and the reason why particular life-cycles predominate. For instance, many bacterial species exist as single cells which reproduce via binary fission while many multicellular organisms grow to large sizes and reproduce via single-celled offspring. However, there are many more options and it is not clear if or why these strategies are evolutionarily optimal or how multicellularity evolved from a binary fission ancestor. Here I ask the question, which multicellular life-cycles can invade a unicellular population producing via binary fission?</p>"},{"location":"syllabus/final_project/partI_example/#model-description","title":"Model description","text":"<p>I will start by modeling a unicellular population where individuals divide, die, and compete in continuous time. The variable in this model is the number of individuals, \\(n\\). The parameters are the birth rate (\\(b_1\\)), death rate (\\(d_1\\)), and amount of competition. To describe competition I will assume there are a total of \\(k\\) places for individuals to exist. When a cell divides, each of the two resulting offspring randomly choose one of these \\(k\\) locations. If an offspring lands on an empty spot it survives. If an offspring lands on an occupied spot it \"wins\" that spot with probability 1/2, killing the resident, and otherwise dies. The amount of competition is therefore controlled by parameter \\(k\\).</p> <p>This model can be described by the following flow diagram</p> <pre><code>graph LR;\n    A((n)) --\"b1 n (1 - n/k)\" --&gt; A;\n    A --d1 n--&gt; B[ ];\n\n    style B height:0px;</code></pre> <p>From the resulting differential equation I will determine the equilibrium number of unicellular individuals, \\(\\hat{n}\\).</p> <p>Next I will model the dynamics of an invading multicellular population. For instance, imagine a life-cycle where offspring are unicellular, they divide to form an individual with two cells, and then divide again to produce three single-celled offspring. In this case we need to track the number of individuals with one, \\(n_1\\), and two, \\(n_2\\), cells (i.e., this is a structured population). I assume that each cell in an individual with \\(i\\) cells divides at rate \\(b_i\\) and that individuals with \\(i\\) cells die at rate \\(d_i\\). I also assume that this invading population is so rare (and \\(k\\) so large) that offspring produced by this life-history strategy never land on other individuals with this same strategy. The offspring may, however, land on an individual from the unicellular population, which will reduce the growth rate of the invader.</p> <p>The dynamics of this invading multicellular population can be described by the following flow diagram</p> <pre><code>graph LR;\n    A((n1)) --b1 n1--&gt; B((n2));\n    A --d1 n1--&gt; C1[ ];\n    B --d2 n2--&gt; C2[ ];\n    B --\"6b2 n2 (1-n/(2k))\"--&gt; A;\n\n    style C1 height:0px;\n    style C2 height:0px;</code></pre> <p>The goal is then to see if the growth rate of this invading multicellular population is positive (i.e., it can invade) or negative (i.e., it cannot invade). This will require calculating the leading eigenvalue from the system of (linear) differential equations describing the rate of change in \\(n_i\\).</p>"},{"location":"syllabus/final_project/partI_example/#equations","title":"Equations","text":"<p>The equation for the unicellular population size is</p> \\[\\frac{\\mathrm{d}n}{\\mathrm{d}t} = b_1 n \\left(1 - \\frac{n}{k}\\right) - d_1 n\\] <p>and the equations for the invading multicellular population described above are</p> \\[\\begin{aligned} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &amp;= -(b_1 + d_1) n_1 + 6 b_2 n_2 \\left(1 - \\frac{n}{2k}\\right)\\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} &amp;= b_1 n_1 - (2b_2 + d_2) n_2 \\end{aligned}\\]"},{"location":"syllabus/final_project/partI_example/#hypothesis","title":"Hypothesis","text":"<p>I hypothesize that, provided the \\(b_2\\), \\(b_3\\), ... are large enough relative to \\(b_1\\) (ie, that cells divide fast enough in multicellular organisms) and the \\(d_2\\), \\(d_3\\), ... are small enough relative to \\(d_1\\) (ie, that multicellular individuals don't die too quickly), there will be multicellular life-cycles that can invade a unicellular population. </p>"}]}