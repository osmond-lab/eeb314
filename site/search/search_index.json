{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mathematical modelling in ecology and evolution (EEB314) Mathematics is central to science because it provides a rigorous way to go from a set of assumptions to their logical consequences. In ecology & evolution this might be how we think a virus will spread and evolve, how climate change will impact a threatened population, or how much genetic diversity we expect to see in a randomly mating population. In this course you'll learn how to build, analyze, and interpret mathematical models of increasing complexity through readings, lectures, homeworks, computer labs, and a final project. Our focus is on deterministic dynamical models (recursions and differential equations), which requires us to learn and use some calculus and linear algebra. Please see the University of Toronto Academic Calendar for more details on the course prerequisites and additional information on the distribution/breadth requirements this course satisfies. Next taught: Fall 2024 (EEB314) Previously taught: Fall 2022 (EEB430), Fall 2021 (EEB430)","title":"Overview"},{"location":"acknowledgements/","text":"Acknowledgements This course is based on a fantastic textbook by Sally Otto and Troy Day , A biologist's guide to mathematical modeling in ecology and evolution . I've been fortunate to have been taught by both of these excellent mentors and I hope I can do them and their book justice. I'm also very grateful to Leithen M'Gonigle who has slickly elaborated on Sally's lecture notes in LaTeX and kindly shared them with me. Thanks also to Ailene McPherson who, along with Sally and Leithen, continues to help me teach this course and inspires me to become a better teacher. Thanks are also due to Puneeth Deraje who went above and beyond as the course's first TA. And you wouldn't be reading this without the help of Tom Ouellette who did an amazing job creating this website as a course development TA. Finally, thanks to all the students who have taken the course and given me feedback (explicit or not) -- thanks for your patience!","title":"Acknowledgements"},{"location":"acknowledgements/#acknowledgements","text":"This course is based on a fantastic textbook by Sally Otto and Troy Day , A biologist's guide to mathematical modeling in ecology and evolution . I've been fortunate to have been taught by both of these excellent mentors and I hope I can do them and their book justice. I'm also very grateful to Leithen M'Gonigle who has slickly elaborated on Sally's lecture notes in LaTeX and kindly shared them with me. Thanks also to Ailene McPherson who, along with Sally and Leithen, continues to help me teach this course and inspires me to become a better teacher. Thanks are also due to Puneeth Deraje who went above and beyond as the course's first TA. And you wouldn't be reading this without the help of Tom Ouellette who did an amazing job creating this website as a course development TA. Finally, thanks to all the students who have taken the course and given me feedback (explicit or not) -- thanks for your patience!","title":"Acknowledgements"},{"location":"labs/schedule/","text":"Schedule Lab Topic Notebook 1 Introduction to Jupyter, Python, and SymPy 2 Simulating recursion equations 3 Numerical and graphical techniques 4 Equilibria and stability (univariate) 5 General solutions (univariate) 6 Linear algebra 7 Eigenvalues, eigenvectors, and linear multivariate solutions 8 Demography 9 Equilibria and stability (nonlinear multivariate) 10 Evolutionary invasion analysis 11 Genetic drift 12 Demographic stochasticity To access a lab, click on the image. The easiest option is then to \"Open with Google Colaboratory\", but you can also download the notebook and open it locally (this requires you install Jupyter, Python, and required Python libraries).","title":"Schedule"},{"location":"labs/schedule/#schedule","text":"Lab Topic Notebook 1 Introduction to Jupyter, Python, and SymPy 2 Simulating recursion equations 3 Numerical and graphical techniques 4 Equilibria and stability (univariate) 5 General solutions (univariate) 6 Linear algebra 7 Eigenvalues, eigenvectors, and linear multivariate solutions 8 Demography 9 Equilibria and stability (nonlinear multivariate) 10 Evolutionary invasion analysis 11 Genetic drift 12 Demographic stochasticity To access a lab, click on the image. The easiest option is then to \"Open with Google Colaboratory\", but you can also download the notebook and open it locally (this requires you install Jupyter, Python, and required Python libraries).","title":"Schedule"},{"location":"lectures/lecture-01/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 1: Introduction Run notes interactively? Lecture overview Why use mathematical models in ecology and evolution? Syllabus 1. Why use mathematical models in ecology and evolution? Mathematics permeates ecology and evolution, from simple back-of-the-envelope calculations to the development of sophisticated mathematical models. This is because mathematics is a unique tool that can take what we already know (or assume to be true) and rigorously lead us to the logical conclusions. To see this (while also introducing you to the kind of models I work with), let's look at two examples. Example 1: HIV See sections 1.2-1.4 of the text for more info. Human immunodeficiency virus (HIV) is, as the name suggests, a virus that infects humans and causes acquired immunodeficiency syndrome (AIDS). This is a terrible diesase that has caused over 20 million deaths worldwide. It is transmitted via bodily fluids. Once inside the body HIV particles (virions) attach to a protein called CD4 on the cell membrane of helper T cells (part of our immune system) and others. Once attached, the virus inserts its RNA into the host cell, which is reverse transcribed into DNA and becomes part of the host genome. The host cell can remain in the 'latently infected' state for some time. When the viral DNA is eventually transcribed by the host cell, starting an 'active infection', hundreds of new virions are produced, often killing the host cell. These new virions then go on to infect other CD4+ cells. A large decline in the number of helper T cells is what causes AIDS. This is because helper T cells bind to viruses and secrete chemical signals to stimulate the immune system. So without helper T cells the immune system is very comprimised. HIV and T cells. Source: bio.libretexts.org Based on this, once infected with HIV (and without treatment) we expect the number of virions to rapidly increase and the number of helper T cells to decline. This is generally what is observed. However, the number of virions then tends to steeply decline. Why might this be? Here are two hypotheses: the immune system recognizes HIV and suppresses it the decline in helper T cells prevents HIV from replicating To decide whether the second hypothesis is valid Phillips (1996) built a mathematical model describing the rate of change in the number of CD4+ cells and free virions. See the note below for the details of the model if you are interested (but don't worry if you don't understand much of this yet!). Details of the Philips (1996) model See Box 2.4 in the text for more details. Philips built a dynamical model of the infection process using 4 differential equations: \\(\\frac{dR}{dt} = \\Gamma \\tau - \\mu R - \\beta V R\\) \\(\\frac{dL}{dt} = p \\beta V R - \\mu L - \\alpha L\\) \\(\\frac{dE}{dt} = (1 - p) \\beta V R + \\alpha L - \\delta E\\) \\(\\frac{dV}{dt} = \\pi E - \\sigma V\\) These equations describe the rate of change in the number of susceptible CD4+ cells (R), latently infected cells (L), actively infected cells (E), and free virions (V). These are the 4 \"variables\" of the model, as their values change over time. The remainder of the symbols represent \"parameters\", whose values do not change (they are constants). The meaning of the parameters and the values used by Phillips (1996) are shown in the table below. Symbol Description Value (units/day) \\(\\Gamma\\) Rate that CD4+ cells are produced 1.36 \\(\\tau\\) Proportion of CD4+ cells that are susceptible to attack 0.2 \\(\\mu\\) HIV-independent death rate of susceptible CD4+ cells 1.36 x 10^-3 \\(\\beta\\) Rate of CD4+ cell infection per HIV virion 0.00027 \\(p\\) Proportion of newly infected cells becoming latently infected 0.1 \\(\\alpha\\) Activation rate of latently infected cells 3.6 x 10^-2 \\(\\delta\\) Death rate of actively infected cells 0.33 \\(\\pi\\) Rate of production of virions by actively infected cells 100 \\(\\sigma\\) Removal rate of cell-free virus 2 Given these parameter descriptions, can you \"read\" the differential equations above? For example, what does \\(\\Gamma \\tau\\) represent, biologically? Using these equations and parameter values Philips (1996) asked how the number of CD4+ cells (R + L + E) and free virions (V) changed over time following infection. To see how Philips' model behaves, activate the kernel at the top of the page and run the code below import numpy as np #numerical tools import matplotlib.pyplot as plt #plotting tools # define a function to iterate through the model def philips_model(R=200, L=0, E=0, V=4e-5, days=120, steps=120): #default initial conditions and timesteps #choose parameter values gamma, mu, tau, beta, p, alpha, sigma, delta, pi = [1.36, 1.36e-3, 0.2, 0.00027, 0.1, 3.6e-2, 2, 0.33, 100] #update variables record = [] ts = np.linspace(0, days, steps) #times we record population sizes at for t in ts: #looping over timesteps dRdt = gamma * tau - mu * R - beta * V * R R += dRdt #uninfected T cells dLdt = p * beta * V * R - mu * L - alpha * L L += dLdt #latently infected T cells dEdt = (1-p) * beta * V * R + alpha * L - delta * E E += dEdt #actively infected T cells dVdt = pi * E - sigma * V V += dVdt #free virions record += ([[R + L + E, V]]) #total T cells, virions return ts, np.array(record) # iterate and record population sizes ts, ns = philips_model() #times and population sizes # initialize plot fig, left_ax = plt.subplots() right_ax = left_ax.twinx() fig.set_size_inches(8,4) # plot T cell data on left axis left_ax.plot(ts, ns[:,0], label='R'); left_ax.set_ylabel('CD4+ cells (R + E + L)', fontsize=14) left_ax.legend(frameon=False, bbox_to_anchor=(0.99, 0.99)) left_ax.set_xlabel('Days from infection', fontsize=14) # plot virion data on right axis right_ax.plot(ts, ns[:,1], label='V', color=plt.cm.tab10(1)); right_ax.set_ylabel('Free virions V', fontsize=14) right_ax.legend(frameon=False, bbox_to_anchor=(0.99, 0.85)) # format plot and show it plt.yscale('log') fig.tight_layout() plt.show() Compare to Figure 1.3 and Figure 1.4 in the text. We see the initial increase in virions (orange) and decline in CD4+ cells (blue), followed by a decline virions (note the log scale on the right axis -- this is a big decline, from over 1000 to about 10). Because this model does not include an immune response against the virions but still exhibits the decline in virions, we conclude that the second hypothesis is valid, that it is theoretically plausible that the decline in virions is due to a lack of CD4+ cells to infect. A few years later this hypothesis was empirically tested and validated -- a nice example of theory guiding science. Feel free to play around with the code above, changing parameter values or even the structure of the model. Do the dynamics change as you expected? Example 2: Extreme Events A second example is a model that I helped Dr. Kelsey Lyberger (then a PhD student at UC Davis) with in Lyberger et al 2021 . Kelsey Lyberger, doing Daphnia fieldwork I suppose. Kelsey was interested in how populations respond to extreme climatic events, like lizards to hurricanes. It has long been clear that such events can impact the size of a population, e.g., by causing extra mortality, and may in fact put populations at risk of extinction. More recently it has become apparent that extreme events can also impose strong natural selection, and that populations can quickly adapt to the new environment. Some examples include: Hurricanes select on lizard limbs and toe pads Ice-storms select on sparrow body size Droughts select on Darwin finch beaks Droughts select on flowering time in Brassica Lizard being blown off perch by a leaf blower. Source: colindonihue.com Now, how should such rapid adaptive evolution impact population size? This is the question Kelsey set out to answer with a mathematical model. Details of Kelsey's model Kelsey assumed each individual has a quantitative genetic trait, such as lizard limb length, that is determined by many alleles of small effect plus some environmental noise. Fitness is assumed to be a bell-shaped function of the difference between the optimum trait value, \\(\\theta\\) , and an individual's trait value, \\(z\\) , which we write as \\(W(\\theta - z)\\) . The change in population size, \\(N\\) , from one generation to rate the next is the current population size times mean fitness, \\(\\overline{W}(\\theta - \\bar{z})\\) , the latter depending on the population mean trait value, \\(\\bar{z}\\) . This gives \\(\\Delta N = N \\overline{W}(\\theta - \\bar{z}).\\) The change in the mean trait value from one generation to the next is roughly the product of genetic variance in the trait, \\(V_g\\) , and the strength of selection. The strength of selection is defined as the derivative of the natural logarithm of population mean fitness with respect to the mean trait value, \\(\\mathrm{d}\\ln\\overline{W}(\\theta - \\bar{z})/\\mathrm{d}\\bar{z}\\) . This gives \\(\\Delta \\bar{z} = V_g \\frac{\\mathrm{d}}{\\mathrm{d}\\bar{z}}\\ln\\overline{W}(\\theta - \\bar{z}).\\) Together, these coupled recursion equations, \\(\\Delta N\\) and \\(\\Delta \\bar{z}\\) , can be used to describe how evolution affects population size under an extreme event, which is modeled as a sudden but temporary change in the optimum phenotype, \\(\\theta\\) . Below is a stochastic simulation much like that used by Kelsey. With an activated kernel, run the code below to create a plot very similar to Figure 1 in Lyberger et al. (this may take a minute). import numpy as np import matplotlib.pyplot as plt def lyberger_model(Vo=0.75, Ve=0, event_duration=1, seed=0, other_parameters=[120, 500, 1, 2, 100, 0, 2.5]): # unpack parameters generations, K, w, lmbda, event_time, initial_theta_t, dtheta_t = other_parameters # initialize population members = np.random.normal(initial_theta_t, Vo**0.5, K) #K individuals with trait values distributed around the optimum # run simulations np.random.seed(seed) population_size, mean_breeding_value = [], [] for g in range(generations): # optimum trait value if g in np.arange(event_time, event_time + event_duration): theta_t = initial_theta_t + dtheta_t #extreme event else: theta_t = initial_theta_t #normal # viability selection prob_survival = np.array([np.exp(-(theta_t - z + np.random.normal(0, Ve))**2 / (2*w**2)) for z in members]) survived = np.array([True if p > np.random.uniform(0,1) else False for p in prob_survival]) if len(survived) == 0: break members = members[survived] # random mating offspring = [] for m in np.random.choice(members, len(members)): if len(offspring) > K: #if more than K offspring already then choose K and stop matings offspring = np.random.choice(offspring, K) break else: n_off = np.random.poisson(lmbda) mate = np.random.choice(members) offspring += [(m + mate)/2 for _ in range(n_off)] #give offspring mean parental value # sample new trait values for offspring offspring = np.array(offspring) offspring = np.random.normal(offspring, Vo) #add segregation variance # record statistics population_size.append(len(offspring)) mean_breeding_value.append(np.mean(offspring)) # update for next generation members = offspring return ( np.arange(0, generations-event_time+1), #times np.array(population_size[event_time-1:]), #population size np.array(mean_breeding_value[event_time-1:]) #mean trait value ) # initialize plot fig, ax = plt.subplots(2, sharex=True) fig.set_size_inches(8,6) # length of extreme event event_duration = 1 # run 10 simulations per segregation (Vo) and environment variance (Ve) parameter combination for Vo, Ve, c, lab in [[0.75, 0, 'black', 'with evolution'], [0, 1, 'red', 'without evolution']]: # plot simulations simulations = np.array([lyberger_model(Vo=Vo, Ve=Ve, event_duration=event_duration, seed=s) for s in range(10)]) ax[0].plot(simulations[:,0].T, simulations[:,1].T, color=c, alpha=0.3); ax[1].plot(simulations[:,0].T, simulations[:,2].T, color=c, alpha=0.3); # hack together only one instance of the legend ax[0].plot([np.min(simulations[:,0].T)], [np.min(simulations[:,1].T)], alpha=1, label = lab, color=c) ax[1].plot([np.min(simulations[:,0].T)], [np.min(simulations[:,2].T)], alpha=1, label = lab, color=c) # add environmental event duration ax[0].fill_between([0,event_duration], y1=500, alpha=0.2) ax[1].fill_between([0,event_duration], y1=-0.2, y2=2, alpha=0.2) # add labels ax[0].set_ylabel('Population size', fontsize=12) ax[1].set_ylabel('Mean trait value', fontsize=12) ax[1].set_xlabel('Generation', fontsize=14) # add legend plt.legend(frameon=False) plt.show() The key result, that you can see in the plot above, is that when extreme events are short, adaptive evolution (black lines) can paradoxically reduce population size (relative to the red lines, where there is no evolution). The reason for this is that, while during the extreme event (shaded section) evolution is adaptive, once the extreme event ends the population finds itself maladapted to the original environment. Adaptive evolution can therefore hamper population persistence, and this is an important thing to keep in mind when documenting rapid adaptive evolution in response to extreme events -- it is not necessarily a good thing for the species (or our conservation goals). 2. Syllabus OK, now that we've gone over some motivating examples of modeling in ecology and evolution, let's take a look at how we're going to learn to become modelers in this course. Point your browser over to the syllabus and read each of the pages there.","title":"Lecture 1"},{"location":"lectures/lecture-01/#lecture-1-introduction","text":"Run notes interactively?","title":"Lecture 1: Introduction"},{"location":"lectures/lecture-01/#lecture-overview","text":"Why use mathematical models in ecology and evolution? Syllabus","title":"Lecture overview"},{"location":"lectures/lecture-01/#1-why-use-mathematical-models-in-ecology-and-evolution","text":"Mathematics permeates ecology and evolution, from simple back-of-the-envelope calculations to the development of sophisticated mathematical models. This is because mathematics is a unique tool that can take what we already know (or assume to be true) and rigorously lead us to the logical conclusions. To see this (while also introducing you to the kind of models I work with), let's look at two examples.","title":"1. Why use mathematical models in ecology and evolution?"},{"location":"lectures/lecture-01/#example-1-hiv","text":"See sections 1.2-1.4 of the text for more info. Human immunodeficiency virus (HIV) is, as the name suggests, a virus that infects humans and causes acquired immunodeficiency syndrome (AIDS). This is a terrible diesase that has caused over 20 million deaths worldwide. It is transmitted via bodily fluids. Once inside the body HIV particles (virions) attach to a protein called CD4 on the cell membrane of helper T cells (part of our immune system) and others. Once attached, the virus inserts its RNA into the host cell, which is reverse transcribed into DNA and becomes part of the host genome. The host cell can remain in the 'latently infected' state for some time. When the viral DNA is eventually transcribed by the host cell, starting an 'active infection', hundreds of new virions are produced, often killing the host cell. These new virions then go on to infect other CD4+ cells. A large decline in the number of helper T cells is what causes AIDS. This is because helper T cells bind to viruses and secrete chemical signals to stimulate the immune system. So without helper T cells the immune system is very comprimised. HIV and T cells. Source: bio.libretexts.org Based on this, once infected with HIV (and without treatment) we expect the number of virions to rapidly increase and the number of helper T cells to decline. This is generally what is observed. However, the number of virions then tends to steeply decline. Why might this be? Here are two hypotheses: the immune system recognizes HIV and suppresses it the decline in helper T cells prevents HIV from replicating To decide whether the second hypothesis is valid Phillips (1996) built a mathematical model describing the rate of change in the number of CD4+ cells and free virions. See the note below for the details of the model if you are interested (but don't worry if you don't understand much of this yet!). Details of the Philips (1996) model See Box 2.4 in the text for more details. Philips built a dynamical model of the infection process using 4 differential equations: \\(\\frac{dR}{dt} = \\Gamma \\tau - \\mu R - \\beta V R\\) \\(\\frac{dL}{dt} = p \\beta V R - \\mu L - \\alpha L\\) \\(\\frac{dE}{dt} = (1 - p) \\beta V R + \\alpha L - \\delta E\\) \\(\\frac{dV}{dt} = \\pi E - \\sigma V\\) These equations describe the rate of change in the number of susceptible CD4+ cells (R), latently infected cells (L), actively infected cells (E), and free virions (V). These are the 4 \"variables\" of the model, as their values change over time. The remainder of the symbols represent \"parameters\", whose values do not change (they are constants). The meaning of the parameters and the values used by Phillips (1996) are shown in the table below. Symbol Description Value (units/day) \\(\\Gamma\\) Rate that CD4+ cells are produced 1.36 \\(\\tau\\) Proportion of CD4+ cells that are susceptible to attack 0.2 \\(\\mu\\) HIV-independent death rate of susceptible CD4+ cells 1.36 x 10^-3 \\(\\beta\\) Rate of CD4+ cell infection per HIV virion 0.00027 \\(p\\) Proportion of newly infected cells becoming latently infected 0.1 \\(\\alpha\\) Activation rate of latently infected cells 3.6 x 10^-2 \\(\\delta\\) Death rate of actively infected cells 0.33 \\(\\pi\\) Rate of production of virions by actively infected cells 100 \\(\\sigma\\) Removal rate of cell-free virus 2 Given these parameter descriptions, can you \"read\" the differential equations above? For example, what does \\(\\Gamma \\tau\\) represent, biologically? Using these equations and parameter values Philips (1996) asked how the number of CD4+ cells (R + L + E) and free virions (V) changed over time following infection. To see how Philips' model behaves, activate the kernel at the top of the page and run the code below import numpy as np #numerical tools import matplotlib.pyplot as plt #plotting tools # define a function to iterate through the model def philips_model(R=200, L=0, E=0, V=4e-5, days=120, steps=120): #default initial conditions and timesteps #choose parameter values gamma, mu, tau, beta, p, alpha, sigma, delta, pi = [1.36, 1.36e-3, 0.2, 0.00027, 0.1, 3.6e-2, 2, 0.33, 100] #update variables record = [] ts = np.linspace(0, days, steps) #times we record population sizes at for t in ts: #looping over timesteps dRdt = gamma * tau - mu * R - beta * V * R R += dRdt #uninfected T cells dLdt = p * beta * V * R - mu * L - alpha * L L += dLdt #latently infected T cells dEdt = (1-p) * beta * V * R + alpha * L - delta * E E += dEdt #actively infected T cells dVdt = pi * E - sigma * V V += dVdt #free virions record += ([[R + L + E, V]]) #total T cells, virions return ts, np.array(record) # iterate and record population sizes ts, ns = philips_model() #times and population sizes # initialize plot fig, left_ax = plt.subplots() right_ax = left_ax.twinx() fig.set_size_inches(8,4) # plot T cell data on left axis left_ax.plot(ts, ns[:,0], label='R'); left_ax.set_ylabel('CD4+ cells (R + E + L)', fontsize=14) left_ax.legend(frameon=False, bbox_to_anchor=(0.99, 0.99)) left_ax.set_xlabel('Days from infection', fontsize=14) # plot virion data on right axis right_ax.plot(ts, ns[:,1], label='V', color=plt.cm.tab10(1)); right_ax.set_ylabel('Free virions V', fontsize=14) right_ax.legend(frameon=False, bbox_to_anchor=(0.99, 0.85)) # format plot and show it plt.yscale('log') fig.tight_layout() plt.show() Compare to Figure 1.3 and Figure 1.4 in the text. We see the initial increase in virions (orange) and decline in CD4+ cells (blue), followed by a decline virions (note the log scale on the right axis -- this is a big decline, from over 1000 to about 10). Because this model does not include an immune response against the virions but still exhibits the decline in virions, we conclude that the second hypothesis is valid, that it is theoretically plausible that the decline in virions is due to a lack of CD4+ cells to infect. A few years later this hypothesis was empirically tested and validated -- a nice example of theory guiding science. Feel free to play around with the code above, changing parameter values or even the structure of the model. Do the dynamics change as you expected?","title":"Example 1: HIV"},{"location":"lectures/lecture-01/#example-2-extreme-events","text":"A second example is a model that I helped Dr. Kelsey Lyberger (then a PhD student at UC Davis) with in Lyberger et al 2021 . Kelsey Lyberger, doing Daphnia fieldwork I suppose. Kelsey was interested in how populations respond to extreme climatic events, like lizards to hurricanes. It has long been clear that such events can impact the size of a population, e.g., by causing extra mortality, and may in fact put populations at risk of extinction. More recently it has become apparent that extreme events can also impose strong natural selection, and that populations can quickly adapt to the new environment. Some examples include: Hurricanes select on lizard limbs and toe pads Ice-storms select on sparrow body size Droughts select on Darwin finch beaks Droughts select on flowering time in Brassica Lizard being blown off perch by a leaf blower. Source: colindonihue.com Now, how should such rapid adaptive evolution impact population size? This is the question Kelsey set out to answer with a mathematical model. Details of Kelsey's model Kelsey assumed each individual has a quantitative genetic trait, such as lizard limb length, that is determined by many alleles of small effect plus some environmental noise. Fitness is assumed to be a bell-shaped function of the difference between the optimum trait value, \\(\\theta\\) , and an individual's trait value, \\(z\\) , which we write as \\(W(\\theta - z)\\) . The change in population size, \\(N\\) , from one generation to rate the next is the current population size times mean fitness, \\(\\overline{W}(\\theta - \\bar{z})\\) , the latter depending on the population mean trait value, \\(\\bar{z}\\) . This gives \\(\\Delta N = N \\overline{W}(\\theta - \\bar{z}).\\) The change in the mean trait value from one generation to the next is roughly the product of genetic variance in the trait, \\(V_g\\) , and the strength of selection. The strength of selection is defined as the derivative of the natural logarithm of population mean fitness with respect to the mean trait value, \\(\\mathrm{d}\\ln\\overline{W}(\\theta - \\bar{z})/\\mathrm{d}\\bar{z}\\) . This gives \\(\\Delta \\bar{z} = V_g \\frac{\\mathrm{d}}{\\mathrm{d}\\bar{z}}\\ln\\overline{W}(\\theta - \\bar{z}).\\) Together, these coupled recursion equations, \\(\\Delta N\\) and \\(\\Delta \\bar{z}\\) , can be used to describe how evolution affects population size under an extreme event, which is modeled as a sudden but temporary change in the optimum phenotype, \\(\\theta\\) . Below is a stochastic simulation much like that used by Kelsey. With an activated kernel, run the code below to create a plot very similar to Figure 1 in Lyberger et al. (this may take a minute). import numpy as np import matplotlib.pyplot as plt def lyberger_model(Vo=0.75, Ve=0, event_duration=1, seed=0, other_parameters=[120, 500, 1, 2, 100, 0, 2.5]): # unpack parameters generations, K, w, lmbda, event_time, initial_theta_t, dtheta_t = other_parameters # initialize population members = np.random.normal(initial_theta_t, Vo**0.5, K) #K individuals with trait values distributed around the optimum # run simulations np.random.seed(seed) population_size, mean_breeding_value = [], [] for g in range(generations): # optimum trait value if g in np.arange(event_time, event_time + event_duration): theta_t = initial_theta_t + dtheta_t #extreme event else: theta_t = initial_theta_t #normal # viability selection prob_survival = np.array([np.exp(-(theta_t - z + np.random.normal(0, Ve))**2 / (2*w**2)) for z in members]) survived = np.array([True if p > np.random.uniform(0,1) else False for p in prob_survival]) if len(survived) == 0: break members = members[survived] # random mating offspring = [] for m in np.random.choice(members, len(members)): if len(offspring) > K: #if more than K offspring already then choose K and stop matings offspring = np.random.choice(offspring, K) break else: n_off = np.random.poisson(lmbda) mate = np.random.choice(members) offspring += [(m + mate)/2 for _ in range(n_off)] #give offspring mean parental value # sample new trait values for offspring offspring = np.array(offspring) offspring = np.random.normal(offspring, Vo) #add segregation variance # record statistics population_size.append(len(offspring)) mean_breeding_value.append(np.mean(offspring)) # update for next generation members = offspring return ( np.arange(0, generations-event_time+1), #times np.array(population_size[event_time-1:]), #population size np.array(mean_breeding_value[event_time-1:]) #mean trait value ) # initialize plot fig, ax = plt.subplots(2, sharex=True) fig.set_size_inches(8,6) # length of extreme event event_duration = 1 # run 10 simulations per segregation (Vo) and environment variance (Ve) parameter combination for Vo, Ve, c, lab in [[0.75, 0, 'black', 'with evolution'], [0, 1, 'red', 'without evolution']]: # plot simulations simulations = np.array([lyberger_model(Vo=Vo, Ve=Ve, event_duration=event_duration, seed=s) for s in range(10)]) ax[0].plot(simulations[:,0].T, simulations[:,1].T, color=c, alpha=0.3); ax[1].plot(simulations[:,0].T, simulations[:,2].T, color=c, alpha=0.3); # hack together only one instance of the legend ax[0].plot([np.min(simulations[:,0].T)], [np.min(simulations[:,1].T)], alpha=1, label = lab, color=c) ax[1].plot([np.min(simulations[:,0].T)], [np.min(simulations[:,2].T)], alpha=1, label = lab, color=c) # add environmental event duration ax[0].fill_between([0,event_duration], y1=500, alpha=0.2) ax[1].fill_between([0,event_duration], y1=-0.2, y2=2, alpha=0.2) # add labels ax[0].set_ylabel('Population size', fontsize=12) ax[1].set_ylabel('Mean trait value', fontsize=12) ax[1].set_xlabel('Generation', fontsize=14) # add legend plt.legend(frameon=False) plt.show() The key result, that you can see in the plot above, is that when extreme events are short, adaptive evolution (black lines) can paradoxically reduce population size (relative to the red lines, where there is no evolution). The reason for this is that, while during the extreme event (shaded section) evolution is adaptive, once the extreme event ends the population finds itself maladapted to the original environment. Adaptive evolution can therefore hamper population persistence, and this is an important thing to keep in mind when documenting rapid adaptive evolution in response to extreme events -- it is not necessarily a good thing for the species (or our conservation goals).","title":"Example 2: Extreme Events"},{"location":"lectures/lecture-01/#2-syllabus","text":"OK, now that we've gone over some motivating examples of modeling in ecology and evolution, let's take a look at how we're going to learn to become modelers in this course. Point your browser over to the syllabus and read each of the pages there.","title":"2. Syllabus"},{"location":"lectures/lecture-02/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 2: Model construction Run notes interactively? Lecture overview Constructing a model Example: Hardy-Weinberg equilibrium 1. Constructing a model Although many problems require specific formulations and assumptions, there are a few general principles to constructing a model. i. Formulate the question What do you want to know? Describe the model in the form of a question. Simplify, Simplify! Start with the simplest, biologically reasonable description of the problem. For example, we might ask: how does immigration affect population size? ii. Determine the basic ingredients Define the variables in the model. Describe any constraints on the variables. Describe any interactions between variables. Decide whether you will treat time as continuous or discrete. Choose a time scale (i.e., decide what a time step equals in discrete time and specify whether rates will be measured per second, minute, day, year, generation, etc.). Define the parameters in the model. Describe any constraints on the parameters. In our example, there is a single variable, \\(n(t)\\) , denoting population size at time \\(t\\) . This must be non-negative, \\(n(t)\\geq0\\) , to be biologically valid. We'll look at both continuous and discrete time, with an arbitrary time scale (if we wanted to plug in some numerical parameter values we'd have to specify this, but here we won't). In continuous time our parameters will be immigration rate ( \\(m\\) ), per capita birth rate ( \\(b\\) ), and per capita death rate ( \\(d\\) ). These must all be non-negative under our interpretation. The units of \\(m\\) are individuals/time while the units of \\(b\\) and \\(d\\) are simply 1/time (e.g., for birth we have the number of individuals produced per individual per time, so that individuals cancels out and we are left with 1/time). In discrete time the parameters will be the average number of immigrants per time step ( \\(M\\) ), the average number of offspring per individual per time step ( \\(B\\) ), and the fraction of individuals that die each time step ( \\(D\\) ). These must all be non-negative and \\(D\\) must also be less than or equal to 1 as it is a fraction. iii. Qualitatively describe the biological system For continuous-time models, draw a flow diagram to describe changes to the variables over time. For our example we could draw the following: graph LR; A((n)) --birth--> A; B[ ] --migration--> A; A --death--> C[ ]; style B height:0px; style C height:0px; For discrete-time models, draw a life-cycle diagram with all the events that occur each time unit. For our example, if we assume migration, then birth, then death each time step, we could draw the following: graph LR; A((n)) --migration--> B((n')); B --birth--> C((n'')); C --death--> A; For discrete time models with multiple variables and events, construct a table listing the outcome of every event. We'll see an example of this below. iv. Quantitatively describe the biological system Using the diagrams and tables as a guide, write down the equations. Perform checks. Are the constraints on the variables still met as time passes? Make sure that the units of the right-hand side equal those on the left-hand side. For continuous-time models, add rates for arrows coming in to the flow diagram and subtract rates for arrows coming out of the flow diagram. For example, in the model shown above the rate of change in the number of individuals, \\(\\frac{\\mathrm{d} n}{\\mathrm{d} t}\\) , is \\[ \\begin{aligned} \\frac{\\mathrm{d} n}{\\mathrm{d} t} &= m + b n(t) - d n(t)\\\\ &= m + (b - d) n(t) \\end{aligned} \\] This is a differential equation . In discrete time, you must take into account the order of events when constructing equations. To build an equation for the population size in the next generation, \\(n(t+1)\\) , based on the life-cycle diagram above, first construct an equation for each event \\[n' = n(t) + m\\] \\[n'' = n' + bn'\\] \\[n(t+1) = n'' - dn''\\] Next, substitute \\(n''\\) and then \\(n'\\) into the equation for \\(n(t+1)\\) to write \\(n(t+1)\\) in terms of \\(n(t)\\) \\[ \\begin{aligned} n(t+1) &= n'' \u2212 dn'' \\\\ &= (n' + bn\u2032) \u2212 d(n' + bn\u2032) \\\\ &= n'(1 + b \u2212 d \u2212 db) \\\\ &= (n(t) + m)(1 + b \u2212 d \u2212 db) \\\\ \\end{aligned} \\] We now have a recursion equation that correctly takes into account the order of the life cycle (migration, birth, death) and the point at which the census is taken (immediately after death). v. Analyze the equations Start by using the equations to simulate and graph the changes to the system over time. Choose and perform appropriate analyses. Make sure that the analyses can address the problem. We'll save the mathematical analyses for later in the course, but we can use the above equations to simulate the model. import numpy as np import matplotlib.pyplot as plt # define def discrete_model(nt, b, d, m): '''recursion equation giving population size in next time step as a function of the population size at this time, nt, and the birth, death, and immigration rates, b, d, and m.''' return (nt + m) * (1 + b - d - b*d) #the recursion equation we derived above def continuous_model(nt, b, d, m, dt): '''approximate differential equation giving population size in next time step as a function of the population size at this time, nt, and the birth, death, and immigration rates, b, d, and m. The dt is the size of the time step: as this approaches 0 we get the differential equation.''' return nt + (m + (b - d)*nt)*dt #the differential equation we derived above # simulate nd, nc = [], [] #empty lists to store data ntd, ntc = 1, 1 #initial popn size b, d, m, T, dt = 0.05, 0.1, 1, 100, 0.1 #parameter values for t in np.arange(0,T,dt): if t%1==0: ntd = discrete_model(ntd, b, d, m) #get the next population size from the recursion equation nd.append(ntd) #and append it to the list ntc = continuous_model(ntc, b, d, m, dt) #get the next population size from the differential equation nc.append(ntc) #and append it to the list # plot fig, ax = plt.subplots() ax.scatter(np.arange(0, T), nd, label='discrete') #plot population size at each time ax.scatter(np.arange(0, T, dt), nc, label='continuous') #plot population size at each time ax.set_xlabel('time, $t$') ax.set_ylabel('population size, $n(t)$') plt.legend() plt.show() vi. Checks and balances Check the results against data or any known special cases. Determine how general the results are. Consider alternatives to the simplest model. Extend or simplify the model, as appropriate, and repeat steps 2-5. If \\(b>d\\) the population grows without bound (try this in the code above). But competition should prevent unbounded population growth. We could therefore extend the model to include competition. We'll see one way to do this in the next lecture. vii. Relate the results back to the question Do the results answer the biological question? Are the results counter-intuitive? Why? Interpret the results verbally, and describe conceptually any new insights into the biological process. Describe potential experiments. Immigration appears to increase the population size (try setting \\(m=0\\) for comparison in the code above), though we'd need to do the mathematical analysis to be more confident in this statement. There is some counter-intuitiveness in the discrete model: try creating the recursion equation for a different order of events in the lifecycle. The recursion equation depends on the order and will therefore lead to different dynamics. Potential experiments incude manipulating immigration in lab populations (e.g., bacteria) or comparing population sizes on islands that are different distances from the mainland. 2. Example: Hardy-Weinberg equilibrium To demonstrate how to use a table of events let's revisit what should be a familiar evolutionary concept: Hardy-Weinberg equilibrium. i. Formulate the question What do you want to know How do genotype frequencies change due to random mating and segregation alone? Boil the question down In a population with two variant \"alleles\" of a gene (A and a), how will the frequencies of the AA, Aa, and aa diploid genotypes change over time? Simple, biologically reasonable description We assume all individuals have equal fitness and that individuals reproduce and then die (non-overlapping generations). We also assume that individuals produce haploid gametes via meiosis (segregation) to form a gamete pool. Gametes within the gamete pool unite at random to produce the next generation of diploid individuals. ii. Determine the basic ingredients Variables \\(x\\) = frequency of AA individuals \\(y\\) = frequency of Aa individuals \\(z\\) = frequency of aa individuals From these we can extract the allele frequencies: the frequency of A is \\(p = x + y/2\\) (ie, all of the alleles in genotype AA ( \\(x\\) ) are A but only 1/2 of the alleles in genotype Aa ( \\(y\\) ) are A) the frequency of a is \\(q = 1 - p = y/2 + z\\) Constraints on these variables \\(x\\) , \\(y\\) , \\(z\\) are \\(\u22650\\) and \\(\u22641\\) \\(x+y+z=1\\) How we'll treat time We will follow the genotype frequencies from one generation to the next, using a discrete-time model Parameters there are no parameters in this model (which is a little bit unusual!) iii. Qualitatively describe the biological system Diploid adults undergo meiosis, creating haploid gametes, and die. Gametes unite at random in the gamete pool to produce diploid offspring, which form the adults in next generation. Below we draw the life-cycle diagram. ```mermaid graph LR; A((x,y,z)) --meiosis--> B((p')); B --union--> A; Since there are no diploid genotypes in the gamete pooled formed after meiosis, there we track the frequency of the A allele, \\(p'\\) . iv. Quantiatively describe the biological system If all individuals create the same number of gametes and each individual produces an equal number of gametes with each allele then meiosis/segregation does not change allele frequencies, \\(p'=p(t)=x(t)+y(t)/2\\) . To calculate the frequency of each genoytpe in the next generation, via random union of gametes, we use a table of events. The \"Union\" column indicates the pair of gametes that are meeting each other, the \"Frequency\" column indicates the proportion of gamete pairs in the population with this particular union, and the remaining columns indicate which diploid genotype is created by the union. Union Frequency AA Aa aa A x A \\(p'^2\\) 1 0 0 A x a \\(p'q'\\) 0 1 0 a x A \\(q'p'\\) 0 1 0 a x a \\(q'^2\\) 0 0 1 \\(p'^2\\) \\(2p'q'\\) \\(q'^2\\) The genotype frequencies denoted in red are known as the Hardy-Weinberg frequencies . They relate the genotype frequencies with the allele frequency. This table shows that populations not at \"Hardy-Weinberg equilibrium\" reach Hardy-Weinberg equilibrium after only one generation of random mating. We now have the frequency of genotypes in the next generation in terms of the allele frequencies in the previous generation, \\[x(t+1) = p'^2 = p(t)^2\\] \\[y(t+1) = 2p'q' = 2p(t)q(t)\\] \\[z(t+1) = q'^2 = q(t)^2\\] To understand how the genotype frequencies at Hardy-Weinberg vary with allele frequency, we can plot them. from sympy import * var('p') x = p**2 #frequency of AA at Hardy-Weinberg as a function of the frequency of allele A y = 2*p*(1-p) #freq of Aa z = (1-p)**2 #freq of aa p = plot(x, y, z, #functions that we are plotting (p,0,1), #plot as a function of allele frequency from 0 to 1 xlabel=\"allele frequency, $p$\", ylabel=\"genotype frequency\", legend=True, show=False ) p[0].label='AA' #give legend genotype labels p[1].label='Aa' p[2].label='aa' p.show() From this plot we see, for example, that the frequency of heterozygotes, Aa, is maximized at intermediate allele frequencies. Subbing in \\(p=x+y/2\\) and \\(q=y/2+z\\) we get the recursion equations describing the frequency of diploid genotypes in the next generation as a function of the diploid genotypes in the current generation, \\[x(t+1) = (x(t) + y(t)/2)^2\\] \\[y(t+1) = 2(x(t) + y(t)/2)(y(t)/2 + z(t))\\] \\[z(t+1) = (y(t)/2 + z(t))^2\\] v. Analyze the equations Now back to our question. How do the genotype frequencies change over time? We could simulate or analyze the recursion equations above, but there is an easier way in this case. First examine how the allele frequency changes: \\[p(t+1) = x(t+1) + y(t+1)/2\\] \\[p(t+1) = p(t)^2 + 2p(t)q(t)/2\\] \\[p(t+1) = p(t)(p(t) + q(t))\\] \\[p(t+1) = p(t)\\] It doesn't! Since \\(p(t)\\) is a constant and the genotype frequencies reach Hardy-Weinberg in a single generation, e.g., \\(x(t+1) = p(t)^2\\) , then the genotype frequencies are also constant after that first generation. vi. Checks and balances Does \\(x(t+1) + y(t+1) + z(t+1) = 1\\) ? \\[ \\begin{aligned} x(t+1) + y(t+1) + z(t+1) &= p(t+1)^2 + 2p(t+1)q(t+1) + q(t+1)^2\\\\ &= (p(t+1)+q(t+1))^2\\\\ &= (1)^2\\\\ &= 1 \\end{aligned} \\] vii. Relate the results back to the question How do genotype frequencies change over time due to random mating and segregation? They reach Hardy-Weinberg after a single generation and never change again Data example: blood types Below is a table describing the frequency of three different blood types (and their associated genotype) in a sample of humans from the USA. Blood type M MN N Genotype MM MN NN Observed frequency in USA 0.292 0.496 0.212 We ask, are these genotypes near Hardy-Weinberg equilibrium? First, write the frequency of each genotype as \\(x\\) , \\(y\\) , and \\(z\\) \\[x = 0.292\\] \\[y = 0.496\\] \\[z = 0.212\\] Next, convert the genotype frequencies into allele frequencies \\[p = x + y/2 = 0.540\\] \\[q = y/2 + z = 0.460\\] Now predict what the genotype frequencies would be at Hardy-Weinberg equilibrium \\[x = p^2 = 0.2916\\] \\[y = 2pq = 0.4985\\] \\[z = q^2 = 0.2116\\] These predicted genotype frequencies are exceptionally close to those actually observed, indicating Hardy-Weinberg equilibrium.","title":"Lecture 2"},{"location":"lectures/lecture-02/#lecture-2-model-construction","text":"Run notes interactively?","title":"Lecture 2: Model construction"},{"location":"lectures/lecture-02/#lecture-overview","text":"Constructing a model Example: Hardy-Weinberg equilibrium","title":"Lecture overview"},{"location":"lectures/lecture-02/#1-constructing-a-model","text":"Although many problems require specific formulations and assumptions, there are a few general principles to constructing a model.","title":"1. Constructing a model"},{"location":"lectures/lecture-02/#i-formulate-the-question","text":"What do you want to know? Describe the model in the form of a question. Simplify, Simplify! Start with the simplest, biologically reasonable description of the problem. For example, we might ask: how does immigration affect population size?","title":"i. Formulate the question"},{"location":"lectures/lecture-02/#ii-determine-the-basic-ingredients","text":"Define the variables in the model. Describe any constraints on the variables. Describe any interactions between variables. Decide whether you will treat time as continuous or discrete. Choose a time scale (i.e., decide what a time step equals in discrete time and specify whether rates will be measured per second, minute, day, year, generation, etc.). Define the parameters in the model. Describe any constraints on the parameters. In our example, there is a single variable, \\(n(t)\\) , denoting population size at time \\(t\\) . This must be non-negative, \\(n(t)\\geq0\\) , to be biologically valid. We'll look at both continuous and discrete time, with an arbitrary time scale (if we wanted to plug in some numerical parameter values we'd have to specify this, but here we won't). In continuous time our parameters will be immigration rate ( \\(m\\) ), per capita birth rate ( \\(b\\) ), and per capita death rate ( \\(d\\) ). These must all be non-negative under our interpretation. The units of \\(m\\) are individuals/time while the units of \\(b\\) and \\(d\\) are simply 1/time (e.g., for birth we have the number of individuals produced per individual per time, so that individuals cancels out and we are left with 1/time). In discrete time the parameters will be the average number of immigrants per time step ( \\(M\\) ), the average number of offspring per individual per time step ( \\(B\\) ), and the fraction of individuals that die each time step ( \\(D\\) ). These must all be non-negative and \\(D\\) must also be less than or equal to 1 as it is a fraction.","title":"ii. Determine the basic ingredients"},{"location":"lectures/lecture-02/#iii-qualitatively-describe-the-biological-system","text":"For continuous-time models, draw a flow diagram to describe changes to the variables over time. For our example we could draw the following: graph LR; A((n)) --birth--> A; B[ ] --migration--> A; A --death--> C[ ]; style B height:0px; style C height:0px; For discrete-time models, draw a life-cycle diagram with all the events that occur each time unit. For our example, if we assume migration, then birth, then death each time step, we could draw the following: graph LR; A((n)) --migration--> B((n')); B --birth--> C((n'')); C --death--> A; For discrete time models with multiple variables and events, construct a table listing the outcome of every event. We'll see an example of this below.","title":"iii. Qualitatively describe the biological system"},{"location":"lectures/lecture-02/#iv-quantitatively-describe-the-biological-system","text":"Using the diagrams and tables as a guide, write down the equations. Perform checks. Are the constraints on the variables still met as time passes? Make sure that the units of the right-hand side equal those on the left-hand side. For continuous-time models, add rates for arrows coming in to the flow diagram and subtract rates for arrows coming out of the flow diagram. For example, in the model shown above the rate of change in the number of individuals, \\(\\frac{\\mathrm{d} n}{\\mathrm{d} t}\\) , is \\[ \\begin{aligned} \\frac{\\mathrm{d} n}{\\mathrm{d} t} &= m + b n(t) - d n(t)\\\\ &= m + (b - d) n(t) \\end{aligned} \\] This is a differential equation . In discrete time, you must take into account the order of events when constructing equations. To build an equation for the population size in the next generation, \\(n(t+1)\\) , based on the life-cycle diagram above, first construct an equation for each event \\[n' = n(t) + m\\] \\[n'' = n' + bn'\\] \\[n(t+1) = n'' - dn''\\] Next, substitute \\(n''\\) and then \\(n'\\) into the equation for \\(n(t+1)\\) to write \\(n(t+1)\\) in terms of \\(n(t)\\) \\[ \\begin{aligned} n(t+1) &= n'' \u2212 dn'' \\\\ &= (n' + bn\u2032) \u2212 d(n' + bn\u2032) \\\\ &= n'(1 + b \u2212 d \u2212 db) \\\\ &= (n(t) + m)(1 + b \u2212 d \u2212 db) \\\\ \\end{aligned} \\] We now have a recursion equation that correctly takes into account the order of the life cycle (migration, birth, death) and the point at which the census is taken (immediately after death).","title":"iv. Quantitatively describe the biological system"},{"location":"lectures/lecture-02/#v-analyze-the-equations","text":"Start by using the equations to simulate and graph the changes to the system over time. Choose and perform appropriate analyses. Make sure that the analyses can address the problem. We'll save the mathematical analyses for later in the course, but we can use the above equations to simulate the model. import numpy as np import matplotlib.pyplot as plt # define def discrete_model(nt, b, d, m): '''recursion equation giving population size in next time step as a function of the population size at this time, nt, and the birth, death, and immigration rates, b, d, and m.''' return (nt + m) * (1 + b - d - b*d) #the recursion equation we derived above def continuous_model(nt, b, d, m, dt): '''approximate differential equation giving population size in next time step as a function of the population size at this time, nt, and the birth, death, and immigration rates, b, d, and m. The dt is the size of the time step: as this approaches 0 we get the differential equation.''' return nt + (m + (b - d)*nt)*dt #the differential equation we derived above # simulate nd, nc = [], [] #empty lists to store data ntd, ntc = 1, 1 #initial popn size b, d, m, T, dt = 0.05, 0.1, 1, 100, 0.1 #parameter values for t in np.arange(0,T,dt): if t%1==0: ntd = discrete_model(ntd, b, d, m) #get the next population size from the recursion equation nd.append(ntd) #and append it to the list ntc = continuous_model(ntc, b, d, m, dt) #get the next population size from the differential equation nc.append(ntc) #and append it to the list # plot fig, ax = plt.subplots() ax.scatter(np.arange(0, T), nd, label='discrete') #plot population size at each time ax.scatter(np.arange(0, T, dt), nc, label='continuous') #plot population size at each time ax.set_xlabel('time, $t$') ax.set_ylabel('population size, $n(t)$') plt.legend() plt.show()","title":"v. Analyze the equations"},{"location":"lectures/lecture-02/#vi-checks-and-balances","text":"Check the results against data or any known special cases. Determine how general the results are. Consider alternatives to the simplest model. Extend or simplify the model, as appropriate, and repeat steps 2-5. If \\(b>d\\) the population grows without bound (try this in the code above). But competition should prevent unbounded population growth. We could therefore extend the model to include competition. We'll see one way to do this in the next lecture.","title":"vi. Checks and balances"},{"location":"lectures/lecture-02/#vii-relate-the-results-back-to-the-question","text":"Do the results answer the biological question? Are the results counter-intuitive? Why? Interpret the results verbally, and describe conceptually any new insights into the biological process. Describe potential experiments. Immigration appears to increase the population size (try setting \\(m=0\\) for comparison in the code above), though we'd need to do the mathematical analysis to be more confident in this statement. There is some counter-intuitiveness in the discrete model: try creating the recursion equation for a different order of events in the lifecycle. The recursion equation depends on the order and will therefore lead to different dynamics. Potential experiments incude manipulating immigration in lab populations (e.g., bacteria) or comparing population sizes on islands that are different distances from the mainland.","title":"vii. Relate the results back to the question"},{"location":"lectures/lecture-02/#2-example-hardy-weinberg-equilibrium","text":"To demonstrate how to use a table of events let's revisit what should be a familiar evolutionary concept: Hardy-Weinberg equilibrium.","title":"2. Example: Hardy-Weinberg equilibrium"},{"location":"lectures/lecture-02/#i-formulate-the-question_1","text":"What do you want to know How do genotype frequencies change due to random mating and segregation alone? Boil the question down In a population with two variant \"alleles\" of a gene (A and a), how will the frequencies of the AA, Aa, and aa diploid genotypes change over time? Simple, biologically reasonable description We assume all individuals have equal fitness and that individuals reproduce and then die (non-overlapping generations). We also assume that individuals produce haploid gametes via meiosis (segregation) to form a gamete pool. Gametes within the gamete pool unite at random to produce the next generation of diploid individuals.","title":"i. Formulate the question"},{"location":"lectures/lecture-02/#ii-determine-the-basic-ingredients_1","text":"Variables \\(x\\) = frequency of AA individuals \\(y\\) = frequency of Aa individuals \\(z\\) = frequency of aa individuals From these we can extract the allele frequencies: the frequency of A is \\(p = x + y/2\\) (ie, all of the alleles in genotype AA ( \\(x\\) ) are A but only 1/2 of the alleles in genotype Aa ( \\(y\\) ) are A) the frequency of a is \\(q = 1 - p = y/2 + z\\) Constraints on these variables \\(x\\) , \\(y\\) , \\(z\\) are \\(\u22650\\) and \\(\u22641\\) \\(x+y+z=1\\) How we'll treat time We will follow the genotype frequencies from one generation to the next, using a discrete-time model Parameters there are no parameters in this model (which is a little bit unusual!)","title":"ii. Determine the basic ingredients"},{"location":"lectures/lecture-02/#iii-qualitatively-describe-the-biological-system_1","text":"Diploid adults undergo meiosis, creating haploid gametes, and die. Gametes unite at random in the gamete pool to produce diploid offspring, which form the adults in next generation. Below we draw the life-cycle diagram. ```mermaid graph LR; A((x,y,z)) --meiosis--> B((p')); B --union--> A; Since there are no diploid genotypes in the gamete pooled formed after meiosis, there we track the frequency of the A allele, \\(p'\\) .","title":"iii. Qualitatively describe the biological system"},{"location":"lectures/lecture-02/#iv-quantiatively-describe-the-biological-system","text":"If all individuals create the same number of gametes and each individual produces an equal number of gametes with each allele then meiosis/segregation does not change allele frequencies, \\(p'=p(t)=x(t)+y(t)/2\\) . To calculate the frequency of each genoytpe in the next generation, via random union of gametes, we use a table of events. The \"Union\" column indicates the pair of gametes that are meeting each other, the \"Frequency\" column indicates the proportion of gamete pairs in the population with this particular union, and the remaining columns indicate which diploid genotype is created by the union. Union Frequency AA Aa aa A x A \\(p'^2\\) 1 0 0 A x a \\(p'q'\\) 0 1 0 a x A \\(q'p'\\) 0 1 0 a x a \\(q'^2\\) 0 0 1 \\(p'^2\\) \\(2p'q'\\) \\(q'^2\\) The genotype frequencies denoted in red are known as the Hardy-Weinberg frequencies . They relate the genotype frequencies with the allele frequency. This table shows that populations not at \"Hardy-Weinberg equilibrium\" reach Hardy-Weinberg equilibrium after only one generation of random mating. We now have the frequency of genotypes in the next generation in terms of the allele frequencies in the previous generation, \\[x(t+1) = p'^2 = p(t)^2\\] \\[y(t+1) = 2p'q' = 2p(t)q(t)\\] \\[z(t+1) = q'^2 = q(t)^2\\] To understand how the genotype frequencies at Hardy-Weinberg vary with allele frequency, we can plot them. from sympy import * var('p') x = p**2 #frequency of AA at Hardy-Weinberg as a function of the frequency of allele A y = 2*p*(1-p) #freq of Aa z = (1-p)**2 #freq of aa p = plot(x, y, z, #functions that we are plotting (p,0,1), #plot as a function of allele frequency from 0 to 1 xlabel=\"allele frequency, $p$\", ylabel=\"genotype frequency\", legend=True, show=False ) p[0].label='AA' #give legend genotype labels p[1].label='Aa' p[2].label='aa' p.show() From this plot we see, for example, that the frequency of heterozygotes, Aa, is maximized at intermediate allele frequencies. Subbing in \\(p=x+y/2\\) and \\(q=y/2+z\\) we get the recursion equations describing the frequency of diploid genotypes in the next generation as a function of the diploid genotypes in the current generation, \\[x(t+1) = (x(t) + y(t)/2)^2\\] \\[y(t+1) = 2(x(t) + y(t)/2)(y(t)/2 + z(t))\\] \\[z(t+1) = (y(t)/2 + z(t))^2\\]","title":"iv. Quantiatively describe the biological system"},{"location":"lectures/lecture-02/#v-analyze-the-equations_1","text":"Now back to our question. How do the genotype frequencies change over time? We could simulate or analyze the recursion equations above, but there is an easier way in this case. First examine how the allele frequency changes: \\[p(t+1) = x(t+1) + y(t+1)/2\\] \\[p(t+1) = p(t)^2 + 2p(t)q(t)/2\\] \\[p(t+1) = p(t)(p(t) + q(t))\\] \\[p(t+1) = p(t)\\] It doesn't! Since \\(p(t)\\) is a constant and the genotype frequencies reach Hardy-Weinberg in a single generation, e.g., \\(x(t+1) = p(t)^2\\) , then the genotype frequencies are also constant after that first generation.","title":"v. Analyze the equations"},{"location":"lectures/lecture-02/#vi-checks-and-balances_1","text":"Does \\(x(t+1) + y(t+1) + z(t+1) = 1\\) ? \\[ \\begin{aligned} x(t+1) + y(t+1) + z(t+1) &= p(t+1)^2 + 2p(t+1)q(t+1) + q(t+1)^2\\\\ &= (p(t+1)+q(t+1))^2\\\\ &= (1)^2\\\\ &= 1 \\end{aligned} \\]","title":"vi. Checks and balances"},{"location":"lectures/lecture-02/#vii-relate-the-results-back-to-the-question_1","text":"How do genotype frequencies change over time due to random mating and segregation? They reach Hardy-Weinberg after a single generation and never change again Data example: blood types Below is a table describing the frequency of three different blood types (and their associated genotype) in a sample of humans from the USA. Blood type M MN N Genotype MM MN NN Observed frequency in USA 0.292 0.496 0.212 We ask, are these genotypes near Hardy-Weinberg equilibrium? First, write the frequency of each genotype as \\(x\\) , \\(y\\) , and \\(z\\) \\[x = 0.292\\] \\[y = 0.496\\] \\[z = 0.212\\] Next, convert the genotype frequencies into allele frequencies \\[p = x + y/2 = 0.540\\] \\[q = y/2 + z = 0.460\\] Now predict what the genotype frequencies would be at Hardy-Weinberg equilibrium \\[x = p^2 = 0.2916\\] \\[y = 2pq = 0.4985\\] \\[z = q^2 = 0.2116\\] These predicted genotype frequencies are exceptionally close to those actually observed, indicating Hardy-Weinberg equilibrium.","title":"vii. Relate the results back to the question"},{"location":"lectures/lecture-03/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 3: Exponential & logistic growth Run notes interactively? Lecture overview Introduction Exponential growth Logistic growth 1. Introduction In nature, population sizes change over time in response to a myriad of factors, such as weather competition, predation, disease, ... resource availability The simplest models describing changes in population size are exponential growth and logistic growth which assume a constant environment no interactions with other species The exponential model also assumes no competition among the members of a species for the available resources ( density-independent growth ), while the logistic model includes competition within a species ( density-dependent growth ). Both of these models can be described in discrete and continuous time. We\u2019ll start with the simpler exponential model. 2. Exponential growth Exponential growth in discrete time Imagine we start with \\(n(t)\\) individuals at some time \\(t\\) . If we assume that each of these individuals produces \\(b\\) offspring, the number of individuals after reproduction is \\(n(t) + n(t) b = n(t)(1 + b)\\) . If we then assume a fraction \\(d\\) die, the number of individuals remaining after death is \\(n(t)(1+b) - n(t)(1+b)d = n(t)(1+b)(1-d)\\) . With no further events in the life-cycle, this is the expected number of individuals in the next generation, \\(n(t+1)\\) , which we can write as \\[ \\begin{aligned} n(t+1) &= n(t)(1+b)(1-d) \\\\ &= n(t) R \\\\ \\end{aligned} \\] where \\(R=(1+b)(1-d)\\) is a constant referred to as the reproductive factor . This equation, \\(n(t+1)=n(t) R\\) , is the recursion equation for exponential growth. Exponential vs geometric growth Technically this recursion equation describes \"geometric\" growth, since \\(n(t)\\) will grow with \\(t\\) as a geometric series, but here we simply call it \"exponential growth in discrete time\" to make a clear connection with exponential growth in continuous time. We can also describe the change in the number of individuals by subtracting off the current number, \\[ \\begin{aligned} \\Delta n &= n(t+1) - n(t)\\\\ &= n(t)(R-1) \\end{aligned} \\] This is the difference equation for exponential growth, with discrete-time growth rate \\(r_d = R-1 = (1+b)(1-d)-1 = b - d - bd\\) . Let's plot the dynamics described by these equations for a particular set of parameter values. import numpy as np import matplotlib.pyplot as plt def exponential_discrete(nt, b, d): '''recursion equation giving population size in next time step as a function of the population size at this time, nt, and the birth and death rates, b and d.''' return nt * (1 + b) * (1 - d) # Grow population nd, nt, b, d = [], 1, 0.2, 0.1 #define empty list nd to store population sizes and choose parameter values for t in np.arange(0,100): #for time from 0 to 99 nt = exponential_discrete(nt, b, d) #get the next population size from the recursion equation nd.append(nt) #and append it to the list # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100), nd) #plot population size at each time ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n(t)$') plt.show() Exponential growth in continuous time Now assume that each individual continuously gives birth at rate \\(b\\) and dies at rate \\(d\\) . If there are \\(n(t)\\) individuals in the population at time \\(t\\) , then the instantaneous rate of change in the number of individuals is \\[ \\begin{aligned} \\frac{\\mathrm{d} n}{\\mathrm{d} t} &= n(t) b - n(t) d\\\\ &= (b - d) n(t)\\\\ &= r_c n(t) \\end{aligned} \\] This is the differential equation for exponential growth with continuous-time growth rate \\(r_c = b - d\\) . Note that the growth rate in the discrete-time model was \\(r_d = b - d - b d\\) . The difference between the two growth rates reflects the fact that birth and death cannot happen at the exact same time in the continuous-time model (so there is no \\(b d\\) term), while offspring that are born can die before the next generation in the discrete-time model (causing the \\(b d\\) term). Let's also plot these dynamics. Approximating a differential equation The differential equation describes the change in the population size in an \"infinitesimally\" small amount of time, \\(\\mathrm{d}t\\) . To plot these dynamics we therefore make an approximation, taking \\(\\mathrm{d}t\\) to be small, but not infinitely so. Rearranging the differential equation gives \\(\\mathrm{d}n(t) = n(t)(b-d)\\mathrm{d}t\\) and the population size after \\(\\mathrm{d}t\\) is therefore \\(n_{t+\\mathrm{d}t} = n(t) + \\mathrm{d}n(t)\\) . This is a recursion equation that approximates our differential equation. import numpy as np import matplotlib.pyplot as plt def exponential_continuous(nt, b, d, dt): '''approximation of the differential equation giving population size after a small time interval, dt, as a function of the population size at this time, nt, and the birth and death rates, b and d.''' return nt + nt * (b - d) * dt # Grow population nc, nt, dt = [], 1, 0.1 #define empty list nc to store population sizes and choose parameter values (keep b and d as above) for t in np.arange(0,100,dt): #for time from 0 to 99 by increments of dt nt = exponential_continuous(nt, b, d, dt) #get the next population size from the recursion equation nc.append(nt) #and append it to the list # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100, dt), nc) #plot population size at each time ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n(t)$') plt.show() We can now combine our two plots to compare these two predictions, exponential growth in discrete vs. continuous time. # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100), nd, label='discrete time') #plot population size at each time ax.scatter(np.arange(0, 100, dt), nc, label='continuous time') #plot population size at each time ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n(t)$') plt.legend() plt.show() Why the difference? The predictions are very similar at first but then noticeably diverge. The time of this divergence depends on the values of parameters \\(b\\) and \\(d\\) . Try increasing or decreasing both \\(b\\) and \\(d\\) and think about why it has that effect. Also think about why the continuous time model predicts a larger population size than the discrete time model. Hint: remember the difference in the growth rates between the two models is \\(bd\\) . The trouble with exponential growth Exponential growth cannot continue indefinitely. Take for example a population of pheasants on an island off the coast of Washington State. Just 8 pheasants were introduced in 1937, but the population then grew exponentially, tripling in size every year ( \\(R=3\\) ) for the first 5 years. Had this population continued to grow exponentially there would have been 7 million of them by the year 1950 and \\(10^{28}\\) by now \u2013 which at 2 kg per pheasant is 3000 times the mass of the earth!! Although populations may initially experience exponential growth, resources eventually become depleted and competition becomes more severe. This suggests that we should change our model assumptions. 3. Logistic growth Exponential growth assumes the growth rate ( \\(r_d\\) , \\(r_c\\) ) is constant. Logistic growth relaxes this assumption, and instead assumes that the growth rate decreases linearly with population size, due to competition for resources within the population. Logistic growth in discrete time In discrete-time, the reproductive factor under logistic growth can be written as \\[R(n(t)) = 1 + r\\left(1 - \\frac{n(t)}{K}\\right)\\] Notice that each individual is expected to have one offspring ( \\(R=1\\) ) if the intrinsic growth rate (ie, growth rate when rare) is zero, \\(r = 0\\) , or if the population size is at carrying capacity , \\(n(t)=K\\) . Try plotting the reproductive factor as a function of \\(n(t)\\) for a few different values of \\(r\\) and \\(K\\) . # Reproductive factor for logistic growth def logistic_discrete(nt, r, K): '''reproductive factor in discrete logistic model with growth rate r and carrying capacity k''' return 1 + r * (1 - nt/K) # Compare a few different growth rates and carrying capacities fig, ax = plt.subplots() for r, K in zip([1, 2, 1], [100, 100, 50]): #for each pair of r and K values nt = np.linspace(0, 200) #for a range of population sizes from 0 to 200 R = logistic_discrete(nt, r, K) #calculate the reproductive factor ax.plot(nt, R, label=f\"r = {r}, K = {K}\") #and plot ax.plot(nt, [1 for i in nt], '--', color='gray') #1 line for reference ax.set_xlabel('Population size, $n(t)$') ax.set_ylabel('Reproductive factor, $R$') ax.legend(frameon=False) plt.ylim(0,None) plt.show() The population size in the next generation is the expected number of offspring per parent times the the total number of parents \\[n(t+1) = \\left(1 + r\\left(1-\\frac{n(t)}{K}\\right)\\right)n(t)\\] This is the recursion equation for logistic growth. This recursion is a non-linear function of \\(n(t)\\) ( non-linear means that there is a term in the equation where the term is taken to some power other than 1; here if we expand out the recursion we get a \\(n(t)^2\\) term ). This reflects the fact that logistic growth models an interaction between individuals (competition). The change in population size from one generation to the next, \\(\\Delta n\\) , is therefore \\[\\Delta n = n(t+1) - n(t) = r\\left(1 - \\frac{n(t)}{K}\\right)n(t)\\] Based on this difference equation, when will the population grow in size? Test out your answer by plotting population size over time in the discrete-time logistic model. Try changing the initial population size or carrying capacity so that \\(n(t) > K\\) . # Initialize parameters n, nt, r, K = [], 1, 0.1, 100 #list, initial population size, intrinsic growth rate, carrying capacity # Grow population under logistic growth for t in np.arange(0, 100): nt = nt * logistic_discrete(nt, r, K) n.append(nt) # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100), n) ax.axhline(100, label=f\"K = {K}\", linestyle='dashed', color='gray') #carrying capacity as dashed line # Add annotations ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n(t)$') plt.show() Complex dynamics Logistic growth in discrete time can show some surprisingly complex dynamics, which we'll explore in lab. Logistic growth in continuous time The model of logistic growth in continuous time, as with discrete time, follows from the assumption that each individual has a growth rate that decreases as a linear function of the population size \\(r(1 - n(t)/K)\\) . If there are \\(n(t)\\) individuals in the population at time \\(t\\) , then the rate of change of the population size will be \\[\\frac{\\mathrm{d}n}{\\mathrm{d}t} = r\\left(1 - \\frac{n(t)}{K}\\right)n(t)\\] This is a differential equation of logistic growth. Note that in both discrete and continuous time the logistic growth model reduces to the exponential growth model as \\(n/K\\) approaches 0, i.e., when the population size is much smaller than the carrying capacity \\(n << K\\) . An example of logistic growth Dr. Sarah Otto cultured haploid (one copy of each chromosome) and diploid (two copies of each chromosome) populations of Saccharomyces cereviseae . She observed the following population sizes for the two types of cells: Figure. The size of haploid and diploid yeast populations in a controlled laboratory experiment. Although the populations grow nearly exponentially at first, growth decreased as population size increased (i.e., density-dependent growth was observed). The carrying capacity ( \\(K\\) ) is clearly larger for the haploid cells, but do haploid and diploid cells have different intrinsic growth rates ( \\(r\\) )? By fitting the logistic growth model described above to the data, Dr. Otto estimated the parameter values to be Haploid: r = 0.55, K = 3.7 x 10^8 Diploid: r = 0.55, K = 2.3 x 10^8 The growth rates therefore do not differ (visibly or statistically). With these parameter estimates, the logistic model nicely fits the data: Figure. Population size of haploid and diploid yeast fit with logistic growth models. Note: This may be a bit misleading, as such excellent model fits are rarely observed, especially outside the lab! To do: make the above plots in Python","title":"Lecture 3"},{"location":"lectures/lecture-03/#lecture-3-exponential-logistic-growth","text":"Run notes interactively?","title":"Lecture 3: Exponential &amp; logistic growth"},{"location":"lectures/lecture-03/#lecture-overview","text":"Introduction Exponential growth Logistic growth","title":"Lecture overview"},{"location":"lectures/lecture-03/#1-introduction","text":"In nature, population sizes change over time in response to a myriad of factors, such as weather competition, predation, disease, ... resource availability The simplest models describing changes in population size are exponential growth and logistic growth which assume a constant environment no interactions with other species The exponential model also assumes no competition among the members of a species for the available resources ( density-independent growth ), while the logistic model includes competition within a species ( density-dependent growth ). Both of these models can be described in discrete and continuous time. We\u2019ll start with the simpler exponential model.","title":"1. Introduction"},{"location":"lectures/lecture-03/#2-exponential-growth","text":"","title":"2. Exponential growth"},{"location":"lectures/lecture-03/#exponential-growth-in-discrete-time","text":"Imagine we start with \\(n(t)\\) individuals at some time \\(t\\) . If we assume that each of these individuals produces \\(b\\) offspring, the number of individuals after reproduction is \\(n(t) + n(t) b = n(t)(1 + b)\\) . If we then assume a fraction \\(d\\) die, the number of individuals remaining after death is \\(n(t)(1+b) - n(t)(1+b)d = n(t)(1+b)(1-d)\\) . With no further events in the life-cycle, this is the expected number of individuals in the next generation, \\(n(t+1)\\) , which we can write as \\[ \\begin{aligned} n(t+1) &= n(t)(1+b)(1-d) \\\\ &= n(t) R \\\\ \\end{aligned} \\] where \\(R=(1+b)(1-d)\\) is a constant referred to as the reproductive factor . This equation, \\(n(t+1)=n(t) R\\) , is the recursion equation for exponential growth. Exponential vs geometric growth Technically this recursion equation describes \"geometric\" growth, since \\(n(t)\\) will grow with \\(t\\) as a geometric series, but here we simply call it \"exponential growth in discrete time\" to make a clear connection with exponential growth in continuous time. We can also describe the change in the number of individuals by subtracting off the current number, \\[ \\begin{aligned} \\Delta n &= n(t+1) - n(t)\\\\ &= n(t)(R-1) \\end{aligned} \\] This is the difference equation for exponential growth, with discrete-time growth rate \\(r_d = R-1 = (1+b)(1-d)-1 = b - d - bd\\) . Let's plot the dynamics described by these equations for a particular set of parameter values. import numpy as np import matplotlib.pyplot as plt def exponential_discrete(nt, b, d): '''recursion equation giving population size in next time step as a function of the population size at this time, nt, and the birth and death rates, b and d.''' return nt * (1 + b) * (1 - d) # Grow population nd, nt, b, d = [], 1, 0.2, 0.1 #define empty list nd to store population sizes and choose parameter values for t in np.arange(0,100): #for time from 0 to 99 nt = exponential_discrete(nt, b, d) #get the next population size from the recursion equation nd.append(nt) #and append it to the list # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100), nd) #plot population size at each time ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n(t)$') plt.show()","title":"Exponential growth in discrete time"},{"location":"lectures/lecture-03/#exponential-growth-in-continuous-time","text":"Now assume that each individual continuously gives birth at rate \\(b\\) and dies at rate \\(d\\) . If there are \\(n(t)\\) individuals in the population at time \\(t\\) , then the instantaneous rate of change in the number of individuals is \\[ \\begin{aligned} \\frac{\\mathrm{d} n}{\\mathrm{d} t} &= n(t) b - n(t) d\\\\ &= (b - d) n(t)\\\\ &= r_c n(t) \\end{aligned} \\] This is the differential equation for exponential growth with continuous-time growth rate \\(r_c = b - d\\) . Note that the growth rate in the discrete-time model was \\(r_d = b - d - b d\\) . The difference between the two growth rates reflects the fact that birth and death cannot happen at the exact same time in the continuous-time model (so there is no \\(b d\\) term), while offspring that are born can die before the next generation in the discrete-time model (causing the \\(b d\\) term). Let's also plot these dynamics. Approximating a differential equation The differential equation describes the change in the population size in an \"infinitesimally\" small amount of time, \\(\\mathrm{d}t\\) . To plot these dynamics we therefore make an approximation, taking \\(\\mathrm{d}t\\) to be small, but not infinitely so. Rearranging the differential equation gives \\(\\mathrm{d}n(t) = n(t)(b-d)\\mathrm{d}t\\) and the population size after \\(\\mathrm{d}t\\) is therefore \\(n_{t+\\mathrm{d}t} = n(t) + \\mathrm{d}n(t)\\) . This is a recursion equation that approximates our differential equation. import numpy as np import matplotlib.pyplot as plt def exponential_continuous(nt, b, d, dt): '''approximation of the differential equation giving population size after a small time interval, dt, as a function of the population size at this time, nt, and the birth and death rates, b and d.''' return nt + nt * (b - d) * dt # Grow population nc, nt, dt = [], 1, 0.1 #define empty list nc to store population sizes and choose parameter values (keep b and d as above) for t in np.arange(0,100,dt): #for time from 0 to 99 by increments of dt nt = exponential_continuous(nt, b, d, dt) #get the next population size from the recursion equation nc.append(nt) #and append it to the list # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100, dt), nc) #plot population size at each time ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n(t)$') plt.show() We can now combine our two plots to compare these two predictions, exponential growth in discrete vs. continuous time. # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100), nd, label='discrete time') #plot population size at each time ax.scatter(np.arange(0, 100, dt), nc, label='continuous time') #plot population size at each time ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n(t)$') plt.legend() plt.show() Why the difference? The predictions are very similar at first but then noticeably diverge. The time of this divergence depends on the values of parameters \\(b\\) and \\(d\\) . Try increasing or decreasing both \\(b\\) and \\(d\\) and think about why it has that effect. Also think about why the continuous time model predicts a larger population size than the discrete time model. Hint: remember the difference in the growth rates between the two models is \\(bd\\) .","title":"Exponential growth in continuous time"},{"location":"lectures/lecture-03/#the-trouble-with-exponential-growth","text":"Exponential growth cannot continue indefinitely. Take for example a population of pheasants on an island off the coast of Washington State. Just 8 pheasants were introduced in 1937, but the population then grew exponentially, tripling in size every year ( \\(R=3\\) ) for the first 5 years. Had this population continued to grow exponentially there would have been 7 million of them by the year 1950 and \\(10^{28}\\) by now \u2013 which at 2 kg per pheasant is 3000 times the mass of the earth!! Although populations may initially experience exponential growth, resources eventually become depleted and competition becomes more severe. This suggests that we should change our model assumptions.","title":"The trouble with exponential growth"},{"location":"lectures/lecture-03/#3-logistic-growth","text":"Exponential growth assumes the growth rate ( \\(r_d\\) , \\(r_c\\) ) is constant. Logistic growth relaxes this assumption, and instead assumes that the growth rate decreases linearly with population size, due to competition for resources within the population.","title":"3. Logistic growth"},{"location":"lectures/lecture-03/#logistic-growth-in-discrete-time","text":"In discrete-time, the reproductive factor under logistic growth can be written as \\[R(n(t)) = 1 + r\\left(1 - \\frac{n(t)}{K}\\right)\\] Notice that each individual is expected to have one offspring ( \\(R=1\\) ) if the intrinsic growth rate (ie, growth rate when rare) is zero, \\(r = 0\\) , or if the population size is at carrying capacity , \\(n(t)=K\\) . Try plotting the reproductive factor as a function of \\(n(t)\\) for a few different values of \\(r\\) and \\(K\\) . # Reproductive factor for logistic growth def logistic_discrete(nt, r, K): '''reproductive factor in discrete logistic model with growth rate r and carrying capacity k''' return 1 + r * (1 - nt/K) # Compare a few different growth rates and carrying capacities fig, ax = plt.subplots() for r, K in zip([1, 2, 1], [100, 100, 50]): #for each pair of r and K values nt = np.linspace(0, 200) #for a range of population sizes from 0 to 200 R = logistic_discrete(nt, r, K) #calculate the reproductive factor ax.plot(nt, R, label=f\"r = {r}, K = {K}\") #and plot ax.plot(nt, [1 for i in nt], '--', color='gray') #1 line for reference ax.set_xlabel('Population size, $n(t)$') ax.set_ylabel('Reproductive factor, $R$') ax.legend(frameon=False) plt.ylim(0,None) plt.show() The population size in the next generation is the expected number of offspring per parent times the the total number of parents \\[n(t+1) = \\left(1 + r\\left(1-\\frac{n(t)}{K}\\right)\\right)n(t)\\] This is the recursion equation for logistic growth. This recursion is a non-linear function of \\(n(t)\\) ( non-linear means that there is a term in the equation where the term is taken to some power other than 1; here if we expand out the recursion we get a \\(n(t)^2\\) term ). This reflects the fact that logistic growth models an interaction between individuals (competition). The change in population size from one generation to the next, \\(\\Delta n\\) , is therefore \\[\\Delta n = n(t+1) - n(t) = r\\left(1 - \\frac{n(t)}{K}\\right)n(t)\\] Based on this difference equation, when will the population grow in size? Test out your answer by plotting population size over time in the discrete-time logistic model. Try changing the initial population size or carrying capacity so that \\(n(t) > K\\) . # Initialize parameters n, nt, r, K = [], 1, 0.1, 100 #list, initial population size, intrinsic growth rate, carrying capacity # Grow population under logistic growth for t in np.arange(0, 100): nt = nt * logistic_discrete(nt, r, K) n.append(nt) # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100), n) ax.axhline(100, label=f\"K = {K}\", linestyle='dashed', color='gray') #carrying capacity as dashed line # Add annotations ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n(t)$') plt.show() Complex dynamics Logistic growth in discrete time can show some surprisingly complex dynamics, which we'll explore in lab.","title":"Logistic growth in discrete time"},{"location":"lectures/lecture-03/#logistic-growth-in-continuous-time","text":"The model of logistic growth in continuous time, as with discrete time, follows from the assumption that each individual has a growth rate that decreases as a linear function of the population size \\(r(1 - n(t)/K)\\) . If there are \\(n(t)\\) individuals in the population at time \\(t\\) , then the rate of change of the population size will be \\[\\frac{\\mathrm{d}n}{\\mathrm{d}t} = r\\left(1 - \\frac{n(t)}{K}\\right)n(t)\\] This is a differential equation of logistic growth. Note that in both discrete and continuous time the logistic growth model reduces to the exponential growth model as \\(n/K\\) approaches 0, i.e., when the population size is much smaller than the carrying capacity \\(n << K\\) . An example of logistic growth Dr. Sarah Otto cultured haploid (one copy of each chromosome) and diploid (two copies of each chromosome) populations of Saccharomyces cereviseae . She observed the following population sizes for the two types of cells: Figure. The size of haploid and diploid yeast populations in a controlled laboratory experiment. Although the populations grow nearly exponentially at first, growth decreased as population size increased (i.e., density-dependent growth was observed). The carrying capacity ( \\(K\\) ) is clearly larger for the haploid cells, but do haploid and diploid cells have different intrinsic growth rates ( \\(r\\) )? By fitting the logistic growth model described above to the data, Dr. Otto estimated the parameter values to be Haploid: r = 0.55, K = 3.7 x 10^8 Diploid: r = 0.55, K = 2.3 x 10^8 The growth rates therefore do not differ (visibly or statistically). With these parameter estimates, the logistic model nicely fits the data: Figure. Population size of haploid and diploid yeast fit with logistic growth models. Note: This may be a bit misleading, as such excellent model fits are rarely observed, especially outside the lab! To do: make the above plots in Python","title":"Logistic growth in continuous time"},{"location":"lectures/lecture-04/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 4: One-locus selection Run notes interactively? Lecture overview Review of Hardy-Weinberg One-locus haploid selection in discrete time One-locus diploid selection in discrete time Comparing haploid and diploid selection One-locus haploid selection in continuous time Comparing haploid selection in discrete and continuous time The models we've covered 1. Review of Hardy-Weinberg In Lecture 2 we saw that one round of random union among gametes (equivalently, random mating among diploids) causes the frequency of diploid genotypes at a locus with two alleles, \\(A\\) and \\(a\\) , to become \\(AA\\) : \\(p^2\\) \\(Aa\\) : \\(2pq\\) \\(aa\\) : \\(q^2\\) where \\(p\\) is the frequency of allele \\(A\\) and \\(q=1-p\\) is the frequency of allele \\(a\\) . A population with these diploid genotype frequencies is said to be in Hardy-Weinberg equilibrium. Furthermore, we showed that under this model the allele frequencies do not change, \\(p' = p\\) . But what if there is natural selection? 2. One-locus haploid selection in discrete time We begin by examining a model where selection acts during the haploid phase of the life-cycle. This is a little simpler than diploid selection because there are only two haploid genotypes, \\(A\\) and \\(a\\) . It is also a very relevant model for species with long haploid phases (eg, some algae and fungi) or with strong competition during the haploid phase (eg, pollen competiting for ovules). Life-cycle diagram We first illustrate the structure of the model with a life-cycle diagram. graph LR; A((p)) --gamete union--> B((p')); B --meiosis--> C((p'')); C --selection--> A; We will census the population at the beginning of the haploid phase (immediately after meiosis). Deriving the equations Let the number of haploid individuals with each allele be \\(n_A(t) =\\) number of individuals with the \\(A\\) allele in generation \\(t\\) \\(n_a(t) =\\) number of individuals with the \\(a\\) allele in generation \\(t\\) The frequency of \\(A\\) is therefore \\(p(t) = \\frac{n_a(t)}{n_A(t) + n_a(t)}\\) . Now, let\u2019s assume that during selection each haploid individual has reproductive factor \\(W_A =\\) reproductive factor of individuals with the \\(A\\) allele \\(W_a =\\) reproductive factor of individuals with the \\(a\\) allele These reproductive factors are referred to as the absolute fitnesses as they determine the (absolute) numbers of individuals after selection, \\(n_i' = W_i n_i(t)\\) for \\(i=A\\) and \\(i=a\\) . To relate back to the models of population growth in Lecture 3, this is exponential growth of each allele. After selection these alleles randomly pair, go through the dipliod phase of the life cycle, and then segregate back into haploids after meiosis. From our analysis of Hardy-Weinberg we know random mating and segregation don't affect allele frequency. The frequency of \\(A\\) in the next generation is therefore \\[ \\begin{aligned} p(t+1) &= \\frac{n_A(t+1)}{n_A(t+1) + v_a n_a(t)} \\\\ &= \\frac{W_A n_A(t)}{W_A n_A(t) + W_a n_a(t)}. \\end{aligned} \\] To make this a recursion equation we need to write \\(p(t+1)\\) in terms of \\(p(t)\\) , so that if we knew \\(p\\) at some point in time we can recursively calculate it in all future times. To do this we divide both the numerator and denominator by the total number of individuals, \\(n_A(t) + n_a(t)\\) , and simplify \\[ \\begin{aligned} p(t+1) &= \\frac{W_A n_A(t)}{W_A n_A(t) + W_a n_a(t)} \\\\ &= \\frac{W_A\\frac{n_A(t)}{n_A(t) + n_a(t)}}{W_A\\frac{n_A(t)}{n_A(t) + n_a(t)} + W_a\\frac{n_a(t)}{n_A(t) + n_a(t)}}\\\\ &= \\frac{W_A p(t)}{W_A p(t) + W_a q(t)}. \\end{aligned} \\] This is now a recursion equation for the allele frequency in our model of haploid selection. Simplifying Our current recursion is a function of two parameters, the absolute fitnesses \\(W_A\\) and \\(W_a\\) . Now notice that if we divide both the numerator and denominator by one of these fitnesses, say \\(W_a\\) , then we can reduce the recursion to a function of only one parameter, \\(w_A = W_A/W_a\\) , the fitness of \\(A\\) relative to the fitness of \\(a\\) , \\[ \\begin{aligned} p(t+1) &= \\frac{(W_A/W_a)p(t)}{(W_A/W_a)p(t) + (W_a/W_a)q(t)}\\\\ &= \\frac{w_A p(t)}{w_A p(t) + q(t)}. \\end{aligned} \\] Note that the allele frequency dynamics depend only on the relative fitnesses , and not on the absolute fitnesses, meaning that evolution does not depend on how the size of the population changes. It is therefore possible to study evolution while ignoring population dynamics under this simple model. Let's plot the recursion to get a sense of the dynamics. import matplotlib.pyplot as plt # calculate allele frequency over time with recursion w_A = 1.1 #relative fitness of allele A p_now = 0.01 #initial allele frequency, p_0 ps = [] #list to hold allele frequencies over time ts = range(100) #list of time steps for t in ts: #for each time ps.append(p_now) #add current allele frequency to list p_now = w_A*p_now/(w_A*p_now+(1-p_now)) #update allele frequency with our recursion equation # plot plt.scatter(ts, ps) #plot the (t,p) pairs plt.xlabel(\"generation, $t$\") #label axes plt.ylabel(\"frequency of $A$ allele, $p(t)$\") plt.show() 3. One-locus diploid selection in discrete time Since we are all currently in the diploid phase of our life-cycle, it is natural to ask: Does selection in the diploid phase work the same way? Life-cycle diagram We now have the following life-cycle diagram. graph LR; A((p)) --gamete union--> B((p')); B --selection--> C((p'')); C --meiosis--> A; We will census at the beginning of the diploid phase (immediately after gamete union). Deriving the equations Let the number of diploid individuals with each genotype be \\(n_{AA}(t) =\\) number of individuals with the \\(AA\\) genotype in generation \\(t\\) \\(n_{Aa}(t) =\\) number of individuals with the \\(Aa\\) genotype in generation \\(t\\) \\(n_{aa}(t) =\\) number of individuals wite the \\(aa\\) genotype in generation \\(t\\) The frequency of allele \\(A\\) is then \\[ \\begin{aligned} p(t) &= \\frac{2n_{AA}(t) + n_{Aa}(t)}{2n_{AA}(t) + n_{Aa}(t) + n_{aa}(t)} \\\\ &= \\frac{n_{AA}(t) + \\frac{1}{2}n_{Aa}(t)}{n_{AA}(t) + n_{Aa}(t) + n_{aa}(t)} \\end{aligned} \\] Now, let\u2019s assume that during selection each diploid individual has reproductive factor \\(W_{AA} =\\) reproductive factor of individuals with the \\(AA\\) genotype \\(W_{Aa} =\\) reproductive factor of individuals with the \\(Aa\\) genotype \\(W_{aa} =\\) reproductive factor of individuals with the \\(aa\\) genotype These reproductive factors are again referred to as the absolute fitnesses as they determine the (absolute) numbers of individuals after selection, \\(n_i' = W_i n_i(t)\\) for \\(i=AA\\) , \\(i=Aa\\) , and \\(i=aa\\) . After selection these genotypes segregate into haploids via meiosis, go through the haploid phase of the life cycle, and then random join to create diploids again. Again, we know random union and segregation don't affect allele frequency. The frequency of \\(A\\) in the next generation is therefore \\[ \\begin{aligned} p(t+1) &= \\frac{n_{AA}(t+1) + \\frac{1}{2}n_{Aa}(t+1)}{n_{AA}(t+1) + n_{Aa}(t+1) + n_{aa}(t+1)}\\\\ &= \\frac{W_{AA}n_{AA}(t) + \\frac{1}{2}W_{Aa}n_{Aa}(t)}{W_{AA}n_{AA}(t) + \\frac{1}{2}W_{Aa}n_{Aa}(t) + W_{aa}n_{aa}(t)}. \\end{aligned} \\] As above, we want a recursion equation in terms of allele frequency, so we want to replace the \\(n_i\\) 's in the right hand side of this equation with \\(p\\) 's. To do this we note that with the random union of gametes the diploid offspring are in Hardy-Weinberg proportions, so that \\[ \\begin{aligned} n_{AA}(t) &= p(t)^2 n(t) \\\\ n_{Aa}(t) &= 2p(t) q(t) n(t) \\\\ n_{aa}(t) &= q(t)^2 n(t) \\end{aligned} \\] where \\(n(t) = n_{AA}(t) + n_{Aa}(t) + n_{aa}(t)\\) is the total population size. Substituting these Hardy-Weinberg proportions in and simplifying, the total population size cancels out and we can rewrite the above equation in terms of allele frequency alone, \\[ \\begin{aligned} p(t+1) &= \\frac{W_{AA}p(t)^2 n(t) + W_{Aa}p(t) q(t) n(t)}{W_{AA}p(t)^2 n(t) + 2W_{Aa}p(t) q(t) n(t) + W_{aa}q(t)^2 n(t)}\\\\ &= \\frac{W_{AA}p(t)^2 + W_{Aa}p(t) q(t)}{W_{AA}p(t)^2 + 2W_{Aa}p(t) q(t) + W_{aa}q(t)^2}. \\end{aligned} \\] This is a recursion equation for allele frequency in our model of diploid selection. Simplifying As in the haploid selection case, we can divide by one of the absolute fitnesses, say \\(W_{aa}\\) , to remove one parameter from the model \\[ \\begin{aligned} p(t+1) &= \\frac{(W_{AA}/W_{aa})p(t)^2 + (W_{Aa}/W_{aa})p(t) q(t)}{(W_{AA}/W_{aa})p(t)^2 + 2(W_{Aa}/W_{aa})p(t) q(t) + (W_{aa}/W_{aa})q(t)^2} \\\\ &= \\frac{w_{AA}p(t)^2 + w_{Aa}p(t) q(t)}{w_{AA}p(t)^2 + 2w_{Aa}p(t) q(t) + q(t)^2} \\end{aligned} \\] This recursion is a function of only two relative fitnesses, \\(w_{AA} = W_{AA}/W_{aa}\\) and \\(w_{Aa} = W_{Aa}/W_{aa}\\) . Let's plot the recursion to get a sense of the dynamics. import matplotlib.pyplot as plt # calculate allele frequency over time with recursion w_AA = 1.2 #relative fitness of genotype AA w_Aa = 1.1 #relative fitness of genotype Aa p_now = 0.01 #initial allele frequency, p_0 ps = [] #list to hold allele frequencies over time ts = range(100) #list of time steps for t in ts: #for each time ps.append(p_now) #add current allele frequency to list p_now = (w_AA*p_now**2 + w_Aa*p_now*(1-p_now))/(w_AA*p_now**2 + 2*w_Aa*p_now*(1-p_now) + (1-p_now)**2) #update allele frequency with our recursion equation # plot plt.scatter(ts, ps) plt.xlabel(\"generation, $t$\") plt.ylabel(\"frequency of $A$ allele, $p(t)$\") plt.show() 4. Comparing haploid and diploid selection So, returning to our original question, let's compare evolution under haploid selection \\[ p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a q(t)} \\] to evolution under diploid selection \\[ p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t) q(t)}{W_{AA}p(t)^2 + 2W_{Aa}p(t) q(t) + W_{aa}q(t)^2}. \\] To facilitate this, let\u2019s assume the fitness of a diploid genotype is the product of the haploid fitnesses, i.e., \\(W_{AA} = W_A W_A\\) , \\(W_{Aa} = W_A W_a\\) , and \\(W_{aa} = W_a W_a\\) . It then happens that our diploid recursion reduces to the haploid recursion, \\[ \\begin{aligned} p(t+1) &= \\frac{W_A W_A p(t)^2 + W_A W_a p(t) q(t)}{W_A W_A p(t)^2 + 2W_A W_a p(t) q(t) + W_a W_a q(t)^2}\\\\ &= \\frac{W_A p(t)(p(t) + W_a q(t))}{W_A p(t)(W_A p(t) + W_a q(t)) + W_a q(t)(W_A p(t) + W_a q(t))}\\\\ &= \\frac{W_A p(t)}{W_A p(t) + W_a q(t)} \\end{aligned} \\] This shows that we need twice as much selection under diploid selection relative to that under haploid selection (e.g., \\(W_{AA} = W_A W_A\\) ) for evolution to proceed as quickly. Why is evolution slower under diploid selection? Let's check this visually (try breaking the assumption above, \\(W_{ij}=W_i W_j\\) , to see what happens). import matplotlib.pyplot as plt # calculate allele frequency over time with recursion w_A = 1.1 #relative fitness of allele A in the haploid model w_AA = w_A*w_A #relative fitness of genotype AA in the diploid model w_Aa = w_A #relative fitness of genotype Aa in the diploid model p_now_hap = 0.01 #initial allele frequency, p_0 p_now_dip = p_now_hap ps_hap, ps_dip = [], [] #lists to hold allele frequencies over time ts = range(100) #list of time steps for t in ts: #for each time ps_hap.append(p_now_hap) #add current allele frequency to list p_now_hap = w_A*p_now_hap/(w_A*p_now_hap+(1-p_now_hap)) #update allele frequency with haploid recursion equation ps_dip.append(p_now_dip) #add current allele frequency to list p_now_dip = (w_AA*p_now_dip**2 + w_Aa*p_now_dip*(1-p_now_dip))/(w_AA*p_now_dip**2 + 2*w_Aa*p_now_dip*(1-p_now_dip) + (1-p_now_dip)**2) #update allele frequency with diploid recursion equation # plot plt.scatter(ts, ps_hap, label='haploid') plt.scatter(ts, ps_dip, s=10, label='diploid') plt.xlabel(\"generation, $t$\") plt.ylabel(\"frequency of $A$ allele, $p(t)$\") plt.legend() #add legend to know which points belong to which model plt.show() 5. One-locus haploid selection in continuous time To model haploid selection in continuous-time we assume that during the selective phase each haploid individual has growth rate \\(r_A =\\) growth rate of individuals with the \\(A\\) allele \\(r_a =\\) growth rate of individuals with the \\(a\\) allele and the relative numbers of each type don't change otherwise (ie, during union, diploidy, or meiosis). We therefore have exponential growth of both genotypes, \\(\\frac{\\mathrm{d} n_i}{dt} = r_i n_i\\) . At any particular point in time, \\(t\\) , the frequency of allele \\(A\\) is, \\(p(t) = n_A(t)/(n_A(t) + n_a(t))\\) . We can therefore derive the rate of change in the frequency of allele \\(A\\) , \\(\\mathrm{d}p/\\mathrm{d}t\\) , using the qoutient rule (see Appendix 2 in the text for help with this and other math tricks) \\[ \\begin{aligned} \\frac{\\mathrm{d}p}{\\mathrm{d}t} &= \\frac{\\mathrm{d}\\frac{n_A}{n_A + n_a}}{\\mathrm{d}t} \\\\ &= \\frac{\\frac{\\mathrm{d}n_A}{\\mathrm{d}t} (n_A(t) + n_a(t)) - n_A(t) \\frac{\\mathrm{d}(n_A+n_a)}{\\mathrm{d}t}}{(n_A(t) + n_a(t))^2}\\\\ &= \\frac{r_A n_A(t) (n_A(t) + n_a(t)) - n_A(t) (r_A n_A(t) + r_a n_a(t))}{(n_A(t) + n_a(t))^2}\\\\ &= \\frac{r_A n_A(t) n_a(t) - r_a n_A(t) n_a(t))}{(n_A(t) + n_a(t))^2}\\\\ &= \\frac{(r_A -r_a)n_A(t) n_a(t)}{(n_A(t) + n_a(t))^2}\\\\ &= (r_A -r_a)p(t) q(t), \\end{aligned} \\] where \\(s_c = r_A - r_a\\) is the continuous-time selection coefficient of allele \\(A\\) . A similar equation can be derived for the model of diploid-selection in continuous time, but we will not study it (see Problem 3.16 in the text if you are curious). 6. Comparing haploid selection in discrete and continuous time Are the discrete- and continuous-time models of haploid selection as different as they look? \\[ p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a q(t)} \\] \\[ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = s_c p(t) q(t) \\] Not really. Discrete and continuous time models generally behave in a similar fashion when changes occur slowly over time. For this model of haploid selection, this implies that the discrete and continuous models will be similar when the fitnesses of the two alleles are nearly equal, i.e., when \\(W_A - W_a\\) is small. This is called \"weak selection\". In the discrete model, the change in the allele frequency is \\[ \\begin{aligned} \\Delta p &= p(t+1) - p(t) \\\\ &= \\frac{W_A p(t)}{W_A p(t) + W_a q(t)} - p(t) \\\\ &= \\frac{(W_A - W_a)p(t) q(t)}{W_A p(t) + W_a q(t)}. \\end{aligned} \\] Now define \\(s_d = (W_A - W_a)/W_a\\) as the discrete time selection coefficient. Subbing this in as \\(W_A = s_d W_a + W_a\\) gives \\[ \\begin{aligned} \\Delta p &= \\frac{(W_A-W_a)p(t) q(t)}{W_A p(t) + W_a q(t)}\\\\ &= \\frac{(s_d W_a + W_a - W_a) p(t) q(t)}{(s_d W_a + W_a) p(t) + W_a q(t)}\\\\ &= \\frac{s_d W_a p(t) q(t)}{s_d W_a p(t) + W_a (p(t) + q(t))}\\\\ &= \\frac{s_d W_a p(t) q(t)}{s_d W_a p(t) + W_a}\\\\ &= \\frac{s_d p(t) q(t)}{s_d p(t) + 1}\\\\ \\end{aligned} \\] Now assume weak selection, i.e., that \\(s_d\\) is small. This implies \\(s_d p(t) + 1 \\approx 1\\) . Making this approximation we have \\[ \\Delta p \\approx s_d p(t) q(t) \\] This is equivalent to the continuous-time model when the selection coefficients are equal, \\(s_c = s_d\\) . 7. The models we've covered To summarize the last two lectures, we've derived four of the most classic models in ecology and evolution: Model Discrete time Continous time Exponential growth \\(n(t+1) = R n(t)\\) \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t} = r n(t)\\) Logistic growth \\(n(t+1) = (1 + r(1 - \\frac{n(t)}{K}))n(t)\\) \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t} = r(1 - \\frac{n(t)}{K})n(t)\\) Haploid selection \\(p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a q(t)}\\) \\(\\frac{\\mathrm{d}p}{\\mathrm{d}t} = s p(t)(1-p(t))\\) Diploid selection \\(p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t) q(t)}{W_{AA}p(t)^2 + 2 W_{Aa} p(t) q(t) + W_{aa} q(t)^2}\\) Not derived See textbook Sections 3.4 and 3.5 for models of interacting species and epidemiology, respectively, which we won't cover in class.","title":"Lecture 4"},{"location":"lectures/lecture-04/#lecture-4-one-locus-selection","text":"Run notes interactively?","title":"Lecture 4: One-locus selection"},{"location":"lectures/lecture-04/#lecture-overview","text":"Review of Hardy-Weinberg One-locus haploid selection in discrete time One-locus diploid selection in discrete time Comparing haploid and diploid selection One-locus haploid selection in continuous time Comparing haploid selection in discrete and continuous time The models we've covered","title":"Lecture overview"},{"location":"lectures/lecture-04/#1-review-of-hardy-weinberg","text":"In Lecture 2 we saw that one round of random union among gametes (equivalently, random mating among diploids) causes the frequency of diploid genotypes at a locus with two alleles, \\(A\\) and \\(a\\) , to become \\(AA\\) : \\(p^2\\) \\(Aa\\) : \\(2pq\\) \\(aa\\) : \\(q^2\\) where \\(p\\) is the frequency of allele \\(A\\) and \\(q=1-p\\) is the frequency of allele \\(a\\) . A population with these diploid genotype frequencies is said to be in Hardy-Weinberg equilibrium. Furthermore, we showed that under this model the allele frequencies do not change, \\(p' = p\\) . But what if there is natural selection?","title":"1. Review of Hardy-Weinberg"},{"location":"lectures/lecture-04/#2-one-locus-haploid-selection-in-discrete-time","text":"We begin by examining a model where selection acts during the haploid phase of the life-cycle. This is a little simpler than diploid selection because there are only two haploid genotypes, \\(A\\) and \\(a\\) . It is also a very relevant model for species with long haploid phases (eg, some algae and fungi) or with strong competition during the haploid phase (eg, pollen competiting for ovules).","title":"2. One-locus haploid selection in discrete time"},{"location":"lectures/lecture-04/#life-cycle-diagram","text":"We first illustrate the structure of the model with a life-cycle diagram. graph LR; A((p)) --gamete union--> B((p')); B --meiosis--> C((p'')); C --selection--> A; We will census the population at the beginning of the haploid phase (immediately after meiosis).","title":"Life-cycle diagram"},{"location":"lectures/lecture-04/#deriving-the-equations","text":"Let the number of haploid individuals with each allele be \\(n_A(t) =\\) number of individuals with the \\(A\\) allele in generation \\(t\\) \\(n_a(t) =\\) number of individuals with the \\(a\\) allele in generation \\(t\\) The frequency of \\(A\\) is therefore \\(p(t) = \\frac{n_a(t)}{n_A(t) + n_a(t)}\\) . Now, let\u2019s assume that during selection each haploid individual has reproductive factor \\(W_A =\\) reproductive factor of individuals with the \\(A\\) allele \\(W_a =\\) reproductive factor of individuals with the \\(a\\) allele These reproductive factors are referred to as the absolute fitnesses as they determine the (absolute) numbers of individuals after selection, \\(n_i' = W_i n_i(t)\\) for \\(i=A\\) and \\(i=a\\) . To relate back to the models of population growth in Lecture 3, this is exponential growth of each allele. After selection these alleles randomly pair, go through the dipliod phase of the life cycle, and then segregate back into haploids after meiosis. From our analysis of Hardy-Weinberg we know random mating and segregation don't affect allele frequency. The frequency of \\(A\\) in the next generation is therefore \\[ \\begin{aligned} p(t+1) &= \\frac{n_A(t+1)}{n_A(t+1) + v_a n_a(t)} \\\\ &= \\frac{W_A n_A(t)}{W_A n_A(t) + W_a n_a(t)}. \\end{aligned} \\] To make this a recursion equation we need to write \\(p(t+1)\\) in terms of \\(p(t)\\) , so that if we knew \\(p\\) at some point in time we can recursively calculate it in all future times. To do this we divide both the numerator and denominator by the total number of individuals, \\(n_A(t) + n_a(t)\\) , and simplify \\[ \\begin{aligned} p(t+1) &= \\frac{W_A n_A(t)}{W_A n_A(t) + W_a n_a(t)} \\\\ &= \\frac{W_A\\frac{n_A(t)}{n_A(t) + n_a(t)}}{W_A\\frac{n_A(t)}{n_A(t) + n_a(t)} + W_a\\frac{n_a(t)}{n_A(t) + n_a(t)}}\\\\ &= \\frac{W_A p(t)}{W_A p(t) + W_a q(t)}. \\end{aligned} \\] This is now a recursion equation for the allele frequency in our model of haploid selection.","title":"Deriving the equations"},{"location":"lectures/lecture-04/#simplifying","text":"Our current recursion is a function of two parameters, the absolute fitnesses \\(W_A\\) and \\(W_a\\) . Now notice that if we divide both the numerator and denominator by one of these fitnesses, say \\(W_a\\) , then we can reduce the recursion to a function of only one parameter, \\(w_A = W_A/W_a\\) , the fitness of \\(A\\) relative to the fitness of \\(a\\) , \\[ \\begin{aligned} p(t+1) &= \\frac{(W_A/W_a)p(t)}{(W_A/W_a)p(t) + (W_a/W_a)q(t)}\\\\ &= \\frac{w_A p(t)}{w_A p(t) + q(t)}. \\end{aligned} \\] Note that the allele frequency dynamics depend only on the relative fitnesses , and not on the absolute fitnesses, meaning that evolution does not depend on how the size of the population changes. It is therefore possible to study evolution while ignoring population dynamics under this simple model. Let's plot the recursion to get a sense of the dynamics. import matplotlib.pyplot as plt # calculate allele frequency over time with recursion w_A = 1.1 #relative fitness of allele A p_now = 0.01 #initial allele frequency, p_0 ps = [] #list to hold allele frequencies over time ts = range(100) #list of time steps for t in ts: #for each time ps.append(p_now) #add current allele frequency to list p_now = w_A*p_now/(w_A*p_now+(1-p_now)) #update allele frequency with our recursion equation # plot plt.scatter(ts, ps) #plot the (t,p) pairs plt.xlabel(\"generation, $t$\") #label axes plt.ylabel(\"frequency of $A$ allele, $p(t)$\") plt.show()","title":"Simplifying"},{"location":"lectures/lecture-04/#3-one-locus-diploid-selection-in-discrete-time","text":"Since we are all currently in the diploid phase of our life-cycle, it is natural to ask: Does selection in the diploid phase work the same way?","title":"3. One-locus diploid selection in discrete time"},{"location":"lectures/lecture-04/#life-cycle-diagram_1","text":"We now have the following life-cycle diagram. graph LR; A((p)) --gamete union--> B((p')); B --selection--> C((p'')); C --meiosis--> A; We will census at the beginning of the diploid phase (immediately after gamete union).","title":"Life-cycle diagram"},{"location":"lectures/lecture-04/#deriving-the-equations_1","text":"Let the number of diploid individuals with each genotype be \\(n_{AA}(t) =\\) number of individuals with the \\(AA\\) genotype in generation \\(t\\) \\(n_{Aa}(t) =\\) number of individuals with the \\(Aa\\) genotype in generation \\(t\\) \\(n_{aa}(t) =\\) number of individuals wite the \\(aa\\) genotype in generation \\(t\\) The frequency of allele \\(A\\) is then \\[ \\begin{aligned} p(t) &= \\frac{2n_{AA}(t) + n_{Aa}(t)}{2n_{AA}(t) + n_{Aa}(t) + n_{aa}(t)} \\\\ &= \\frac{n_{AA}(t) + \\frac{1}{2}n_{Aa}(t)}{n_{AA}(t) + n_{Aa}(t) + n_{aa}(t)} \\end{aligned} \\] Now, let\u2019s assume that during selection each diploid individual has reproductive factor \\(W_{AA} =\\) reproductive factor of individuals with the \\(AA\\) genotype \\(W_{Aa} =\\) reproductive factor of individuals with the \\(Aa\\) genotype \\(W_{aa} =\\) reproductive factor of individuals with the \\(aa\\) genotype These reproductive factors are again referred to as the absolute fitnesses as they determine the (absolute) numbers of individuals after selection, \\(n_i' = W_i n_i(t)\\) for \\(i=AA\\) , \\(i=Aa\\) , and \\(i=aa\\) . After selection these genotypes segregate into haploids via meiosis, go through the haploid phase of the life cycle, and then random join to create diploids again. Again, we know random union and segregation don't affect allele frequency. The frequency of \\(A\\) in the next generation is therefore \\[ \\begin{aligned} p(t+1) &= \\frac{n_{AA}(t+1) + \\frac{1}{2}n_{Aa}(t+1)}{n_{AA}(t+1) + n_{Aa}(t+1) + n_{aa}(t+1)}\\\\ &= \\frac{W_{AA}n_{AA}(t) + \\frac{1}{2}W_{Aa}n_{Aa}(t)}{W_{AA}n_{AA}(t) + \\frac{1}{2}W_{Aa}n_{Aa}(t) + W_{aa}n_{aa}(t)}. \\end{aligned} \\] As above, we want a recursion equation in terms of allele frequency, so we want to replace the \\(n_i\\) 's in the right hand side of this equation with \\(p\\) 's. To do this we note that with the random union of gametes the diploid offspring are in Hardy-Weinberg proportions, so that \\[ \\begin{aligned} n_{AA}(t) &= p(t)^2 n(t) \\\\ n_{Aa}(t) &= 2p(t) q(t) n(t) \\\\ n_{aa}(t) &= q(t)^2 n(t) \\end{aligned} \\] where \\(n(t) = n_{AA}(t) + n_{Aa}(t) + n_{aa}(t)\\) is the total population size. Substituting these Hardy-Weinberg proportions in and simplifying, the total population size cancels out and we can rewrite the above equation in terms of allele frequency alone, \\[ \\begin{aligned} p(t+1) &= \\frac{W_{AA}p(t)^2 n(t) + W_{Aa}p(t) q(t) n(t)}{W_{AA}p(t)^2 n(t) + 2W_{Aa}p(t) q(t) n(t) + W_{aa}q(t)^2 n(t)}\\\\ &= \\frac{W_{AA}p(t)^2 + W_{Aa}p(t) q(t)}{W_{AA}p(t)^2 + 2W_{Aa}p(t) q(t) + W_{aa}q(t)^2}. \\end{aligned} \\] This is a recursion equation for allele frequency in our model of diploid selection.","title":"Deriving the equations"},{"location":"lectures/lecture-04/#simplifying_1","text":"As in the haploid selection case, we can divide by one of the absolute fitnesses, say \\(W_{aa}\\) , to remove one parameter from the model \\[ \\begin{aligned} p(t+1) &= \\frac{(W_{AA}/W_{aa})p(t)^2 + (W_{Aa}/W_{aa})p(t) q(t)}{(W_{AA}/W_{aa})p(t)^2 + 2(W_{Aa}/W_{aa})p(t) q(t) + (W_{aa}/W_{aa})q(t)^2} \\\\ &= \\frac{w_{AA}p(t)^2 + w_{Aa}p(t) q(t)}{w_{AA}p(t)^2 + 2w_{Aa}p(t) q(t) + q(t)^2} \\end{aligned} \\] This recursion is a function of only two relative fitnesses, \\(w_{AA} = W_{AA}/W_{aa}\\) and \\(w_{Aa} = W_{Aa}/W_{aa}\\) . Let's plot the recursion to get a sense of the dynamics. import matplotlib.pyplot as plt # calculate allele frequency over time with recursion w_AA = 1.2 #relative fitness of genotype AA w_Aa = 1.1 #relative fitness of genotype Aa p_now = 0.01 #initial allele frequency, p_0 ps = [] #list to hold allele frequencies over time ts = range(100) #list of time steps for t in ts: #for each time ps.append(p_now) #add current allele frequency to list p_now = (w_AA*p_now**2 + w_Aa*p_now*(1-p_now))/(w_AA*p_now**2 + 2*w_Aa*p_now*(1-p_now) + (1-p_now)**2) #update allele frequency with our recursion equation # plot plt.scatter(ts, ps) plt.xlabel(\"generation, $t$\") plt.ylabel(\"frequency of $A$ allele, $p(t)$\") plt.show()","title":"Simplifying"},{"location":"lectures/lecture-04/#4-comparing-haploid-and-diploid-selection","text":"So, returning to our original question, let's compare evolution under haploid selection \\[ p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a q(t)} \\] to evolution under diploid selection \\[ p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t) q(t)}{W_{AA}p(t)^2 + 2W_{Aa}p(t) q(t) + W_{aa}q(t)^2}. \\] To facilitate this, let\u2019s assume the fitness of a diploid genotype is the product of the haploid fitnesses, i.e., \\(W_{AA} = W_A W_A\\) , \\(W_{Aa} = W_A W_a\\) , and \\(W_{aa} = W_a W_a\\) . It then happens that our diploid recursion reduces to the haploid recursion, \\[ \\begin{aligned} p(t+1) &= \\frac{W_A W_A p(t)^2 + W_A W_a p(t) q(t)}{W_A W_A p(t)^2 + 2W_A W_a p(t) q(t) + W_a W_a q(t)^2}\\\\ &= \\frac{W_A p(t)(p(t) + W_a q(t))}{W_A p(t)(W_A p(t) + W_a q(t)) + W_a q(t)(W_A p(t) + W_a q(t))}\\\\ &= \\frac{W_A p(t)}{W_A p(t) + W_a q(t)} \\end{aligned} \\] This shows that we need twice as much selection under diploid selection relative to that under haploid selection (e.g., \\(W_{AA} = W_A W_A\\) ) for evolution to proceed as quickly. Why is evolution slower under diploid selection? Let's check this visually (try breaking the assumption above, \\(W_{ij}=W_i W_j\\) , to see what happens). import matplotlib.pyplot as plt # calculate allele frequency over time with recursion w_A = 1.1 #relative fitness of allele A in the haploid model w_AA = w_A*w_A #relative fitness of genotype AA in the diploid model w_Aa = w_A #relative fitness of genotype Aa in the diploid model p_now_hap = 0.01 #initial allele frequency, p_0 p_now_dip = p_now_hap ps_hap, ps_dip = [], [] #lists to hold allele frequencies over time ts = range(100) #list of time steps for t in ts: #for each time ps_hap.append(p_now_hap) #add current allele frequency to list p_now_hap = w_A*p_now_hap/(w_A*p_now_hap+(1-p_now_hap)) #update allele frequency with haploid recursion equation ps_dip.append(p_now_dip) #add current allele frequency to list p_now_dip = (w_AA*p_now_dip**2 + w_Aa*p_now_dip*(1-p_now_dip))/(w_AA*p_now_dip**2 + 2*w_Aa*p_now_dip*(1-p_now_dip) + (1-p_now_dip)**2) #update allele frequency with diploid recursion equation # plot plt.scatter(ts, ps_hap, label='haploid') plt.scatter(ts, ps_dip, s=10, label='diploid') plt.xlabel(\"generation, $t$\") plt.ylabel(\"frequency of $A$ allele, $p(t)$\") plt.legend() #add legend to know which points belong to which model plt.show()","title":"4. Comparing haploid and diploid selection"},{"location":"lectures/lecture-04/#5-one-locus-haploid-selection-in-continuous-time","text":"To model haploid selection in continuous-time we assume that during the selective phase each haploid individual has growth rate \\(r_A =\\) growth rate of individuals with the \\(A\\) allele \\(r_a =\\) growth rate of individuals with the \\(a\\) allele and the relative numbers of each type don't change otherwise (ie, during union, diploidy, or meiosis). We therefore have exponential growth of both genotypes, \\(\\frac{\\mathrm{d} n_i}{dt} = r_i n_i\\) . At any particular point in time, \\(t\\) , the frequency of allele \\(A\\) is, \\(p(t) = n_A(t)/(n_A(t) + n_a(t))\\) . We can therefore derive the rate of change in the frequency of allele \\(A\\) , \\(\\mathrm{d}p/\\mathrm{d}t\\) , using the qoutient rule (see Appendix 2 in the text for help with this and other math tricks) \\[ \\begin{aligned} \\frac{\\mathrm{d}p}{\\mathrm{d}t} &= \\frac{\\mathrm{d}\\frac{n_A}{n_A + n_a}}{\\mathrm{d}t} \\\\ &= \\frac{\\frac{\\mathrm{d}n_A}{\\mathrm{d}t} (n_A(t) + n_a(t)) - n_A(t) \\frac{\\mathrm{d}(n_A+n_a)}{\\mathrm{d}t}}{(n_A(t) + n_a(t))^2}\\\\ &= \\frac{r_A n_A(t) (n_A(t) + n_a(t)) - n_A(t) (r_A n_A(t) + r_a n_a(t))}{(n_A(t) + n_a(t))^2}\\\\ &= \\frac{r_A n_A(t) n_a(t) - r_a n_A(t) n_a(t))}{(n_A(t) + n_a(t))^2}\\\\ &= \\frac{(r_A -r_a)n_A(t) n_a(t)}{(n_A(t) + n_a(t))^2}\\\\ &= (r_A -r_a)p(t) q(t), \\end{aligned} \\] where \\(s_c = r_A - r_a\\) is the continuous-time selection coefficient of allele \\(A\\) . A similar equation can be derived for the model of diploid-selection in continuous time, but we will not study it (see Problem 3.16 in the text if you are curious).","title":"5. One-locus haploid selection in continuous time"},{"location":"lectures/lecture-04/#6-comparing-haploid-selection-in-discrete-and-continuous-time","text":"Are the discrete- and continuous-time models of haploid selection as different as they look? \\[ p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a q(t)} \\] \\[ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = s_c p(t) q(t) \\] Not really. Discrete and continuous time models generally behave in a similar fashion when changes occur slowly over time. For this model of haploid selection, this implies that the discrete and continuous models will be similar when the fitnesses of the two alleles are nearly equal, i.e., when \\(W_A - W_a\\) is small. This is called \"weak selection\". In the discrete model, the change in the allele frequency is \\[ \\begin{aligned} \\Delta p &= p(t+1) - p(t) \\\\ &= \\frac{W_A p(t)}{W_A p(t) + W_a q(t)} - p(t) \\\\ &= \\frac{(W_A - W_a)p(t) q(t)}{W_A p(t) + W_a q(t)}. \\end{aligned} \\] Now define \\(s_d = (W_A - W_a)/W_a\\) as the discrete time selection coefficient. Subbing this in as \\(W_A = s_d W_a + W_a\\) gives \\[ \\begin{aligned} \\Delta p &= \\frac{(W_A-W_a)p(t) q(t)}{W_A p(t) + W_a q(t)}\\\\ &= \\frac{(s_d W_a + W_a - W_a) p(t) q(t)}{(s_d W_a + W_a) p(t) + W_a q(t)}\\\\ &= \\frac{s_d W_a p(t) q(t)}{s_d W_a p(t) + W_a (p(t) + q(t))}\\\\ &= \\frac{s_d W_a p(t) q(t)}{s_d W_a p(t) + W_a}\\\\ &= \\frac{s_d p(t) q(t)}{s_d p(t) + 1}\\\\ \\end{aligned} \\] Now assume weak selection, i.e., that \\(s_d\\) is small. This implies \\(s_d p(t) + 1 \\approx 1\\) . Making this approximation we have \\[ \\Delta p \\approx s_d p(t) q(t) \\] This is equivalent to the continuous-time model when the selection coefficients are equal, \\(s_c = s_d\\) .","title":"6. Comparing haploid selection in discrete and continuous time"},{"location":"lectures/lecture-04/#7-the-models-weve-covered","text":"To summarize the last two lectures, we've derived four of the most classic models in ecology and evolution: Model Discrete time Continous time Exponential growth \\(n(t+1) = R n(t)\\) \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t} = r n(t)\\) Logistic growth \\(n(t+1) = (1 + r(1 - \\frac{n(t)}{K}))n(t)\\) \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t} = r(1 - \\frac{n(t)}{K})n(t)\\) Haploid selection \\(p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a q(t)}\\) \\(\\frac{\\mathrm{d}p}{\\mathrm{d}t} = s p(t)(1-p(t))\\) Diploid selection \\(p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t) q(t)}{W_{AA}p(t)^2 + 2 W_{Aa} p(t) q(t) + W_{aa} q(t)^2}\\) Not derived See textbook Sections 3.4 and 3.5 for models of interacting species and epidemiology, respectively, which we won't cover in class.","title":"7. The models we've covered"},{"location":"lectures/lecture-05/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 5: Numerical and graphical techniques I (univariate) Run notes interactively? Lecture overview Numerical and graphical techniques Plots of variables over time Plots of variables as a function of the variables themselves Summary 1. Numerical and graphical techniques Before we jump into more rigorous mathematical analyses, we\u2019re first going to learn how to get a feel for the dynamics of our models. To do so we\u2019re going to choose some particular numerical values for our parameters and then use our models to predict what happens over time. The downside of this approach is that we often won\u2019t know the parameter values to choose and, regardless, choosing particular values doesn\u2019t tell us about the dynamics of our model more generally. The upside is that this approach can highlight errors or reveal unexpected patterns that guide future mathematical analyses. 2. Plots of variables over time Exponential growth model In the discrete exponential growth model, there is one parameter, \\(R\\) , the number of offspring per parent (\"reproductive factor\"). In last week\u2019s lab we wrote a recursive function (actually, a generator) to generate values of \\(n(t)\\) , the population size, at sequential time points. import numpy as np def n(t0, n0, R, max=np.inf): # Set the initial value of t and n(t) t, nt = t0, n0 # Yield new values of n(t) if t hasn't gone past the max value while t < max: yield nt # Then update t and n(t) t, nt = t + 1, nt * R We then chose some parameter values (reproductive factor, \\(R = 2\\) ) and initial conditions (initial population size, \\(n(0) = 1\\) ) to get the values of \\(n(t)\\) from the initial ( \\(t = 0\\) ) to final ( \\(t = 10\\) ) time. nt = n(t0=0, n0=1, R=2, max=10) #choose some parameter values nts = [n for n in nt] #get all the t, n(t) values nts [1, 2, 4, 8, 16, 32, 64, 128, 256, 512] And we then plotted \\(n(t)\\) as a function of \\(t\\) import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.plot(range(10), nts, marker = '.', markersize = 10) ax.set_xlabel('time step, $t$') ax.set_ylabel('population size, $n(t)$') plt.show() This allowed us to compare what happens for different values of the reproductive factor, \\(R\\) . colors = ['black','blue','red'] fig, ax = plt.subplots() for i, R in enumerate([1.1,1,0.9]): nt = n(t0=0, n0=100, R=R, max=10) nts = [n for n in nt] ax.plot(range(10), nts, color=colors[i], label=f\"R = {R}\", marker = '.', markersize = 10) ax.set_xlabel('time step, $t$') ax.set_ylabel('population size, $n(t)$') ax.legend() plt.show() From this we can deduce that when \\(R>1\\) the population grows, when \\(R<1\\) the population declines, and when \\(R=1\\) the population size remains constant. Logistic growth model In the discrete logistic growth model there are two parameters, the intrinsic growth rate, \\(r\\) , and the carrying capacity, \\(K\\) . The behaviour doesn\u2019t change much with different values of \\(K\\) but it is extremely sensitive to the value of \\(r\\) , as you may remember from Lab 2. def n(n0, r, k, max=np.inf): t, nt = 0, n0 while t < max: yield nt t, nt = t + 1, nt + r * nt * (1 - nt / k) # Initialize plots fig, ax = plt.subplots(1, 2, sharex=True, sharey=True) fig.set_size_inches(12,4) # Logistic growth with smaller r values for r in [0.40, 0.70, 1.80, 2.10]: ax[0].plot( #plot lines connecting values for visual clarity range(25), [nt for nt in n(1, r, 1000, max=25)], label = f\"r = {r}\", marker = '.', markersize = 10 ) # Logistic growth with larger r values for r in [2.70, 3.0995]: ax[1].plot( range(25), [nt for nt in n(1, r, 1000, max=25)], label = f\"r = {r}\", marker = '.', markersize = 10 ) # Add titles and annotations ax[0].set_title('smaller $r$') ax[1].set_title('larger $r$') for i in range(2): ax[i].set_xlabel('time step, $t$') ax[i].set_ylabel('population size, $n(t)$') ax[i].legend() fig.tight_layout() plt.show() Bifurcation diagrams and chaos We can examine how this model behaves as we change \\(r\\) by making a bifurcation diagram , which plots the values the system takes on after a long time for a given parameter value. Check out the very complex and potentially strange dynamics in the plot below. What does it mean? # Sample the periodicity of the oscillations # by taking unique values after reaching carrying capacity (here we use t between 30 and 75) def log_map(r, n0=900, k=1000): return np.unique([nt for t, nt in enumerate(n(n0, r, k, max=75)) if t > 30]) # Compute the logistic map for different growth rates in discrete time r, Nr = np.array([]), np.array([]) #list of r and n(t) values we will plot for i in np.linspace(1.5, 3, 1000): #these are the r values we will simulate nl = log_map(i) #get the unique values after carrying capacity r = np.hstack((r, [i for _ in range(len(nl))])) #add the r value to plotting list (repeat the value of r for each unique n(t) value (for plotting)) Nr = np.hstack((Nr, nl)) #add the n(t) values to plotting list # Plot the logistic map on a black background (why not?) fig, ax = plt.subplots() ax.patch.set_facecolor('black') ax.scatter(r, Nr, s=0.075, color='white') plt.xlabel('intrinsic growth rate, $r$') plt.ylabel('population size, $n(t)$') plt.show() 3. Plots of variables as a function of the variables themselves OK, so now we\u2019ll move on to a plot that is easier to generate and is very useful for models with just one variable (which is what we\u2019ve been working with so far). Instead of plotting the variable as a function of time, we\u2019ll plot the variable as a function of the variable in the previous time, e.g., plotting \\(n(t+1)\\) as a function of \\(n(t)\\) . Haploid selection Let's start with our model of haploid selection \\[ p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a (1-p(t))} \\] and plot for two different sets of parameter values, where \\(A\\) has a higher or lower fitness than \\(a\\) . import sympy # Build cobweb plotting function def cobweb_haploid(p0, WA, Wa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow, pnext #current value of p(t) and p_(t+1) pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) #update p_(t+1) yield pnow, pnext #current value of p(t) and p_(t+1) pnow = pnext #update p(t) t += 1 #update t # Build function for generating figure def plot_haploid_selection(WA, Wa, p0=0.5, ax=None): pt = sympy.symbols('pt') #define our variable p(t) # Write out sympy equation f = (WA * pt) / (WA * pt + Wa * (1 - pt)) #the recursion equation # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation (turn it into a function) t = np.linspace(0,1,100) fy = sympy.lambdify(pt, f)(t) # Build plot if ax == None: fig, ax = plt.subplots() ax.plot(t, fy, color='black', label=f\"$W_A$ = {WA}, $W_a$ = {Wa}\") #plot p_(t+1) as function of p(t) ax.plot(t, t, color='black', linestyle='--') #draw 1:1 line for reference # Add cobweb cobweb = np.array([p for p in cobweb_haploid(p0, WA, Wa, max=100)]) ax.plot(cobweb[:,0], cobweb[:,1]) # Annotate and label plot ax.set_xlim(0,1) ax.set_ylim(0,1) ax.set_xlabel(\"allele frequency at $t$, $p(t)$\") ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\") ax.legend(frameon=False) return ax # Plot figure fig, ax = plt.subplots(1,2) fig.set_size_inches(12,4) # First cobweb with WA > Wa plot_haploid_selection(WA = 1, Wa = 0.5, ax=ax[0]) # Second cobweb with WA < Wa plot_haploid_selection(WA = 0.5, Wa = 1, ax=ax[1]) plt.show() There are three components to this plot. First, the solid curve gives the recursion itself ( \\(p(t+1)\\) as a function of \\(p(t)\\) ). Second, the dashed line shows where \\(p(t+1)=p(t)\\) . And third, the blue lines show how the variable changes over multiple time steps. Foreshadowing what is to come (Lecture 7), the dashed line is helpful for two reasons. First, it indicates where the variable does not change over time. So wherever the recursion (solid line) intersects with the dashed line is an equilibrium . Second, it reflects \\(p(t+1)\\) back onto \\(p(t)\\) , updating the variable. For example, in the left panel above we start with an allele frequency of \\(p(t)=0.5\\) , draw a blue vertical line to the recursion to find \\(p(t+1)\\) , and then update \\(p(t)\\) to \\(p(t+1)\\) by drawing the horizontal blue line to the dashed line. Now we can ask what \\(p(t+1)\\) is given this updated value of \\(p(t)\\) by drawing another vertical blue line, and so on. Following the blue line we can therefore see where the system is heading, which tells us about the stability of the equilibria. What are the stable equilibria in the two panels above? Diploid selection To demonstrate the utility of this method, let\u2019s move on to the slightly more complex model of diploid selection \\[ p(t+1) = \\frac{W_{AA} p(t)^2 + W_{Aa}p(t) q_t}{W_{AA}p(t)^2 + W_{Aa}2p(t)q_t + W_{aa}q_t^2} \\] To show some different behaviour than above, this time let's set \\(W_{AA} < W_{Aa} > W_{aa}\\) and plot for two different starting frequencies, \\(p_0\\) . def cobweb_diploid(p0, WAA, WAa, Waa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow, pnext #current value of p(t) and p(t+1) pnext = (WAA * pnow**2 + WAa * pnow * (1 - pnow)) / (WAA * pnow**2 + WAa * 2 * pnow * (1 - pnow) + Waa * (1 - pnow)**2) #update p(t+1) yield pnow, pnext #current value of p(t) and p(t+1) pnow = pnext #update p(t) t += 1 #update t # Build function for generating figure def plot_diploid_selection(WAA, WAa, Waa, ax=None, p0=0.5): pt = sympy.symbols('pt') #define our variable p(t) # Write out sympy equation f = (WAA * pt**2 + WAa * pt * (1- pt) ) / (WAA * pt**2 + WAa * 2 * pt * (1 - pt) + Waa * (1 - pt)**2) #the recursion equation # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation x = np.linspace(0,1,100) fy = sympy.lambdify(pt, f)(x) # Build plot if ax == None: fig, ax = plt.subplots() # Add cobweb cobweb = np.array([p for p in cobweb_diploid(p0, WAA, WAa, Waa, max=100)]) ax.plot(cobweb[:,0], cobweb[:,1]) # Annotate and label plot ax.plot(x, fy, color='black', label=f\"$W_A$$_A$ = {WAA}, $W_A$$_a$ = {WAa}, $W_a$$_a$ = {Waa}\") ax.plot(x, x, color='black', linestyle='--') ax.set_xlim(0,1) ax.set_ylim(0,1) ax.set_xlabel(\"allele frequency at $t$, $p(t)$\") ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\") ax.legend(frameon=False) return ax # Plot figure fig, ax = plt.subplots(1,2) fig.set_size_inches(12,4) # First cobweb from low starting condition plot_diploid_selection(WAA=1, WAa=2, Waa=1, p0=0.05, ax=ax[0]) # Second cobweb from high starting condition plot_diploid_selection(WAA=1, WAa=2, Waa=1, p0=0.95, ax=ax[1]) plt.show() How many equilibria are there? Which appear to be stable? Difference/differential equations We can do something very similar for difference and differential equations. Now we plot the change in the variable as a function of the current value of the variable, e.g., plot \\(\\Delta n\\) or \\(dn/dt\\) as a function of \\(n(t)\\) . For example, in our model of haploid selection we have \\[ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = sp(1-p) \\] and our plot looks like: # Initialize sympy symbols p0, s, t = sympy.symbols('p0, s, t') p = sympy.Function('t') # Specify differential equation diffeq = sympy.Eq(p(t).diff(t), s * p(t) * (1 - p(t))) # Convert differential equation RHS to pythonic function dp = sympy.lambdify((s, p(t)), diffeq.rhs) # Plot the curve fig, ax = plt.subplots() for s_coeff in [0.01, -0.01]: ax.plot( np.linspace(0, 1, 100), dp(s_coeff, np.linspace(0,1, 100)), label=f\"s = {s_coeff}\" ) ax.set_xlabel('allele frequency at $t, p$') ax.set_ylabel('change in allele frequency, $\\mathrm{d}p/\\mathrm{d}t$') ax.legend(frameon=False) plt.show() What does this tell us about how allele frequency will change when \\(s>0\\) vs. \\(s<0\\) ? And what allele frequencies, \\(p\\) , cause more rapid evolution? 4. Summary To get a feel for our model it is helpful to graph some numerical examples: Plot the variable as a function of time (\"simulate\") Plot the variable (or change in variable) as a function of itself (only works for models with one variable) Next lecture we\u2019ll look at a graphical technique for models with multiple variables...","title":"Lecture 5"},{"location":"lectures/lecture-05/#lecture-5-numerical-and-graphical-techniques-i-univariate","text":"Run notes interactively?","title":"Lecture 5: Numerical and graphical techniques I (univariate)"},{"location":"lectures/lecture-05/#lecture-overview","text":"Numerical and graphical techniques Plots of variables over time Plots of variables as a function of the variables themselves Summary","title":"Lecture overview"},{"location":"lectures/lecture-05/#1-numerical-and-graphical-techniques","text":"Before we jump into more rigorous mathematical analyses, we\u2019re first going to learn how to get a feel for the dynamics of our models. To do so we\u2019re going to choose some particular numerical values for our parameters and then use our models to predict what happens over time. The downside of this approach is that we often won\u2019t know the parameter values to choose and, regardless, choosing particular values doesn\u2019t tell us about the dynamics of our model more generally. The upside is that this approach can highlight errors or reveal unexpected patterns that guide future mathematical analyses.","title":"1. Numerical and graphical techniques"},{"location":"lectures/lecture-05/#2-plots-of-variables-over-time","text":"","title":"2. Plots of variables over time"},{"location":"lectures/lecture-05/#exponential-growth-model","text":"In the discrete exponential growth model, there is one parameter, \\(R\\) , the number of offspring per parent (\"reproductive factor\"). In last week\u2019s lab we wrote a recursive function (actually, a generator) to generate values of \\(n(t)\\) , the population size, at sequential time points. import numpy as np def n(t0, n0, R, max=np.inf): # Set the initial value of t and n(t) t, nt = t0, n0 # Yield new values of n(t) if t hasn't gone past the max value while t < max: yield nt # Then update t and n(t) t, nt = t + 1, nt * R We then chose some parameter values (reproductive factor, \\(R = 2\\) ) and initial conditions (initial population size, \\(n(0) = 1\\) ) to get the values of \\(n(t)\\) from the initial ( \\(t = 0\\) ) to final ( \\(t = 10\\) ) time. nt = n(t0=0, n0=1, R=2, max=10) #choose some parameter values nts = [n for n in nt] #get all the t, n(t) values nts [1, 2, 4, 8, 16, 32, 64, 128, 256, 512] And we then plotted \\(n(t)\\) as a function of \\(t\\) import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.plot(range(10), nts, marker = '.', markersize = 10) ax.set_xlabel('time step, $t$') ax.set_ylabel('population size, $n(t)$') plt.show() This allowed us to compare what happens for different values of the reproductive factor, \\(R\\) . colors = ['black','blue','red'] fig, ax = plt.subplots() for i, R in enumerate([1.1,1,0.9]): nt = n(t0=0, n0=100, R=R, max=10) nts = [n for n in nt] ax.plot(range(10), nts, color=colors[i], label=f\"R = {R}\", marker = '.', markersize = 10) ax.set_xlabel('time step, $t$') ax.set_ylabel('population size, $n(t)$') ax.legend() plt.show() From this we can deduce that when \\(R>1\\) the population grows, when \\(R<1\\) the population declines, and when \\(R=1\\) the population size remains constant.","title":"Exponential growth model"},{"location":"lectures/lecture-05/#logistic-growth-model","text":"In the discrete logistic growth model there are two parameters, the intrinsic growth rate, \\(r\\) , and the carrying capacity, \\(K\\) . The behaviour doesn\u2019t change much with different values of \\(K\\) but it is extremely sensitive to the value of \\(r\\) , as you may remember from Lab 2. def n(n0, r, k, max=np.inf): t, nt = 0, n0 while t < max: yield nt t, nt = t + 1, nt + r * nt * (1 - nt / k) # Initialize plots fig, ax = plt.subplots(1, 2, sharex=True, sharey=True) fig.set_size_inches(12,4) # Logistic growth with smaller r values for r in [0.40, 0.70, 1.80, 2.10]: ax[0].plot( #plot lines connecting values for visual clarity range(25), [nt for nt in n(1, r, 1000, max=25)], label = f\"r = {r}\", marker = '.', markersize = 10 ) # Logistic growth with larger r values for r in [2.70, 3.0995]: ax[1].plot( range(25), [nt for nt in n(1, r, 1000, max=25)], label = f\"r = {r}\", marker = '.', markersize = 10 ) # Add titles and annotations ax[0].set_title('smaller $r$') ax[1].set_title('larger $r$') for i in range(2): ax[i].set_xlabel('time step, $t$') ax[i].set_ylabel('population size, $n(t)$') ax[i].legend() fig.tight_layout() plt.show() Bifurcation diagrams and chaos We can examine how this model behaves as we change \\(r\\) by making a bifurcation diagram , which plots the values the system takes on after a long time for a given parameter value. Check out the very complex and potentially strange dynamics in the plot below. What does it mean? # Sample the periodicity of the oscillations # by taking unique values after reaching carrying capacity (here we use t between 30 and 75) def log_map(r, n0=900, k=1000): return np.unique([nt for t, nt in enumerate(n(n0, r, k, max=75)) if t > 30]) # Compute the logistic map for different growth rates in discrete time r, Nr = np.array([]), np.array([]) #list of r and n(t) values we will plot for i in np.linspace(1.5, 3, 1000): #these are the r values we will simulate nl = log_map(i) #get the unique values after carrying capacity r = np.hstack((r, [i for _ in range(len(nl))])) #add the r value to plotting list (repeat the value of r for each unique n(t) value (for plotting)) Nr = np.hstack((Nr, nl)) #add the n(t) values to plotting list # Plot the logistic map on a black background (why not?) fig, ax = plt.subplots() ax.patch.set_facecolor('black') ax.scatter(r, Nr, s=0.075, color='white') plt.xlabel('intrinsic growth rate, $r$') plt.ylabel('population size, $n(t)$') plt.show()","title":"Logistic growth model"},{"location":"lectures/lecture-05/#3-plots-of-variables-as-a-function-of-the-variables-themselves","text":"OK, so now we\u2019ll move on to a plot that is easier to generate and is very useful for models with just one variable (which is what we\u2019ve been working with so far). Instead of plotting the variable as a function of time, we\u2019ll plot the variable as a function of the variable in the previous time, e.g., plotting \\(n(t+1)\\) as a function of \\(n(t)\\) .","title":"3. Plots of variables as a function of the variables themselves"},{"location":"lectures/lecture-05/#haploid-selection","text":"Let's start with our model of haploid selection \\[ p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a (1-p(t))} \\] and plot for two different sets of parameter values, where \\(A\\) has a higher or lower fitness than \\(a\\) . import sympy # Build cobweb plotting function def cobweb_haploid(p0, WA, Wa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow, pnext #current value of p(t) and p_(t+1) pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) #update p_(t+1) yield pnow, pnext #current value of p(t) and p_(t+1) pnow = pnext #update p(t) t += 1 #update t # Build function for generating figure def plot_haploid_selection(WA, Wa, p0=0.5, ax=None): pt = sympy.symbols('pt') #define our variable p(t) # Write out sympy equation f = (WA * pt) / (WA * pt + Wa * (1 - pt)) #the recursion equation # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation (turn it into a function) t = np.linspace(0,1,100) fy = sympy.lambdify(pt, f)(t) # Build plot if ax == None: fig, ax = plt.subplots() ax.plot(t, fy, color='black', label=f\"$W_A$ = {WA}, $W_a$ = {Wa}\") #plot p_(t+1) as function of p(t) ax.plot(t, t, color='black', linestyle='--') #draw 1:1 line for reference # Add cobweb cobweb = np.array([p for p in cobweb_haploid(p0, WA, Wa, max=100)]) ax.plot(cobweb[:,0], cobweb[:,1]) # Annotate and label plot ax.set_xlim(0,1) ax.set_ylim(0,1) ax.set_xlabel(\"allele frequency at $t$, $p(t)$\") ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\") ax.legend(frameon=False) return ax # Plot figure fig, ax = plt.subplots(1,2) fig.set_size_inches(12,4) # First cobweb with WA > Wa plot_haploid_selection(WA = 1, Wa = 0.5, ax=ax[0]) # Second cobweb with WA < Wa plot_haploid_selection(WA = 0.5, Wa = 1, ax=ax[1]) plt.show() There are three components to this plot. First, the solid curve gives the recursion itself ( \\(p(t+1)\\) as a function of \\(p(t)\\) ). Second, the dashed line shows where \\(p(t+1)=p(t)\\) . And third, the blue lines show how the variable changes over multiple time steps. Foreshadowing what is to come (Lecture 7), the dashed line is helpful for two reasons. First, it indicates where the variable does not change over time. So wherever the recursion (solid line) intersects with the dashed line is an equilibrium . Second, it reflects \\(p(t+1)\\) back onto \\(p(t)\\) , updating the variable. For example, in the left panel above we start with an allele frequency of \\(p(t)=0.5\\) , draw a blue vertical line to the recursion to find \\(p(t+1)\\) , and then update \\(p(t)\\) to \\(p(t+1)\\) by drawing the horizontal blue line to the dashed line. Now we can ask what \\(p(t+1)\\) is given this updated value of \\(p(t)\\) by drawing another vertical blue line, and so on. Following the blue line we can therefore see where the system is heading, which tells us about the stability of the equilibria. What are the stable equilibria in the two panels above?","title":"Haploid selection"},{"location":"lectures/lecture-05/#diploid-selection","text":"To demonstrate the utility of this method, let\u2019s move on to the slightly more complex model of diploid selection \\[ p(t+1) = \\frac{W_{AA} p(t)^2 + W_{Aa}p(t) q_t}{W_{AA}p(t)^2 + W_{Aa}2p(t)q_t + W_{aa}q_t^2} \\] To show some different behaviour than above, this time let's set \\(W_{AA} < W_{Aa} > W_{aa}\\) and plot for two different starting frequencies, \\(p_0\\) . def cobweb_diploid(p0, WAA, WAa, Waa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow, pnext #current value of p(t) and p(t+1) pnext = (WAA * pnow**2 + WAa * pnow * (1 - pnow)) / (WAA * pnow**2 + WAa * 2 * pnow * (1 - pnow) + Waa * (1 - pnow)**2) #update p(t+1) yield pnow, pnext #current value of p(t) and p(t+1) pnow = pnext #update p(t) t += 1 #update t # Build function for generating figure def plot_diploid_selection(WAA, WAa, Waa, ax=None, p0=0.5): pt = sympy.symbols('pt') #define our variable p(t) # Write out sympy equation f = (WAA * pt**2 + WAa * pt * (1- pt) ) / (WAA * pt**2 + WAa * 2 * pt * (1 - pt) + Waa * (1 - pt)**2) #the recursion equation # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation x = np.linspace(0,1,100) fy = sympy.lambdify(pt, f)(x) # Build plot if ax == None: fig, ax = plt.subplots() # Add cobweb cobweb = np.array([p for p in cobweb_diploid(p0, WAA, WAa, Waa, max=100)]) ax.plot(cobweb[:,0], cobweb[:,1]) # Annotate and label plot ax.plot(x, fy, color='black', label=f\"$W_A$$_A$ = {WAA}, $W_A$$_a$ = {WAa}, $W_a$$_a$ = {Waa}\") ax.plot(x, x, color='black', linestyle='--') ax.set_xlim(0,1) ax.set_ylim(0,1) ax.set_xlabel(\"allele frequency at $t$, $p(t)$\") ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\") ax.legend(frameon=False) return ax # Plot figure fig, ax = plt.subplots(1,2) fig.set_size_inches(12,4) # First cobweb from low starting condition plot_diploid_selection(WAA=1, WAa=2, Waa=1, p0=0.05, ax=ax[0]) # Second cobweb from high starting condition plot_diploid_selection(WAA=1, WAa=2, Waa=1, p0=0.95, ax=ax[1]) plt.show() How many equilibria are there? Which appear to be stable?","title":"Diploid selection"},{"location":"lectures/lecture-05/#differencedifferential-equations","text":"We can do something very similar for difference and differential equations. Now we plot the change in the variable as a function of the current value of the variable, e.g., plot \\(\\Delta n\\) or \\(dn/dt\\) as a function of \\(n(t)\\) . For example, in our model of haploid selection we have \\[ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = sp(1-p) \\] and our plot looks like: # Initialize sympy symbols p0, s, t = sympy.symbols('p0, s, t') p = sympy.Function('t') # Specify differential equation diffeq = sympy.Eq(p(t).diff(t), s * p(t) * (1 - p(t))) # Convert differential equation RHS to pythonic function dp = sympy.lambdify((s, p(t)), diffeq.rhs) # Plot the curve fig, ax = plt.subplots() for s_coeff in [0.01, -0.01]: ax.plot( np.linspace(0, 1, 100), dp(s_coeff, np.linspace(0,1, 100)), label=f\"s = {s_coeff}\" ) ax.set_xlabel('allele frequency at $t, p$') ax.set_ylabel('change in allele frequency, $\\mathrm{d}p/\\mathrm{d}t$') ax.legend(frameon=False) plt.show() What does this tell us about how allele frequency will change when \\(s>0\\) vs. \\(s<0\\) ? And what allele frequencies, \\(p\\) , cause more rapid evolution?","title":"Difference/differential equations"},{"location":"lectures/lecture-05/#4-summary","text":"To get a feel for our model it is helpful to graph some numerical examples: Plot the variable as a function of time (\"simulate\") Plot the variable (or change in variable) as a function of itself (only works for models with one variable) Next lecture we\u2019ll look at a graphical technique for models with multiple variables...","title":"4. Summary"},{"location":"lectures/lecture-06/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 6: Numerical and graphical techniques II (multivariate) Run notes interactively? Lecture overview Numerical and graphical techniques Phase-line diagrams Phase-plane diagrams 1. Numerical and graphical techniques In the last lecture we talked about two numerical/graphical approaches to get a better understanding of our models: Plotting a variable as a function of time (eg, \\(p(t)\\) as a function of \\(t\\) ) Plotting a variable as a function of itself (eg, \\(p(t+1)\\) as a function of \\(p(t)\\) ). The latter works well for models with one variable. In this lecture we\u2019re going to talk about a third numerical technique, a phase-plane diagram , which is especially useful for models that have two variables. 2. Phase-line diagrams Before looking at models with two variables, let\u2019s first consider some with only one. Consider again haploid selection where \\[ p(t+1) = \\frac{W_Ap(t)}{W_Ap(t) + W_a(1-p(t))} \\] Last time we plotted \\(p_{t + 1}\\) as a function of \\(p(t)\\) and used this to examine the dynamics starting from any initial value. We called this plot a cob-web plot. import sympy import numpy as np import matplotlib.pyplot as plt # Build cobweb plotting function def cobweb_haploid(p0, WA, Wa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow, pnext #current value of p(t) and p_(t+1) pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) #update p_(t+1) yield pnow, pnext #current value of p(t) and p_(t+1) pnow = pnext #update p(t) t += 1 #update t # Build function for generating figure def plot_haploid_selection(WA, Wa, p0=0.5, ax=None): pt = sympy.symbols('pt') #define our variable p(t) # Write out sympy equation f = (WA * pt) / (WA * pt + Wa * (1 - pt)) #the recursion equation # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation (turn it into a function) t = np.linspace(0,1,100) fy = sympy.lambdify(pt, f)(t) # Build plot if ax == None: fig, ax = plt.subplots() ax.plot(t, fy, color='black', label=f\"$W_A$ = {WA}, $W_a$ = {Wa}\") #plot p_(t+1) as function of p(t) ax.plot(t, t, color='black', linestyle='--') #draw 1:1 line for reference # Add cobweb cobweb = np.array([p for p in cobweb_haploid(p0, WA, Wa, max=100)]) ax.plot(cobweb[:,0], cobweb[:,1]) # Annotate and label plot ax.set_xlim(0,1) ax.set_ylim(0,1) ax.set_xlabel(\"allele frequency at $t$, $p(t)$\") ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\") ax.legend(frameon=False) return ax # Plot figure fig, ax = plt.subplots(1,2) fig.set_size_inches(12,4) # First cobweb with WA > Wa plot_haploid_selection(WA = 1, Wa = 0.5, ax=ax[0]) # Second cobweb with WA < Wa plot_haploid_selection(WA = 0.5, Wa = 1, ax=ax[1]) plt.show() Now let's simplify the cob-web plot and just indicate the direction (and magnitude) of change in \\(p(t)\\) with time. This is known as a phase-line diagram with a vector field (the arrows). def phase_line_haploid(p0, WA, Wa, max=np.inf): 'generator for p(t)' t, pnow, pnext = 0, p0, 0 #initial conditions while t < max: yield pnow #current value of p(t) and p_(t+1) pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) pnow = pnext #update p(t) t += 1 #update t def plot_phase_line_haploid(WA, Wa, p0, max=20, ax=None): 'plot phase line' # Set up figure if ax==None: fig, ax = plt.subplots() fig.set_size_inches(8,0.25) ax.axhline(0, color='black', linewidth=0.5) # Plot phase-line pts = [pt for pt in phase_line_haploid(p0, WA, Wa, max=max)] #pt values ax.plot( pts, np.zeros(max) #dummy y values (0 for all x values) because we want to plot a 1d line ) # Plot vector field marker = '>' if pts[2] > pts[1] else ' < ' #determine which direction to point based on first 2 time points ax.scatter( pts, np.zeros(max),#dummy y again marker=marker, s=150 ) # Remove background axes ax.set_ylabel('$p$', rotation=0) ax.set_xlabel(f\"$W_A$ = {WA}, $W_a$ = {Wa}, $p_0$ = {p0}\") ax.get_yaxis().set_ticks([]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.spines['bottom'].set_visible(False) ax.spines['left'].set_visible(False) ax.set_xlim(0,1) plt.show() plot_phase_line_haploid(WA=1, Wa=0.5, p0=0.01) plot_phase_line_haploid(WA=0.5, Wa=1, p0=0.99) As in the cob-web plots, we see the allele frequency approaches \\(p=1\\) when \\(W_A>W_a\\) and \\(p=0\\) when \\(W_a>W_A\\) . We also notice, as above, the changes are fastest (fewer, longer arrows) at intermediate frequencies. Similarly, with the more complex model of diploid selection \\[ p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t)q(t)}{W_{AA}p(t)^2 + W_{Aa}p(t)q(t) + W_{aa}q(t)^2} \\] we can draw a phase-line diagram and vector field for a set of parameter values. def phase_line_diploid(p0, WAA, WAa, Waa, max=np.inf): 'generator for p(t)' t, pnow, pnext = 0, p0, 0 #initial conditions while t < max: yield pnow #current value of p(t) and p(t+1) pnext = (WAA * pnow**2 + WAa * pnow * (1 - pnow)) / (WAA * pnow**2 + WAa * 2 * pnow * (1 - pnow) + Waa * (1 - pnow)**2) #update p(t+1) pnow = pnext #update p(t) t += 1 #update t def plot_phase_line_diploid(WAA, WAa, Waa, p0, max=20, ax=None): 'plot phase line' # set up figure if ax==None: fig, ax = plt.subplots() fig.set_size_inches(8,0.25) ax.axhline(0, color='black', linewidth=0.5) # Plot phase-line pts = [pt for pt in phase_line_diploid(p0, WAA, WAa, Waa, max=max)] ax.plot( pts, np.zeros(max), alpha=1 ) # Plot phase-line markers marker = '>' if pts[2] > pts[1] else ' < ' ax.scatter( pts, np.zeros(max), marker=marker, s=150 ) ax.set_xlabel(f\"$WAA$ = {WAA}, $WAa$ = {WAa}, $Waa$ = {Waa}, $p_0$ = {p0}\") return ax # Plot figure fig, ax = plt.subplots() fig.set_size_inches(8,0.25) plot_phase_line_diploid(WAA=1, WAa=2, Waa=1, p0=0.99, max=100, ax=ax) #higher starting allele frequency plot_phase_line_diploid(WAA=1, WAa=2, Waa=1, p0=0.01, max=100, ax=ax) #low starting allele frequency # Remove background axes ax.set_ylabel('$p$', rotation=0) ax.get_yaxis().set_ticks([]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.spines['bottom'].set_visible(False) ax.spines['left'].set_visible(False) ax.set_xlim(0,1) plt.show() Notice that this time we chose two initial frequencies for the same plot (orange vs blue), to show that under heterozygote advantage ( \\(W_{AA}<W_{Aa}>W_{aa}\\) ) the allele frequency approaches an intermediate value from either direction. 3. Phase-plane diagrams Now let\u2019s extend this technique from one to two variables. Lotka-Volterra model We'll introduce a new model for this purpose, the Lotka-Volterra model of competition (see section 3.4.1 in the text). This is an extension of the logistic growth model (Lecture 3) to include competition between multiple species (in our case two). Let the population size of each species be \\(n_1(t)\\) and \\(n_2(t)\\) . These are our two variables. And let them have different intrinsic growth rates, \\(r_1\\) and \\(r_2\\) , and carrying capacities, \\(K_1\\) and \\(K_2\\) . To model competition, we\u2019ll assume that, for an individual of species \\(i\\) , an individual of species \\(j\\) is the competitive equivalent of \\(\\alpha_{ij}\\) individuals of species \\(i\\) . We then have \\[ n_1(t+1) = n_1(t)\\left( 1 + r_1 \\left(1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1}\\right)\\right) \\] \\[ n_2(t+1) = n_2(t) \\left(1 + r_2 \\left(1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2}\\right)\\right) \\] Often individuals of the same species will use more similar resources and therefore competition will be less severe with individuals of the other species, \\(0 < \\alpha_{ij} < 1\\) , but not always. And, in fact, we could model other types of interactions (eg, mutualism) by making some of the interactions beneficial, \\(\\alpha_{ij} < 0\\) . Phase-planes and vector fields So why did we introduce the Lotka-Volterra model? Well, phase-plane diagrams are plots of one variable against another ( \\(n_1\\) vs. \\(n_2\\) ), on which we can plot vector fields , vectors originating from many different starting conditions that indicate the direction and magnitude of change in the two variables. With this we can graphically investigate the dynamics of the Lotka-Volterra model by first defining the rates of change in our two variables, \\(\\Delta n_1\\) and \\(\\Delta n_2\\) and then choosing some parameter values to explore. \\[ \\Delta n_1 \\equiv n_1(t+1) - n_1(t) = n_1(t)r_1\\left(1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1}\\right) \\] \\[ \\Delta n_2 \\equiv n_2(t+1) - n_2(t) = n_2(t)r_2\\left(1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2}\\right) \\] Let's plot a phase-plane for the Lotka-Volterra with the following parameter values: \\(r_1 = 0.5, r_2 = 0.5, K_1 = 1000, K_2 = 1000, \\alpha_{12} = 0.5, \\alpha_{21} = 0.5\\) . # Define a function to plot the phase plane and vector field for n1 and n2 def plot_vector_field(dn1, dn2, xlim=(0,1200), ylim=(0,1200), n_steps=25, width=8, height=6, show=False, axes_labels=[None, None]): # Set x and y ranges xrange, yrange = np.linspace(xlim[0], xlim[1], n_steps), np.linspace(ylim[0], ylim[1], n_steps) # Initialize 2D grid with x,y values and additional grids to track derivatives X, Y = np.meshgrid(xrange, yrange) U, V = np.zeros(X.shape), np.zeros(Y.shape) # Compute the gradient at each x,y position for i in range(len(xrange)): for j in range(len(xrange)): U[i,j] = sympy.lambdify((n1, n2), dn1)(X[i,j], Y[i,j]) #change in n1 V[i,j] = sympy.lambdify((n1, n2), dn2)(X[i,j], Y[i,j]) #change in n2 # Plot figure fig, ax = plt.subplots() fig.set_size_inches(width, height) ax.set_xlabel(axes_labels[0]) ax.set_ylabel(axes_labels[1]) ax.quiver(X,Y,U,V, linewidth=1) #from point X,Y draw arrow moving U in x-axis and V in y-axis if show == True: plt.show() else: return ax # Initialize the sympy variables n1, n2 = sympy.symbols('n1, n2') # Choose the parameter values r1, r2 = 0.5, 0.5 k1, k2 = 1000, 1000 a12, a21 = 0.5, 0.5 # Specify the difference equations dn1 = r1 * n1 * (1 - (n1 + a12 * n2) / k1) dn2 = r2 * n2 * (1 - (n2 + a21 * n1) / k2) # Plot the vector field plot_vector_field(dn1, dn2, axes_labels=[\"number of species 1, $n_1$\", \"number of species 2, $n_2$\"]) plt.show() With this approach we see that the dynamics appear to be approaching a value near \\(n_1 = 700, n_2 = 700\\) from nearly any initial condition. Null clines To better understand the dynamics, we can ask for what values of our variables ( \\(n_1, n_2\\) ) is the change in our variables zero ( \\(\\Delta n_1 = 0\\) , \\(\\Delta n_2 = 0\\) ). These values are known as null clines . Concretely, going back to our previous formula for the change in \\(n_1\\) and \\(n_2\\) in the Lotka-Volterra model \\[ \\Delta n_1 = n_1(t)r_1\\left(1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1}\\right) \\] \\[ \\Delta n_2 = n_2(t)r_2\\left(1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2}\\right) \\] We want to know when \\(\\Delta n_1\\) and \\(\\Delta n_2\\) are 0. Solving for these inequalities shows that \\[ \\Delta n_1 = 0 \\Longrightarrow n_1(t) = 0, 1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1} = 0 \\] \\[ \\Delta n_2 = 0 \\Longrightarrow n_2(t) = 0, 1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2} = 0 \\] Plotting these null clines on the phase-plane diagram, we get # Initialize plot and ranges ax = plot_vector_field(dn1, dn2, axes_labels=[\"number of species 1, $n_1$\", \"number of species 2, $n_2$\"]) xrange, yrange = np.linspace(0, 1200, 100), np.linspace(0, 1200, 100) def plot_nullclines(ax): #plot the null clines for species 1 (blue) nullcline_1 = [list(i.values())[0] for i in sympy.solve(sympy.Eq(dn1, 0))] ax.plot(xrange, sympy.lambdify(n1, nullcline_1[1])(xrange), color=plt.cm.tab10(0)) # this null cline is a function of n1 (i.e. x) ax.plot([nullcline_1[0] for _ in xrange], yrange, color=plt.cm.tab10(0)) # #plot the null clines for species 2 (red) nullcline_2 = [list(i.values())[0] for i in sympy.solve(sympy.Eq(dn2, 0))] ax.plot(sympy.lambdify(n2, nullcline_2[0])(yrange), yrange, color=plt.cm.tab10(1)) # this null cline is a function of n2 (i.e. y) ax.plot(xrange, [nullcline_2[1] for _ in yrange], color=plt.cm.tab10(1)) ax.set_ylim(-10, 1210) ax.set_xlim(-10, 1210) return ax plot_nullclines(ax) plt.show() The null clines (blue for \\(n_1\\) and orange for \\(n_2\\) ) help us understand the dynamics. In each area bounded by null clines the vectors point in the same general direction (eg, in the top right area they point down and to the left). This helps us see where the dynamics are heading -- in this case most initial conditions head to the intersection of the non-zero null clines for \\(n_1\\) and \\(n_2\\) , near \\(n_1=700\\) and \\(n_2=700\\) . Note that where the null cline of one variable intersects a null cline of the other variable neither variable is changing, indicating equilibria . We can also make phase diagrams for continuous-time models, just using differential equations in place of difference equations. We\u2019ll see an example of that for another model, of predator and prey, in Lab 3.","title":"Lecture 6"},{"location":"lectures/lecture-06/#lecture-6-numerical-and-graphical-techniques-ii-multivariate","text":"Run notes interactively?","title":"Lecture 6: Numerical and graphical techniques II (multivariate)"},{"location":"lectures/lecture-06/#lecture-overview","text":"Numerical and graphical techniques Phase-line diagrams Phase-plane diagrams","title":"Lecture overview"},{"location":"lectures/lecture-06/#1-numerical-and-graphical-techniques","text":"In the last lecture we talked about two numerical/graphical approaches to get a better understanding of our models: Plotting a variable as a function of time (eg, \\(p(t)\\) as a function of \\(t\\) ) Plotting a variable as a function of itself (eg, \\(p(t+1)\\) as a function of \\(p(t)\\) ). The latter works well for models with one variable. In this lecture we\u2019re going to talk about a third numerical technique, a phase-plane diagram , which is especially useful for models that have two variables.","title":"1. Numerical and graphical techniques"},{"location":"lectures/lecture-06/#2-phase-line-diagrams","text":"Before looking at models with two variables, let\u2019s first consider some with only one. Consider again haploid selection where \\[ p(t+1) = \\frac{W_Ap(t)}{W_Ap(t) + W_a(1-p(t))} \\] Last time we plotted \\(p_{t + 1}\\) as a function of \\(p(t)\\) and used this to examine the dynamics starting from any initial value. We called this plot a cob-web plot. import sympy import numpy as np import matplotlib.pyplot as plt # Build cobweb plotting function def cobweb_haploid(p0, WA, Wa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow, pnext #current value of p(t) and p_(t+1) pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) #update p_(t+1) yield pnow, pnext #current value of p(t) and p_(t+1) pnow = pnext #update p(t) t += 1 #update t # Build function for generating figure def plot_haploid_selection(WA, Wa, p0=0.5, ax=None): pt = sympy.symbols('pt') #define our variable p(t) # Write out sympy equation f = (WA * pt) / (WA * pt + Wa * (1 - pt)) #the recursion equation # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation (turn it into a function) t = np.linspace(0,1,100) fy = sympy.lambdify(pt, f)(t) # Build plot if ax == None: fig, ax = plt.subplots() ax.plot(t, fy, color='black', label=f\"$W_A$ = {WA}, $W_a$ = {Wa}\") #plot p_(t+1) as function of p(t) ax.plot(t, t, color='black', linestyle='--') #draw 1:1 line for reference # Add cobweb cobweb = np.array([p for p in cobweb_haploid(p0, WA, Wa, max=100)]) ax.plot(cobweb[:,0], cobweb[:,1]) # Annotate and label plot ax.set_xlim(0,1) ax.set_ylim(0,1) ax.set_xlabel(\"allele frequency at $t$, $p(t)$\") ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\") ax.legend(frameon=False) return ax # Plot figure fig, ax = plt.subplots(1,2) fig.set_size_inches(12,4) # First cobweb with WA > Wa plot_haploid_selection(WA = 1, Wa = 0.5, ax=ax[0]) # Second cobweb with WA < Wa plot_haploid_selection(WA = 0.5, Wa = 1, ax=ax[1]) plt.show() Now let's simplify the cob-web plot and just indicate the direction (and magnitude) of change in \\(p(t)\\) with time. This is known as a phase-line diagram with a vector field (the arrows). def phase_line_haploid(p0, WA, Wa, max=np.inf): 'generator for p(t)' t, pnow, pnext = 0, p0, 0 #initial conditions while t < max: yield pnow #current value of p(t) and p_(t+1) pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) pnow = pnext #update p(t) t += 1 #update t def plot_phase_line_haploid(WA, Wa, p0, max=20, ax=None): 'plot phase line' # Set up figure if ax==None: fig, ax = plt.subplots() fig.set_size_inches(8,0.25) ax.axhline(0, color='black', linewidth=0.5) # Plot phase-line pts = [pt for pt in phase_line_haploid(p0, WA, Wa, max=max)] #pt values ax.plot( pts, np.zeros(max) #dummy y values (0 for all x values) because we want to plot a 1d line ) # Plot vector field marker = '>' if pts[2] > pts[1] else ' < ' #determine which direction to point based on first 2 time points ax.scatter( pts, np.zeros(max),#dummy y again marker=marker, s=150 ) # Remove background axes ax.set_ylabel('$p$', rotation=0) ax.set_xlabel(f\"$W_A$ = {WA}, $W_a$ = {Wa}, $p_0$ = {p0}\") ax.get_yaxis().set_ticks([]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.spines['bottom'].set_visible(False) ax.spines['left'].set_visible(False) ax.set_xlim(0,1) plt.show() plot_phase_line_haploid(WA=1, Wa=0.5, p0=0.01) plot_phase_line_haploid(WA=0.5, Wa=1, p0=0.99) As in the cob-web plots, we see the allele frequency approaches \\(p=1\\) when \\(W_A>W_a\\) and \\(p=0\\) when \\(W_a>W_A\\) . We also notice, as above, the changes are fastest (fewer, longer arrows) at intermediate frequencies. Similarly, with the more complex model of diploid selection \\[ p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t)q(t)}{W_{AA}p(t)^2 + W_{Aa}p(t)q(t) + W_{aa}q(t)^2} \\] we can draw a phase-line diagram and vector field for a set of parameter values. def phase_line_diploid(p0, WAA, WAa, Waa, max=np.inf): 'generator for p(t)' t, pnow, pnext = 0, p0, 0 #initial conditions while t < max: yield pnow #current value of p(t) and p(t+1) pnext = (WAA * pnow**2 + WAa * pnow * (1 - pnow)) / (WAA * pnow**2 + WAa * 2 * pnow * (1 - pnow) + Waa * (1 - pnow)**2) #update p(t+1) pnow = pnext #update p(t) t += 1 #update t def plot_phase_line_diploid(WAA, WAa, Waa, p0, max=20, ax=None): 'plot phase line' # set up figure if ax==None: fig, ax = plt.subplots() fig.set_size_inches(8,0.25) ax.axhline(0, color='black', linewidth=0.5) # Plot phase-line pts = [pt for pt in phase_line_diploid(p0, WAA, WAa, Waa, max=max)] ax.plot( pts, np.zeros(max), alpha=1 ) # Plot phase-line markers marker = '>' if pts[2] > pts[1] else ' < ' ax.scatter( pts, np.zeros(max), marker=marker, s=150 ) ax.set_xlabel(f\"$WAA$ = {WAA}, $WAa$ = {WAa}, $Waa$ = {Waa}, $p_0$ = {p0}\") return ax # Plot figure fig, ax = plt.subplots() fig.set_size_inches(8,0.25) plot_phase_line_diploid(WAA=1, WAa=2, Waa=1, p0=0.99, max=100, ax=ax) #higher starting allele frequency plot_phase_line_diploid(WAA=1, WAa=2, Waa=1, p0=0.01, max=100, ax=ax) #low starting allele frequency # Remove background axes ax.set_ylabel('$p$', rotation=0) ax.get_yaxis().set_ticks([]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.spines['bottom'].set_visible(False) ax.spines['left'].set_visible(False) ax.set_xlim(0,1) plt.show() Notice that this time we chose two initial frequencies for the same plot (orange vs blue), to show that under heterozygote advantage ( \\(W_{AA}<W_{Aa}>W_{aa}\\) ) the allele frequency approaches an intermediate value from either direction.","title":"2. Phase-line diagrams"},{"location":"lectures/lecture-06/#3-phase-plane-diagrams","text":"Now let\u2019s extend this technique from one to two variables.","title":"3. Phase-plane diagrams"},{"location":"lectures/lecture-06/#lotka-volterra-model","text":"We'll introduce a new model for this purpose, the Lotka-Volterra model of competition (see section 3.4.1 in the text). This is an extension of the logistic growth model (Lecture 3) to include competition between multiple species (in our case two). Let the population size of each species be \\(n_1(t)\\) and \\(n_2(t)\\) . These are our two variables. And let them have different intrinsic growth rates, \\(r_1\\) and \\(r_2\\) , and carrying capacities, \\(K_1\\) and \\(K_2\\) . To model competition, we\u2019ll assume that, for an individual of species \\(i\\) , an individual of species \\(j\\) is the competitive equivalent of \\(\\alpha_{ij}\\) individuals of species \\(i\\) . We then have \\[ n_1(t+1) = n_1(t)\\left( 1 + r_1 \\left(1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1}\\right)\\right) \\] \\[ n_2(t+1) = n_2(t) \\left(1 + r_2 \\left(1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2}\\right)\\right) \\] Often individuals of the same species will use more similar resources and therefore competition will be less severe with individuals of the other species, \\(0 < \\alpha_{ij} < 1\\) , but not always. And, in fact, we could model other types of interactions (eg, mutualism) by making some of the interactions beneficial, \\(\\alpha_{ij} < 0\\) .","title":"Lotka-Volterra model"},{"location":"lectures/lecture-06/#phase-planes-and-vector-fields","text":"So why did we introduce the Lotka-Volterra model? Well, phase-plane diagrams are plots of one variable against another ( \\(n_1\\) vs. \\(n_2\\) ), on which we can plot vector fields , vectors originating from many different starting conditions that indicate the direction and magnitude of change in the two variables. With this we can graphically investigate the dynamics of the Lotka-Volterra model by first defining the rates of change in our two variables, \\(\\Delta n_1\\) and \\(\\Delta n_2\\) and then choosing some parameter values to explore. \\[ \\Delta n_1 \\equiv n_1(t+1) - n_1(t) = n_1(t)r_1\\left(1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1}\\right) \\] \\[ \\Delta n_2 \\equiv n_2(t+1) - n_2(t) = n_2(t)r_2\\left(1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2}\\right) \\] Let's plot a phase-plane for the Lotka-Volterra with the following parameter values: \\(r_1 = 0.5, r_2 = 0.5, K_1 = 1000, K_2 = 1000, \\alpha_{12} = 0.5, \\alpha_{21} = 0.5\\) . # Define a function to plot the phase plane and vector field for n1 and n2 def plot_vector_field(dn1, dn2, xlim=(0,1200), ylim=(0,1200), n_steps=25, width=8, height=6, show=False, axes_labels=[None, None]): # Set x and y ranges xrange, yrange = np.linspace(xlim[0], xlim[1], n_steps), np.linspace(ylim[0], ylim[1], n_steps) # Initialize 2D grid with x,y values and additional grids to track derivatives X, Y = np.meshgrid(xrange, yrange) U, V = np.zeros(X.shape), np.zeros(Y.shape) # Compute the gradient at each x,y position for i in range(len(xrange)): for j in range(len(xrange)): U[i,j] = sympy.lambdify((n1, n2), dn1)(X[i,j], Y[i,j]) #change in n1 V[i,j] = sympy.lambdify((n1, n2), dn2)(X[i,j], Y[i,j]) #change in n2 # Plot figure fig, ax = plt.subplots() fig.set_size_inches(width, height) ax.set_xlabel(axes_labels[0]) ax.set_ylabel(axes_labels[1]) ax.quiver(X,Y,U,V, linewidth=1) #from point X,Y draw arrow moving U in x-axis and V in y-axis if show == True: plt.show() else: return ax # Initialize the sympy variables n1, n2 = sympy.symbols('n1, n2') # Choose the parameter values r1, r2 = 0.5, 0.5 k1, k2 = 1000, 1000 a12, a21 = 0.5, 0.5 # Specify the difference equations dn1 = r1 * n1 * (1 - (n1 + a12 * n2) / k1) dn2 = r2 * n2 * (1 - (n2 + a21 * n1) / k2) # Plot the vector field plot_vector_field(dn1, dn2, axes_labels=[\"number of species 1, $n_1$\", \"number of species 2, $n_2$\"]) plt.show() With this approach we see that the dynamics appear to be approaching a value near \\(n_1 = 700, n_2 = 700\\) from nearly any initial condition.","title":"Phase-planes and vector fields"},{"location":"lectures/lecture-06/#null-clines","text":"To better understand the dynamics, we can ask for what values of our variables ( \\(n_1, n_2\\) ) is the change in our variables zero ( \\(\\Delta n_1 = 0\\) , \\(\\Delta n_2 = 0\\) ). These values are known as null clines . Concretely, going back to our previous formula for the change in \\(n_1\\) and \\(n_2\\) in the Lotka-Volterra model \\[ \\Delta n_1 = n_1(t)r_1\\left(1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1}\\right) \\] \\[ \\Delta n_2 = n_2(t)r_2\\left(1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2}\\right) \\] We want to know when \\(\\Delta n_1\\) and \\(\\Delta n_2\\) are 0. Solving for these inequalities shows that \\[ \\Delta n_1 = 0 \\Longrightarrow n_1(t) = 0, 1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1} = 0 \\] \\[ \\Delta n_2 = 0 \\Longrightarrow n_2(t) = 0, 1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2} = 0 \\] Plotting these null clines on the phase-plane diagram, we get # Initialize plot and ranges ax = plot_vector_field(dn1, dn2, axes_labels=[\"number of species 1, $n_1$\", \"number of species 2, $n_2$\"]) xrange, yrange = np.linspace(0, 1200, 100), np.linspace(0, 1200, 100) def plot_nullclines(ax): #plot the null clines for species 1 (blue) nullcline_1 = [list(i.values())[0] for i in sympy.solve(sympy.Eq(dn1, 0))] ax.plot(xrange, sympy.lambdify(n1, nullcline_1[1])(xrange), color=plt.cm.tab10(0)) # this null cline is a function of n1 (i.e. x) ax.plot([nullcline_1[0] for _ in xrange], yrange, color=plt.cm.tab10(0)) # #plot the null clines for species 2 (red) nullcline_2 = [list(i.values())[0] for i in sympy.solve(sympy.Eq(dn2, 0))] ax.plot(sympy.lambdify(n2, nullcline_2[0])(yrange), yrange, color=plt.cm.tab10(1)) # this null cline is a function of n2 (i.e. y) ax.plot(xrange, [nullcline_2[1] for _ in yrange], color=plt.cm.tab10(1)) ax.set_ylim(-10, 1210) ax.set_xlim(-10, 1210) return ax plot_nullclines(ax) plt.show() The null clines (blue for \\(n_1\\) and orange for \\(n_2\\) ) help us understand the dynamics. In each area bounded by null clines the vectors point in the same general direction (eg, in the top right area they point down and to the left). This helps us see where the dynamics are heading -- in this case most initial conditions head to the intersection of the non-zero null clines for \\(n_1\\) and \\(n_2\\) , near \\(n_1=700\\) and \\(n_2=700\\) . Note that where the null cline of one variable intersects a null cline of the other variable neither variable is changing, indicating equilibria . We can also make phase diagrams for continuous-time models, just using differential equations in place of difference equations. We\u2019ll see an example of that for another model, of predator and prey, in Lab 3.","title":"Null clines"},{"location":"lectures/lecture-07/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 7: Equilibria Run notes interactively? Lecture overview Equilibria Exponential growth Logistic growth Haploid selection Diploid selection Summary 1. Equilibria An equilibrium is any state of a system which tends to persist unchanged over time. For discrete-time models, the equilibria are defined as those values of the variables where no changes occur from one time step to the next. For example, those values of allele frequency \\(p(t)\\) where \\[ \\begin{aligned} \\Delta p &= 0\\\\ p(t+1) - p(t) &= 0\\\\ p(t+1) &= p(t) \\end{aligned} \\] Similarly, for continuous-time models, the equilibria are defined as those values of the variables for which the rate of change in the variables equals zero. For example, those values of allele frequency \\(p(t)\\) where \\[ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = 0 \\] What are the equilibria for the following models? Model Discrete time Continous time Exponential growth \\(n(t+1) = R n(t)\\) \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t} = r n(t)\\) Logistic growth \\(n(t+1) = (1 + r(1 - \\frac{n(t)}{K}))n(t)\\) \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t} = r(1 - \\frac{n(t)}{K})n(t)\\) Haploid selection \\(p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a q(t)}\\) \\(\\frac{\\mathrm{d}p}{\\mathrm{d}t} = s p(t)(1-p(t))\\) Diploid selection \\(p(t+1) = \\frac{p(t)^2W_{AA} + p(t) q(t)W_{Aa}}{p(t)^2W_{AA} + 2 p(t) q(t)W_{Aa} + q(t)^2W_{aa}}\\) Not derived 2. Exponential growth Here, we will solve for the equilibria in both the discrete- and continuous-time exponential-growth models. Discrete time \\[ n(t+1) = Rn(t) \\] Set \\(n(t+1) = n(t) = \\hat n\\) and solve for \\(\\hat{n}\\) \\[ \\begin{aligned} \\hat n &= R\\hat n\\\\ \\hat n &= 0 \\end{aligned} \\] Continuous time \\[ \\frac{\\mathrm{d}n}{\\mathrm{d}t} = r n(t) \\] Set \\(\\mathrm{d}n/\\mathrm{d}t = 0\\) and \\(n(t) = \\hat n\\) and solve for \\(\\hat n\\) \\[ \\begin{aligned} 0 &= r \\hat{n}\\\\ \\hat n &= 0 \\end{aligned} \\] So the only equilibrium in both discrete- and continuous-time exponential growth is extinction, \\(\\hat{n}=0\\) . Special case of parameters Notice above that \\(R=1\\) and \\(r=0\\) also satisfy the conditions for an equilibrium. These are called special cases of parameters . Here this refers to the case where individuals perfectly replace themselves so that the population remains constant from any starting value of \\(n\\) . 3. Logistic growth Here, we will solve for the equilibria in both the discrete- and continuous-time logistic-growth models. Discrete time \\[ n(t+1) = \\left(1 + r\\left(1 - \\frac{n(t)}{K}\\right)\\right)n(t) \\] As above, we substitute \\(n(t+1) = n(t) = \\hat n\\) and want to solve for \\(\\hat{n}\\) . \\[ \\hat n = \\left(1 + r\\left(1 - \\frac{\\hat n}{K}\\right)\\right)\\hat{n} \\] Notice that one equilibrium is \\(\\hat n = 0\\) . However, this isn't the only equilibrium because dividing both sides by \\(\\hat n\\) results in \\[ \\begin{aligned} 1 &= 1 + r\\left(1 - \\frac{\\hat n}{K}\\right)\\\\ 0 &= r\\left(1 - \\frac{\\hat n}{K}\\right) \\end{aligned} \\] Here we have a special case of parameters, \\(r=0\\) , or \\[ \\begin{aligned} 0 &= 1 - \\frac{\\hat n}{K}\\\\ \\hat n &= K \\end{aligned} \\] There are therefore two equilibria: extinction, \\(\\hat{n}=0\\) , or carrying capacity, \\(\\hat{n}=K\\) . Continuous time \\[ \\frac{\\mathrm{d}n}{\\mathrm{d}t} = r \\left(1 - \\frac{n}{K}\\right)n \\] We set \\(\\mathrm{d}n/\\mathrm{d}t=0\\) and \\(n=\\hat{n}\\) \\[ 0 = r \\left(1 - \\frac{\\hat n}{K}\\right)\\hat{n} \\] which is the same equation we had above in discrete-time, so the equilibria ( \\(\\hat n = 0,K\\) ) and the special case of parameters ( \\(r = 0\\) ) are also the same. 4. Haploid selection Here, we will solve for the equilibria in both the discrete- and continuous-time haploid-selection models. Discrete time \\[ p(t+1) = \\frac{p(t)W_A}{p(t)W_A + q(t)W_a} \\] Replace \\(p(t+1)\\) and \\(p(t)\\) with \\(\\hat p\\) and replace \\(q(t)\\) with \\(\\hat q\\) and solve for \\(\\hat p\\) and \\(\\hat q\\) \\[ \\begin{aligned} \\hat{p} &= \\frac{\\hat p W_A}{\\hat p W_A + \\hat q W_a}\\\\ \\end{aligned} \\] We first see that \\(\\hat{p}=0\\) is an equilibrium. But there is more, since dividing by \\(\\hat p\\) gives \\[ \\begin{aligned} 1 &= \\frac{W_A}{\\hat p W_A + \\hat q W_a}\\\\ \\hat p W_A + \\hat q W_a &= W_A\\\\ \\hat q W_a &= (1-\\hat p) W_A \\end{aligned} \\] At this point we use \\(q=1-p\\) to write this in terms of \\(p\\) only \\[ (1-\\hat p) W_a = (1-\\hat p) W_A \\] So \\(\\hat p =1\\) is another equilibrium. And finally, dividing by \\((1-\\hat p)\\) gives a special case of parameters, \\(W_A=W_a\\) . To summarize, the allele frequency will not change from one generation to the next in our discrete-time haploid-selection model when \\(\\hat p = 0 \\Longrightarrow\\) the population is \"fixed\" for the \\(a\\) allele \\(\\hat p = 1 \\Longrightarrow\\) the population is fixed for the \\(A\\) allele \\(W_A = W_a \\Longrightarrow\\) the two alleles have equal fitness (\"neutrality\") Continuous time \\[ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = sp(t)(1-p(t)) \\] In the continuous-time model, we set the derivative equal to zero and \\(p(t)=\\hat{p}\\) \\[ \\begin{aligned} 0 &= s\\hat p(1 -\\hat p) \\end{aligned} \\] And we again find the same equilibria ( \\(\\hat p=0,1\\) ) and special case of parameters ( \\(s=0\\) , i.e., neutrality). 5. Diploid selection Discrete time Here, we will solve for the equilibria in the discrete-time diploid-selection model \\[ p(t+1) = \\frac{p(t)^2 W_{AA} + p(t) q(t) W_{Aa}}{p(t)^2 W_{AA} + 2 p(t) q(t) W_{Aa} + q(t)^2 W_{aa}} \\] We replace \\(p(t+1)\\) and \\(p(t)\\) with \\(\\hat p\\) and \\(q(t)\\) with \\(\\hat{q}\\) and solve for \\(\\hat p\\) and \\(\\hat q\\) \\[ \\begin{aligned} \\hat{p} &= \\frac{\\hat{p}^2 W_{AA} + \\hat{p} \\hat{q} W_{Aa}}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}}\\\\ \\hat{p} &= \\frac{\\hat{p}(\\hat p W_{AA} + \\hat{q} W_{Aa})}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}} \\end{aligned} \\] We see that \\(\\hat{p}=0\\) is one equilibrium. Moving on, dividing by \\(\\hat p\\) gives \\[ \\begin{aligned} 1 &= \\frac{\\hat p W_{AA} + \\hat{q} W_{Aa}}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}}\\\\ \\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa} &= \\hat p W_{AA} + \\hat{q} W_{Aa}\\\\ 0 &= (\\hat{p} - \\hat{p}^2) W_{AA} + (\\hat{q} - 2 \\hat{p} \\hat{q}) W_{Aa} - \\hat{q}^2 W_{aa}\\\\ 0 &= \\hat{p}(1 - \\hat{p}) W_{AA} + \\hat{q}(1 - 2 \\hat{p}) W_{Aa} - \\hat{q}^2 W_{aa}\\\\ 0 &= \\hat{q}(\\hat{p} W_{AA} + (1 - 2 \\hat{p}) W_{Aa} - \\hat{q} W_{aa}) \\end{aligned} \\] And so \\(\\hat{q}=0\\implies\\hat{p}=1\\) is another equilibrium. Dividing by \\(\\hat{q}\\) and putting everything in terms of \\(p\\) we have \\[ \\begin{aligned} 0 &= \\hat{p} W_{AA} + (1 - 2 \\hat{p}) W_{Aa} - \\hat{q} W_{aa}\\\\ 0 &= \\hat{p} W_{AA} + (1 - 2 \\hat{p}) W_{Aa} - (1 - \\hat{p}) W_{aa}\\\\ 0 &= \\hat{p}(W_{AA} -2W_{Aa} + W_{aa}) + W_{Aa} - W_{aa}\\\\ W_{aa} - W_{Aa} &= \\hat p(W_{AA} -2W_{Aa} + W_{aa})\\\\ \\frac{W_{Aa} - W_{aa}}{2W_{Aa} - W_{AA} - W_{aa}} &= \\hat p\\\\ \\end{aligned} \\] We therefore have three equilibria under diploid selection: \\(\\hat{p}=0,\\frac{W_{Aa} - W_{aa}}{2W_{Aa} - W_{AA} - W_{aa}},1\\) . Since a frequency is bounded between 0 and 1, we must have \\(0 \\leq p \\leq 1\\) . We therefore call \\(\\hat{p}=0\\) and \\(\\hat{p}=1\\) boundary equilibria . These bounds also imply the third equilibrium is only biologically valid when \\[ 0 \\leq \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} \\leq 1 \\] When \\(W_{Aa} = W_{aa}\\) this equilibrium reduces to \\(\\hat{p}=0\\) and when \\(W_{Aa} = W_{AA}\\) this reduces to \\(\\hat{p}=1\\) (check this for yourself). The third equilibrium will be an internal equilibrium , representing a population with both \\(A\\) and \\(a\\) alleles, when \\[ 0 < \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} < 1 \\] The equilibrium is positive when the numerator and denominator have the same sign (i.e., are both positive or both negative). Let's split this into two \"cases\". Case A will have a positive numerator, \\(W_{Aa} > W_{aa}\\) , and Case B will have a negative numerator, \\(W_{Aa} < W_{aa}\\) . So, in Case A, the equilibrium is positive when the denominator is positive, \\(2 W_{Aa} - W_{AA} - W_{aa} > 0\\) . While in case B the equilibrium is positive when the denominator is negative, \\(2 W_{Aa} - W_{AA} - W_{aa} < 0\\) . Now we can rearrange the equilibrium to show that it is less than 1 when \\[ \\begin{aligned} \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} &< 1\\\\ \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} - 1 &< 0\\\\ \\frac{W_{Aa} - W_{aa} - (2 W_{Aa} -W_{AA} - W_{aa})}{2 W_{Aa} -W_{AA} - W_{aa}} &< 0\\\\ \\frac{W_{AA} - W_{Aa}}{2 W_{Aa} -W_{AA} - W_{aa}} &< 0\\\\ \\frac{W_{Aa} - W_{AA}}{2 W_{Aa} -W_{AA} - W_{aa}} &> 0\\\\ \\end{aligned} \\] Again we need the numerator and denominator to have the same sign for this inequality to hold. In case A, where we've said that denominator is positive, this means we also need the numerator to be positive, \\(W_{Aa} > W_{AA}\\) . While in case B we said that the denominator is negative, so we also need the numerator to be negative, \\(W_{Aa} < W_{AA}\\) . Putting this all together, there is a biologically-relevant internal equilibrium when either Case A: \\(W_{Aa} > W_{aa}\\) and \\(W_{Aa} > W_{AA}\\) (which ensures \\(2 W_{Aa} - W_{AA} - W_{aa} > 0\\) ; go ahead and check!) Case B: \\(W_{Aa} < W_{aa}\\) and \\(W_{Aa} < W_{AA}\\) (which ensures \\(2 W_{Aa} - W_{AA} - W_{aa} < 0\\) ) Case A therefore represents \"heterozygote advantage\", \\(W_{AA} < W_{Aa} > W_{aa}\\) , while Case B represents \"heterozygote disadvantage\", \\(W_{AA} > W_{Aa} < W_{aa}\\) . 6. Summary In summary, the equilibria for the models we have looked at are: Model Discrete-time equilibria Continuous-time equilibria Exponential growth \\(\\hat n = 0\\) \\(\\hat n = 0\\) Logistic growth \\(\\hat n = 0, \\hat n = K\\) \\(\\hat n = 0, \\hat n = K\\) Haploid selection \\(\\hat p = 0, \\hat p = 1\\) \\(\\hat p = 0, \\hat p = 1\\) Diploid selection \\(\\hat p = 0, \\hat p = 1, \\hat p = \\frac{W_{Aa} - W_{aa}}{2W_{Aa} - W_{AA} - W_{aa}}\\) Not derived Make sure that you understand how to determine equilibria in discrete- and continuous-time and can derive the equilibria of the models above on your own.","title":"Lecture 7"},{"location":"lectures/lecture-07/#lecture-7-equilibria","text":"Run notes interactively?","title":"Lecture 7: Equilibria"},{"location":"lectures/lecture-07/#lecture-overview","text":"Equilibria Exponential growth Logistic growth Haploid selection Diploid selection Summary","title":"Lecture overview"},{"location":"lectures/lecture-07/#1-equilibria","text":"An equilibrium is any state of a system which tends to persist unchanged over time. For discrete-time models, the equilibria are defined as those values of the variables where no changes occur from one time step to the next. For example, those values of allele frequency \\(p(t)\\) where \\[ \\begin{aligned} \\Delta p &= 0\\\\ p(t+1) - p(t) &= 0\\\\ p(t+1) &= p(t) \\end{aligned} \\] Similarly, for continuous-time models, the equilibria are defined as those values of the variables for which the rate of change in the variables equals zero. For example, those values of allele frequency \\(p(t)\\) where \\[ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = 0 \\] What are the equilibria for the following models? Model Discrete time Continous time Exponential growth \\(n(t+1) = R n(t)\\) \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t} = r n(t)\\) Logistic growth \\(n(t+1) = (1 + r(1 - \\frac{n(t)}{K}))n(t)\\) \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t} = r(1 - \\frac{n(t)}{K})n(t)\\) Haploid selection \\(p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a q(t)}\\) \\(\\frac{\\mathrm{d}p}{\\mathrm{d}t} = s p(t)(1-p(t))\\) Diploid selection \\(p(t+1) = \\frac{p(t)^2W_{AA} + p(t) q(t)W_{Aa}}{p(t)^2W_{AA} + 2 p(t) q(t)W_{Aa} + q(t)^2W_{aa}}\\) Not derived","title":"1. Equilibria"},{"location":"lectures/lecture-07/#2-exponential-growth","text":"Here, we will solve for the equilibria in both the discrete- and continuous-time exponential-growth models.","title":"2. Exponential growth"},{"location":"lectures/lecture-07/#discrete-time","text":"\\[ n(t+1) = Rn(t) \\] Set \\(n(t+1) = n(t) = \\hat n\\) and solve for \\(\\hat{n}\\) \\[ \\begin{aligned} \\hat n &= R\\hat n\\\\ \\hat n &= 0 \\end{aligned} \\]","title":"Discrete time"},{"location":"lectures/lecture-07/#continuous-time","text":"\\[ \\frac{\\mathrm{d}n}{\\mathrm{d}t} = r n(t) \\] Set \\(\\mathrm{d}n/\\mathrm{d}t = 0\\) and \\(n(t) = \\hat n\\) and solve for \\(\\hat n\\) \\[ \\begin{aligned} 0 &= r \\hat{n}\\\\ \\hat n &= 0 \\end{aligned} \\] So the only equilibrium in both discrete- and continuous-time exponential growth is extinction, \\(\\hat{n}=0\\) . Special case of parameters Notice above that \\(R=1\\) and \\(r=0\\) also satisfy the conditions for an equilibrium. These are called special cases of parameters . Here this refers to the case where individuals perfectly replace themselves so that the population remains constant from any starting value of \\(n\\) .","title":"Continuous time"},{"location":"lectures/lecture-07/#3-logistic-growth","text":"Here, we will solve for the equilibria in both the discrete- and continuous-time logistic-growth models.","title":"3. Logistic growth"},{"location":"lectures/lecture-07/#discrete-time_1","text":"\\[ n(t+1) = \\left(1 + r\\left(1 - \\frac{n(t)}{K}\\right)\\right)n(t) \\] As above, we substitute \\(n(t+1) = n(t) = \\hat n\\) and want to solve for \\(\\hat{n}\\) . \\[ \\hat n = \\left(1 + r\\left(1 - \\frac{\\hat n}{K}\\right)\\right)\\hat{n} \\] Notice that one equilibrium is \\(\\hat n = 0\\) . However, this isn't the only equilibrium because dividing both sides by \\(\\hat n\\) results in \\[ \\begin{aligned} 1 &= 1 + r\\left(1 - \\frac{\\hat n}{K}\\right)\\\\ 0 &= r\\left(1 - \\frac{\\hat n}{K}\\right) \\end{aligned} \\] Here we have a special case of parameters, \\(r=0\\) , or \\[ \\begin{aligned} 0 &= 1 - \\frac{\\hat n}{K}\\\\ \\hat n &= K \\end{aligned} \\] There are therefore two equilibria: extinction, \\(\\hat{n}=0\\) , or carrying capacity, \\(\\hat{n}=K\\) .","title":"Discrete time"},{"location":"lectures/lecture-07/#continuous-time_1","text":"\\[ \\frac{\\mathrm{d}n}{\\mathrm{d}t} = r \\left(1 - \\frac{n}{K}\\right)n \\] We set \\(\\mathrm{d}n/\\mathrm{d}t=0\\) and \\(n=\\hat{n}\\) \\[ 0 = r \\left(1 - \\frac{\\hat n}{K}\\right)\\hat{n} \\] which is the same equation we had above in discrete-time, so the equilibria ( \\(\\hat n = 0,K\\) ) and the special case of parameters ( \\(r = 0\\) ) are also the same.","title":"Continuous time"},{"location":"lectures/lecture-07/#4-haploid-selection","text":"Here, we will solve for the equilibria in both the discrete- and continuous-time haploid-selection models.","title":"4. Haploid selection"},{"location":"lectures/lecture-07/#discrete-time_2","text":"\\[ p(t+1) = \\frac{p(t)W_A}{p(t)W_A + q(t)W_a} \\] Replace \\(p(t+1)\\) and \\(p(t)\\) with \\(\\hat p\\) and replace \\(q(t)\\) with \\(\\hat q\\) and solve for \\(\\hat p\\) and \\(\\hat q\\) \\[ \\begin{aligned} \\hat{p} &= \\frac{\\hat p W_A}{\\hat p W_A + \\hat q W_a}\\\\ \\end{aligned} \\] We first see that \\(\\hat{p}=0\\) is an equilibrium. But there is more, since dividing by \\(\\hat p\\) gives \\[ \\begin{aligned} 1 &= \\frac{W_A}{\\hat p W_A + \\hat q W_a}\\\\ \\hat p W_A + \\hat q W_a &= W_A\\\\ \\hat q W_a &= (1-\\hat p) W_A \\end{aligned} \\] At this point we use \\(q=1-p\\) to write this in terms of \\(p\\) only \\[ (1-\\hat p) W_a = (1-\\hat p) W_A \\] So \\(\\hat p =1\\) is another equilibrium. And finally, dividing by \\((1-\\hat p)\\) gives a special case of parameters, \\(W_A=W_a\\) . To summarize, the allele frequency will not change from one generation to the next in our discrete-time haploid-selection model when \\(\\hat p = 0 \\Longrightarrow\\) the population is \"fixed\" for the \\(a\\) allele \\(\\hat p = 1 \\Longrightarrow\\) the population is fixed for the \\(A\\) allele \\(W_A = W_a \\Longrightarrow\\) the two alleles have equal fitness (\"neutrality\")","title":"Discrete time"},{"location":"lectures/lecture-07/#continuous-time_2","text":"\\[ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = sp(t)(1-p(t)) \\] In the continuous-time model, we set the derivative equal to zero and \\(p(t)=\\hat{p}\\) \\[ \\begin{aligned} 0 &= s\\hat p(1 -\\hat p) \\end{aligned} \\] And we again find the same equilibria ( \\(\\hat p=0,1\\) ) and special case of parameters ( \\(s=0\\) , i.e., neutrality).","title":"Continuous time"},{"location":"lectures/lecture-07/#5-diploid-selection","text":"","title":"5. Diploid selection"},{"location":"lectures/lecture-07/#discrete-time_3","text":"Here, we will solve for the equilibria in the discrete-time diploid-selection model \\[ p(t+1) = \\frac{p(t)^2 W_{AA} + p(t) q(t) W_{Aa}}{p(t)^2 W_{AA} + 2 p(t) q(t) W_{Aa} + q(t)^2 W_{aa}} \\] We replace \\(p(t+1)\\) and \\(p(t)\\) with \\(\\hat p\\) and \\(q(t)\\) with \\(\\hat{q}\\) and solve for \\(\\hat p\\) and \\(\\hat q\\) \\[ \\begin{aligned} \\hat{p} &= \\frac{\\hat{p}^2 W_{AA} + \\hat{p} \\hat{q} W_{Aa}}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}}\\\\ \\hat{p} &= \\frac{\\hat{p}(\\hat p W_{AA} + \\hat{q} W_{Aa})}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}} \\end{aligned} \\] We see that \\(\\hat{p}=0\\) is one equilibrium. Moving on, dividing by \\(\\hat p\\) gives \\[ \\begin{aligned} 1 &= \\frac{\\hat p W_{AA} + \\hat{q} W_{Aa}}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}}\\\\ \\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa} &= \\hat p W_{AA} + \\hat{q} W_{Aa}\\\\ 0 &= (\\hat{p} - \\hat{p}^2) W_{AA} + (\\hat{q} - 2 \\hat{p} \\hat{q}) W_{Aa} - \\hat{q}^2 W_{aa}\\\\ 0 &= \\hat{p}(1 - \\hat{p}) W_{AA} + \\hat{q}(1 - 2 \\hat{p}) W_{Aa} - \\hat{q}^2 W_{aa}\\\\ 0 &= \\hat{q}(\\hat{p} W_{AA} + (1 - 2 \\hat{p}) W_{Aa} - \\hat{q} W_{aa}) \\end{aligned} \\] And so \\(\\hat{q}=0\\implies\\hat{p}=1\\) is another equilibrium. Dividing by \\(\\hat{q}\\) and putting everything in terms of \\(p\\) we have \\[ \\begin{aligned} 0 &= \\hat{p} W_{AA} + (1 - 2 \\hat{p}) W_{Aa} - \\hat{q} W_{aa}\\\\ 0 &= \\hat{p} W_{AA} + (1 - 2 \\hat{p}) W_{Aa} - (1 - \\hat{p}) W_{aa}\\\\ 0 &= \\hat{p}(W_{AA} -2W_{Aa} + W_{aa}) + W_{Aa} - W_{aa}\\\\ W_{aa} - W_{Aa} &= \\hat p(W_{AA} -2W_{Aa} + W_{aa})\\\\ \\frac{W_{Aa} - W_{aa}}{2W_{Aa} - W_{AA} - W_{aa}} &= \\hat p\\\\ \\end{aligned} \\] We therefore have three equilibria under diploid selection: \\(\\hat{p}=0,\\frac{W_{Aa} - W_{aa}}{2W_{Aa} - W_{AA} - W_{aa}},1\\) . Since a frequency is bounded between 0 and 1, we must have \\(0 \\leq p \\leq 1\\) . We therefore call \\(\\hat{p}=0\\) and \\(\\hat{p}=1\\) boundary equilibria . These bounds also imply the third equilibrium is only biologically valid when \\[ 0 \\leq \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} \\leq 1 \\] When \\(W_{Aa} = W_{aa}\\) this equilibrium reduces to \\(\\hat{p}=0\\) and when \\(W_{Aa} = W_{AA}\\) this reduces to \\(\\hat{p}=1\\) (check this for yourself). The third equilibrium will be an internal equilibrium , representing a population with both \\(A\\) and \\(a\\) alleles, when \\[ 0 < \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} < 1 \\] The equilibrium is positive when the numerator and denominator have the same sign (i.e., are both positive or both negative). Let's split this into two \"cases\". Case A will have a positive numerator, \\(W_{Aa} > W_{aa}\\) , and Case B will have a negative numerator, \\(W_{Aa} < W_{aa}\\) . So, in Case A, the equilibrium is positive when the denominator is positive, \\(2 W_{Aa} - W_{AA} - W_{aa} > 0\\) . While in case B the equilibrium is positive when the denominator is negative, \\(2 W_{Aa} - W_{AA} - W_{aa} < 0\\) . Now we can rearrange the equilibrium to show that it is less than 1 when \\[ \\begin{aligned} \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} &< 1\\\\ \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} - 1 &< 0\\\\ \\frac{W_{Aa} - W_{aa} - (2 W_{Aa} -W_{AA} - W_{aa})}{2 W_{Aa} -W_{AA} - W_{aa}} &< 0\\\\ \\frac{W_{AA} - W_{Aa}}{2 W_{Aa} -W_{AA} - W_{aa}} &< 0\\\\ \\frac{W_{Aa} - W_{AA}}{2 W_{Aa} -W_{AA} - W_{aa}} &> 0\\\\ \\end{aligned} \\] Again we need the numerator and denominator to have the same sign for this inequality to hold. In case A, where we've said that denominator is positive, this means we also need the numerator to be positive, \\(W_{Aa} > W_{AA}\\) . While in case B we said that the denominator is negative, so we also need the numerator to be negative, \\(W_{Aa} < W_{AA}\\) . Putting this all together, there is a biologically-relevant internal equilibrium when either Case A: \\(W_{Aa} > W_{aa}\\) and \\(W_{Aa} > W_{AA}\\) (which ensures \\(2 W_{Aa} - W_{AA} - W_{aa} > 0\\) ; go ahead and check!) Case B: \\(W_{Aa} < W_{aa}\\) and \\(W_{Aa} < W_{AA}\\) (which ensures \\(2 W_{Aa} - W_{AA} - W_{aa} < 0\\) ) Case A therefore represents \"heterozygote advantage\", \\(W_{AA} < W_{Aa} > W_{aa}\\) , while Case B represents \"heterozygote disadvantage\", \\(W_{AA} > W_{Aa} < W_{aa}\\) .","title":"Discrete time"},{"location":"lectures/lecture-07/#6-summary","text":"In summary, the equilibria for the models we have looked at are: Model Discrete-time equilibria Continuous-time equilibria Exponential growth \\(\\hat n = 0\\) \\(\\hat n = 0\\) Logistic growth \\(\\hat n = 0, \\hat n = K\\) \\(\\hat n = 0, \\hat n = K\\) Haploid selection \\(\\hat p = 0, \\hat p = 1\\) \\(\\hat p = 0, \\hat p = 1\\) Diploid selection \\(\\hat p = 0, \\hat p = 1, \\hat p = \\frac{W_{Aa} - W_{aa}}{2W_{Aa} - W_{AA} - W_{aa}}\\) Not derived Make sure that you understand how to determine equilibria in discrete- and continuous-time and can derive the equilibria of the models above on your own.","title":"6. Summary"},{"location":"lectures/lecture-08/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 8: Local stability (univariate) Run notes interactively? Lecture overview Stability Local stability analysis in continuous-time one-variable models Local stability analysis in discrete-time one-variable models Summary 1. Stability When a variable is exactly at an equilibrium its value will never change. But what happens when we are not exactly at, but just near an equilibrium? Starting near an equilibrium, if the system moves towards the equilibrium over time the equilibrium is said to be locally stable . In contrast, if the system moves away from the equilibrium over time the equilibrium is said to be unstable . An equilibrium point is said to be globally stable if any starting point leads to it. We've seen these possibilities already graphically, eg., in phase-line plots for discrete-time diploid selection: when the heterozygote has the lowest fitness the fixation equilibria are locally (but not globally) stable and the polymorphic equilibrium is unstable. import numpy as np import matplotlib.pyplot as plt def phase_line_diploid(p0, WAA, WAa, Waa, max=np.inf): 'generator for p_t' t, pnow, pnext = 0, p0, 0 #initial conditions while t < max: yield pnow #current value of p(t) and p(t+1) pnext = (WAA * pnow**2 + WAa * pnow * (1 - pnow)) / (WAA * pnow**2 + WAa * 2 * pnow * (1 - pnow) + Waa * (1 - pnow)**2) #update p(t+1) pnow = pnext #update p(t) t += 1 #update t def plot_phase_line_diploid(WAA, WAa, Waa, p0, max=20, ax=None): 'plot phase line' # set up figure if ax==None: fig, ax = plt.subplots() fig.set_size_inches(8,0.25) # Plot phase-line ax.axhline(0, color='black', linewidth=0.5) # Plot vector field pts = [pt for pt in phase_line_diploid(p0, WAA, WAa, Waa, max=max)] marker = '>' if pts[2] > pts[1] else ' < ' ax.scatter( pts, np.zeros(max), marker=marker, s=50, c='black' ) ax.set_xlabel(f\"$WAA$ = {WAA}, $WAa$ = {WAa}, $Waa$ = {Waa}\") return ax # Plot figure fig, ax = plt.subplots() fig.set_size_inches(8,0.25) # phase line and vector field plot_phase_line_diploid(WAA=2, WAa=1, Waa=2, p0=0.45, max=12, ax=ax) #higher starting allele frequency plot_phase_line_diploid(WAA=2, WAa=1, Waa=2, p0=0.55, max=12, ax=ax) #low starting allele frequency # equilibria plt.scatter([0,1],[0,0],s=200,c='black') plt.scatter([0.5],[0],s=200,c='white',edgecolors='black') # Remove background axes ax.set_ylabel('$p$', rotation=0) ax.get_yaxis().set_ticks([]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.spines['bottom'].set_visible(False) ax.spines['left'].set_visible(False) ax.set_xlim(-0.05,1.05) plt.show() Our goal in this lecture is to mathematically determine whether a small perturbation away from an equilibrium point will grow or shrink in magnitude over time \\(\\Longrightarrow\\) local stability analysis . Motivating example Consider logistic growth in continuous time, with \\(K = 1000\\) and \\(r = 0.5\\) . For populations started near carrying capacity, a plot of \\(\\mathrm{d}n/\\mathrm{d}t\\) vs. \\(n\\) shows that they move closer to the carrying capacity over time, since the rate of change in \\(n\\) is positive when \\(n<K\\) and negative when \\(K<n\\) . r,k = 0.5,1000 xs = np.linspace(0,1.5 k,100) ys = [r n*(1-n/k) for n in xs] plt.plot(xs,ys) plt.plot(xs,[0 for _ in xs], color='black') plt.xlabel('population size, \\(n\\) ') plt.ylabel('rate of change in population size, \\(dn/dt\\) ') plt.show() If we drew the same plot for \\(r=-0.5\\) we see that the population size then moves away from \\(n=K\\) , as the rate of change in \\(n\\) is negative when \\(n<K\\) and positive when \\(K<n\\) . r,k=-0.5,1000 xs = np.linspace(0,1.5 k,100) ys = [r n*(1-n/k) for n in xs] plt.plot(xs,ys) plt.plot(xs,[0 for _ in xs], color='black') plt.xlabel('population size, \\(n\\) ') plt.ylabel('rate of change in population size, \\(dn/dt\\) ') plt.show() The key difference between these plots near the equilibrium of interest, \\(\\hat n=K\\) , is that 1. in the first case ($r=0.5$), where the equilibrium is stable, $\\mathrm{d}n/\\mathrm{d}t$ goes from positive to negative, meaning its slope is negative 2. in the second case ($r = -0.5$), where the equilibrium is unstable, $\\mathrm{d}n/\\mathrm{d}t$, goes from negative to positive, meaning its slope is positive This suggests that we can determine the local stability of an equilibrium by looking at the slope of the differential equation at that equilibrium. 2. Local stability analysis in continuous-time one-variable models To determine local stability mathematically, we focus on a small perturbation ( \\(\\epsilon\\) ) away from an equilibrium ( \\(\\hat{x}\\) ) and determine whether this perturbation will grow or shrink. If, at time \\(t\\) , the population is a small distance from equilibrium, \\(x = \\hat{x} + \\epsilon\\) , the rate of change in \\(x\\) will be \\(\\mathrm{d}x/\\mathrm{d}t = \\mathrm{d}(\\hat{x} + \\epsilon)/\\mathrm{d}t\\) . Let us call this derivative \\(f(x)=\\mathrm{d}x/\\mathrm{d}t\\) , which we can write as \\(f(\\hat{x} + \\epsilon) = \\mathrm{d}(\\hat{x} + \\epsilon)/\\mathrm{d}t\\) . To work with this arbitrary function, \\(f\\) , let's introduce a pretty remarkable mathematical fact, the Taylor Series. Taylor Series Any function \\(f(x)\\) can be written as an infinite series of derivatives evaluated at \\(x=a\\) \\[ f(x) = \\sum_{k = 0}^{\\infty}\\frac{f^{(k)}(a)}{k!}(x-a)^k \\] where \\(f^{(k)}(a)\\) is the \\(k^{\\mathrm{th}}\\) derivative of the function with respect to \\(x\\) , evaluated at point \\(a\\) . (See section P1.3 in the text for more information). In this case we want to write \\(f(\\hat{x} + \\epsilon)\\) as a Taylor Series evaluated at the equilibrium, \\(\\hat{x} + \\epsilon = \\hat{x}\\) \\[ \\begin{aligned} \\frac{\\mathrm{d}(\\hat{x} + \\epsilon)}{\\mathrm{d}t} &= f(\\hat{x} + \\epsilon)\\\\ &= f(\\hat{x}) + f^{(1)}(\\hat{x})(\\hat{x} + \\epsilon - \\hat{x}) + \\frac{f^{(2)}(\\hat{x})}{2}(\\hat{x} + \\epsilon - \\hat{x})^2 + \\cdots\\\\ &= f(\\hat{x}) + f^{(1)}(\\hat{x})\\epsilon + \\frac{f^{(2)}(\\hat{x})}{2}\\epsilon^2 + \\cdots\\\\ \\end{aligned} \\] Now, to work with this infinite series we will make an assumption, that we are very the equilibrium, meaning the deviation is small, \\(\\epsilon<<1\\) . This means that \\(\\epsilon^2\\) is even smaller, and \\(\\epsilon^3\\) even smaller than that, and so on. By considering small \\(\\epsilon\\) we can therefore cut-off our infinite series by ignoring any term with \\(\\epsilon\\) to a power greater than 1. This is called a \"first order\" Taylor series approximation of \\(f\\) around \\(\\epsilon=0\\) . This assumption is what limits us to determining only local stability. Global stability would require us to consider large deviations from the equilibrium as well, which is not possible for even mildly complicated functions, \\(f\\) . OK, so making this assumption of small \\(\\epsilon\\) we have \\[ \\begin{aligned} \\frac{\\mathrm{d}(\\hat{x} + \\epsilon)}{\\mathrm{d}t} &= f(\\hat{x}) + f^{(1)}(\\hat{x})\\epsilon\\\\ \\end{aligned} \\] Since \\(\\hat{x}\\) is an equilibrium, \\(f(\\hat{x})\\) equals zero, leaving just \\(f^{(1)}(\\hat{x})\\epsilon\\) on the right-hand side. Furthermore, the left-hand side can be expanded and simplified ( \\(\\hat{x}\\) is a constant that does not change in time) \\[ \\begin{aligned} \\frac{\\mathrm{d}(\\hat{x}+\\epsilon)}{\\mathrm{d}t} &= \\frac{\\mathrm{d}\\hat{x}}{\\mathrm{d}t} + \\frac{\\mathrm{d}\\epsilon}{\\mathrm{d}t}\\\\ &= \\frac{\\mathrm{d}\\epsilon} {\\mathrm{d}t} \\end{aligned} \\] Combining the above, the deviation from the equilibrium will change over time at a rate \\[ \\frac{\\mathrm{d} \\epsilon}{\\mathrm{d}t} = f^{(1)}(\\hat{x}) \\epsilon \\] This is the same as exponential growth with growth rate \\(r=f^{(1)}(\\hat{x})\\) . The deviation will therefore grow if \\(f^{(1)}(\\hat{x})>0\\) \\(\\implies\\hat{x}\\) unstable shrink if \\(f^{(1)}(\\hat{x})<0\\) \\(\\implies\\hat{x}\\) locally stable Stability in continuous time therefore requires the slope of the differential equation to be negative at the equilibrium, \\(f^{(1)}(\\hat{x}) = \\left.\\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(\\frac{\\mathrm{d}x}{\\mathrm{d}t}\\right)\\right|_{x=\\hat{x}} < 0\\) . E.g., logistic growth Let's again look at the model of logistic growth in continuous time, where \\[ f(n) = \\frac{\\mathrm{d}n}{\\mathrm{d}t} = rn \\left(1 - \\frac{n}{K}\\right) \\] The derivative of \\(f\\) with respect to \\(n\\) is \\[ f'(n) = r - 2 r\\frac{n}{K} \\] Plugging in \\(n=K\\) gives \\[ f'(K) = r - 2 r = -r \\] This implies that \\(r>0\\) causes local stability of \\(\\hat{n}=K\\) . We can check this is consistent with a graphical analysis, below. def f(n,r,k): 'differential equation for logistic growth' return n*r*(1-n/k) def plot_logistic_de(r,k,ax=None): 'plot differential equation for logistic growth as function of n' xs = np.linspace(0,k*1.5,100) #n values if ax == None: fig, ax = plt.subplots() # 0 line ax.plot(xs, [0 for _ in xs], color='black', linestyle='--') # differential equation ax.plot(xs, [f(x,r,k) for x in xs], color='black') #aesthetics ax.set_xlabel('$n$') ax.set_ylabel('$dn/dt$') ax.set_title('$r=$%.1f'%r) return ax plot_logistic_de(r=0.5, k=1000) plt.show() plot_logistic_de(r=-0.5, k=1000) plt.show() 3. Local stability analysis in discrete-time one-variable models In discrete time we instead work with a recursion equation, \\(x(t+1) = f(x(t))\\) . Again, consider a system that is a small distance from the equilibrium at time \\(t\\) : \\(x(t) = \\hat{x} + \\epsilon(t)\\) . At time \\(t+1\\) , the population will be at \\(x(t+1) = \\hat{x} + \\epsilon(t+1) = f(\\hat{x} + \\epsilon(t))\\) . Taking the Taylor Series of \\(f(\\hat{x} + \\epsilon(t))\\) around \\(\\epsilon(t)=0\\) and truncating to first order (under our assumption of small \\(\\epsilon\\) ) we have \\[ \\begin{aligned} \\hat{x} + \\epsilon(t+1) &= f(\\hat{x} + \\epsilon(t))\\\\ &= f(\\hat{x}) + f^{(1)}(\\hat{x})(\\hat{x} + \\epsilon(t) - \\hat{x})\\\\ &= f(\\hat{x}) + f^{(1)}(\\hat{x})\\epsilon(t) \\end{aligned} \\] We know that \\(f(\\hat{x}) = \\hat{x}\\) because \\(\\hat{x}\\) is an equilibrium, which implies \\[ \\epsilon(t+1) = f^{(1)}(\\hat{x})\\epsilon(t) \\] This is the recursion for exponential growth, with reproductive factor \\(\\lambda = f^{(1)}(\\hat{x})\\) . Based on our knowledge of discrete-time exponential growth, we therefore know that the deviation from equilibrium will: move from one side of the equilibrium to the other (i.e., oscillate) if \\(\\lambda\\) is negative grow if \\(\\lambda<-1\\) \\(\\implies\\hat{x}\\) unstable shrink if \\(-1<\\lambda<0\\) \\(\\implies\\hat{x}\\) locally stable stay on the same side of the equilibrium (i.e., not oscillate) if \\(\\lambda\\) is positive shrink if \\(0<\\lambda<1\\) \\(\\implies\\hat{x}\\) locally stable grow if \\(1<\\lambda\\) \\(\\implies\\hat{x}\\) unstable Local stability in discrete time therefore requires the slope of the recursion to be between -1 and 1 at the equilibrium, \\(-1<f^{(1)}(\\hat{x})=\\left.\\frac{\\mathrm{d}x_{t+1}}{\\mathrm{d}x_t}\\right|_{x_t=\\hat x}<1\\) . E.g., logistic growth To see how this works, let's look at the logistic growth model in discrete time. Here the recursion is \\[ f(n) = n \\left(1 + r\\left(1 - \\frac{n}{K}\\right)\\right) \\] We first take the derivative of \\(f\\) with respect to \\(n\\) \\[ f'(n) = 1 + r - 2 r \\frac{n}{K} \\] Then we plug in the equilirbium value of interest, \\(n=K\\) \\[ \\begin{aligned} f'(K) &= 1 + r - 2 r \\\\ &= 1 - r \\end{aligned} \\] This will be negative when \\(r > 1\\) , creating oscillations. The equilibrium will be stable when \\(-1 < 1 - r < 1 \\implies 0 < r < 2\\) . This is consistent with what we've seen in cob-web plots, as below. # logistic growth recursion def f(nt,r,k): return nt * (1 + r * (1 - nt / k)) # Build cobweb plotting function def cobweb_logistic(n0, r, k, max=np.inf): t, nnow, nnext = 0, n0, 0 #initial conditions while t < max: yield nnow, nnext #current value of n(t) and n(t+1) nnext = f(nnow,r,k) yield nnow, nnext #current value of n(t) and n(t+1) nnow = nnext #update n(t) t += 1 #update t # Plot def plot_logistic_with_cobweb(r, k, n0, ncobs=10, ax=None): # Plot the curves (add an additional curve past equilibrium to show stability) xs = np.linspace(0,k*1.5,100) #x values if ax == None: fig, ax = plt.subplots() #1:1 line ax.plot(xs, xs, color='black', linestyle='dashed') # recursion ax.plot(xs, [f(x,r,k) for x in xs], color='black') # cobweb cobweb = np.array([i for i in cobweb_logistic(n0, r, k, ncobs)]) plt.plot(cobweb[:,0], cobweb[:,1], color='blue') #aesthetics ax.set_ylim(0,None) ax.set_xlim(0,None) ax.set_xlabel('$n_t$') ax.set_ylabel('$n_{t+1}$') ax.set_title('$r=$%.1f'%r) return ax plot_logistic_with_cobweb(r=0.5, k=1000, n0=900) plot_logistic_with_cobweb(r=1.5, k=1000, n0=900) plot_logistic_with_cobweb(r=2.5, k=1000, n0=900) plt.show() 4. Summary Local stability analysis for continuous- and discrete-time models with one variable: take the derivative of the differential equation or recursion with respect to the variable, \\(f^{(1)}(x)\\) plug in the equilibrium value of the variable, \\(f^{(1)}(\\hat x)\\) determine the sign (and magnitude in discrete-time)","title":"Lecture 8"},{"location":"lectures/lecture-08/#lecture-8-local-stability-univariate","text":"Run notes interactively?","title":"Lecture 8: Local stability (univariate)"},{"location":"lectures/lecture-08/#lecture-overview","text":"Stability Local stability analysis in continuous-time one-variable models Local stability analysis in discrete-time one-variable models Summary","title":"Lecture overview"},{"location":"lectures/lecture-08/#1-stability","text":"When a variable is exactly at an equilibrium its value will never change. But what happens when we are not exactly at, but just near an equilibrium? Starting near an equilibrium, if the system moves towards the equilibrium over time the equilibrium is said to be locally stable . In contrast, if the system moves away from the equilibrium over time the equilibrium is said to be unstable . An equilibrium point is said to be globally stable if any starting point leads to it. We've seen these possibilities already graphically, eg., in phase-line plots for discrete-time diploid selection: when the heterozygote has the lowest fitness the fixation equilibria are locally (but not globally) stable and the polymorphic equilibrium is unstable. import numpy as np import matplotlib.pyplot as plt def phase_line_diploid(p0, WAA, WAa, Waa, max=np.inf): 'generator for p_t' t, pnow, pnext = 0, p0, 0 #initial conditions while t < max: yield pnow #current value of p(t) and p(t+1) pnext = (WAA * pnow**2 + WAa * pnow * (1 - pnow)) / (WAA * pnow**2 + WAa * 2 * pnow * (1 - pnow) + Waa * (1 - pnow)**2) #update p(t+1) pnow = pnext #update p(t) t += 1 #update t def plot_phase_line_diploid(WAA, WAa, Waa, p0, max=20, ax=None): 'plot phase line' # set up figure if ax==None: fig, ax = plt.subplots() fig.set_size_inches(8,0.25) # Plot phase-line ax.axhline(0, color='black', linewidth=0.5) # Plot vector field pts = [pt for pt in phase_line_diploid(p0, WAA, WAa, Waa, max=max)] marker = '>' if pts[2] > pts[1] else ' < ' ax.scatter( pts, np.zeros(max), marker=marker, s=50, c='black' ) ax.set_xlabel(f\"$WAA$ = {WAA}, $WAa$ = {WAa}, $Waa$ = {Waa}\") return ax # Plot figure fig, ax = plt.subplots() fig.set_size_inches(8,0.25) # phase line and vector field plot_phase_line_diploid(WAA=2, WAa=1, Waa=2, p0=0.45, max=12, ax=ax) #higher starting allele frequency plot_phase_line_diploid(WAA=2, WAa=1, Waa=2, p0=0.55, max=12, ax=ax) #low starting allele frequency # equilibria plt.scatter([0,1],[0,0],s=200,c='black') plt.scatter([0.5],[0],s=200,c='white',edgecolors='black') # Remove background axes ax.set_ylabel('$p$', rotation=0) ax.get_yaxis().set_ticks([]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.spines['bottom'].set_visible(False) ax.spines['left'].set_visible(False) ax.set_xlim(-0.05,1.05) plt.show() Our goal in this lecture is to mathematically determine whether a small perturbation away from an equilibrium point will grow or shrink in magnitude over time \\(\\Longrightarrow\\) local stability analysis .","title":"1. Stability"},{"location":"lectures/lecture-08/#motivating-example","text":"Consider logistic growth in continuous time, with \\(K = 1000\\) and \\(r = 0.5\\) . For populations started near carrying capacity, a plot of \\(\\mathrm{d}n/\\mathrm{d}t\\) vs. \\(n\\) shows that they move closer to the carrying capacity over time, since the rate of change in \\(n\\) is positive when \\(n<K\\) and negative when \\(K<n\\) . r,k = 0.5,1000 xs = np.linspace(0,1.5 k,100) ys = [r n*(1-n/k) for n in xs] plt.plot(xs,ys) plt.plot(xs,[0 for _ in xs], color='black') plt.xlabel('population size, \\(n\\) ') plt.ylabel('rate of change in population size, \\(dn/dt\\) ') plt.show() If we drew the same plot for \\(r=-0.5\\) we see that the population size then moves away from \\(n=K\\) , as the rate of change in \\(n\\) is negative when \\(n<K\\) and positive when \\(K<n\\) . r,k=-0.5,1000 xs = np.linspace(0,1.5 k,100) ys = [r n*(1-n/k) for n in xs] plt.plot(xs,ys) plt.plot(xs,[0 for _ in xs], color='black') plt.xlabel('population size, \\(n\\) ') plt.ylabel('rate of change in population size, \\(dn/dt\\) ') plt.show() The key difference between these plots near the equilibrium of interest, \\(\\hat n=K\\) , is that 1. in the first case ($r=0.5$), where the equilibrium is stable, $\\mathrm{d}n/\\mathrm{d}t$ goes from positive to negative, meaning its slope is negative 2. in the second case ($r = -0.5$), where the equilibrium is unstable, $\\mathrm{d}n/\\mathrm{d}t$, goes from negative to positive, meaning its slope is positive This suggests that we can determine the local stability of an equilibrium by looking at the slope of the differential equation at that equilibrium.","title":"Motivating example"},{"location":"lectures/lecture-08/#2-local-stability-analysis-in-continuous-time-one-variable-models","text":"To determine local stability mathematically, we focus on a small perturbation ( \\(\\epsilon\\) ) away from an equilibrium ( \\(\\hat{x}\\) ) and determine whether this perturbation will grow or shrink. If, at time \\(t\\) , the population is a small distance from equilibrium, \\(x = \\hat{x} + \\epsilon\\) , the rate of change in \\(x\\) will be \\(\\mathrm{d}x/\\mathrm{d}t = \\mathrm{d}(\\hat{x} + \\epsilon)/\\mathrm{d}t\\) . Let us call this derivative \\(f(x)=\\mathrm{d}x/\\mathrm{d}t\\) , which we can write as \\(f(\\hat{x} + \\epsilon) = \\mathrm{d}(\\hat{x} + \\epsilon)/\\mathrm{d}t\\) . To work with this arbitrary function, \\(f\\) , let's introduce a pretty remarkable mathematical fact, the Taylor Series. Taylor Series Any function \\(f(x)\\) can be written as an infinite series of derivatives evaluated at \\(x=a\\) \\[ f(x) = \\sum_{k = 0}^{\\infty}\\frac{f^{(k)}(a)}{k!}(x-a)^k \\] where \\(f^{(k)}(a)\\) is the \\(k^{\\mathrm{th}}\\) derivative of the function with respect to \\(x\\) , evaluated at point \\(a\\) . (See section P1.3 in the text for more information). In this case we want to write \\(f(\\hat{x} + \\epsilon)\\) as a Taylor Series evaluated at the equilibrium, \\(\\hat{x} + \\epsilon = \\hat{x}\\) \\[ \\begin{aligned} \\frac{\\mathrm{d}(\\hat{x} + \\epsilon)}{\\mathrm{d}t} &= f(\\hat{x} + \\epsilon)\\\\ &= f(\\hat{x}) + f^{(1)}(\\hat{x})(\\hat{x} + \\epsilon - \\hat{x}) + \\frac{f^{(2)}(\\hat{x})}{2}(\\hat{x} + \\epsilon - \\hat{x})^2 + \\cdots\\\\ &= f(\\hat{x}) + f^{(1)}(\\hat{x})\\epsilon + \\frac{f^{(2)}(\\hat{x})}{2}\\epsilon^2 + \\cdots\\\\ \\end{aligned} \\] Now, to work with this infinite series we will make an assumption, that we are very the equilibrium, meaning the deviation is small, \\(\\epsilon<<1\\) . This means that \\(\\epsilon^2\\) is even smaller, and \\(\\epsilon^3\\) even smaller than that, and so on. By considering small \\(\\epsilon\\) we can therefore cut-off our infinite series by ignoring any term with \\(\\epsilon\\) to a power greater than 1. This is called a \"first order\" Taylor series approximation of \\(f\\) around \\(\\epsilon=0\\) . This assumption is what limits us to determining only local stability. Global stability would require us to consider large deviations from the equilibrium as well, which is not possible for even mildly complicated functions, \\(f\\) . OK, so making this assumption of small \\(\\epsilon\\) we have \\[ \\begin{aligned} \\frac{\\mathrm{d}(\\hat{x} + \\epsilon)}{\\mathrm{d}t} &= f(\\hat{x}) + f^{(1)}(\\hat{x})\\epsilon\\\\ \\end{aligned} \\] Since \\(\\hat{x}\\) is an equilibrium, \\(f(\\hat{x})\\) equals zero, leaving just \\(f^{(1)}(\\hat{x})\\epsilon\\) on the right-hand side. Furthermore, the left-hand side can be expanded and simplified ( \\(\\hat{x}\\) is a constant that does not change in time) \\[ \\begin{aligned} \\frac{\\mathrm{d}(\\hat{x}+\\epsilon)}{\\mathrm{d}t} &= \\frac{\\mathrm{d}\\hat{x}}{\\mathrm{d}t} + \\frac{\\mathrm{d}\\epsilon}{\\mathrm{d}t}\\\\ &= \\frac{\\mathrm{d}\\epsilon} {\\mathrm{d}t} \\end{aligned} \\] Combining the above, the deviation from the equilibrium will change over time at a rate \\[ \\frac{\\mathrm{d} \\epsilon}{\\mathrm{d}t} = f^{(1)}(\\hat{x}) \\epsilon \\] This is the same as exponential growth with growth rate \\(r=f^{(1)}(\\hat{x})\\) . The deviation will therefore grow if \\(f^{(1)}(\\hat{x})>0\\) \\(\\implies\\hat{x}\\) unstable shrink if \\(f^{(1)}(\\hat{x})<0\\) \\(\\implies\\hat{x}\\) locally stable Stability in continuous time therefore requires the slope of the differential equation to be negative at the equilibrium, \\(f^{(1)}(\\hat{x}) = \\left.\\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(\\frac{\\mathrm{d}x}{\\mathrm{d}t}\\right)\\right|_{x=\\hat{x}} < 0\\) .","title":"2. Local stability analysis in continuous-time one-variable models"},{"location":"lectures/lecture-08/#eg-logistic-growth","text":"Let's again look at the model of logistic growth in continuous time, where \\[ f(n) = \\frac{\\mathrm{d}n}{\\mathrm{d}t} = rn \\left(1 - \\frac{n}{K}\\right) \\] The derivative of \\(f\\) with respect to \\(n\\) is \\[ f'(n) = r - 2 r\\frac{n}{K} \\] Plugging in \\(n=K\\) gives \\[ f'(K) = r - 2 r = -r \\] This implies that \\(r>0\\) causes local stability of \\(\\hat{n}=K\\) . We can check this is consistent with a graphical analysis, below. def f(n,r,k): 'differential equation for logistic growth' return n*r*(1-n/k) def plot_logistic_de(r,k,ax=None): 'plot differential equation for logistic growth as function of n' xs = np.linspace(0,k*1.5,100) #n values if ax == None: fig, ax = plt.subplots() # 0 line ax.plot(xs, [0 for _ in xs], color='black', linestyle='--') # differential equation ax.plot(xs, [f(x,r,k) for x in xs], color='black') #aesthetics ax.set_xlabel('$n$') ax.set_ylabel('$dn/dt$') ax.set_title('$r=$%.1f'%r) return ax plot_logistic_de(r=0.5, k=1000) plt.show() plot_logistic_de(r=-0.5, k=1000) plt.show()","title":"E.g., logistic growth"},{"location":"lectures/lecture-08/#3-local-stability-analysis-in-discrete-time-one-variable-models","text":"In discrete time we instead work with a recursion equation, \\(x(t+1) = f(x(t))\\) . Again, consider a system that is a small distance from the equilibrium at time \\(t\\) : \\(x(t) = \\hat{x} + \\epsilon(t)\\) . At time \\(t+1\\) , the population will be at \\(x(t+1) = \\hat{x} + \\epsilon(t+1) = f(\\hat{x} + \\epsilon(t))\\) . Taking the Taylor Series of \\(f(\\hat{x} + \\epsilon(t))\\) around \\(\\epsilon(t)=0\\) and truncating to first order (under our assumption of small \\(\\epsilon\\) ) we have \\[ \\begin{aligned} \\hat{x} + \\epsilon(t+1) &= f(\\hat{x} + \\epsilon(t))\\\\ &= f(\\hat{x}) + f^{(1)}(\\hat{x})(\\hat{x} + \\epsilon(t) - \\hat{x})\\\\ &= f(\\hat{x}) + f^{(1)}(\\hat{x})\\epsilon(t) \\end{aligned} \\] We know that \\(f(\\hat{x}) = \\hat{x}\\) because \\(\\hat{x}\\) is an equilibrium, which implies \\[ \\epsilon(t+1) = f^{(1)}(\\hat{x})\\epsilon(t) \\] This is the recursion for exponential growth, with reproductive factor \\(\\lambda = f^{(1)}(\\hat{x})\\) . Based on our knowledge of discrete-time exponential growth, we therefore know that the deviation from equilibrium will: move from one side of the equilibrium to the other (i.e., oscillate) if \\(\\lambda\\) is negative grow if \\(\\lambda<-1\\) \\(\\implies\\hat{x}\\) unstable shrink if \\(-1<\\lambda<0\\) \\(\\implies\\hat{x}\\) locally stable stay on the same side of the equilibrium (i.e., not oscillate) if \\(\\lambda\\) is positive shrink if \\(0<\\lambda<1\\) \\(\\implies\\hat{x}\\) locally stable grow if \\(1<\\lambda\\) \\(\\implies\\hat{x}\\) unstable Local stability in discrete time therefore requires the slope of the recursion to be between -1 and 1 at the equilibrium, \\(-1<f^{(1)}(\\hat{x})=\\left.\\frac{\\mathrm{d}x_{t+1}}{\\mathrm{d}x_t}\\right|_{x_t=\\hat x}<1\\) .","title":"3. Local stability analysis in discrete-time one-variable models"},{"location":"lectures/lecture-08/#eg-logistic-growth_1","text":"To see how this works, let's look at the logistic growth model in discrete time. Here the recursion is \\[ f(n) = n \\left(1 + r\\left(1 - \\frac{n}{K}\\right)\\right) \\] We first take the derivative of \\(f\\) with respect to \\(n\\) \\[ f'(n) = 1 + r - 2 r \\frac{n}{K} \\] Then we plug in the equilirbium value of interest, \\(n=K\\) \\[ \\begin{aligned} f'(K) &= 1 + r - 2 r \\\\ &= 1 - r \\end{aligned} \\] This will be negative when \\(r > 1\\) , creating oscillations. The equilibrium will be stable when \\(-1 < 1 - r < 1 \\implies 0 < r < 2\\) . This is consistent with what we've seen in cob-web plots, as below. # logistic growth recursion def f(nt,r,k): return nt * (1 + r * (1 - nt / k)) # Build cobweb plotting function def cobweb_logistic(n0, r, k, max=np.inf): t, nnow, nnext = 0, n0, 0 #initial conditions while t < max: yield nnow, nnext #current value of n(t) and n(t+1) nnext = f(nnow,r,k) yield nnow, nnext #current value of n(t) and n(t+1) nnow = nnext #update n(t) t += 1 #update t # Plot def plot_logistic_with_cobweb(r, k, n0, ncobs=10, ax=None): # Plot the curves (add an additional curve past equilibrium to show stability) xs = np.linspace(0,k*1.5,100) #x values if ax == None: fig, ax = plt.subplots() #1:1 line ax.plot(xs, xs, color='black', linestyle='dashed') # recursion ax.plot(xs, [f(x,r,k) for x in xs], color='black') # cobweb cobweb = np.array([i for i in cobweb_logistic(n0, r, k, ncobs)]) plt.plot(cobweb[:,0], cobweb[:,1], color='blue') #aesthetics ax.set_ylim(0,None) ax.set_xlim(0,None) ax.set_xlabel('$n_t$') ax.set_ylabel('$n_{t+1}$') ax.set_title('$r=$%.1f'%r) return ax plot_logistic_with_cobweb(r=0.5, k=1000, n0=900) plot_logistic_with_cobweb(r=1.5, k=1000, n0=900) plot_logistic_with_cobweb(r=2.5, k=1000, n0=900) plt.show()","title":"E.g., logistic growth"},{"location":"lectures/lecture-08/#4-summary","text":"Local stability analysis for continuous- and discrete-time models with one variable: take the derivative of the differential equation or recursion with respect to the variable, \\(f^{(1)}(x)\\) plug in the equilibrium value of the variable, \\(f^{(1)}(\\hat x)\\) determine the sign (and magnitude in discrete-time)","title":"4. Summary"},{"location":"lectures/lecture-09/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 9: General solutions (univariate) Run notes interactively? Lecture overview General solutions Linear models in discrete time Nonlinear models in discrete time Linear models in continuous time Nonlinear models in continuous time Summary 1. General solutions Last week we learned how to find equilibria and determine their local stability in models with one variable (univariate). Those analyses describe the long-term dynamics of our models, i.e., what we expect after a long time has passed. This week we\u2019ll look at some simple cases where we can describe the entire dynamics, including the short-term, by solving for the variable as a function of time, \\(x_t = f(t)\\) This is called a general solution . 2. Linear models in discrete time With a single variable, \\(x\\) , in discrete time all linear models can be written \\[ x_{t+1} = a x_t + b \\] There are two cases that we will consider separately: 1) \\(b = 0\\) and 2) \\(b \\neq 0\\) . Brute force iteration When \\(b = 0\\) we can use brute force iteration \\[ \\begin{aligned} x_t &= a x_{t-1}\\\\ &= a a x_{t-2}\\\\ &= a a a x_{t-3}\\\\ &\\vdots\\\\ &= a\\cdots a x_0\\\\ &= a^t x_0 \\end{aligned} \\] This is the general solution for exponential growth in discrete time, with reproductive factor \\(a\\) . We can see that our variable will oscillate around the equilibrium ( \\(\\hat{x}=0\\) ) if \\(a<0\\) and will either approach the equilibrium ( \\(|a|<1\\) ) or depart from it ( \\(|a|>1\\) ), consistent with our local stability analysis. See this for yourself by playing with the value of \\(a\\) in the plot below. import matplotlib.pyplot as plt a, x0 = 0.99, 10 #define parameter values and initial condition ts = range(1000) #time values xs = [a**t * x0 for t in ts] #variable values from general solution plt.scatter(ts, xs) #plot discretely plt.ylabel('$x(t)$') plt.xlabel('$t$') plt.show() Solving affine models When \\(b \\neq 0\\) (which gives us what is called an affine model ) we need to use a transformation , much like we did when determining local stability. Step 1 : Solve for the equilibrium \\[ \\begin{aligned} \\hat{x} &= a \\hat{x} + b \\\\ \\hat{x} &= \\frac{b}{1 - a} \\end{aligned} \\] Note Note that if \\(a=1\\) there is no equilibrium for \\(b\\neq0\\) , and instead you can use brute force iteration to show that \\(x_t = x_0 + b t\\) . Step 2 : Define \\(\\delta_t = x_t - \\hat{x}\\) , the deviation of our variable from the equilibrium (this is our transformation). Step 3 : Write the recursion equation for the transformed variable \\[ \\begin{aligned} \\delta_{t+1} &= x_{t+1} - \\hat{x} \\\\ &= a x_t + b - \\hat{x} \\\\ &= a(\\delta_t + \\hat{x}) + b - \\hat{x}\\\\ &= a \\left(\\delta_t + \\frac{b}{1 - a}\\right) + b - \\frac{b}{1 - a}\\\\ &= a \\delta_t \\end{aligned} \\] Step 4 : This is the same recursion we derived above for \\(x\\) when \\(b=0\\) . So the general solution for the transformed variable is \\(\\delta_t = a^t \\delta_0\\) . Step 5 : Reverse transform back to \\(x_t\\) \\[ \\begin{aligned} x_t &= \\delta_t + \\hat{x}\\\\ &= a^t \\delta_0 + \\hat{x}\\\\ &= a^t (x_0 - \\hat{x}) + \\hat{x}\\\\ &= a^t x_0 + (1 - a^t)\\hat{x} \\end{aligned} \\] This says that our variable moves from \\(x_0\\) towards/away from \\(\\hat{x}\\) by a factor \\(a\\) per time step. Note that if \\(b=0\\) then \\(\\hat{x}=0\\) and this reduces to what we derived above, \\(x_t=a^t x_0\\) . Below we plot the general solution for a given value of \\(a\\) and \\(b\\) from a number of different intitial conditions. Try playing with the values of \\(a\\) and \\(b\\) and observe the different dynamics. a, b, x0 = 0.99, 1, 10 #define parameter values and initial condition ts = range(1000) #time values xs = [a**t * x0 + (1-a**t)*b/(1-a) for t in ts] #variable values from general solution plt.scatter(ts, xs) #plot discretely plt.ylabel('$x(t)$') plt.xlabel('$t$') plt.show() 3. Nonlinear models in discrete time Unfortunately there is no recipe to solve nonlinear models in discrete time, even with one variable. In fact, most of the time there is no general solution. To get a sense of why that might be, remember the chaos of logistic growth! import numpy as np # Generator for logistic growth def n(n0, r, k, max=np.inf): t, nt = 0, n0 while t < max: yield nt t, nt = t + 1, nt + r * nt * (1 - nt / k) # Sample the periodicity of the oscillations by taking unique values after reaching carrying capacity def log_map(r, n0=900, k=1000): return np.unique([nt for t, nt in enumerate(n(n0, r, k, max=75)) if t > 30]) # Compute the logistic map for different growth rates in discrete time r, Nr = np.array([]), np.array([]) for i in np.linspace(1.5, 3, 1000): nl = log_map(i) r = np.hstack((r, [i for _ in range(len(nl))])) Nr = np.hstack((Nr, nl)) # Plot the logistic map on a black background fig, ax = plt.subplots() ax.patch.set_facecolor('black') ax.scatter(r, Nr, s=0.075, color='white') ax.set_xlabel('intrinsic growth rate, $r$') ax.set_ylabel('population size, $n$') plt.show() Solving with transformations Sometimes, however, you can find a transformation that works. For example, with haploid selection we have \\[ p_{t+1} = \\frac{W_A p_t}{W_A p_t + W_a q_t} \\] Brute force iteration will create a giant mess. But what about if we let \\(f_t = p_t/q_t\\) ? Noting that \\(q_{t+1} = 1 - p_{t+1} = (W_a p_t)/(W_A p_t + W_a q_t)\\) we have \\[ \\begin{aligned} f_{t+1} &= \\frac{p_{t+1}}{q_{t+1}}\\\\ &= \\frac{W_A p_t}{W_a q_t}\\\\ &= \\frac{W_A}{W_a} f_t \\end{aligned} \\] This implies that \\(f_t = (W_A/W_a)^t f_0\\) ! Converting back to \\(p_t\\) we see \\[ p_t = \\frac{f_t}{1-f_t} = \\frac{W_A^t p_0}{W_A^t p_0 + W_a^t q_0} \\] Solving with conceptualization An alternative way to derive this general solution is to think about (\"conceptualize\") the \\(A\\) and \\(a\\) alleles as two competing populations that each grow exponentially according to their fitness \\[ \\begin{aligned} n_A(t) &= W_A^t n_A(0)\\\\ n_a(t) &= W_a^t n_a(0) \\end{aligned} \\] Then the frequency of allele \\(A\\) at time \\(t\\) is \\[ p_t = \\frac{n_A(t)}{n_A(t) + n_a(t)} = \\frac{W_A^t n_A(0)}{W_A^t n_A(0) + W_a^t n_a(0)} \\] Dividing numerator and denominator by the total initial population size \\(n_A(0) + n_a(0)\\) \\[ p_t = \\frac{W_A^t p_0}{W_A^t p_0 + W_a^t q_0} \\] Below we plot this general solution for a given \\(W_A\\) , \\(W_a\\) , and \\(p_0\\) . WA, Wa, p0 = 1.1, 1, 0.01 #define parameter values and initial condition ts = range(100) #time values ps = [(WA**t * p0)/(WA**t * p0 + Wa**t * (1-p0)) for t in ts] #variable values from general solution plt.scatter(ts, ps) #plot discretely plt.ylabel('allele frequency, $p(t)$') plt.xlabel('generation, $t$') plt.show() 4. Linear models in continuous time In continuous time, a linear differential equation of one variable can be written \\[ \\frac{\\mathrm{d}x}{\\mathrm{d}t} = a x + b \\] Let's first look at the case where \\(b=0\\) . Separation of variables Here we can use a method called seperation of variables . That is, our differential equation can be written \\(\\mathrm{d}x/\\mathrm{d}t = f(x) g(t)\\) , i.e., we can separate the variables \\(x\\) and \\(t\\) . We can then re-write the equation as \\(\\mathrm{d}x/f(x) = g(t)\\mathrm{d}t\\) and take the indefinite integral of both sides. In our case we have \\(f(x)=a x\\) and \\(g(t)=1\\) so \\[ \\begin{aligned} \\int \\frac{\\mathrm{d}x}{a x} &= \\int \\mathrm{d}t \\\\ \\frac{\\ln(x)}{a} + c_1 &= t + c_2\\\\ \\ln(x) &= a t + c \\; \\text{(where } c = c_2 - c_1\\text{)}\\\\ x_t &= e^{a t} e^{a c} \\end{aligned} \\] Plugging in \\(t=0\\) we have \\(x_0 = e^{a c}\\) and so our general solution is \\[ x_t = x_0 e^{at} \\] This is the general solution for exponential growth in continuous time with growth rate \\(a\\) . We see that variable will either converge on ( \\(a<0\\) ) or depart from ( \\(a>0\\) ) the equilibrium ( \\(\\hat x=0\\) ), consistent with our local stability analysis. import matplotlib.pyplot as plt a, x0 = 0.01, 10 #define parameter values and initial condition ts = range(1000) #time values xs = [exp(a*t) * x0 for t in ts] #variable values from general solution plt.plot(ts, xs) #plot continuously plt.ylabel('$x(t)$') plt.xlabel('$t$') plt.show() Using transformations Now let's consider the case where \\(b\\neq0\\) . This can also be solved by the method of separation of variables but let's do it with a transformation, like we did in discrete time. Step 1 : Solve for the equilibrium, \\[ \\begin{aligned} 0 &= a \\hat{x} + b\\\\ \\hat{x} &= -b/a \\end{aligned} \\] Step 2 : Define \\(\\delta = x - \\hat{x}\\) as the deviation of the variable from equilibrium. Step 3 : Derive the differential equation for \\(\\delta\\) \\[ \\begin{aligned} \\frac{\\mathrm{d}\\delta}{\\mathrm{d}t} &= \\frac{\\mathrm{d}(x - \\hat{x})}{\\mathrm{d}t}\\\\ &= \\frac{\\mathrm{d}x}{\\mathrm{d}t} - \\frac{\\mathrm{d}\\hat{x}}{\\mathrm{d}t}\\\\ &= \\frac{\\mathrm{d}x}{\\mathrm{d}t}\\\\ &= a x + b\\\\ &= a (\\delta + \\hat{x}) + b\\\\ &= a(\\delta + -b/a) + b\\\\ &= a \\delta \\end{aligned} \\] Step 4 : This is the same differential equation we had above for \\(x\\) when \\(b=0\\) . The general solution is therefore \\(\\delta\\) is \\(\\delta_t = \\delta_0 e^{a t}\\) . Step 5 : Replace \\(\\delta\\) with \\(x - \\hat{x}\\) to back transform to our original variable \\[ x_t = e^{a t} x_0 + (1 - e^{a t})\\hat{x} \\] Similar to the discrete case (but without the oscillations), this tells us there is an exponential approach to ( \\(a<0\\) ) or departure from ( \\(a>0\\) ) the equilibrium, \\(\\hat{x}\\) . Below we plot this general solution for given values of \\(a\\) and \\(b\\) and a range of initial conditions, \\(x_0\\) . a, b, x0 = -0.01, 1, 10 #define parameter values and initial condition ts = range(1000) #time values xs = [exp(a*t) * x0 + (1-exp(a*t))*(-b/a) for t in ts] #variable values from general solution plt.plot(ts, xs) #plot continuously plt.ylabel('$x(t)$') plt.xlabel('$t$') plt.show() 5. Nonlinear models in continuous time Some nonlinear differential equations can also be solved. But there is no general recipe. Separation of variables Sometimes separation of variables works, as in the case of logistic growth and haploid selection (which are equivalent in form in continuous-time!). \\[ \\begin{aligned} \\frac{\\mathrm{d}p}{\\mathrm{d}t} &= s p (1-p)\\\\ \\frac{\\mathrm{d}p}{s p (1-p)} &= \\mathrm{d}t\\\\ \\left(\\frac{1}{s p} + \\frac{1/s}{1-p}\\right) \\mathrm{d}p &= \\mathrm{d}t \\;\\text{(method of partial fractions, rule A.19 in the text)}\\\\ \\int \\frac{1}{s p} \\mathrm{d}p + \\int \\frac{1}{s(1-p)} \\mathrm{d}p &= \\int \\mathrm{d}t\\\\ \\ln(p)/s - \\ln(1 - p)/s + c_1 &= t + c_2 \\\\ \\ln\\left(\\frac{p}{1-p}\\right) &= s t + s c \\; \\text{(where } c = c_2 - c_1\\text{)}\\\\ \\frac{p}{1-p} &= e^{st} e^{sc} \\end{aligned} \\] Plugging in \\(t=0\\) we have \\(p_0/(1-p_0) = e^{sc}\\) . Then solving this linear equation for \\(p\\) \\[ p_t = \\frac{e^{st} p_0}{1 - p_0 + e^{st} p_0} \\] This shows essentially the same dynamics as haploid selection in discrete time. s, p0 = 0.1, 0.01 #define parameter values and initial condition ts = range(100) #time values ps = [(exp(s*t) * p0)/(1 - p0 + exp(s*t)*p0) for t in ts] #variable values from general solution plt.plot(ts, ps) #plot continuously plt.ylabel('allele frequency, $p(t)$') plt.xlabel('generation, $t$') plt.show() Alternative methods Separation of variables does not always work as it may not be possible to solve the integrals. However, separation of variables is not the only method. Box 6.2 in the text describes how to solve three forms of differential equations that are not amenable to separation of variables (ie, that cannot be written like \\(\\mathrm{d}x/\\mathrm{d}t = f(x) g(t)\\) ). 6. Summary Today we've covered how to find the general solution for some univariate models. We now have three methods to analyze univariate models: numerical and graphical analyses (for particular parameter values) finding equilibria and determining their local stability (general long-term dynamics) finding the general solution (general short- and long-term dynamics)","title":"Lecture 9"},{"location":"lectures/lecture-09/#lecture-9-general-solutions-univariate","text":"Run notes interactively?","title":"Lecture 9: General solutions (univariate)"},{"location":"lectures/lecture-09/#lecture-overview","text":"General solutions Linear models in discrete time Nonlinear models in discrete time Linear models in continuous time Nonlinear models in continuous time Summary","title":"Lecture overview"},{"location":"lectures/lecture-09/#1-general-solutions","text":"Last week we learned how to find equilibria and determine their local stability in models with one variable (univariate). Those analyses describe the long-term dynamics of our models, i.e., what we expect after a long time has passed. This week we\u2019ll look at some simple cases where we can describe the entire dynamics, including the short-term, by solving for the variable as a function of time, \\(x_t = f(t)\\) This is called a general solution .","title":"1. General solutions"},{"location":"lectures/lecture-09/#2-linear-models-in-discrete-time","text":"With a single variable, \\(x\\) , in discrete time all linear models can be written \\[ x_{t+1} = a x_t + b \\] There are two cases that we will consider separately: 1) \\(b = 0\\) and 2) \\(b \\neq 0\\) .","title":"2. Linear models in discrete time"},{"location":"lectures/lecture-09/#brute-force-iteration","text":"When \\(b = 0\\) we can use brute force iteration \\[ \\begin{aligned} x_t &= a x_{t-1}\\\\ &= a a x_{t-2}\\\\ &= a a a x_{t-3}\\\\ &\\vdots\\\\ &= a\\cdots a x_0\\\\ &= a^t x_0 \\end{aligned} \\] This is the general solution for exponential growth in discrete time, with reproductive factor \\(a\\) . We can see that our variable will oscillate around the equilibrium ( \\(\\hat{x}=0\\) ) if \\(a<0\\) and will either approach the equilibrium ( \\(|a|<1\\) ) or depart from it ( \\(|a|>1\\) ), consistent with our local stability analysis. See this for yourself by playing with the value of \\(a\\) in the plot below. import matplotlib.pyplot as plt a, x0 = 0.99, 10 #define parameter values and initial condition ts = range(1000) #time values xs = [a**t * x0 for t in ts] #variable values from general solution plt.scatter(ts, xs) #plot discretely plt.ylabel('$x(t)$') plt.xlabel('$t$') plt.show()","title":"Brute force iteration"},{"location":"lectures/lecture-09/#solving-affine-models","text":"When \\(b \\neq 0\\) (which gives us what is called an affine model ) we need to use a transformation , much like we did when determining local stability. Step 1 : Solve for the equilibrium \\[ \\begin{aligned} \\hat{x} &= a \\hat{x} + b \\\\ \\hat{x} &= \\frac{b}{1 - a} \\end{aligned} \\] Note Note that if \\(a=1\\) there is no equilibrium for \\(b\\neq0\\) , and instead you can use brute force iteration to show that \\(x_t = x_0 + b t\\) . Step 2 : Define \\(\\delta_t = x_t - \\hat{x}\\) , the deviation of our variable from the equilibrium (this is our transformation). Step 3 : Write the recursion equation for the transformed variable \\[ \\begin{aligned} \\delta_{t+1} &= x_{t+1} - \\hat{x} \\\\ &= a x_t + b - \\hat{x} \\\\ &= a(\\delta_t + \\hat{x}) + b - \\hat{x}\\\\ &= a \\left(\\delta_t + \\frac{b}{1 - a}\\right) + b - \\frac{b}{1 - a}\\\\ &= a \\delta_t \\end{aligned} \\] Step 4 : This is the same recursion we derived above for \\(x\\) when \\(b=0\\) . So the general solution for the transformed variable is \\(\\delta_t = a^t \\delta_0\\) . Step 5 : Reverse transform back to \\(x_t\\) \\[ \\begin{aligned} x_t &= \\delta_t + \\hat{x}\\\\ &= a^t \\delta_0 + \\hat{x}\\\\ &= a^t (x_0 - \\hat{x}) + \\hat{x}\\\\ &= a^t x_0 + (1 - a^t)\\hat{x} \\end{aligned} \\] This says that our variable moves from \\(x_0\\) towards/away from \\(\\hat{x}\\) by a factor \\(a\\) per time step. Note that if \\(b=0\\) then \\(\\hat{x}=0\\) and this reduces to what we derived above, \\(x_t=a^t x_0\\) . Below we plot the general solution for a given value of \\(a\\) and \\(b\\) from a number of different intitial conditions. Try playing with the values of \\(a\\) and \\(b\\) and observe the different dynamics. a, b, x0 = 0.99, 1, 10 #define parameter values and initial condition ts = range(1000) #time values xs = [a**t * x0 + (1-a**t)*b/(1-a) for t in ts] #variable values from general solution plt.scatter(ts, xs) #plot discretely plt.ylabel('$x(t)$') plt.xlabel('$t$') plt.show()","title":"Solving affine models"},{"location":"lectures/lecture-09/#3-nonlinear-models-in-discrete-time","text":"Unfortunately there is no recipe to solve nonlinear models in discrete time, even with one variable. In fact, most of the time there is no general solution. To get a sense of why that might be, remember the chaos of logistic growth! import numpy as np # Generator for logistic growth def n(n0, r, k, max=np.inf): t, nt = 0, n0 while t < max: yield nt t, nt = t + 1, nt + r * nt * (1 - nt / k) # Sample the periodicity of the oscillations by taking unique values after reaching carrying capacity def log_map(r, n0=900, k=1000): return np.unique([nt for t, nt in enumerate(n(n0, r, k, max=75)) if t > 30]) # Compute the logistic map for different growth rates in discrete time r, Nr = np.array([]), np.array([]) for i in np.linspace(1.5, 3, 1000): nl = log_map(i) r = np.hstack((r, [i for _ in range(len(nl))])) Nr = np.hstack((Nr, nl)) # Plot the logistic map on a black background fig, ax = plt.subplots() ax.patch.set_facecolor('black') ax.scatter(r, Nr, s=0.075, color='white') ax.set_xlabel('intrinsic growth rate, $r$') ax.set_ylabel('population size, $n$') plt.show()","title":"3. Nonlinear models in discrete time"},{"location":"lectures/lecture-09/#solving-with-transformations","text":"Sometimes, however, you can find a transformation that works. For example, with haploid selection we have \\[ p_{t+1} = \\frac{W_A p_t}{W_A p_t + W_a q_t} \\] Brute force iteration will create a giant mess. But what about if we let \\(f_t = p_t/q_t\\) ? Noting that \\(q_{t+1} = 1 - p_{t+1} = (W_a p_t)/(W_A p_t + W_a q_t)\\) we have \\[ \\begin{aligned} f_{t+1} &= \\frac{p_{t+1}}{q_{t+1}}\\\\ &= \\frac{W_A p_t}{W_a q_t}\\\\ &= \\frac{W_A}{W_a} f_t \\end{aligned} \\] This implies that \\(f_t = (W_A/W_a)^t f_0\\) ! Converting back to \\(p_t\\) we see \\[ p_t = \\frac{f_t}{1-f_t} = \\frac{W_A^t p_0}{W_A^t p_0 + W_a^t q_0} \\]","title":"Solving with transformations"},{"location":"lectures/lecture-09/#solving-with-conceptualization","text":"An alternative way to derive this general solution is to think about (\"conceptualize\") the \\(A\\) and \\(a\\) alleles as two competing populations that each grow exponentially according to their fitness \\[ \\begin{aligned} n_A(t) &= W_A^t n_A(0)\\\\ n_a(t) &= W_a^t n_a(0) \\end{aligned} \\] Then the frequency of allele \\(A\\) at time \\(t\\) is \\[ p_t = \\frac{n_A(t)}{n_A(t) + n_a(t)} = \\frac{W_A^t n_A(0)}{W_A^t n_A(0) + W_a^t n_a(0)} \\] Dividing numerator and denominator by the total initial population size \\(n_A(0) + n_a(0)\\) \\[ p_t = \\frac{W_A^t p_0}{W_A^t p_0 + W_a^t q_0} \\] Below we plot this general solution for a given \\(W_A\\) , \\(W_a\\) , and \\(p_0\\) . WA, Wa, p0 = 1.1, 1, 0.01 #define parameter values and initial condition ts = range(100) #time values ps = [(WA**t * p0)/(WA**t * p0 + Wa**t * (1-p0)) for t in ts] #variable values from general solution plt.scatter(ts, ps) #plot discretely plt.ylabel('allele frequency, $p(t)$') plt.xlabel('generation, $t$') plt.show()","title":"Solving with conceptualization"},{"location":"lectures/lecture-09/#4-linear-models-in-continuous-time","text":"In continuous time, a linear differential equation of one variable can be written \\[ \\frac{\\mathrm{d}x}{\\mathrm{d}t} = a x + b \\] Let's first look at the case where \\(b=0\\) .","title":"4. Linear models in continuous time"},{"location":"lectures/lecture-09/#separation-of-variables","text":"Here we can use a method called seperation of variables . That is, our differential equation can be written \\(\\mathrm{d}x/\\mathrm{d}t = f(x) g(t)\\) , i.e., we can separate the variables \\(x\\) and \\(t\\) . We can then re-write the equation as \\(\\mathrm{d}x/f(x) = g(t)\\mathrm{d}t\\) and take the indefinite integral of both sides. In our case we have \\(f(x)=a x\\) and \\(g(t)=1\\) so \\[ \\begin{aligned} \\int \\frac{\\mathrm{d}x}{a x} &= \\int \\mathrm{d}t \\\\ \\frac{\\ln(x)}{a} + c_1 &= t + c_2\\\\ \\ln(x) &= a t + c \\; \\text{(where } c = c_2 - c_1\\text{)}\\\\ x_t &= e^{a t} e^{a c} \\end{aligned} \\] Plugging in \\(t=0\\) we have \\(x_0 = e^{a c}\\) and so our general solution is \\[ x_t = x_0 e^{at} \\] This is the general solution for exponential growth in continuous time with growth rate \\(a\\) . We see that variable will either converge on ( \\(a<0\\) ) or depart from ( \\(a>0\\) ) the equilibrium ( \\(\\hat x=0\\) ), consistent with our local stability analysis. import matplotlib.pyplot as plt a, x0 = 0.01, 10 #define parameter values and initial condition ts = range(1000) #time values xs = [exp(a*t) * x0 for t in ts] #variable values from general solution plt.plot(ts, xs) #plot continuously plt.ylabel('$x(t)$') plt.xlabel('$t$') plt.show()","title":"Separation of variables"},{"location":"lectures/lecture-09/#using-transformations","text":"Now let's consider the case where \\(b\\neq0\\) . This can also be solved by the method of separation of variables but let's do it with a transformation, like we did in discrete time. Step 1 : Solve for the equilibrium, \\[ \\begin{aligned} 0 &= a \\hat{x} + b\\\\ \\hat{x} &= -b/a \\end{aligned} \\] Step 2 : Define \\(\\delta = x - \\hat{x}\\) as the deviation of the variable from equilibrium. Step 3 : Derive the differential equation for \\(\\delta\\) \\[ \\begin{aligned} \\frac{\\mathrm{d}\\delta}{\\mathrm{d}t} &= \\frac{\\mathrm{d}(x - \\hat{x})}{\\mathrm{d}t}\\\\ &= \\frac{\\mathrm{d}x}{\\mathrm{d}t} - \\frac{\\mathrm{d}\\hat{x}}{\\mathrm{d}t}\\\\ &= \\frac{\\mathrm{d}x}{\\mathrm{d}t}\\\\ &= a x + b\\\\ &= a (\\delta + \\hat{x}) + b\\\\ &= a(\\delta + -b/a) + b\\\\ &= a \\delta \\end{aligned} \\] Step 4 : This is the same differential equation we had above for \\(x\\) when \\(b=0\\) . The general solution is therefore \\(\\delta\\) is \\(\\delta_t = \\delta_0 e^{a t}\\) . Step 5 : Replace \\(\\delta\\) with \\(x - \\hat{x}\\) to back transform to our original variable \\[ x_t = e^{a t} x_0 + (1 - e^{a t})\\hat{x} \\] Similar to the discrete case (but without the oscillations), this tells us there is an exponential approach to ( \\(a<0\\) ) or departure from ( \\(a>0\\) ) the equilibrium, \\(\\hat{x}\\) . Below we plot this general solution for given values of \\(a\\) and \\(b\\) and a range of initial conditions, \\(x_0\\) . a, b, x0 = -0.01, 1, 10 #define parameter values and initial condition ts = range(1000) #time values xs = [exp(a*t) * x0 + (1-exp(a*t))*(-b/a) for t in ts] #variable values from general solution plt.plot(ts, xs) #plot continuously plt.ylabel('$x(t)$') plt.xlabel('$t$') plt.show()","title":"Using transformations"},{"location":"lectures/lecture-09/#5-nonlinear-models-in-continuous-time","text":"Some nonlinear differential equations can also be solved. But there is no general recipe.","title":"5. Nonlinear models in continuous time"},{"location":"lectures/lecture-09/#separation-of-variables_1","text":"Sometimes separation of variables works, as in the case of logistic growth and haploid selection (which are equivalent in form in continuous-time!). \\[ \\begin{aligned} \\frac{\\mathrm{d}p}{\\mathrm{d}t} &= s p (1-p)\\\\ \\frac{\\mathrm{d}p}{s p (1-p)} &= \\mathrm{d}t\\\\ \\left(\\frac{1}{s p} + \\frac{1/s}{1-p}\\right) \\mathrm{d}p &= \\mathrm{d}t \\;\\text{(method of partial fractions, rule A.19 in the text)}\\\\ \\int \\frac{1}{s p} \\mathrm{d}p + \\int \\frac{1}{s(1-p)} \\mathrm{d}p &= \\int \\mathrm{d}t\\\\ \\ln(p)/s - \\ln(1 - p)/s + c_1 &= t + c_2 \\\\ \\ln\\left(\\frac{p}{1-p}\\right) &= s t + s c \\; \\text{(where } c = c_2 - c_1\\text{)}\\\\ \\frac{p}{1-p} &= e^{st} e^{sc} \\end{aligned} \\] Plugging in \\(t=0\\) we have \\(p_0/(1-p_0) = e^{sc}\\) . Then solving this linear equation for \\(p\\) \\[ p_t = \\frac{e^{st} p_0}{1 - p_0 + e^{st} p_0} \\] This shows essentially the same dynamics as haploid selection in discrete time. s, p0 = 0.1, 0.01 #define parameter values and initial condition ts = range(100) #time values ps = [(exp(s*t) * p0)/(1 - p0 + exp(s*t)*p0) for t in ts] #variable values from general solution plt.plot(ts, ps) #plot continuously plt.ylabel('allele frequency, $p(t)$') plt.xlabel('generation, $t$') plt.show()","title":"Separation of variables"},{"location":"lectures/lecture-09/#alternative-methods","text":"Separation of variables does not always work as it may not be possible to solve the integrals. However, separation of variables is not the only method. Box 6.2 in the text describes how to solve three forms of differential equations that are not amenable to separation of variables (ie, that cannot be written like \\(\\mathrm{d}x/\\mathrm{d}t = f(x) g(t)\\) ).","title":"Alternative methods"},{"location":"lectures/lecture-09/#6-summary","text":"Today we've covered how to find the general solution for some univariate models. We now have three methods to analyze univariate models: numerical and graphical analyses (for particular parameter values) finding equilibria and determining their local stability (general long-term dynamics) finding the general solution (general short- and long-term dynamics)","title":"6. Summary"},{"location":"lectures/lecture-10/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 10: Linear algebra I Run notes interactively? Lecture overview Motivation What are vectors? What is a matrix? Vector and matrix operations 1. Motivation Until now, we have been dealing with problems in a single variable changing over time. Often, dynamical systems involve more than one variable (ie, they are multivariate ). For instance, we may be interested in how the numbers of two species change as they interact (e.g., compete) with one another. As a simple example with more than one variable, consider a model tracking the number of birds on two islands. Let the number of birds on island 1 be \\(n_1\\) and let the number of birds on island 2 be \\(n_2\\) . We assume the birds migrate between the islands at per capita rates \\(m_{12}\\) and \\(m_{21}\\) , the birds on each island give birth at per capita rates \\(b_1\\) and \\(b_2\\) , the birds on each island die at per capita rates \\(d_1\\) and \\(d_2\\) , and new birds arrive on each island at rates \\(m_1\\) and \\(m_2\\) . This is captured in the following flow diagram graph LR; A1((n1)) --b1 n1--> A1; B1[ ] --m1--> A1; A1 --d1 n1--> C1[ ]; A2((n2)) --b2 n2--> A2; B2[ ] --m2--> A2; A2 --d2 n2--> C2[ ]; A1 --m12 n1--> A2; A2 --m21 n2--> A1; style B1 height:0px; style C1 height:0px; style B2 height:0px; style C2 height:0px; The rate of change in \\(n_1\\) and \\(n_2\\) are then described by the following system of differential equations \\[ \\begin{aligned} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &= (b_1 - d_1 - m_{12})n_1 + m_{21} n_2 + m_1 \\\\ \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &= m_{12} n_1 + (b_2 - d_2 - m_{21})n_2 + m_1 \\end{aligned} \\] These equations are linear functions of the variables (i.e., they contain only constant multiples of \\(n_1\\) and \\(n_2\\) and nothing more complicated such as \\(x^2\\) or \\(e^x\\) ). Linear systems of equations like these can also be written in matrix form \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} \\end{pmatrix} &= \\begin{pmatrix} b_1 - d_1 - m_{12} & m_{21} \\\\ m_{12} & b_2 - d_2 - m_{21} \\end{pmatrix} \\begin{pmatrix} n_1 \\\\ n_2 \\end{pmatrix} + \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}\\\\ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} &= \\mathbf{M}\\vec{n} + \\vec{m} \\end{aligned} \\] Not only is this a nice compact expression, there are rules of linear algebra that can help us conveniently solve this (and any other) set of linear equations. So let's get to know these rules. 2. What are vectors? Vectors are lists of elements (elements being numbers, parameters, functions, or variables). A column vector has elements arranged from top to bottom \\[ \\begin{equation*} \\begin{pmatrix} 5 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 1 \\\\ 5 \\\\ 9 \\\\ 7 \\end{pmatrix}, \\begin{pmatrix} x \\\\ y \\end{pmatrix}, \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}, \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\end{equation*} \\] A row vector has elements arranged from left to right \\[ \\begin{pmatrix}5 & 2\\end{pmatrix}, \\begin{pmatrix} 1 & 5 & 9 & 7\\end{pmatrix}, \\begin{pmatrix} x & y \\end{pmatrix}, \\begin{pmatrix} x & y & z\\end{pmatrix}, \\begin{pmatrix} x_1 & x_2 & \\cdots & x_n \\end{pmatrix} \\] We will indicate vectors by placing an arrow on top of the symbol \\[ \\vec{x} = \\begin{pmatrix} x_1 & x_2 & \\cdots & x_n \\end{pmatrix} \\] The number of elements in the vector indicates its dimension , \\(n\\) . For example, the row vector \\(\\begin{pmatrix}x & y\\end{pmatrix}\\) has dimension \\(n=2\\) . You can represent a vector as an arrow in \\(n\\) dimensions, connecting the origin with a point whose coordinates are given by elements in the vector. For example, the vector \\(\\vec{v} = \\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) can be depicted as below import matplotlib.pyplot as plt #import plotting library plt.arrow(0, 0, #starting x and y values of arrow 1, 2, #change in x and y head_width=0.1, color='black') #aesthetics plt.xlim(0,2.5) #set bounds on x axis plt.ylim(0,2.5) #set bounds on y axis plt.show() 3. What is a matrix? An \\(m \\times n\\) matrix has \\(m\\) rows and \\(n\\) columns \\[ \\begin{equation*} \\begin{pmatrix} x_{11} & x_{12} & \\cdots & x_{1n}\\\\ x_{21} & x_{22} & \\cdots & x_{2n}\\\\ \\vdots & \\vdots & & \\vdots\\\\ x_{m1} & x_{m2} & \\cdots & x_{mn}\\\\ \\end{pmatrix}, \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}, \\begin{pmatrix} 75 & 67 \\\\ 66 & 34 \\\\ 12 & 14 \\end{pmatrix}, \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\end{equation*} \\] We will indicate matrices by bolding the symbol (and using capital letters) \\[ \\mathbf{X} = \\begin{pmatrix} x_{11} & x_{12} & \\cdots & x_{1n}\\\\ x_{21} & x_{22} & \\cdots & x_{2n}\\\\ \\vdots & \\vdots & & \\vdots\\\\ x_{m1} & x_{m2} & \\cdots & x_{mn}\\\\ \\end{pmatrix} \\] A matrix with an equal number of rows and columns, \\(m=n\\) , is a square matrix . A matrix with zeros everywhere except along the diagonal is called a diagonal matrix \\[ \\begin{pmatrix} a & 0 & 0 \\\\ 0 & b & 0 \\\\ 0 & 0 & c \\end{pmatrix} \\] And a special case of this with 1s along the diagonal is called the identity matrix \\[ \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\] A matrix with all zeros below the diagonal is called an upper trianglular matrix \\[ \\begin{pmatrix} a & b & c \\\\ 0 & d & e \\\\ 0 & 0 & f \\end{pmatrix} \\] A matrix with all zeros above the diagonal is called an lower trianglular matrix \\[ \\begin{pmatrix} a & 0 & 0 \\\\ b & d & 0 \\\\ c & e & f \\end{pmatrix} \\] It is sometimes useful to chop a matrix up into multiple blocks, creating a block matrix \\[ \\begin{pmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{pmatrix} = \\begin{pmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{C} & \\mathbf{D} \\end{pmatrix} \\] where \\(\\mathbf{A}=\\begin{pmatrix} a & b \\\\ d & e\\end{pmatrix}\\) , \\(\\mathbf{B}=\\begin{pmatrix} c\\\\ f\\end{pmatrix}\\) , \\(\\mathbf{C}=\\begin{pmatrix} g & h \\end{pmatrix}\\) , and \\(\\mathbf{D}=\\begin{pmatrix} i\\end{pmatrix}\\) . This is especially helpful when the block form has off-diagonal submatrices consisting of all zeros. For instance, when \\(\\mathbf{B}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\\) or \\(\\mathbf{C}=\\begin{pmatrix} 0 & 0 \\end{pmatrix}\\) , we have a block triangular matrix . And when \\(\\mathbf{B}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\\) and \\(\\mathbf{C}=\\begin{pmatrix} 0 & 0 \\end{pmatrix}\\) , we have a block diagonal matrix . Finally, it is sometimes useful to transpose a matrix, which exchanges the rows and columns (an element in row \\(i\\) column \\(j\\) moves to row \\(j\\) column \\(i\\) ) \\[ \\begin{pmatrix} a_1 & a_2 & a_3 \\\\ b_1 & b_2 & b_3 \\end{pmatrix}^\\intercal = \\begin{pmatrix} a_1 & b_1 \\\\ a_2 & b_2 \\\\ a_3 & b_3 \\end{pmatrix} \\] Like vectors, matrices have a graphical/geometrical interpretation: they stretch and rotate vectors (as we will see shortly). 4. Vector and matrix operations Addition Vector and matrix addition (and subtraction) is straightforward, entry-by-entry: \\[ \\begin{equation*} \\begin{pmatrix} a \\\\ b \\end{pmatrix} + \\begin{pmatrix} c \\\\ d \\end{pmatrix} = \\begin{pmatrix} a+c \\\\ b+d \\end{pmatrix} \\end{equation*} \\] \\[ \\begin{equation*} \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} + \\begin{pmatrix} e & f \\\\ g & h \\end{pmatrix} = \\begin{pmatrix} a+e & b+f \\\\ c+g & d+h \\end{pmatrix} \\end{equation*} \\] Warning The vectors or matrices added together must have the same dimension! Geometrically, adding vectors is like placing the second vector at the end of the first. Below we add the black and red vectors together to get the blue vector. import matplotlib.pyplot as plt #import plotting library v1 = [1,2] #vector 1 v2 = [1,0] #vector 2 v12 = [i+j for i,j in zip(v1,v2)] #sum of the two vectors #first vector plt.arrow(0, 0, #starting x and y values of arrow v1[0], v1[1], #change in x and y head_width=0.1, color='black') #aesthetics #second vector placed at the end of first vector plt.arrow(v1[0], v1[1], #starting x and y values of arrow v2[0], v2[1], #change in x and y head_width=0.1, color='red') #aesthetics #sum of the vectors plt.arrow(0, 0, #starting x and y values of arrow v12[0], v12[1], #change in x and y head_width=0.1, color='blue') #aesthetics plt.xlim(0,2.5) #set bounds on x axis plt.ylim(0,2.5) #set bounds on y axis plt.show() Multiplication Vector and matrix multiplication by a scalar (which may be a constant, a variable, or a function, but not a matrix or a vector) is also straightforward, we just multiply every element by the scalar: \\[ \\begin{equation*} \\alpha * \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} \\alpha a \\\\ \\alpha b \\end{pmatrix} \\end{equation*} \\] \\[ \\begin{equation*} \\alpha * \\begin{pmatrix} a & b\\\\ c & d \\end{pmatrix} = \\begin{pmatrix} \\alpha a & \\alpha b\\\\ \\alpha c & \\alpha d \\end{pmatrix} \\end{equation*} \\] Geometrically, multiplying by a scalar stretches (if \\(\\alpha>1\\) ) or compresses (if \\(\\alpha<1\\) ) a vector. Below we multiply the black vector by \\(1/2\\) to get the red vector. import matplotlib.pyplot as plt #import plotting library v1 = [1,2] #vector 1 alpha = 1/2 #scalar v2 = [i*alpha for i in v1] #multiplication by a scalar #original vector plt.arrow(0, 0, #starting x and y values of arrow v1[0], v1[1], #change in x and y head_width=0.1, color='black') #aesthetics #stretched vector plt.arrow(0, 0, #starting x and y values of arrow v2[0], v2[1], #change in x and y head_width=0.1, color='red') #aesthetics plt.xlim(0,2.5) #set bounds on x axis plt.ylim(0,2.5) #set bounds on y axis plt.show() Multiplying vectors and matrices together is a bit trickier, but is based on the fact that a row vector times a column vector is equal to the sum of the products of their respective entries \\[ \\begin{equation*} \\begin{pmatrix} a & b & c \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = ax + by + cz \\end{equation*} \\] This is referred to as the dot product . (There are other types of products for vectors and matrices, which we won't cover in this class.) To multiply a matrix by a vector, this procedure is repeated first for the first row of the matrix, then for the second row of the matrix, etc: \\[ \\begin{equation*} \\begin{pmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} ax + by + cz \\\\ dx + ey + fz \\\\ gx + hy + iz \\\\ \\end{pmatrix} \\end{equation*} \\] Geometrically, multiplying a vector by a matrix stretches and rotates a vector. Below we multiply the black vector my a matrix to get the red vector. import matplotlib.pyplot as plt #import plotting library from sympy import * v = Matrix([[2],[1]]) #column vector M = Matrix([[1,-1],[1,1/4]]) #matrix u = M*v #original vector plt.arrow(0, 0, #starting x and y values of arrow float(v[0]), float(v[1]), #change in x and y head_width=0.1, color='black') #aesthetics #stretched and rotated vector plt.arrow(0, 0, #starting x and y values of arrow float(u[0]), float(u[1]), #change in x and y head_width=0.1, color='red') #aesthetics plt.xlim(0,2.5) #set bounds on x axis plt.ylim(0,2.5) #set bounds on y axis plt.show() To multiply a matrix by a matrix, this procedure is then repeated first for the first column of the second matrix and then for the second column of the second matrix, etc: \\[ \\begin{equation*} \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\begin{pmatrix} e & f \\\\ g & h \\end{pmatrix} = \\begin{pmatrix} ae + bg & af + bh \\\\ ce + dg & cf + dh \\end{pmatrix} \\end{equation*} \\] Warning An \\(m \\times n\\) matrix (or vector) \\(\\mathbf{A}\\) can be multiplied on the right by \\(\\mathbf{B}\\) only if \\(\\mathbf{B}\\) is an \\(n \\times p\\) matrix (or vector). The resulting matrix (or vector) will then be \\(m \\times p\\) . As opposed to basic algebra, matrix multiplication is not commutative. That is, \\(\\mathbf{AB}\\) does not generally equal \\(\\mathbf{BA}\\) . This means that if we want to multiply both sides of an equation, e.g., \\(\\mathbf{AB} = \\mathbf{C}\\) , by \\(\\mathbf{D}\\) , we need to do so on the same side, \\(\\mathbf{ABD} = \\mathbf{CD}\\) or \\(\\mathbf{DAB} = \\mathbf{DC}\\) . We therefore often need to specify that we are multiplying by a matrix \"on the left\" or \"on the right\". On the other hand, matrix multiplication does satisfy the following laws: \\((\\mathbf{AB})\\mathbf{C} = \\mathbf{A}(\\mathbf{BC})\\) (associative law) \\(\\mathbf{A}(\\mathbf{B+C}) = \\mathbf{AB}+\\mathbf{AC}\\) (distributive law) \\((\\mathbf{A}+\\mathbf{B})\\mathbf{C} = \\mathbf{AC}+\\mathbf{BC}\\) (distributive law) \\(\\alpha(\\mathbf{AB}) = (\\alpha\\mathbf{A})\\mathbf{B} = \\mathbf{A}(\\alpha\\mathbf{B}) = (\\mathbf{A}\\mathbf{B})\\alpha\\) (commutative law for scalars) Multiplication between the identity matrix and any vector, \\(\\vec{v}\\) , or square matrix, \\(\\mathbf{M}\\) , has no effect (it is like a \"1\" in normal algebra) \\[ \\mathbf{I}\\vec{v}=\\vec{v} \\] \\[ \\mathbf{I}\\mathbf{M}=\\mathbf{M}\\mathbf{I}=\\mathbf{M} \\]","title":"Lecture 10"},{"location":"lectures/lecture-10/#lecture-10-linear-algebra-i","text":"Run notes interactively?","title":"Lecture 10: Linear algebra I"},{"location":"lectures/lecture-10/#lecture-overview","text":"Motivation What are vectors? What is a matrix? Vector and matrix operations","title":"Lecture overview"},{"location":"lectures/lecture-10/#1-motivation","text":"Until now, we have been dealing with problems in a single variable changing over time. Often, dynamical systems involve more than one variable (ie, they are multivariate ). For instance, we may be interested in how the numbers of two species change as they interact (e.g., compete) with one another. As a simple example with more than one variable, consider a model tracking the number of birds on two islands. Let the number of birds on island 1 be \\(n_1\\) and let the number of birds on island 2 be \\(n_2\\) . We assume the birds migrate between the islands at per capita rates \\(m_{12}\\) and \\(m_{21}\\) , the birds on each island give birth at per capita rates \\(b_1\\) and \\(b_2\\) , the birds on each island die at per capita rates \\(d_1\\) and \\(d_2\\) , and new birds arrive on each island at rates \\(m_1\\) and \\(m_2\\) . This is captured in the following flow diagram graph LR; A1((n1)) --b1 n1--> A1; B1[ ] --m1--> A1; A1 --d1 n1--> C1[ ]; A2((n2)) --b2 n2--> A2; B2[ ] --m2--> A2; A2 --d2 n2--> C2[ ]; A1 --m12 n1--> A2; A2 --m21 n2--> A1; style B1 height:0px; style C1 height:0px; style B2 height:0px; style C2 height:0px; The rate of change in \\(n_1\\) and \\(n_2\\) are then described by the following system of differential equations \\[ \\begin{aligned} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &= (b_1 - d_1 - m_{12})n_1 + m_{21} n_2 + m_1 \\\\ \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &= m_{12} n_1 + (b_2 - d_2 - m_{21})n_2 + m_1 \\end{aligned} \\] These equations are linear functions of the variables (i.e., they contain only constant multiples of \\(n_1\\) and \\(n_2\\) and nothing more complicated such as \\(x^2\\) or \\(e^x\\) ). Linear systems of equations like these can also be written in matrix form \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} \\end{pmatrix} &= \\begin{pmatrix} b_1 - d_1 - m_{12} & m_{21} \\\\ m_{12} & b_2 - d_2 - m_{21} \\end{pmatrix} \\begin{pmatrix} n_1 \\\\ n_2 \\end{pmatrix} + \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}\\\\ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} &= \\mathbf{M}\\vec{n} + \\vec{m} \\end{aligned} \\] Not only is this a nice compact expression, there are rules of linear algebra that can help us conveniently solve this (and any other) set of linear equations. So let's get to know these rules.","title":"1. Motivation"},{"location":"lectures/lecture-10/#2-what-are-vectors","text":"Vectors are lists of elements (elements being numbers, parameters, functions, or variables). A column vector has elements arranged from top to bottom \\[ \\begin{equation*} \\begin{pmatrix} 5 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 1 \\\\ 5 \\\\ 9 \\\\ 7 \\end{pmatrix}, \\begin{pmatrix} x \\\\ y \\end{pmatrix}, \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}, \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\end{equation*} \\] A row vector has elements arranged from left to right \\[ \\begin{pmatrix}5 & 2\\end{pmatrix}, \\begin{pmatrix} 1 & 5 & 9 & 7\\end{pmatrix}, \\begin{pmatrix} x & y \\end{pmatrix}, \\begin{pmatrix} x & y & z\\end{pmatrix}, \\begin{pmatrix} x_1 & x_2 & \\cdots & x_n \\end{pmatrix} \\] We will indicate vectors by placing an arrow on top of the symbol \\[ \\vec{x} = \\begin{pmatrix} x_1 & x_2 & \\cdots & x_n \\end{pmatrix} \\] The number of elements in the vector indicates its dimension , \\(n\\) . For example, the row vector \\(\\begin{pmatrix}x & y\\end{pmatrix}\\) has dimension \\(n=2\\) . You can represent a vector as an arrow in \\(n\\) dimensions, connecting the origin with a point whose coordinates are given by elements in the vector. For example, the vector \\(\\vec{v} = \\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) can be depicted as below import matplotlib.pyplot as plt #import plotting library plt.arrow(0, 0, #starting x and y values of arrow 1, 2, #change in x and y head_width=0.1, color='black') #aesthetics plt.xlim(0,2.5) #set bounds on x axis plt.ylim(0,2.5) #set bounds on y axis plt.show()","title":"2. What are vectors?"},{"location":"lectures/lecture-10/#3-what-is-a-matrix","text":"An \\(m \\times n\\) matrix has \\(m\\) rows and \\(n\\) columns \\[ \\begin{equation*} \\begin{pmatrix} x_{11} & x_{12} & \\cdots & x_{1n}\\\\ x_{21} & x_{22} & \\cdots & x_{2n}\\\\ \\vdots & \\vdots & & \\vdots\\\\ x_{m1} & x_{m2} & \\cdots & x_{mn}\\\\ \\end{pmatrix}, \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}, \\begin{pmatrix} 75 & 67 \\\\ 66 & 34 \\\\ 12 & 14 \\end{pmatrix}, \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\end{equation*} \\] We will indicate matrices by bolding the symbol (and using capital letters) \\[ \\mathbf{X} = \\begin{pmatrix} x_{11} & x_{12} & \\cdots & x_{1n}\\\\ x_{21} & x_{22} & \\cdots & x_{2n}\\\\ \\vdots & \\vdots & & \\vdots\\\\ x_{m1} & x_{m2} & \\cdots & x_{mn}\\\\ \\end{pmatrix} \\] A matrix with an equal number of rows and columns, \\(m=n\\) , is a square matrix . A matrix with zeros everywhere except along the diagonal is called a diagonal matrix \\[ \\begin{pmatrix} a & 0 & 0 \\\\ 0 & b & 0 \\\\ 0 & 0 & c \\end{pmatrix} \\] And a special case of this with 1s along the diagonal is called the identity matrix \\[ \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\] A matrix with all zeros below the diagonal is called an upper trianglular matrix \\[ \\begin{pmatrix} a & b & c \\\\ 0 & d & e \\\\ 0 & 0 & f \\end{pmatrix} \\] A matrix with all zeros above the diagonal is called an lower trianglular matrix \\[ \\begin{pmatrix} a & 0 & 0 \\\\ b & d & 0 \\\\ c & e & f \\end{pmatrix} \\] It is sometimes useful to chop a matrix up into multiple blocks, creating a block matrix \\[ \\begin{pmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{pmatrix} = \\begin{pmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{C} & \\mathbf{D} \\end{pmatrix} \\] where \\(\\mathbf{A}=\\begin{pmatrix} a & b \\\\ d & e\\end{pmatrix}\\) , \\(\\mathbf{B}=\\begin{pmatrix} c\\\\ f\\end{pmatrix}\\) , \\(\\mathbf{C}=\\begin{pmatrix} g & h \\end{pmatrix}\\) , and \\(\\mathbf{D}=\\begin{pmatrix} i\\end{pmatrix}\\) . This is especially helpful when the block form has off-diagonal submatrices consisting of all zeros. For instance, when \\(\\mathbf{B}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\\) or \\(\\mathbf{C}=\\begin{pmatrix} 0 & 0 \\end{pmatrix}\\) , we have a block triangular matrix . And when \\(\\mathbf{B}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\\) and \\(\\mathbf{C}=\\begin{pmatrix} 0 & 0 \\end{pmatrix}\\) , we have a block diagonal matrix . Finally, it is sometimes useful to transpose a matrix, which exchanges the rows and columns (an element in row \\(i\\) column \\(j\\) moves to row \\(j\\) column \\(i\\) ) \\[ \\begin{pmatrix} a_1 & a_2 & a_3 \\\\ b_1 & b_2 & b_3 \\end{pmatrix}^\\intercal = \\begin{pmatrix} a_1 & b_1 \\\\ a_2 & b_2 \\\\ a_3 & b_3 \\end{pmatrix} \\] Like vectors, matrices have a graphical/geometrical interpretation: they stretch and rotate vectors (as we will see shortly).","title":"3. What is a matrix?"},{"location":"lectures/lecture-10/#4-vector-and-matrix-operations","text":"","title":"4. Vector and matrix operations"},{"location":"lectures/lecture-10/#addition","text":"Vector and matrix addition (and subtraction) is straightforward, entry-by-entry: \\[ \\begin{equation*} \\begin{pmatrix} a \\\\ b \\end{pmatrix} + \\begin{pmatrix} c \\\\ d \\end{pmatrix} = \\begin{pmatrix} a+c \\\\ b+d \\end{pmatrix} \\end{equation*} \\] \\[ \\begin{equation*} \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} + \\begin{pmatrix} e & f \\\\ g & h \\end{pmatrix} = \\begin{pmatrix} a+e & b+f \\\\ c+g & d+h \\end{pmatrix} \\end{equation*} \\] Warning The vectors or matrices added together must have the same dimension! Geometrically, adding vectors is like placing the second vector at the end of the first. Below we add the black and red vectors together to get the blue vector. import matplotlib.pyplot as plt #import plotting library v1 = [1,2] #vector 1 v2 = [1,0] #vector 2 v12 = [i+j for i,j in zip(v1,v2)] #sum of the two vectors #first vector plt.arrow(0, 0, #starting x and y values of arrow v1[0], v1[1], #change in x and y head_width=0.1, color='black') #aesthetics #second vector placed at the end of first vector plt.arrow(v1[0], v1[1], #starting x and y values of arrow v2[0], v2[1], #change in x and y head_width=0.1, color='red') #aesthetics #sum of the vectors plt.arrow(0, 0, #starting x and y values of arrow v12[0], v12[1], #change in x and y head_width=0.1, color='blue') #aesthetics plt.xlim(0,2.5) #set bounds on x axis plt.ylim(0,2.5) #set bounds on y axis plt.show()","title":"Addition"},{"location":"lectures/lecture-10/#multiplication","text":"Vector and matrix multiplication by a scalar (which may be a constant, a variable, or a function, but not a matrix or a vector) is also straightforward, we just multiply every element by the scalar: \\[ \\begin{equation*} \\alpha * \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} \\alpha a \\\\ \\alpha b \\end{pmatrix} \\end{equation*} \\] \\[ \\begin{equation*} \\alpha * \\begin{pmatrix} a & b\\\\ c & d \\end{pmatrix} = \\begin{pmatrix} \\alpha a & \\alpha b\\\\ \\alpha c & \\alpha d \\end{pmatrix} \\end{equation*} \\] Geometrically, multiplying by a scalar stretches (if \\(\\alpha>1\\) ) or compresses (if \\(\\alpha<1\\) ) a vector. Below we multiply the black vector by \\(1/2\\) to get the red vector. import matplotlib.pyplot as plt #import plotting library v1 = [1,2] #vector 1 alpha = 1/2 #scalar v2 = [i*alpha for i in v1] #multiplication by a scalar #original vector plt.arrow(0, 0, #starting x and y values of arrow v1[0], v1[1], #change in x and y head_width=0.1, color='black') #aesthetics #stretched vector plt.arrow(0, 0, #starting x and y values of arrow v2[0], v2[1], #change in x and y head_width=0.1, color='red') #aesthetics plt.xlim(0,2.5) #set bounds on x axis plt.ylim(0,2.5) #set bounds on y axis plt.show() Multiplying vectors and matrices together is a bit trickier, but is based on the fact that a row vector times a column vector is equal to the sum of the products of their respective entries \\[ \\begin{equation*} \\begin{pmatrix} a & b & c \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = ax + by + cz \\end{equation*} \\] This is referred to as the dot product . (There are other types of products for vectors and matrices, which we won't cover in this class.) To multiply a matrix by a vector, this procedure is repeated first for the first row of the matrix, then for the second row of the matrix, etc: \\[ \\begin{equation*} \\begin{pmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} ax + by + cz \\\\ dx + ey + fz \\\\ gx + hy + iz \\\\ \\end{pmatrix} \\end{equation*} \\] Geometrically, multiplying a vector by a matrix stretches and rotates a vector. Below we multiply the black vector my a matrix to get the red vector. import matplotlib.pyplot as plt #import plotting library from sympy import * v = Matrix([[2],[1]]) #column vector M = Matrix([[1,-1],[1,1/4]]) #matrix u = M*v #original vector plt.arrow(0, 0, #starting x and y values of arrow float(v[0]), float(v[1]), #change in x and y head_width=0.1, color='black') #aesthetics #stretched and rotated vector plt.arrow(0, 0, #starting x and y values of arrow float(u[0]), float(u[1]), #change in x and y head_width=0.1, color='red') #aesthetics plt.xlim(0,2.5) #set bounds on x axis plt.ylim(0,2.5) #set bounds on y axis plt.show() To multiply a matrix by a matrix, this procedure is then repeated first for the first column of the second matrix and then for the second column of the second matrix, etc: \\[ \\begin{equation*} \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\begin{pmatrix} e & f \\\\ g & h \\end{pmatrix} = \\begin{pmatrix} ae + bg & af + bh \\\\ ce + dg & cf + dh \\end{pmatrix} \\end{equation*} \\] Warning An \\(m \\times n\\) matrix (or vector) \\(\\mathbf{A}\\) can be multiplied on the right by \\(\\mathbf{B}\\) only if \\(\\mathbf{B}\\) is an \\(n \\times p\\) matrix (or vector). The resulting matrix (or vector) will then be \\(m \\times p\\) . As opposed to basic algebra, matrix multiplication is not commutative. That is, \\(\\mathbf{AB}\\) does not generally equal \\(\\mathbf{BA}\\) . This means that if we want to multiply both sides of an equation, e.g., \\(\\mathbf{AB} = \\mathbf{C}\\) , by \\(\\mathbf{D}\\) , we need to do so on the same side, \\(\\mathbf{ABD} = \\mathbf{CD}\\) or \\(\\mathbf{DAB} = \\mathbf{DC}\\) . We therefore often need to specify that we are multiplying by a matrix \"on the left\" or \"on the right\". On the other hand, matrix multiplication does satisfy the following laws: \\((\\mathbf{AB})\\mathbf{C} = \\mathbf{A}(\\mathbf{BC})\\) (associative law) \\(\\mathbf{A}(\\mathbf{B+C}) = \\mathbf{AB}+\\mathbf{AC}\\) (distributive law) \\((\\mathbf{A}+\\mathbf{B})\\mathbf{C} = \\mathbf{AC}+\\mathbf{BC}\\) (distributive law) \\(\\alpha(\\mathbf{AB}) = (\\alpha\\mathbf{A})\\mathbf{B} = \\mathbf{A}(\\alpha\\mathbf{B}) = (\\mathbf{A}\\mathbf{B})\\alpha\\) (commutative law for scalars) Multiplication between the identity matrix and any vector, \\(\\vec{v}\\) , or square matrix, \\(\\mathbf{M}\\) , has no effect (it is like a \"1\" in normal algebra) \\[ \\mathbf{I}\\vec{v}=\\vec{v} \\] \\[ \\mathbf{I}\\mathbf{M}=\\mathbf{M}\\mathbf{I}=\\mathbf{M} \\]","title":"Multiplication"},{"location":"lectures/lecture-11/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 11: Linear algebra II Run notes interactively? Lecture overview Matrix operations Solving systems of linear equations 1. Matrix operations Trace The trace of a matrix is the sum of the diagonal elements \\[ \\mathrm{Tr}\\left( \\begin{pmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\\\ \\end{pmatrix}\\right) = a + e + i \\] Determinant The determinant of a \\(2 \\times 2\\) matrix is: \\[ \\begin{equation*} \\text{Det}\\left( \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\right) = \\begin{vmatrix} a & b\\\\ c & d \\end{vmatrix} =ad-bc \\end{equation*} \\] Note You should remember how to calculate the determinant of a 2x2 matrix. The determinant of an \\(n \\times n\\) matrix can be obtained by working along the first row, multiplying the first element of the first row by the determinant of the matrix created by deleting the first row and first column minus the second element of the first row times the determinant of the matrix created by deleting the first row and second column plus the third element... and so on \\[ \\begin{vmatrix} \\mathbf{M} \\end{vmatrix} = \\sum_{j=1}^n (-1)^{j+1} m_{1j} \\begin{vmatrix} \\mathbf{M}_{1j} \\end{vmatrix} \\] where \\(m_{ij}\\) is the element in the \\(i^{\\mathrm{th}}\\) row and \\(j^{\\mathrm{th}}\\) column and \\(\\mathbf{M}_{ij}\\) is the matrix \\(\\mathbf{M}\\) with the \\(i^{\\mathrm{th}}\\) row and the \\(j^{\\mathrm{th}}\\) column deleted. More generally, we can move along any row \\(i\\) \\[ |\\mathbf{M}| = (-1)^{i+1}\\sum_{j=1}^{n}(-1)^{j+1}m_{ij} |\\mathbf{M_{ij}}| \\] or any column \\(j\\) \\[ |\\mathbf{M}| = (-1)^{j+1}\\sum_{i=1}^{n}(-1)^{i+1}m_{ij} |\\mathbf{M_{ij}}| \\] A few useful rules emerge from this: The determinant of a matrix is the same as the determinant of its transpose, \\(|\\mathbf{M}| = |\\mathbf{M}^\\intercal|\\) The determinant of a diagonal or triangular matrix is the product of the diagonal elements, \\(|\\mathbf{M}| = \\prod_{i=1}^n m_{ii} = m_{11}m_{22}\\cdots m_{nn}\\) The determinant of a block-diagonal or -triangular matrix is the product of the determinants of the diagonal submatrices It also suggests that rows or columns with lots of zeros are very helpful when calculating the determinant, for example \\[ \\begin{aligned} \\begin{vmatrix} m_{11} & 0 & 0 \\\\ m_{21} & m_{22} & m_{23} \\\\ m_{31} & m_{32} & m_{33} \\\\ \\end{vmatrix} = m_{11} \\begin{vmatrix} m_{22} & m_{23} \\\\ m_{32} & m_{33} \\\\ \\end{vmatrix}\\\\ \\end{aligned} \\] And why would we want to calculate the determinant of a matrix? Well, when the determinant is zero, \\(|\\mathbf{M}|=0\\) , it means that the rows are not linearly independent, that is, some row \\(\\vec{r}_k\\) can be written as \\(a_1 \\vec{r}_1 + \\cdots + a_{k-1} \\vec{r}_{k-1} + a_{k+1} \\vec{r}_{k+1} + \\cdots + a_n \\vec{r}_n\\) , where the \\(a_i\\) are scalars. As a result, when we multiply a vector by a matrix with a determinant of zero we lose some information, and therefore cannot reverse the operation (as we will see when we discuss inverses ). This is analagous to mutliplying by 0 in normal algebra -- if we multiply a bunch of different numbers by zero we have no way of knowing what the original numbers were. Geometrically, mutliplying multiple vectors by a matrix whose deteriminant is zero causes them to fall along a line. Below we multiply the two black vectors by a matrix whose determinant is zero to get the two red vectors, which line on the same line. matplotlib.pyplot as plt #import plotting library from sympy import * v1 = Matrix([[2],[1]]) #column vector 1 v2 = Matrix([[1],[1]]) #column vector 2 M = Matrix([[1/2,1],[1,2]]) #matrix with determinant of zero #original vectors for v in [v1,v2]: plt.arrow(0, 0, #starting x and y values of arrow float(v[0]), float(v[1]), #change in x and y head_width=0.1, color='black') #aesthetics #stretched and rotated vectors for v in [M*v1,M*v2]: plt.arrow(0, 0, #starting x and y values of arrow float(v[0]), float(v[1]), #change in x and y head_width=0.1, color='red') #aesthetics plt.xlim(0,5) #set bounds on x axis plt.ylim(0,5) #set bounds on y axis plt.show() Inverse In the last lecture we discussed matrix addition/subtraction and multiplication. We did not yet discuss division. In fact, for matrices, there is no such thing as division! The analogy is the inverse . A square \\(m\\times m\\) matrix \\(\\mathbf{M}\\) is invertible if it may be multiplied by another matrix to get the identity matrix. We call this second matrix, \\(\\mathbf{M}^{-1}\\) the inverse of the first \\[ \\mathbf{M}\\mathbf{M}^{-1} = \\mathbf{I} = \\mathbf{M}^{-1}\\mathbf{M} \\] Geometrically, the inverse reverses the stretching and rotating that the original matrix does to a vector \\[\\mathbf{M}^{-1}(\\mathbf{M}\\vec{v}) = (\\mathbf{M}^{-1}\\mathbf{M})\\vec{v} = \\mathbf{I}\\vec{v} = \\vec{v}\\] There are rules to find the inverse of a matrix (when it is invertible). For a 2x2 matrix we do the following \\[ \\begin{align} \\mathbf{M}^{-1} =&\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}^{-1}\\\\ &=\\frac{1}{\\mathrm{Det}(\\mathbf{M})} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}\\\\ &= \\begin{pmatrix} \\frac{d}{ad-bc} & \\frac{-b}{ad-bc} \\\\ \\frac{-c}{ad-bc} & \\frac{a}{ad-bc} \\end{pmatrix} \\end{align} \\] Note Remember how to take the inverse of a 2x2 matrix. Warning As you can see from the 2x2 case above, when the determinant is zero no inverse exists (and this is true for any square matrix). We therefore call a matrix whose determinant is zero non-invertible or singular . You can connect this fact back to the previous lecture, where we saw that mutliplying vectors by a matrix whose determinant is zero caused them to collapse upon one another, losing information. The fact that a matrix whose determinant is zero has no inverse means that we cannot reverse the original matrix multiplication, just like we can't reverse mutliplication by zero in normal algebra. Larger matrices are more difficult to invert, except if they are diagonal, in which case we simply invert each of the diagonal elements \\[ \\mathbf{M}^{-1} = \\begin{pmatrix} 1/m_{11} & 0 & \\cdots & 0\\\\ 0 & 1/m_{22} & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & 1/m_{nn}\\\\ \\end{pmatrix} \\] 2. Solving systems of linear equations With all this linear algebra knowledge in hand, let's use it! Let's return to our model for the number of birds on two islands graph LR; A1((n1)) --b1 n1--> A1; B1[ ] --m1--> A1; A1 --d1 n1--> C1[ ]; A2((n2)) --b2 n2--> A2; B2[ ] --m2--> A2; A2 --d2 n2--> C2[ ]; A1 --m12 n1--> A2; A2 --m21 n2--> A1; style B1 height:0px; style C1 height:0px; style B2 height:0px; style C2 height:0px; \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} \\end{pmatrix} &= \\begin{pmatrix} b_1 - d_1 - m_{12} & m_{21} \\\\ m_{12} & b_2 - d_2 - m_{21} \\end{pmatrix} \\begin{pmatrix} n_1 \\\\ n_2 \\end{pmatrix} + \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}\\\\ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} &= \\mathbf{M}\\vec{n} + \\vec{m} \\end{aligned} \\] The equilibria, \\(\\hat{\\vec{n}}\\) , are then found by setting \\(\\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t}=0\\) , subtracting \\(\\vec{m}\\) from both sides, and multiplying by the inverse matrix \\(\\mathbf{M}^{-1}\\) on the left \\[ \\begin{align*} 0 &= \\mathbf{M}\\hat{\\vec{n}} + \\vec{m}\\\\ -\\vec{m} &= \\mathbf{M}\\hat{\\vec{n}}\\\\ -\\mathbf{M}^{-1}\\vec{m} &= \\mathbf{M}^{-1}\\mathbf{M}\\hat{\\vec{n}}\\\\ -\\mathbf{M}^{-1}\\vec{m} &= \\hat{\\vec{n}} \\end{align*} \\] We can write the left hand side in terms of our parameters by calculating the inverse of this 2x2 matrix and multiplying by the vector \\[ \\begin{align} \\hat{\\vec{n}} &=-\\mathbf{M}^{-1}\\vec{m}\\\\ &=-\\frac{1}{|\\mathbf{M}|} \\begin{pmatrix} b_2 - d_2 - m_{21} & -m_{21} \\\\ -m_{12} & b_1 - d_1 - m_{12} \\end{pmatrix} \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}\\\\ &= -\\frac{1}{(b_1 - d_1 - m_{12})(b_2 - d_2 - m_{21})-m_{21}m_{12}} \\begin{pmatrix} (b_2 - d_2 - m_{21})m_1 -m_{21}m_2 \\\\ -m_{12}m_1 + (b_1 - d_1 - m_{12})m_2 \\end{pmatrix}\\\\ &= \\begin{pmatrix} -\\frac{(b_2 - d_2 - m_{21})m_1 -m_{21}m_2}{(b_1 - d_1 - m_{12})(b_2 - d_2 - m_{21})-m_{21}m_{12}} \\\\ -\\frac{-m_{12}m_1 + (b_1 - d_1 - m_{12})m_2}{(b_1 - d_1 - m_{12})(b_2 - d_2 - m_{21})-m_{21}m_{12}} \\end{pmatrix} \\end{align} \\] Ta-da! Using linear algebra we solved for both equilibria, \\(\\hat{n}_1\\) and \\(\\hat{n}_2\\) , with a single equation. In future lectures we'll see how we can use linear algebra to calculate the local stability and derive general solutions.","title":"Lecture 11"},{"location":"lectures/lecture-11/#lecture-11-linear-algebra-ii","text":"Run notes interactively?","title":"Lecture 11: Linear algebra II"},{"location":"lectures/lecture-11/#lecture-overview","text":"Matrix operations Solving systems of linear equations","title":"Lecture overview"},{"location":"lectures/lecture-11/#1-matrix-operations","text":"","title":"1. Matrix operations"},{"location":"lectures/lecture-11/#trace","text":"The trace of a matrix is the sum of the diagonal elements \\[ \\mathrm{Tr}\\left( \\begin{pmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\\\ \\end{pmatrix}\\right) = a + e + i \\]","title":"Trace"},{"location":"lectures/lecture-11/#determinant","text":"The determinant of a \\(2 \\times 2\\) matrix is: \\[ \\begin{equation*} \\text{Det}\\left( \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\right) = \\begin{vmatrix} a & b\\\\ c & d \\end{vmatrix} =ad-bc \\end{equation*} \\] Note You should remember how to calculate the determinant of a 2x2 matrix. The determinant of an \\(n \\times n\\) matrix can be obtained by working along the first row, multiplying the first element of the first row by the determinant of the matrix created by deleting the first row and first column minus the second element of the first row times the determinant of the matrix created by deleting the first row and second column plus the third element... and so on \\[ \\begin{vmatrix} \\mathbf{M} \\end{vmatrix} = \\sum_{j=1}^n (-1)^{j+1} m_{1j} \\begin{vmatrix} \\mathbf{M}_{1j} \\end{vmatrix} \\] where \\(m_{ij}\\) is the element in the \\(i^{\\mathrm{th}}\\) row and \\(j^{\\mathrm{th}}\\) column and \\(\\mathbf{M}_{ij}\\) is the matrix \\(\\mathbf{M}\\) with the \\(i^{\\mathrm{th}}\\) row and the \\(j^{\\mathrm{th}}\\) column deleted. More generally, we can move along any row \\(i\\) \\[ |\\mathbf{M}| = (-1)^{i+1}\\sum_{j=1}^{n}(-1)^{j+1}m_{ij} |\\mathbf{M_{ij}}| \\] or any column \\(j\\) \\[ |\\mathbf{M}| = (-1)^{j+1}\\sum_{i=1}^{n}(-1)^{i+1}m_{ij} |\\mathbf{M_{ij}}| \\] A few useful rules emerge from this: The determinant of a matrix is the same as the determinant of its transpose, \\(|\\mathbf{M}| = |\\mathbf{M}^\\intercal|\\) The determinant of a diagonal or triangular matrix is the product of the diagonal elements, \\(|\\mathbf{M}| = \\prod_{i=1}^n m_{ii} = m_{11}m_{22}\\cdots m_{nn}\\) The determinant of a block-diagonal or -triangular matrix is the product of the determinants of the diagonal submatrices It also suggests that rows or columns with lots of zeros are very helpful when calculating the determinant, for example \\[ \\begin{aligned} \\begin{vmatrix} m_{11} & 0 & 0 \\\\ m_{21} & m_{22} & m_{23} \\\\ m_{31} & m_{32} & m_{33} \\\\ \\end{vmatrix} = m_{11} \\begin{vmatrix} m_{22} & m_{23} \\\\ m_{32} & m_{33} \\\\ \\end{vmatrix}\\\\ \\end{aligned} \\] And why would we want to calculate the determinant of a matrix? Well, when the determinant is zero, \\(|\\mathbf{M}|=0\\) , it means that the rows are not linearly independent, that is, some row \\(\\vec{r}_k\\) can be written as \\(a_1 \\vec{r}_1 + \\cdots + a_{k-1} \\vec{r}_{k-1} + a_{k+1} \\vec{r}_{k+1} + \\cdots + a_n \\vec{r}_n\\) , where the \\(a_i\\) are scalars. As a result, when we multiply a vector by a matrix with a determinant of zero we lose some information, and therefore cannot reverse the operation (as we will see when we discuss inverses ). This is analagous to mutliplying by 0 in normal algebra -- if we multiply a bunch of different numbers by zero we have no way of knowing what the original numbers were. Geometrically, mutliplying multiple vectors by a matrix whose deteriminant is zero causes them to fall along a line. Below we multiply the two black vectors by a matrix whose determinant is zero to get the two red vectors, which line on the same line. matplotlib.pyplot as plt #import plotting library from sympy import * v1 = Matrix([[2],[1]]) #column vector 1 v2 = Matrix([[1],[1]]) #column vector 2 M = Matrix([[1/2,1],[1,2]]) #matrix with determinant of zero #original vectors for v in [v1,v2]: plt.arrow(0, 0, #starting x and y values of arrow float(v[0]), float(v[1]), #change in x and y head_width=0.1, color='black') #aesthetics #stretched and rotated vectors for v in [M*v1,M*v2]: plt.arrow(0, 0, #starting x and y values of arrow float(v[0]), float(v[1]), #change in x and y head_width=0.1, color='red') #aesthetics plt.xlim(0,5) #set bounds on x axis plt.ylim(0,5) #set bounds on y axis plt.show()","title":"Determinant"},{"location":"lectures/lecture-11/#inverse","text":"In the last lecture we discussed matrix addition/subtraction and multiplication. We did not yet discuss division. In fact, for matrices, there is no such thing as division! The analogy is the inverse . A square \\(m\\times m\\) matrix \\(\\mathbf{M}\\) is invertible if it may be multiplied by another matrix to get the identity matrix. We call this second matrix, \\(\\mathbf{M}^{-1}\\) the inverse of the first \\[ \\mathbf{M}\\mathbf{M}^{-1} = \\mathbf{I} = \\mathbf{M}^{-1}\\mathbf{M} \\] Geometrically, the inverse reverses the stretching and rotating that the original matrix does to a vector \\[\\mathbf{M}^{-1}(\\mathbf{M}\\vec{v}) = (\\mathbf{M}^{-1}\\mathbf{M})\\vec{v} = \\mathbf{I}\\vec{v} = \\vec{v}\\] There are rules to find the inverse of a matrix (when it is invertible). For a 2x2 matrix we do the following \\[ \\begin{align} \\mathbf{M}^{-1} =&\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}^{-1}\\\\ &=\\frac{1}{\\mathrm{Det}(\\mathbf{M})} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}\\\\ &= \\begin{pmatrix} \\frac{d}{ad-bc} & \\frac{-b}{ad-bc} \\\\ \\frac{-c}{ad-bc} & \\frac{a}{ad-bc} \\end{pmatrix} \\end{align} \\] Note Remember how to take the inverse of a 2x2 matrix. Warning As you can see from the 2x2 case above, when the determinant is zero no inverse exists (and this is true for any square matrix). We therefore call a matrix whose determinant is zero non-invertible or singular . You can connect this fact back to the previous lecture, where we saw that mutliplying vectors by a matrix whose determinant is zero caused them to collapse upon one another, losing information. The fact that a matrix whose determinant is zero has no inverse means that we cannot reverse the original matrix multiplication, just like we can't reverse mutliplication by zero in normal algebra. Larger matrices are more difficult to invert, except if they are diagonal, in which case we simply invert each of the diagonal elements \\[ \\mathbf{M}^{-1} = \\begin{pmatrix} 1/m_{11} & 0 & \\cdots & 0\\\\ 0 & 1/m_{22} & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & 1/m_{nn}\\\\ \\end{pmatrix} \\]","title":"Inverse"},{"location":"lectures/lecture-11/#2-solving-systems-of-linear-equations","text":"With all this linear algebra knowledge in hand, let's use it! Let's return to our model for the number of birds on two islands graph LR; A1((n1)) --b1 n1--> A1; B1[ ] --m1--> A1; A1 --d1 n1--> C1[ ]; A2((n2)) --b2 n2--> A2; B2[ ] --m2--> A2; A2 --d2 n2--> C2[ ]; A1 --m12 n1--> A2; A2 --m21 n2--> A1; style B1 height:0px; style C1 height:0px; style B2 height:0px; style C2 height:0px; \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} \\end{pmatrix} &= \\begin{pmatrix} b_1 - d_1 - m_{12} & m_{21} \\\\ m_{12} & b_2 - d_2 - m_{21} \\end{pmatrix} \\begin{pmatrix} n_1 \\\\ n_2 \\end{pmatrix} + \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}\\\\ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} &= \\mathbf{M}\\vec{n} + \\vec{m} \\end{aligned} \\] The equilibria, \\(\\hat{\\vec{n}}\\) , are then found by setting \\(\\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t}=0\\) , subtracting \\(\\vec{m}\\) from both sides, and multiplying by the inverse matrix \\(\\mathbf{M}^{-1}\\) on the left \\[ \\begin{align*} 0 &= \\mathbf{M}\\hat{\\vec{n}} + \\vec{m}\\\\ -\\vec{m} &= \\mathbf{M}\\hat{\\vec{n}}\\\\ -\\mathbf{M}^{-1}\\vec{m} &= \\mathbf{M}^{-1}\\mathbf{M}\\hat{\\vec{n}}\\\\ -\\mathbf{M}^{-1}\\vec{m} &= \\hat{\\vec{n}} \\end{align*} \\] We can write the left hand side in terms of our parameters by calculating the inverse of this 2x2 matrix and multiplying by the vector \\[ \\begin{align} \\hat{\\vec{n}} &=-\\mathbf{M}^{-1}\\vec{m}\\\\ &=-\\frac{1}{|\\mathbf{M}|} \\begin{pmatrix} b_2 - d_2 - m_{21} & -m_{21} \\\\ -m_{12} & b_1 - d_1 - m_{12} \\end{pmatrix} \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}\\\\ &= -\\frac{1}{(b_1 - d_1 - m_{12})(b_2 - d_2 - m_{21})-m_{21}m_{12}} \\begin{pmatrix} (b_2 - d_2 - m_{21})m_1 -m_{21}m_2 \\\\ -m_{12}m_1 + (b_1 - d_1 - m_{12})m_2 \\end{pmatrix}\\\\ &= \\begin{pmatrix} -\\frac{(b_2 - d_2 - m_{21})m_1 -m_{21}m_2}{(b_1 - d_1 - m_{12})(b_2 - d_2 - m_{21})-m_{21}m_{12}} \\\\ -\\frac{-m_{12}m_1 + (b_1 - d_1 - m_{12})m_2}{(b_1 - d_1 - m_{12})(b_2 - d_2 - m_{21})-m_{21}m_{12}} \\end{pmatrix} \\end{align} \\] Ta-da! Using linear algebra we solved for both equilibria, \\(\\hat{n}_1\\) and \\(\\hat{n}_2\\) , with a single equation. In future lectures we'll see how we can use linear algebra to calculate the local stability and derive general solutions.","title":"2. Solving systems of linear equations"},{"location":"lectures/lecture-12/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 12: Linear algebra III Run notes interactively? Lecture overview What are eigenvalues and eigenvectors? Why should be care about eigenvalues and eigenvectors? Finding eigenvalues Finding eigenvectors 1. What are eigenvalues and eigenvectors? A number \\(\\lambda\\) is an eigenvalue of matrix \\(\\textbf{M}\\) if there exists a non-zero vector, \\(\\vec{v}\\) , that satisfies \\[ \\mathbf{M} \\vec{v} = \\lambda \\vec{v} \\] Every non-zero vector \\(\\vec{v}\\) satisfying this equation is a right eigenvector of \\(\\mathbf{M}\\) associated with eigenvalue \\(\\lambda\\) . Every non-zero vector \\(\\vec{u}\\) satisfying \\[ \\vec{u}\\mathbf{M} = \\lambda \\vec{u} \\] is a left eigenvector of \\(\\mathbf{M}\\) associated with eigenvalue \\(\\lambda\\) . 2. Why should we care about eigenvalues and eigenvectors? Let's say \\(\\mathbf{M}\\) describes the dynamics of our biological variables, \\(\\vec{n}\\) , which might be the numbers of different types of individuals or the frequency of alleles at different loci, and for concreteness let's just say we are working in discrete time, \\(\\vec{n}(t+1) = \\mathbf{M} \\vec{n}(t)\\) (but similar arguments hold for continuous time). Now notice that if our system, \\(\\vec{n}(t)\\) , ever approaches a right eigenvalue, \\(\\vec{v}\\) , the dynamics reduce to simple exponential growth \\(\\vec{n}(t+1) = \\mathbf{M} \\vec{n}(t) = \\mathbf{M} \\vec{v} = \\lambda \\vec{v}\\) at rate \\(\\lambda\\) in direction \\(\\vec{v}\\) . In other words, the eigenvalues describe the rate at which our system grows or shrinks along their associated eigenvectors. More generally, even when \\(\\vec{n}\\) is not near a right eigenvector, the right eigenvectors provide a new coordinate system in which the dynamics of our system are easier to understand. To see this, let's take \\(\\mathbf{M} = \\begin{pmatrix} 1 & 1 \\\\ 1/2 & 3/2 \\end{pmatrix}\\) . The eigenvalues of \\(\\mathbf{M}\\) are \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 1/2\\) . The right eigenvectors associated with these two eigenvalues are \\(\\vec{v}_1 = \\begin{pmatrix} 1 \\\\ -1/2 \\end{pmatrix}\\) and \\(\\vec{v}_2 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) , respectively. The right eigenvectors are plotted as red vectors in the plot below. The initial state of the sytem, \\(\\vec{n}(0)\\) , is given by the blue vector. Thinking of the eigenvectors as a new coordinate system for our dynamics, we draw dashed gray lines that are parallel to the two eigenvectors, connecting \\(\\vec{n}(0)\\) to the two eigenvectors. Where these dashed gray lines intersect the eigenvalues indicates the value of \\(\\vec{n}(0)\\) in the new coordinate system. To calculate the state of the system in the next time step, \\(\\vec{n}(1) = \\mathbf{M}\\vec{n}(0)\\) , we can then simply multiply the values of \\(\\vec{n}(0)\\) in the new coordinate system by the eigenvalues. In this example, we multiply the distance of \\(\\vec{n}(0)\\) along the first eigenvector by \\(\\lambda_1\\) and the distance of \\(\\vec{n}(0)\\) along the second eigenvector by \\(\\lambda_2\\) , which gives \\(\\vec{n}(1)\\) in orange. We can continue doing this to calculate \\(\\vec{n}(2)\\) (plotted in green), and so on... Not only do the right eigenvectors define a more convenient coordinate system for our dynamics, you may also notice in the plot below that the system approaches one of the eigenvectors, suggesting that the long-term dynamics of the system can be predicted based on only one of the eigenvalues and it's associated right eigenvector (a fact we will later prove). import numpy as np import matplotlib.pyplot as plt # define objects M = np.array([[1,1],[1/2,3/2]]) #matrix e1, e2 = np.linalg.eigvals(M) #eigenvalues vs = np.linalg.eig(M)[1] #eigenvectors v1 = vs[:,0]; v1 = v1/v1[0] #first eigenvector normalized by first entry v2 = vs[:,1]; v2 = v2/v2[0] #second eigenvector normalized by first entry n0 = np.array([2/3,1/4]) #initial values of our variables n1 = M @ n0 #n1 = M*n0 n2 = M @ n1 #n2 = M*n1 # plot xmin,xmax = -0.1,1.5 #x limits ymin,ymax = -1,1.5 #y limits fig, ax = plt.subplots() # plot the eigenvectors for i,v in enumerate([v1,v2]): ax.plot([0,v[0]], # x values [0,v[1]], # y values c='r', label='eigenvector $v_{%d}$'%(i+1)) #aesthetics # plot the state of the system for i,n in enumerate([n0,n1,n2]): ax.plot([0,n[0]], [0,n[1]], label='$n(%d)$'%i) # parallel lines for a,b in [[v1,v2],[v2,v1]]: yshift = n[1] - a[1]/a[0] * n[0] xshift = yshift/(b[1]/b[0]-a[1]/a[0]) yshift = xshift * b[1]/b[0] ax.plot([0 + xshift, n[0]], [0 + yshift, n[1]], c='gray', ls='--', alpha=0.5) # aesthetics ax.axis('off') #remove frame ax.plot([0,0],[ymin,ymax], c='k') #x axis ax.plot([xmin,xmax],[0,0], c='k') # yaxis ax.legend() plt.show() 3. Finding eigenvalues To find the eigenvalues, first notice that if we try to use linear algebra to solve for the right eigenvector, \\(\\vec{v}\\) , we find \\[ \\begin{aligned} \\mathbf{M}\\vec{v} &= \\lambda\\vec{v}\\\\ \\mathbf{M}\\vec{v} - \\lambda\\vec{v} &= \\vec{0}\\\\ \\mathbf{M}\\vec{v} - \\lambda\\mathbf{I}\\vec{v} &= \\vec{0} \\\\ (\\mathbf{M} - \\lambda\\mathbf{I})\\vec{v} &= \\vec{0}\\\\ \\vec{v} &= (\\mathbf{M} - \\lambda\\mathbf{I})^{-1}\\vec{0} \\\\ \\vec{v} &= \\vec{0} \\end{aligned} \\] where \\(\\mathbf{I}\\) is the identity matrix and \\(\\vec{0}\\) is a vector of zeros. But above we've said that \\(\\vec{v}\\) is a non-zero vector! This is a contradiction. This contradiction implies that we did something wrong in our calculations. The only place we made any assumptions was in our last step, where we assumed \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) was invertible. We then conclude that \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) is non-invertible and therefore must have a determinant of zero, \\(|\\mathbf{M} - \\lambda\\mathbf{I}|=0\\) . Interestingly, this last equation, \\(|\\mathbf{M} - \\lambda\\mathbf{I}|=0\\) , gives us a way to solve for the eigenvalues, \\(\\lambda\\) , without knowing the eigenvectors, \\(\\vec{v}\\) ! The determinant of the \\(n\\times n\\) matrix \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) is an \\(n^{th}\\) degree polynomial in \\(\\lambda\\) , which is called the characteristic polynomial of \\(\\mathbf{M}\\) . Setting this polynomial equal to zero and solving for \\(\\lambda\\) gives the \\(n\\) eigenvalues of \\(\\mathbf{M}\\) : \\(\\lambda_1,\\lambda_2,...,\\lambda_n\\) . For example, in the \\(n=2\\) case we have \\[ \\begin{aligned} \\mathbf{M} - \\lambda \\mathbf{I} &= \\begin{pmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\\\\ &= \\begin{pmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{pmatrix} - \\begin{pmatrix} \\lambda & 0 \\\\ 0 & \\lambda \\end{pmatrix}\\\\ &= \\begin{pmatrix} m_{11} - \\lambda & m_{12} \\\\ m_{21} & m_{22} - \\lambda \\end{pmatrix} \\end{aligned} \\] so that the characteristic polynomial is \\[ \\begin{aligned} |\\mathbf{M} - \\lambda \\mathbf{I} | =& (m_{11}-\\lambda)(m_{22}-\\lambda)-m_{21} m_{12}\\\\ =&\\lambda^2 - (m_{11}+m_{22})\\lambda + (m_{11}m_{22}-m_{21}m_{12})\\\\ =&\\lambda^2 - \\mathrm{Tr}(\\mathbf{M})\\lambda + \\mathrm{Det}(\\mathbf{M}) \\end{aligned} \\] Setting this polynomial equal to zero, the two solutions can be found using the quadratic formula \\[ \\lambda = \\frac{\\mathrm{Tr}(\\mathbf{M}) \\pm \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4\\mathrm{Det}(\\mathbf{M})}}{2} \\] Note that this shows that even a 2x2 matrix composed of all real numbers can have complex eigenvalues if the value within the square root is negative, \\(\\mathrm{Tr}(\\mathbf{M})^2 - 4\\mathrm{Det}(\\mathbf{M}) < 0\\) . Note A complex number has the form \\(A + B i\\) , where \\(A\\) and \\(B\\) are real numbers and \\(i=\\sqrt{-1}\\) . We call \\(A\\) the \"real part\" and \\(B\\) the \"imaginary part\". See Box 7.3 in the text for more fun facts. For example, \\(\\mathbf{M} = \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix}\\) has \\(\\mathrm{Tr}(\\mathbf{M}) = 2\\) and \\(\\mathrm{Det}(\\mathbf{M}) = 2\\) , so that the eigenvalues are \\(\\lambda = 1 \\pm \\sqrt{-1} = 1 \\pm i\\) . We will see in future lectures that complex eigenvalues indicate some cycling in the dynamics of our system. The \\(n=2\\) example shows another interesting fact. When \\(\\mathrm{Det}(\\mathbf{M})=0\\) one of the eigenvalues is 0 (and this holds for larger \\(n\\) too). As we discussed in Lecture 11, if \\(\\mathrm{Det}(\\mathbf{M})=0\\) then multiplying a vector by \\(\\mathbf{M}\\) causes a loss of information (like multiplying by 0 in normal algebra). This can now be understood based on the geometric argument presented above: if one of the eigenvalues is zero then the state of the system immediately goes to zero in new coordinate system defined by the right eigenvectors. Finding the determinant of \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) becomes trickier for larger matrices, but there are some helpful properties of determinants that come in handy (see Lecture 11). For instance, the eigenvalues of a diagonal or triangular matrix are simply the diagonal elements \\[ \\begin{aligned} |\\mathbf{M} - \\lambda \\mathbf{I}| &= \\begin{vmatrix} m_{11} - \\lambda & 0 & 0 \\\\ m_{21} & m_{22} - \\lambda & 0 \\\\ m_{31} & m_{32} & m_{33} - \\lambda \\end{vmatrix}\\\\ &= (m_{11} - \\lambda) (m_{22} - \\lambda) (m_{33} - \\lambda) \\end{aligned} \\] Similarly, the eigenvalues of a block-diagonal or block-triangular matrix are the eigenvalues of the submatrices along the diagonal \\[ \\begin{aligned} \\mathbf{M} - \\lambda \\mathbf{I} &= \\begin{pmatrix} \\begin{pmatrix} m_{11} - \\lambda & 0 \\\\ m_{21} & m_{22} - \\lambda \\end{pmatrix} & \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\\\ \\begin{pmatrix} m_{31} & m_{32} \\end{pmatrix} & \\begin{pmatrix} m_{33} - \\lambda \\end{pmatrix} \\end{pmatrix}\\\\ |\\mathbf{M} - \\lambda \\mathbf{I}| &= \\begin{vmatrix} m_{11} - \\lambda & 0 \\\\ m_{21} & m_{22} - \\lambda \\end{vmatrix} \\begin{vmatrix} m_{33} - \\lambda \\end{vmatrix}\\\\ &= (m_{11} - \\lambda) (m_{22} - \\lambda) (m_{33} - \\lambda) \\end{aligned} \\] 4. Finding eigenvectors Now that we can find an eigenvalue, how do we find its eigenvectors? As we said above, if \\(\\vec{v}\\) is a right eigenvector of the matrix \\(\\mathbf{M}\\) corresponding to the eigenvalue \\(\\lambda\\) , it satisfies \\[ \\mathbf{M}\\vec{v} = \\lambda \\vec{v} \\] We would like to use linear algebra to solve for \\(\\vec{v}\\) from \\(\\mathbf{M}\\vec{v} = \\lambda \\vec{v}\\) , as we attempted above, but we can't since \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) is non-invertible. Instead we need to write out the system of equations represented by \\(\\mathbf{M}\\vec{v} = \\lambda \\vec{v}\\) and solve for one variable after another. For example, for a \\(2 \\times 2\\) matrix \\(\\mathbf{M}\\) with eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\) we know that a right eigenvector \\(\\vec{v}_1\\) associated with \\(\\lambda_1\\) must solve \\[ \\begin{aligned} \\mathbf{M}\\vec{v}_1 &= \\lambda_1 \\vec{v}_1\\\\ \\begin{pmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} &= \\lambda_1 \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} \\end{aligned} \\] Carrying out the matrix multiplication, we can write down a system of equations corresponding the the rows \\[ \\begin{aligned} m_{11} v_1 + m_{12} v_2 &= \\lambda_1 v_1 \\\\ m_{21} v_1 + m_{22} v_2 &= \\lambda_1 v_2 \\end{aligned} \\] This system of equations determines the elements of the right eigenvector, \\(\\vec{v}_1\\) , associated with \\(\\lambda_1\\) . Note from the matrix form above that we can multiply \\(\\vec{v}_1 = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}\\) by any constant and that will also be a solution. This means there are an infinite number of eigenvectors associated with an eigenvalue and we can set one of the elements to an arbitrary value. A typical choice is to set the first entry equal to one, \\(v_1 = 1\\) . Now we have just one unknown, \\(v_2\\) , so we can choose either of the equations above to solve for \\(v_2\\) . We pick the first, giving \\[ \\begin{aligned} m_{11} v_1 + m_{12} v_2 &= \\lambda_1 v_1 \\\\ m_{11} 1 + m_{12} v_2 &= \\lambda_1 1 \\\\ v_2 &= (\\lambda_1 - m_{11}) / m_{12} \\end{aligned} \\] We therefore have right eigenvector \\(\\vec{v}_1 = \\begin{pmatrix} 1 \\\\ (\\lambda_1 - m_{11})/m_{12} \\end{pmatrix}\\) associated with the eigenvalue \\(\\lambda_1\\) . Because we've done this quite generally, we also now know that the right eigenvector associated with the second eigenvalue, \\(\\lambda_2\\) , is \\(\\vec{v}_2 = \\begin{pmatrix} 1 \\\\ (\\lambda_2 - m_{11})/m_{12} \\end{pmatrix}\\) . Solving for the left eigenvectors is done following the same method. For eigenvalue \\(\\lambda_1\\) we want to find the vector \\(\\vec{u}_1\\) that solves \\[ \\begin{aligned} \\vec{u}_1\\mathbf{M} &= \\lambda_1 \\vec{u}_1\\\\ \\begin{pmatrix} u_1 & u_2 \\end{pmatrix} \\begin{pmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{pmatrix} &= \\lambda_1 \\begin{pmatrix} u_1 & u_2 \\end{pmatrix} \\end{aligned} \\] The system of equations is \\[ \\begin{aligned} u_1 m_{11} + u_2 m_{21} &= \\lambda_1 u_1 \\\\ u_1 m_{12} + u_2 m_{22} &= \\lambda_1 u_2 \\end{aligned} \\] Once again we can set the first element to any value, say \\(u_1 = 1\\) , and use one of the equations to solve for the second element \\[ \\begin{aligned} 1 m_{11} + u_2 m_{21} &= \\lambda_1 1 \\\\ u_2 m_{21} &= \\lambda_1 - m_{11} \\\\ u_2 &= (\\lambda_1 - m_{11})/m_{21} \\\\ \\end{aligned} \\] So the left eigenvector associated with eigenvalue \\(\\lambda_1\\) is \\(\\vec{u}_1 = \\begin{pmatrix} 1 & (\\lambda_1 - m_{11})/m_{21} \\end{pmatrix}\\) . Again, because we've done this quite generally, we know that the left eigenvector associated with \\(\\lambda_2\\) is \\(\\vec{u}_2 = \\begin{pmatrix} 1 & (\\lambda_2 - m_{11})/m_{21} \\end{pmatrix}\\) . If you'd like practice finding eigenvalues and eigenvectors, try finding the eigenvalues and eigenvectors of the matrix given in section 2, \\(\\mathbf{M} = \\begin{pmatrix} 1 & 1 \\\\ 1/2 & 3/2 \\end{pmatrix}\\) .","title":"Lecture 12"},{"location":"lectures/lecture-12/#lecture-12-linear-algebra-iii","text":"Run notes interactively?","title":"Lecture 12: Linear algebra III"},{"location":"lectures/lecture-12/#lecture-overview","text":"What are eigenvalues and eigenvectors? Why should be care about eigenvalues and eigenvectors? Finding eigenvalues Finding eigenvectors","title":"Lecture overview"},{"location":"lectures/lecture-12/#1-what-are-eigenvalues-and-eigenvectors","text":"A number \\(\\lambda\\) is an eigenvalue of matrix \\(\\textbf{M}\\) if there exists a non-zero vector, \\(\\vec{v}\\) , that satisfies \\[ \\mathbf{M} \\vec{v} = \\lambda \\vec{v} \\] Every non-zero vector \\(\\vec{v}\\) satisfying this equation is a right eigenvector of \\(\\mathbf{M}\\) associated with eigenvalue \\(\\lambda\\) . Every non-zero vector \\(\\vec{u}\\) satisfying \\[ \\vec{u}\\mathbf{M} = \\lambda \\vec{u} \\] is a left eigenvector of \\(\\mathbf{M}\\) associated with eigenvalue \\(\\lambda\\) .","title":"1. What are eigenvalues and eigenvectors?"},{"location":"lectures/lecture-12/#2-why-should-we-care-about-eigenvalues-and-eigenvectors","text":"Let's say \\(\\mathbf{M}\\) describes the dynamics of our biological variables, \\(\\vec{n}\\) , which might be the numbers of different types of individuals or the frequency of alleles at different loci, and for concreteness let's just say we are working in discrete time, \\(\\vec{n}(t+1) = \\mathbf{M} \\vec{n}(t)\\) (but similar arguments hold for continuous time). Now notice that if our system, \\(\\vec{n}(t)\\) , ever approaches a right eigenvalue, \\(\\vec{v}\\) , the dynamics reduce to simple exponential growth \\(\\vec{n}(t+1) = \\mathbf{M} \\vec{n}(t) = \\mathbf{M} \\vec{v} = \\lambda \\vec{v}\\) at rate \\(\\lambda\\) in direction \\(\\vec{v}\\) . In other words, the eigenvalues describe the rate at which our system grows or shrinks along their associated eigenvectors. More generally, even when \\(\\vec{n}\\) is not near a right eigenvector, the right eigenvectors provide a new coordinate system in which the dynamics of our system are easier to understand. To see this, let's take \\(\\mathbf{M} = \\begin{pmatrix} 1 & 1 \\\\ 1/2 & 3/2 \\end{pmatrix}\\) . The eigenvalues of \\(\\mathbf{M}\\) are \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = 1/2\\) . The right eigenvectors associated with these two eigenvalues are \\(\\vec{v}_1 = \\begin{pmatrix} 1 \\\\ -1/2 \\end{pmatrix}\\) and \\(\\vec{v}_2 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) , respectively. The right eigenvectors are plotted as red vectors in the plot below. The initial state of the sytem, \\(\\vec{n}(0)\\) , is given by the blue vector. Thinking of the eigenvectors as a new coordinate system for our dynamics, we draw dashed gray lines that are parallel to the two eigenvectors, connecting \\(\\vec{n}(0)\\) to the two eigenvectors. Where these dashed gray lines intersect the eigenvalues indicates the value of \\(\\vec{n}(0)\\) in the new coordinate system. To calculate the state of the system in the next time step, \\(\\vec{n}(1) = \\mathbf{M}\\vec{n}(0)\\) , we can then simply multiply the values of \\(\\vec{n}(0)\\) in the new coordinate system by the eigenvalues. In this example, we multiply the distance of \\(\\vec{n}(0)\\) along the first eigenvector by \\(\\lambda_1\\) and the distance of \\(\\vec{n}(0)\\) along the second eigenvector by \\(\\lambda_2\\) , which gives \\(\\vec{n}(1)\\) in orange. We can continue doing this to calculate \\(\\vec{n}(2)\\) (plotted in green), and so on... Not only do the right eigenvectors define a more convenient coordinate system for our dynamics, you may also notice in the plot below that the system approaches one of the eigenvectors, suggesting that the long-term dynamics of the system can be predicted based on only one of the eigenvalues and it's associated right eigenvector (a fact we will later prove). import numpy as np import matplotlib.pyplot as plt # define objects M = np.array([[1,1],[1/2,3/2]]) #matrix e1, e2 = np.linalg.eigvals(M) #eigenvalues vs = np.linalg.eig(M)[1] #eigenvectors v1 = vs[:,0]; v1 = v1/v1[0] #first eigenvector normalized by first entry v2 = vs[:,1]; v2 = v2/v2[0] #second eigenvector normalized by first entry n0 = np.array([2/3,1/4]) #initial values of our variables n1 = M @ n0 #n1 = M*n0 n2 = M @ n1 #n2 = M*n1 # plot xmin,xmax = -0.1,1.5 #x limits ymin,ymax = -1,1.5 #y limits fig, ax = plt.subplots() # plot the eigenvectors for i,v in enumerate([v1,v2]): ax.plot([0,v[0]], # x values [0,v[1]], # y values c='r', label='eigenvector $v_{%d}$'%(i+1)) #aesthetics # plot the state of the system for i,n in enumerate([n0,n1,n2]): ax.plot([0,n[0]], [0,n[1]], label='$n(%d)$'%i) # parallel lines for a,b in [[v1,v2],[v2,v1]]: yshift = n[1] - a[1]/a[0] * n[0] xshift = yshift/(b[1]/b[0]-a[1]/a[0]) yshift = xshift * b[1]/b[0] ax.plot([0 + xshift, n[0]], [0 + yshift, n[1]], c='gray', ls='--', alpha=0.5) # aesthetics ax.axis('off') #remove frame ax.plot([0,0],[ymin,ymax], c='k') #x axis ax.plot([xmin,xmax],[0,0], c='k') # yaxis ax.legend() plt.show()","title":"2. Why should we care about eigenvalues and eigenvectors?"},{"location":"lectures/lecture-12/#3-finding-eigenvalues","text":"To find the eigenvalues, first notice that if we try to use linear algebra to solve for the right eigenvector, \\(\\vec{v}\\) , we find \\[ \\begin{aligned} \\mathbf{M}\\vec{v} &= \\lambda\\vec{v}\\\\ \\mathbf{M}\\vec{v} - \\lambda\\vec{v} &= \\vec{0}\\\\ \\mathbf{M}\\vec{v} - \\lambda\\mathbf{I}\\vec{v} &= \\vec{0} \\\\ (\\mathbf{M} - \\lambda\\mathbf{I})\\vec{v} &= \\vec{0}\\\\ \\vec{v} &= (\\mathbf{M} - \\lambda\\mathbf{I})^{-1}\\vec{0} \\\\ \\vec{v} &= \\vec{0} \\end{aligned} \\] where \\(\\mathbf{I}\\) is the identity matrix and \\(\\vec{0}\\) is a vector of zeros. But above we've said that \\(\\vec{v}\\) is a non-zero vector! This is a contradiction. This contradiction implies that we did something wrong in our calculations. The only place we made any assumptions was in our last step, where we assumed \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) was invertible. We then conclude that \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) is non-invertible and therefore must have a determinant of zero, \\(|\\mathbf{M} - \\lambda\\mathbf{I}|=0\\) . Interestingly, this last equation, \\(|\\mathbf{M} - \\lambda\\mathbf{I}|=0\\) , gives us a way to solve for the eigenvalues, \\(\\lambda\\) , without knowing the eigenvectors, \\(\\vec{v}\\) ! The determinant of the \\(n\\times n\\) matrix \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) is an \\(n^{th}\\) degree polynomial in \\(\\lambda\\) , which is called the characteristic polynomial of \\(\\mathbf{M}\\) . Setting this polynomial equal to zero and solving for \\(\\lambda\\) gives the \\(n\\) eigenvalues of \\(\\mathbf{M}\\) : \\(\\lambda_1,\\lambda_2,...,\\lambda_n\\) . For example, in the \\(n=2\\) case we have \\[ \\begin{aligned} \\mathbf{M} - \\lambda \\mathbf{I} &= \\begin{pmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\\\\ &= \\begin{pmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{pmatrix} - \\begin{pmatrix} \\lambda & 0 \\\\ 0 & \\lambda \\end{pmatrix}\\\\ &= \\begin{pmatrix} m_{11} - \\lambda & m_{12} \\\\ m_{21} & m_{22} - \\lambda \\end{pmatrix} \\end{aligned} \\] so that the characteristic polynomial is \\[ \\begin{aligned} |\\mathbf{M} - \\lambda \\mathbf{I} | =& (m_{11}-\\lambda)(m_{22}-\\lambda)-m_{21} m_{12}\\\\ =&\\lambda^2 - (m_{11}+m_{22})\\lambda + (m_{11}m_{22}-m_{21}m_{12})\\\\ =&\\lambda^2 - \\mathrm{Tr}(\\mathbf{M})\\lambda + \\mathrm{Det}(\\mathbf{M}) \\end{aligned} \\] Setting this polynomial equal to zero, the two solutions can be found using the quadratic formula \\[ \\lambda = \\frac{\\mathrm{Tr}(\\mathbf{M}) \\pm \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4\\mathrm{Det}(\\mathbf{M})}}{2} \\] Note that this shows that even a 2x2 matrix composed of all real numbers can have complex eigenvalues if the value within the square root is negative, \\(\\mathrm{Tr}(\\mathbf{M})^2 - 4\\mathrm{Det}(\\mathbf{M}) < 0\\) . Note A complex number has the form \\(A + B i\\) , where \\(A\\) and \\(B\\) are real numbers and \\(i=\\sqrt{-1}\\) . We call \\(A\\) the \"real part\" and \\(B\\) the \"imaginary part\". See Box 7.3 in the text for more fun facts. For example, \\(\\mathbf{M} = \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix}\\) has \\(\\mathrm{Tr}(\\mathbf{M}) = 2\\) and \\(\\mathrm{Det}(\\mathbf{M}) = 2\\) , so that the eigenvalues are \\(\\lambda = 1 \\pm \\sqrt{-1} = 1 \\pm i\\) . We will see in future lectures that complex eigenvalues indicate some cycling in the dynamics of our system. The \\(n=2\\) example shows another interesting fact. When \\(\\mathrm{Det}(\\mathbf{M})=0\\) one of the eigenvalues is 0 (and this holds for larger \\(n\\) too). As we discussed in Lecture 11, if \\(\\mathrm{Det}(\\mathbf{M})=0\\) then multiplying a vector by \\(\\mathbf{M}\\) causes a loss of information (like multiplying by 0 in normal algebra). This can now be understood based on the geometric argument presented above: if one of the eigenvalues is zero then the state of the system immediately goes to zero in new coordinate system defined by the right eigenvectors. Finding the determinant of \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) becomes trickier for larger matrices, but there are some helpful properties of determinants that come in handy (see Lecture 11). For instance, the eigenvalues of a diagonal or triangular matrix are simply the diagonal elements \\[ \\begin{aligned} |\\mathbf{M} - \\lambda \\mathbf{I}| &= \\begin{vmatrix} m_{11} - \\lambda & 0 & 0 \\\\ m_{21} & m_{22} - \\lambda & 0 \\\\ m_{31} & m_{32} & m_{33} - \\lambda \\end{vmatrix}\\\\ &= (m_{11} - \\lambda) (m_{22} - \\lambda) (m_{33} - \\lambda) \\end{aligned} \\] Similarly, the eigenvalues of a block-diagonal or block-triangular matrix are the eigenvalues of the submatrices along the diagonal \\[ \\begin{aligned} \\mathbf{M} - \\lambda \\mathbf{I} &= \\begin{pmatrix} \\begin{pmatrix} m_{11} - \\lambda & 0 \\\\ m_{21} & m_{22} - \\lambda \\end{pmatrix} & \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\\\ \\begin{pmatrix} m_{31} & m_{32} \\end{pmatrix} & \\begin{pmatrix} m_{33} - \\lambda \\end{pmatrix} \\end{pmatrix}\\\\ |\\mathbf{M} - \\lambda \\mathbf{I}| &= \\begin{vmatrix} m_{11} - \\lambda & 0 \\\\ m_{21} & m_{22} - \\lambda \\end{vmatrix} \\begin{vmatrix} m_{33} - \\lambda \\end{vmatrix}\\\\ &= (m_{11} - \\lambda) (m_{22} - \\lambda) (m_{33} - \\lambda) \\end{aligned} \\]","title":"3. Finding eigenvalues"},{"location":"lectures/lecture-12/#4-finding-eigenvectors","text":"Now that we can find an eigenvalue, how do we find its eigenvectors? As we said above, if \\(\\vec{v}\\) is a right eigenvector of the matrix \\(\\mathbf{M}\\) corresponding to the eigenvalue \\(\\lambda\\) , it satisfies \\[ \\mathbf{M}\\vec{v} = \\lambda \\vec{v} \\] We would like to use linear algebra to solve for \\(\\vec{v}\\) from \\(\\mathbf{M}\\vec{v} = \\lambda \\vec{v}\\) , as we attempted above, but we can't since \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) is non-invertible. Instead we need to write out the system of equations represented by \\(\\mathbf{M}\\vec{v} = \\lambda \\vec{v}\\) and solve for one variable after another. For example, for a \\(2 \\times 2\\) matrix \\(\\mathbf{M}\\) with eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\) we know that a right eigenvector \\(\\vec{v}_1\\) associated with \\(\\lambda_1\\) must solve \\[ \\begin{aligned} \\mathbf{M}\\vec{v}_1 &= \\lambda_1 \\vec{v}_1\\\\ \\begin{pmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} &= \\lambda_1 \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} \\end{aligned} \\] Carrying out the matrix multiplication, we can write down a system of equations corresponding the the rows \\[ \\begin{aligned} m_{11} v_1 + m_{12} v_2 &= \\lambda_1 v_1 \\\\ m_{21} v_1 + m_{22} v_2 &= \\lambda_1 v_2 \\end{aligned} \\] This system of equations determines the elements of the right eigenvector, \\(\\vec{v}_1\\) , associated with \\(\\lambda_1\\) . Note from the matrix form above that we can multiply \\(\\vec{v}_1 = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}\\) by any constant and that will also be a solution. This means there are an infinite number of eigenvectors associated with an eigenvalue and we can set one of the elements to an arbitrary value. A typical choice is to set the first entry equal to one, \\(v_1 = 1\\) . Now we have just one unknown, \\(v_2\\) , so we can choose either of the equations above to solve for \\(v_2\\) . We pick the first, giving \\[ \\begin{aligned} m_{11} v_1 + m_{12} v_2 &= \\lambda_1 v_1 \\\\ m_{11} 1 + m_{12} v_2 &= \\lambda_1 1 \\\\ v_2 &= (\\lambda_1 - m_{11}) / m_{12} \\end{aligned} \\] We therefore have right eigenvector \\(\\vec{v}_1 = \\begin{pmatrix} 1 \\\\ (\\lambda_1 - m_{11})/m_{12} \\end{pmatrix}\\) associated with the eigenvalue \\(\\lambda_1\\) . Because we've done this quite generally, we also now know that the right eigenvector associated with the second eigenvalue, \\(\\lambda_2\\) , is \\(\\vec{v}_2 = \\begin{pmatrix} 1 \\\\ (\\lambda_2 - m_{11})/m_{12} \\end{pmatrix}\\) . Solving for the left eigenvectors is done following the same method. For eigenvalue \\(\\lambda_1\\) we want to find the vector \\(\\vec{u}_1\\) that solves \\[ \\begin{aligned} \\vec{u}_1\\mathbf{M} &= \\lambda_1 \\vec{u}_1\\\\ \\begin{pmatrix} u_1 & u_2 \\end{pmatrix} \\begin{pmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{pmatrix} &= \\lambda_1 \\begin{pmatrix} u_1 & u_2 \\end{pmatrix} \\end{aligned} \\] The system of equations is \\[ \\begin{aligned} u_1 m_{11} + u_2 m_{21} &= \\lambda_1 u_1 \\\\ u_1 m_{12} + u_2 m_{22} &= \\lambda_1 u_2 \\end{aligned} \\] Once again we can set the first element to any value, say \\(u_1 = 1\\) , and use one of the equations to solve for the second element \\[ \\begin{aligned} 1 m_{11} + u_2 m_{21} &= \\lambda_1 1 \\\\ u_2 m_{21} &= \\lambda_1 - m_{11} \\\\ u_2 &= (\\lambda_1 - m_{11})/m_{21} \\\\ \\end{aligned} \\] So the left eigenvector associated with eigenvalue \\(\\lambda_1\\) is \\(\\vec{u}_1 = \\begin{pmatrix} 1 & (\\lambda_1 - m_{11})/m_{21} \\end{pmatrix}\\) . Again, because we've done this quite generally, we know that the left eigenvector associated with \\(\\lambda_2\\) is \\(\\vec{u}_2 = \\begin{pmatrix} 1 & (\\lambda_2 - m_{11})/m_{21} \\end{pmatrix}\\) . If you'd like practice finding eigenvalues and eigenvectors, try finding the eigenvalues and eigenvectors of the matrix given in section 2, \\(\\mathbf{M} = \\begin{pmatrix} 1 & 1 \\\\ 1/2 & 3/2 \\end{pmatrix}\\) .","title":"4. Finding eigenvectors"},{"location":"lectures/lecture-13/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 13: General solutions (linear multivariate) Run notes interactively? Lecture overview Discrete time Continuous time 1. Discrete time Motivating example Let's return to our model of the number of birds on two islands from Lecture 10, which was the motivation for learning linear algebra in the first place. Let's adjust the model slightly, removing foreign immigration and switching to discrete time (assuming migration, then birth, then death). graph LR; A1((n1)) --b1 n1--> A1; A1 --d1 n1--> C1[ ]; A2((n2)) --b2 n2--> A2; A2 --d2 n2--> C2[ ]; A1 --m12 n1--> A2; A2 --m21 n2--> A1; style C1 height:0px; style C2 height:0px; The number of birds on the two islands in the next time step are then \\[ \\begin{aligned} n_1(t+1) &= (n_1(t)(1-m_{12}) + n_2(t)m_{21})(1+b_1)(1-d_1) \\\\ n_2(t+1) &= (n_1(t)m_{12} + n_2(t)(1-m_{21}))(1+b_2)(1-d_2) \\\\ \\end{aligned} \\] As we noted in Lecture 10, we can write a system of linear equations (in this case recursion equations) in matrix form \\[ \\begin{aligned} \\vec{n}(t+1) &= \\mathbf{M}\\vec{n}(t) \\end{aligned} \\] where in this example \\[ \\mathbf{M} = \\begin{pmatrix} (1-m_{12})(1+b_1)(1-d_1) & m_{21}(1+b_1)(1-d_1) \\\\ m_{12}(1+b_2)(1-d_2) & (1-m_{21})(1+b_2)(1-d_2) \\end{pmatrix} \\] The question we now want to answer is, how do the numbers of birds on the two islands change over time? General formulation Instead of analyzing this specific model, let's investigate the dynamics of any system of linear equations in discrete time. We will then return to our motivating example. If there are \\(n\\) variables to keep track of, \\(x_1\\) , \\(x_2\\) , ..., \\(x_n\\) , then there will be \\(n\\) recursion equations \\[ \\begin{aligned} x_1(t+1) &= m_{11} x_1(t) + m_{12} x_2(t) + \\cdots + m_{1n} x_n(t) \\\\ x_2(t+1) &= m_{21} x_1(t) + m_{22} x_2(t) + \\cdots + m_{2n} x_n(t) \\\\ &\\vdots \\\\ x_n(t+1) &= m_{n1} x_1(t) + m_{n2} x_2(t) + \\cdots + m_{nn} x_n(t) \\end{aligned} \\] e.g., in the motivating example above we have \\(n=2\\) . These equations can be written in matrix form \\[ \\begin{aligned} \\begin{pmatrix} x_1(t+1) \\\\ x_2(t+1) \\\\ \\vdots \\\\ x_n(t+1) \\end{pmatrix} &= \\begin{pmatrix} m_{11} & m_{12} & \\cdots & m_{1n} \\\\ m_{21} & m_{22} & \\cdots & m_{2n}\\\\ \\vdots & \\vdots & & \\vdots\\\\ m_{n1} & m_{n2} & \\cdots & m_{nn} \\end{pmatrix} \\begin{pmatrix} x_1(t) \\\\ x_2(t) \\\\ \\vdots \\\\ x_n(t) \\end{pmatrix}\\\\ \\vec{x}(t+1) &= \\mathbf{M} \\vec{x}(t) \\end{aligned} \\] How does \\(\\vec{x}\\) change over time? General solution Since this is just a multivariate version of exponential growth, we can derive the general solution by brute force iteration \\[ \\begin{aligned} \\vec{x}(t) &= \\mathbf{M}\\vec{x}(t-1)\\\\ &= \\mathbf{M}^2\\vec{x}(t-2)\\\\ & \\vdots \\\\ &= \\mathbf{M}^t\\vec{x}(0) \\end{aligned} \\] However, in most cases it will be hard to compute \\(\\mathbf{M}^t\\) (if you don't believe me, try calculating even just \\(\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}^3\\) ). Fortunately there is a trick involving eigenvalues and eigenvectors. Recall the equation for the eigenvalues, \\(\\lambda\\) , and right eigenvectors, \\(\\vec{v}\\) , \\[ \\mathbf{M} \\vec{v} = \\lambda \\vec{v} \\] For our \\(n\\) -dimensional model, there will often be \\(n\\) eigenvalues and eigenvectors that satisfy this equation (sometimes the \\(n\\) eigenvalues are not all distinct). We can actually write all \\(n\\) of these equations in matrix form \\[ \\begin{aligned} \\mathbf{M} \\begin{pmatrix} \\vec{v}_1 & \\vec{v}_2 & \\cdots & \\vec{v}_n \\end{pmatrix} &= \\begin{pmatrix} \\lambda_1 \\vec{v}_1 & \\lambda_2 \\vec{v}_2 & \\cdots & \\lambda_n \\vec{v}_n \\end{pmatrix}\\\\ \\mathbf{M} \\mathbf{A} &= \\mathbf{A} \\mathbf{D} \\end{aligned} \\] where the columns of \\(\\mathbf{A}\\) are the right eigenvectors and \\(\\mathbf{D}\\) is a diagonal matrix of the eigenvalues \\[ \\mathbf{D} = \\begin{pmatrix} \\lambda_1 & 0 & \\cdots & 0\\\\ 0 & \\lambda_2 & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & \\lambda_n\\\\ \\end{pmatrix} \\] Now here is the trick: multiply both sides of \\(\\mathbf{M} \\mathbf{A} = \\mathbf{A} \\mathbf{D}\\) by \\(\\mathbf{A}^{-1}\\) on the right \\[ \\begin{aligned} \\mathbf{M} \\mathbf{A} &= \\mathbf{A} \\mathbf{D}\\\\ \\mathbf{M} \\mathbf{A}\\mathbf{A}^{-1} &= \\mathbf{A} \\mathbf{D}\\mathbf{A}^{-1}\\\\ \\mathbf{M} &= \\mathbf{A} \\mathbf{D} \\mathbf{A}^{-1} \\end{aligned} \\] Note We can only take the inverse of our \\(n\\times n\\) matrix of right eigenvectors, \\(\\mathbf{A}\\) , when \\(\\mathbf{A}\\) is invertible, which requires the determinant be non-zero or, equivalently, the rows (or columns) of \\(\\mathbf{A}\\) to be linearly independent. Fortunately, in many cases the columns (the right eigenvectors) will be linearly independent. However, sometimes the right eigenvectors will not be linearly independent, which only occurs when there are less than \\(n\\) distinct eigenvalues. When the right eigenvectors of \\(\\mathbf{M}\\) are not linearly independent we call \\(\\mathbf{M}\\) defective . In that case we need to derive the general solution in another way, but we won't deal with that in this class. Subbing this alternate version of \\(\\mathbf{M}\\) into our general solution above, we see that most of the \\(\\mathbf{A}\\) and \\(\\mathbf{A}^{-1}\\) matrices cancel, leaving us with \\[ \\begin{aligned} \\vec{x}(t) &= \\mathbf{M}^t\\vec{x}(0)\\\\ &= (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1})^t\\vec{x}(0)\\\\ &= (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) \\cdots (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) \\vec{x}(0)\\\\ &= \\mathbf{A}\\mathbf{D}(\\mathbf{A}^{-1} \\mathbf{A})\\mathbf{D}(\\mathbf{A}^{-1}\\mathbf{A}) \\cdots (\\mathbf{A}^{-1}\\mathbf{A})\\mathbf{D}\\mathbf{A}^{-1} \\vec{x}(0)\\\\ &= \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1} \\vec{x}(0) \\end{aligned} \\] And this is great because \\(\\mathbf{D}\\) is a diagonal matrix, meaning \\(\\mathbf{D}^t\\) is easy to calculate \\[ \\mathbf{D}^t = \\begin{pmatrix} \\lambda_1^t & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2^t & \\cdots & 0\\\\ \\vdots & \\vdots & & \\vdots\\\\ 0 & 0 & \\cdots & \\lambda_n^t \\end{pmatrix} \\] It would not have been so easy to find \\(\\mathbf{M}^t\\) ! Note that \\(\\mathbf{A}^{-1}\\) is a matrix whose rows are the left eigenvectors of \\(\\textbf{M}\\) , which can be seen by deriving \\(\\mathbf{M} = \\mathbf{A} \\mathbf{D}\\mathbf{A}^{-1}\\) starting with the equation for the left eigenvectors \\(\\vec{u}\\mathbf{M}=\\lambda\\vec{u}\\) . Note Another way to arrive at this general solition, \\(\\vec{x}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{x}(0)\\) , is to consider a transformation from our current coordinate system, \\(\\vec{x}\\) , to another defined by \\(\\vec{y} = \\mathbf{A}^{-1}\\vec{x}\\) . If we then attempt to derive a recursion in our new coordinate system we find \\[ \\begin{aligned} \\vec{y}(t+1) &= \\mathbf{A}^{-1}\\vec{x}(t+1)\\\\ &= \\mathbf{A}^{-1}\\mathbf{M}\\vec{x}(t)\\\\ &= \\mathbf{A}^{-1}\\mathbf{M}\\mathbf{A}\\vec{y}(t)\\\\ &= \\mathbf{A}^{-1}(\\mathbf{A} \\mathbf{D} \\mathbf{A}^{-1})\\mathbf{A}\\vec{y}(t)\\\\ &= \\mathbf{D}\\vec{y}(t) \\end{aligned} \\] Voila! In the new coordinate system we just move along each axis by a factor \\(\\lambda_i\\) , just as we saw in Lecture 12. In this way the eigenvectors form a new, more convenient, coordinate system. To convert back to our original coordinate system we multiply both sides of the equation by \\(\\mathbf{A}\\) on the left and then use \\(\\vec{y} = \\mathbf{A}^{-1}\\vec{x}\\) \\[ \\begin{aligned} \\mathbf{A}\\vec{y}(t+1) &= \\mathbf{A}\\mathbf{D}\\vec{y}(t)\\\\ \\vec{x}(t+1) &= \\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}\\vec{x}(t)\\\\ \\end{aligned} \\] Long-term dynamics A major implication from the general solution is that only if all the eigenvalues have an absolute value less than one, \\(|\\lambda_i|<1\\) , will all entries of \\(\\mathbf{D}^t\\) decay to zero and hence \\(\\vec{x}(t)\\) decay to \\(\\vec{0}\\) (the equilibrium) as time increases. Further, as time increases \\(\\mathbf{D}^t\\) will be dominated by the eigenvalue with the largest absolute value, which we will call the leading eigenvalue . Note To see that \\(\\mathbf{D}^t\\) will be dominated by the eigenvalue with the largest absolute value as time increases, let the leading eigenvalue be \\(\\lambda_1\\) and factor it out of \\(\\mathbf{D}^t\\) \\[ \\mathbf{D}^t = \\lambda_1^t \\begin{pmatrix} 1 & 0 & \\cdots & 0\\\\ 0 & (\\lambda_2/\\lambda_1)^t & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & (\\lambda_n/\\lambda_1)^t\\\\ \\end{pmatrix} \\] Since \\(|\\lambda_i/\\lambda_1|<1\\) for all \\(i\\) , for large \\(t\\) these all go to zero and we have \\[ \\mathbf{D}^t \\approx \\tilde{\\mathbf{D}}^t \\equiv \\lambda_1^t \\begin{pmatrix} 1 & 0 & \\cdots & 0\\\\ 0 & 0 & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & 0\\\\ \\end{pmatrix} \\] We can therefore approximate \\(\\vec{x}(t)\\) after a sufficient amount of time as \\[ \\begin{aligned} \\tilde{\\vec{x}}(t) &= \\mathbf{A}\\tilde{\\mathbf{D}}^t\\mathbf{A}^{-1}\\vec{x}(0)\\\\ &= \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{x}(0) \\end{aligned} \\] where \\(\\vec{v}_1\\) and \\(\\vec{u}_1\\) are the right and left eigenvectors associated with the leading eigenvalue, \\(\\lambda_1\\) . Warning Finding the left eigenvectors via the inverse of \\(\\mathbf{A}\\) guarantees that the eigenvectors have been scaled such that \\(\\vec{u}_1\\vec{v}_1 = 1\\) . If the eigenvectors have been derived in another way, make sure you scale them so that this is true, eg, make the left eigenvalue equal to \\(\\vec{u}_1/(\\vec{u}_1 \\vec{v}_1)\\) . Otherwise the long-term approximation will be off by a constant factor. This means that, in the long-term, \\(\\vec{x}(t)\\) will grow like \\(\\lambda_1^t\\) , where \\(\\lambda_1\\) is the leading eigenvalue each variable in \\(\\vec{x}(t)\\) will oscillate around the equilibrium if \\(\\lambda_1<0\\) \\(\\vec{x}(t)\\) will approach the right eigenvector associated with the leading eigenvalue, \\(\\vec{v}_1\\) \\(\\vec{x}(t)\\) is weighted by a constant, \\(\\vec{u}_1 \\vec{x}(0)\\) , describing the \"initial size\" of the system Complex eigenvalues The final complication to discuss is complex eigenvalues. Specifically, what is \\(\\lambda^t\\) when \\(\\lambda = A + B i\\) ? To do this, we can first use some simple geometry on the complex plane (a two-dimensional space with the real part, \\(A\\) , on the x-axis and the imaginary part, \\(B\\) , on the y-axis) to show that any complex number can be written \\[ A + Bi = R (\\cos(\\theta) + i \\sin(\\theta)) \\] where \\(R = \\sqrt{A^2 + B^2}\\) is the absolute value of \\(\\lambda\\) and \\(\\theta = \\arctan(B/A)\\) is the angle between \\(\\lambda\\) and the x-axis. import matplotlib.pyplot as plt import math A,B = 1,1 #real and imaginary parts fig, ax = plt.subplots() ax.arrow(0,0,A,B, head_width=0.05, color='black', length_includes_head=True) #eigenvalue as vector in complex plane dx = 0.05 ax.plot([0-dx/2,A-dx/2],[0+dx,B+dx],marker='o',c='b') ax.text(A/2,B/2+3*dx,r'$R$',rotation=math.atan(B/A)*180/math.pi,c='b',fontsize=15,ha='center',va='center') ax.plot([0,A],[B,B],marker='o',c='r') ax.text(A/2,B+dx,r'$A=R \\cos(\\theta)$',c='r',fontsize=15,ha='center',va='center') ax.plot([A,A],[0,B],marker='o',c='g') ax.text(A+dx,B/2,r'$B=R \\sin(\\theta)$',c='g',fontsize=15,ha='center',va='center',rotation=90) ax.set_xlabel('real part, $A$') ax.set_ylabel('imaginary part, $B$') ax.set_xlim(-dx,A+2*dx) ax.set_ylim(-dx,B+2*dx) dx=A/4 ax.plot([0,dx],[0,0],c='orange') ax.add_patch(Arc((0,0), width=2*dx, height=2*dx, theta1=0, theta2=math.atan(B/A)*180/math.pi, edgecolor='orange')) ax.text(dx/2,dx/6,r'$\\theta$',fontsize=15,c='orange') plt.show() We then use Euler's equation, \\(\\cos(\\theta) + i \\sin(\\theta) = e^{i \\theta}\\) , to write \\[ A + Bi = R e^{i \\theta} \\] And we can now take powers of \\(\\lambda\\) \\[ \\lambda^t = R^t e^{i \\theta t} \\] Summary To summarize, for any system of linear recursion equations, \\(\\vec{x}(t+1)\\) , we can write the general solution in terms of the eigenvalues and eigenvectors, \\(\\vec{x}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{x}(0)\\) define the leading eigenvalue as the one with the largest absolute value say that the equilibrium is stable if the leading eigenvalue is less than 1 approximate the long-term dynamics in terms of the leading eigenvalue and its associated eigenvectors, \\(\\vec{x}(t) = \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{x}(0)\\) Motivating example revisited Now let's return to our motivating example of birds on islands. And let's imagine we have good estimates of the parameter values (after years of tough fieldwork!): \\(m_{12}=m_{21}=0.1\\) , \\(b_1=b_2=0.2\\) , \\(d_1=0.1\\) , \\(d_2=0.2\\) . To derive the general solution, giving the number of birds on the two islands in year \\(t\\) , we first derive the eigenvalues and eigenvectors of \\(\\mathbf{M}\\) . Using the techniques in Lecture 12 we find that the eigenvalues are \\(\\lambda_1\\approx1.03\\) and \\(\\lambda_2\\approx0.8\\) . The associated right eigenvectors are \\(\\vec{v}_1\\approx\\begin{pmatrix} 1 \\\\ 0.57 \\end{pmatrix}\\) and \\(\\vec{v}_2\\approx\\begin{pmatrix} 1 \\\\ -1.57 \\end{pmatrix}\\) . We therefore have \\[ \\mathbf{D} = \\begin{pmatrix} 1.03 & 0 \\\\ 0 & 0.8 \\end{pmatrix} \\] and \\[ \\mathbf{A} = \\begin{pmatrix} 1 & 1 \\\\ 0.57 & -1.57 \\end{pmatrix} \\] This year's census of the islands tells us that there are currently 100 birds on island 1 and 50 on island 2. Taking this as the starting point, \\(\\vec{n}(0) = \\begin{pmatrix} 100 \\\\ 50 \\end{pmatrix}\\) , we can use our general solution to predict the number of birds on the two islands over time. Below we plot the predicted number of birds on the two islands over the next 100 years. m12, m21, b1, b2, d1, d2 = 0.1, 0.1, 0.2, 0.2, 0.1, 0.2 #parameter values # general solution from sympy import * M = Matrix([[(1-m12)*(1+b1)*(1-d1), m21*(1+b1)*(1-d1)], #matrix [m12*(1+b2)*(1-d2), (1-m21)*(1+b2)*(1-d2)]]) A, D = M.diagonalize() #quick way to get matrix of right eigenvectors (A) and eigenvalues (D) n0 = Matrix([100,50]) #note this is made into a column vector automatically nt = A*D**t*A.inv()*n0 #general solution # plot import matplotlib.pyplot as plt fig, ax = plt.subplots() for j in range(2): #for each island ax.plot([nt.subs(t,i)[j] for i in range(100)], label='island %d'%(j+1), marker=\".\") ax.legend() ax.set_xlabel('years from now') ax.set_ylabel('number of birds') plt.show() We see that the population grows, which we should have expected given that the leading eigenvalue, \\(\\lambda_1\\approx1.03\\) , has an absolute value greater than 1. We also see that there are about 0.57 birds on island 2 for every 1 bird on island 1, as predicted by the right eigenvector associated with the leading eigenvector, \\(\\vec{v}_1 \\approx \\begin{pmatrix} 1 \\\\ 0.57 \\end{pmatrix}\\) . More generally, our long-term prediction is \\(\\vec{n}(t) \\approx \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0)\\) . Overlaying this approximation as black curves on the above plot shows that this works very well. # long-term approximation u1 = Matrix(1,2,A.inv()[0:2]) #left eigenvector is first row in A inverse ntapp = l1**t * v1[0] * u1 * n0 # plot import matplotlib.pyplot as plt fig, ax = plt.subplots() for j in range(2): #for each island ax.plot([nt.subs(t,i)[j] for i in range(100)], label='island %d'%(j+1), marker=\".\") #general solution ax.plot([ntapp.subs(t,i)[j] for i in range(100)], c='k') #long-term approx ax.legend() ax.set_xlabel('years from now') ax.set_ylabel('number of birds') plt.show() 2. Continuous time Now let's consider a system of linear equations in continuous time, which we can write in matrix form as \\[ \\frac{\\mathrm{d}\\vec{x}}{\\mathrm{d}t} = \\mathbf{M} \\vec{x} \\] General solution Just as in the univariate case of exponential growth, the general solution is simply \\[ \\vec{x}(t) = e^{\\mathbf{M}t}\\vec{x}(0) \\] But now we have \\(e\\) to the power of a matrix , and \\(e^{\\mathbf{M}t} = \\sum_{i=0}^{\\infty} \\mathbf{M}^i t^i / i!\\) is complicated! Fortunately we can use the same transform as in the discrete time case, \\(\\vec{y}=\\mathbf{A}^{-1}\\vec{x}\\) , to write the general solution as \\[ \\vec{x}(t) = \\mathbf{A} e^{\\mathbf{D}t} \\mathbf{A}^{-1}\\vec{x}(0) \\] This is much simpler because \\(e^{\\mathbf{D}t}\\) is just \\[ e^{\\mathbf{D}t} = \\begin{pmatrix} e^{\\lambda_1 t} & 0 & \\cdots & 0\\\\ 0 & e^{\\lambda_2 t} & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\cdots & 0 & e^{\\lambda_n t} \\end{pmatrix} \\] Long-term dynamics From this general solution we see that the system will approach equilibrium, \\(\\hat{\\vec{x}} = \\vec{0}\\) , only if all the entries of \\(e^{\\mathbf{D}t}\\) approach zero as time increases, which requires that all the eigenvalues are negative. Taking \\(\\lambda_1\\) to be the eigenvalue with the largest value, which we will call the leading eigenvalue , we also see that after sufficient time \\(e^{\\mathbf{D}t}\\) becomes dominated by this entry \\[ e^{\\mathbf{D}t} \\approx \\begin{pmatrix} e^{\\lambda_1 t} & 0 & \\cdots & 0\\\\ 0 & 0 & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\cdots & 0 & 0 \\end{pmatrix} \\] implying that the long-term dynamics can be approximated by \\[ \\vec{x}(t) \\approx e^{\\lambda_1 t} \\vec{v}_1 \\vec{u}_1 \\vec{x}(0) \\] where \\(\\vec{v}_1\\) and \\(\\vec{u}_1\\) are the right and left eigenvectors associated with the leading eigenvalue, \\(\\lambda_1\\) . Complex eigenvalues When we have complex eigenvalues we can again use Euler's equation to write \\[ \\begin{aligned} e^{\\lambda t} &= e^{(A + Bi) t}\\\\ &= e^{At}e^{Bti}\\\\ &= e^{At}(\\cos(Bt) + i\\sin(Bt)) \\end{aligned} \\] Because both \\(\\cos(Bt)\\) and \\(\\sin(Bt)\\) are bounded (between -1 and 1), whether \\(e^{\\lambda t}\\) grows or shrinks in long term depends only on the real part, \\(A\\) . So, in contrast to discrete time, when determining the leading eigenvalue in continuous time we only have to consider the real parts of the eigenvalues. Summary In summary, for any system of linear differential equations, \\(\\frac{\\mathrm{d}\\vec{x}}{\\mathrm{d}t}\\) , we can write the general solution in terms of the eigenvalues and eigenvectors, \\(\\vec{x}(t) = \\mathbf{A} e^{\\mathbf{D}t} \\mathbf{A}^{-1}\\vec{x}(0)\\) define the leading eigenvalue as the eigenvalue with the largest real part say that the equilibrium is stable if the real part of the leading eigenvalue is negative approximate the long-term dynamics in terms of the leading eigenvalue and its associated eigenvectors, \\(\\vec{x}(t) \\approx e^{\\lambda_1 t} \\vec{v}_1 \\vec{u}_1 \\vec{x}(0)\\) Example Let's consider a different example here in continuous time. Instead of thinking about birds on islands, let's think about a model of sexual selection. We will model the mean value of a male trait, \\(\\bar{z}\\) , such as the length of a birds tail, and the mean value of female preference for that trait, \\(\\bar{p}\\) (if \\(\\bar{p}>0\\) females tend to prefer larger male traits, if \\(\\bar{p}<0\\) females tend to prefer smaller male traits). We assume the optimal male trait value in the absence of sexual selection is \\(\\theta\\) , i.e., natural selection always pushes \\(\\bar{z}\\) towards \\(\\theta\\) (we'll take \\(\\theta=0\\) , meaning \\(\\bar{z}\\) is measured relative to the optimum). We assume female choice is costly, i.e., natural selection always pushes \\(\\bar{p}\\) towards 0. Finally we will assume that male traits and female preference share some genetic basis, meaning that they will covary (e.g., there may be some alleles that increase the trait value when in males and increase the preference when in females, causing positive covariance). This covariance means that a change in male trait will cause a change in female preference, and vice-versa. We can describe the dynamics of \\(\\bar{z}\\) and \\(\\bar{p}\\) with a system of linear differential equations \\[ \\begin{align} \\frac{\\mathrm{d}\\bar{z}}{\\mathrm{d}t} = G_z (a \\bar{p} - c \\bar{z}) - B b \\bar{p}\\\\ \\frac{\\mathrm{d}\\bar{p}}{\\mathrm{d}t} = B (a \\bar{p} - c \\bar{z}) - G_p b \\bar{p} \\\\ \\end{align} \\] where \\(G_z\\) and \\(G_p\\) are the amounts of genetic variation in male traits and female preference (this is the \"fuel\" of evolution, so the rates of evolution are proportional to these variances), \\(B\\) is the covariance between male traits and female preference, \\(a\\) is the strength of sexual selection, and \\(c\\) and \\(b\\) are the strengths of natural selection on male traits and female preference. Choosing some parameter values and plotting the general solution, we see the mean male trait and mean female preference cycle over time, decaying towards zero. Gz, Gp, B, a, b, c, z0, p0, tmax = 0.15, 0.8, 0.32, 0.95, 0.3, 0.45, 1, 0, 1000 #parameter values # general solution from sympy import * M = Matrix([[-Gz*c, Gz*a-b*B], [-B*c, -Gp*b+a*B]]) A, D = M.diagonalize() #quick way to get matrix of right eigenvectors (A) and eigenvalues (D) n0 = Matrix([z0,p0]) #note this is made into a column vector automatically nt = A*exp(D*t)*A.inv()*n0 #general solution # plot import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.plot([re(nt.subs(t,i)[0]) for i in range(tmax)], label='male trait') #need to remove the imaginary part because of numerical error ax.plot([re(nt.subs(t,i)[1]) for i in range(tmax)], label='female preference') ax.legend() ax.set_xlabel('time') ax.set_ylabel('value') plt.show() This cycling occurs because initially the mean male trait is positive but there is no mean female preference. This implies that both natural and sexual selection favour smaller male traits, causing the mean to decline. But because of a correlated response, female preference also declines, favouring male traits less than 0. Eventually female preference becomes too costly and begins to increase back toward zero. This causes a correlated increase in the male trait, and so on. With these parameter values the eigenvalues are \\(\\lambda \\approx -0.002 \\pm 0.05 i\\) . We could therefore have predicted this cycling based on these eigenvalues, as they are complex. We could also have predicted the eventual decay to zero (the equilibrium), as the real parts of both eigenvalues are negative. In this case the two eigenvalues have the same real part and therefore there is no one leading eigenvalue, meaning that we cannot use our long-term approximation, \\(\\vec{x}(t) \\approx e^{\\lambda_1 t} \\vec{v}_1 \\vec{u}_1 \\vec{x}(0)\\) .","title":"Lecture 13"},{"location":"lectures/lecture-13/#lecture-13-general-solutions-linear-multivariate","text":"Run notes interactively?","title":"Lecture 13: General solutions (linear multivariate)"},{"location":"lectures/lecture-13/#lecture-overview","text":"Discrete time Continuous time","title":"Lecture overview"},{"location":"lectures/lecture-13/#1-discrete-time","text":"","title":"1. Discrete time"},{"location":"lectures/lecture-13/#motivating-example","text":"Let's return to our model of the number of birds on two islands from Lecture 10, which was the motivation for learning linear algebra in the first place. Let's adjust the model slightly, removing foreign immigration and switching to discrete time (assuming migration, then birth, then death). graph LR; A1((n1)) --b1 n1--> A1; A1 --d1 n1--> C1[ ]; A2((n2)) --b2 n2--> A2; A2 --d2 n2--> C2[ ]; A1 --m12 n1--> A2; A2 --m21 n2--> A1; style C1 height:0px; style C2 height:0px; The number of birds on the two islands in the next time step are then \\[ \\begin{aligned} n_1(t+1) &= (n_1(t)(1-m_{12}) + n_2(t)m_{21})(1+b_1)(1-d_1) \\\\ n_2(t+1) &= (n_1(t)m_{12} + n_2(t)(1-m_{21}))(1+b_2)(1-d_2) \\\\ \\end{aligned} \\] As we noted in Lecture 10, we can write a system of linear equations (in this case recursion equations) in matrix form \\[ \\begin{aligned} \\vec{n}(t+1) &= \\mathbf{M}\\vec{n}(t) \\end{aligned} \\] where in this example \\[ \\mathbf{M} = \\begin{pmatrix} (1-m_{12})(1+b_1)(1-d_1) & m_{21}(1+b_1)(1-d_1) \\\\ m_{12}(1+b_2)(1-d_2) & (1-m_{21})(1+b_2)(1-d_2) \\end{pmatrix} \\] The question we now want to answer is, how do the numbers of birds on the two islands change over time?","title":"Motivating example"},{"location":"lectures/lecture-13/#general-formulation","text":"Instead of analyzing this specific model, let's investigate the dynamics of any system of linear equations in discrete time. We will then return to our motivating example. If there are \\(n\\) variables to keep track of, \\(x_1\\) , \\(x_2\\) , ..., \\(x_n\\) , then there will be \\(n\\) recursion equations \\[ \\begin{aligned} x_1(t+1) &= m_{11} x_1(t) + m_{12} x_2(t) + \\cdots + m_{1n} x_n(t) \\\\ x_2(t+1) &= m_{21} x_1(t) + m_{22} x_2(t) + \\cdots + m_{2n} x_n(t) \\\\ &\\vdots \\\\ x_n(t+1) &= m_{n1} x_1(t) + m_{n2} x_2(t) + \\cdots + m_{nn} x_n(t) \\end{aligned} \\] e.g., in the motivating example above we have \\(n=2\\) . These equations can be written in matrix form \\[ \\begin{aligned} \\begin{pmatrix} x_1(t+1) \\\\ x_2(t+1) \\\\ \\vdots \\\\ x_n(t+1) \\end{pmatrix} &= \\begin{pmatrix} m_{11} & m_{12} & \\cdots & m_{1n} \\\\ m_{21} & m_{22} & \\cdots & m_{2n}\\\\ \\vdots & \\vdots & & \\vdots\\\\ m_{n1} & m_{n2} & \\cdots & m_{nn} \\end{pmatrix} \\begin{pmatrix} x_1(t) \\\\ x_2(t) \\\\ \\vdots \\\\ x_n(t) \\end{pmatrix}\\\\ \\vec{x}(t+1) &= \\mathbf{M} \\vec{x}(t) \\end{aligned} \\] How does \\(\\vec{x}\\) change over time?","title":"General formulation"},{"location":"lectures/lecture-13/#general-solution","text":"Since this is just a multivariate version of exponential growth, we can derive the general solution by brute force iteration \\[ \\begin{aligned} \\vec{x}(t) &= \\mathbf{M}\\vec{x}(t-1)\\\\ &= \\mathbf{M}^2\\vec{x}(t-2)\\\\ & \\vdots \\\\ &= \\mathbf{M}^t\\vec{x}(0) \\end{aligned} \\] However, in most cases it will be hard to compute \\(\\mathbf{M}^t\\) (if you don't believe me, try calculating even just \\(\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}^3\\) ). Fortunately there is a trick involving eigenvalues and eigenvectors. Recall the equation for the eigenvalues, \\(\\lambda\\) , and right eigenvectors, \\(\\vec{v}\\) , \\[ \\mathbf{M} \\vec{v} = \\lambda \\vec{v} \\] For our \\(n\\) -dimensional model, there will often be \\(n\\) eigenvalues and eigenvectors that satisfy this equation (sometimes the \\(n\\) eigenvalues are not all distinct). We can actually write all \\(n\\) of these equations in matrix form \\[ \\begin{aligned} \\mathbf{M} \\begin{pmatrix} \\vec{v}_1 & \\vec{v}_2 & \\cdots & \\vec{v}_n \\end{pmatrix} &= \\begin{pmatrix} \\lambda_1 \\vec{v}_1 & \\lambda_2 \\vec{v}_2 & \\cdots & \\lambda_n \\vec{v}_n \\end{pmatrix}\\\\ \\mathbf{M} \\mathbf{A} &= \\mathbf{A} \\mathbf{D} \\end{aligned} \\] where the columns of \\(\\mathbf{A}\\) are the right eigenvectors and \\(\\mathbf{D}\\) is a diagonal matrix of the eigenvalues \\[ \\mathbf{D} = \\begin{pmatrix} \\lambda_1 & 0 & \\cdots & 0\\\\ 0 & \\lambda_2 & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & \\lambda_n\\\\ \\end{pmatrix} \\] Now here is the trick: multiply both sides of \\(\\mathbf{M} \\mathbf{A} = \\mathbf{A} \\mathbf{D}\\) by \\(\\mathbf{A}^{-1}\\) on the right \\[ \\begin{aligned} \\mathbf{M} \\mathbf{A} &= \\mathbf{A} \\mathbf{D}\\\\ \\mathbf{M} \\mathbf{A}\\mathbf{A}^{-1} &= \\mathbf{A} \\mathbf{D}\\mathbf{A}^{-1}\\\\ \\mathbf{M} &= \\mathbf{A} \\mathbf{D} \\mathbf{A}^{-1} \\end{aligned} \\] Note We can only take the inverse of our \\(n\\times n\\) matrix of right eigenvectors, \\(\\mathbf{A}\\) , when \\(\\mathbf{A}\\) is invertible, which requires the determinant be non-zero or, equivalently, the rows (or columns) of \\(\\mathbf{A}\\) to be linearly independent. Fortunately, in many cases the columns (the right eigenvectors) will be linearly independent. However, sometimes the right eigenvectors will not be linearly independent, which only occurs when there are less than \\(n\\) distinct eigenvalues. When the right eigenvectors of \\(\\mathbf{M}\\) are not linearly independent we call \\(\\mathbf{M}\\) defective . In that case we need to derive the general solution in another way, but we won't deal with that in this class. Subbing this alternate version of \\(\\mathbf{M}\\) into our general solution above, we see that most of the \\(\\mathbf{A}\\) and \\(\\mathbf{A}^{-1}\\) matrices cancel, leaving us with \\[ \\begin{aligned} \\vec{x}(t) &= \\mathbf{M}^t\\vec{x}(0)\\\\ &= (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1})^t\\vec{x}(0)\\\\ &= (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) \\cdots (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) \\vec{x}(0)\\\\ &= \\mathbf{A}\\mathbf{D}(\\mathbf{A}^{-1} \\mathbf{A})\\mathbf{D}(\\mathbf{A}^{-1}\\mathbf{A}) \\cdots (\\mathbf{A}^{-1}\\mathbf{A})\\mathbf{D}\\mathbf{A}^{-1} \\vec{x}(0)\\\\ &= \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1} \\vec{x}(0) \\end{aligned} \\] And this is great because \\(\\mathbf{D}\\) is a diagonal matrix, meaning \\(\\mathbf{D}^t\\) is easy to calculate \\[ \\mathbf{D}^t = \\begin{pmatrix} \\lambda_1^t & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2^t & \\cdots & 0\\\\ \\vdots & \\vdots & & \\vdots\\\\ 0 & 0 & \\cdots & \\lambda_n^t \\end{pmatrix} \\] It would not have been so easy to find \\(\\mathbf{M}^t\\) ! Note that \\(\\mathbf{A}^{-1}\\) is a matrix whose rows are the left eigenvectors of \\(\\textbf{M}\\) , which can be seen by deriving \\(\\mathbf{M} = \\mathbf{A} \\mathbf{D}\\mathbf{A}^{-1}\\) starting with the equation for the left eigenvectors \\(\\vec{u}\\mathbf{M}=\\lambda\\vec{u}\\) . Note Another way to arrive at this general solition, \\(\\vec{x}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{x}(0)\\) , is to consider a transformation from our current coordinate system, \\(\\vec{x}\\) , to another defined by \\(\\vec{y} = \\mathbf{A}^{-1}\\vec{x}\\) . If we then attempt to derive a recursion in our new coordinate system we find \\[ \\begin{aligned} \\vec{y}(t+1) &= \\mathbf{A}^{-1}\\vec{x}(t+1)\\\\ &= \\mathbf{A}^{-1}\\mathbf{M}\\vec{x}(t)\\\\ &= \\mathbf{A}^{-1}\\mathbf{M}\\mathbf{A}\\vec{y}(t)\\\\ &= \\mathbf{A}^{-1}(\\mathbf{A} \\mathbf{D} \\mathbf{A}^{-1})\\mathbf{A}\\vec{y}(t)\\\\ &= \\mathbf{D}\\vec{y}(t) \\end{aligned} \\] Voila! In the new coordinate system we just move along each axis by a factor \\(\\lambda_i\\) , just as we saw in Lecture 12. In this way the eigenvectors form a new, more convenient, coordinate system. To convert back to our original coordinate system we multiply both sides of the equation by \\(\\mathbf{A}\\) on the left and then use \\(\\vec{y} = \\mathbf{A}^{-1}\\vec{x}\\) \\[ \\begin{aligned} \\mathbf{A}\\vec{y}(t+1) &= \\mathbf{A}\\mathbf{D}\\vec{y}(t)\\\\ \\vec{x}(t+1) &= \\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}\\vec{x}(t)\\\\ \\end{aligned} \\]","title":"General solution"},{"location":"lectures/lecture-13/#long-term-dynamics","text":"A major implication from the general solution is that only if all the eigenvalues have an absolute value less than one, \\(|\\lambda_i|<1\\) , will all entries of \\(\\mathbf{D}^t\\) decay to zero and hence \\(\\vec{x}(t)\\) decay to \\(\\vec{0}\\) (the equilibrium) as time increases. Further, as time increases \\(\\mathbf{D}^t\\) will be dominated by the eigenvalue with the largest absolute value, which we will call the leading eigenvalue . Note To see that \\(\\mathbf{D}^t\\) will be dominated by the eigenvalue with the largest absolute value as time increases, let the leading eigenvalue be \\(\\lambda_1\\) and factor it out of \\(\\mathbf{D}^t\\) \\[ \\mathbf{D}^t = \\lambda_1^t \\begin{pmatrix} 1 & 0 & \\cdots & 0\\\\ 0 & (\\lambda_2/\\lambda_1)^t & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & (\\lambda_n/\\lambda_1)^t\\\\ \\end{pmatrix} \\] Since \\(|\\lambda_i/\\lambda_1|<1\\) for all \\(i\\) , for large \\(t\\) these all go to zero and we have \\[ \\mathbf{D}^t \\approx \\tilde{\\mathbf{D}}^t \\equiv \\lambda_1^t \\begin{pmatrix} 1 & 0 & \\cdots & 0\\\\ 0 & 0 & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & 0\\\\ \\end{pmatrix} \\] We can therefore approximate \\(\\vec{x}(t)\\) after a sufficient amount of time as \\[ \\begin{aligned} \\tilde{\\vec{x}}(t) &= \\mathbf{A}\\tilde{\\mathbf{D}}^t\\mathbf{A}^{-1}\\vec{x}(0)\\\\ &= \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{x}(0) \\end{aligned} \\] where \\(\\vec{v}_1\\) and \\(\\vec{u}_1\\) are the right and left eigenvectors associated with the leading eigenvalue, \\(\\lambda_1\\) . Warning Finding the left eigenvectors via the inverse of \\(\\mathbf{A}\\) guarantees that the eigenvectors have been scaled such that \\(\\vec{u}_1\\vec{v}_1 = 1\\) . If the eigenvectors have been derived in another way, make sure you scale them so that this is true, eg, make the left eigenvalue equal to \\(\\vec{u}_1/(\\vec{u}_1 \\vec{v}_1)\\) . Otherwise the long-term approximation will be off by a constant factor. This means that, in the long-term, \\(\\vec{x}(t)\\) will grow like \\(\\lambda_1^t\\) , where \\(\\lambda_1\\) is the leading eigenvalue each variable in \\(\\vec{x}(t)\\) will oscillate around the equilibrium if \\(\\lambda_1<0\\) \\(\\vec{x}(t)\\) will approach the right eigenvector associated with the leading eigenvalue, \\(\\vec{v}_1\\) \\(\\vec{x}(t)\\) is weighted by a constant, \\(\\vec{u}_1 \\vec{x}(0)\\) , describing the \"initial size\" of the system","title":"Long-term dynamics"},{"location":"lectures/lecture-13/#complex-eigenvalues","text":"The final complication to discuss is complex eigenvalues. Specifically, what is \\(\\lambda^t\\) when \\(\\lambda = A + B i\\) ? To do this, we can first use some simple geometry on the complex plane (a two-dimensional space with the real part, \\(A\\) , on the x-axis and the imaginary part, \\(B\\) , on the y-axis) to show that any complex number can be written \\[ A + Bi = R (\\cos(\\theta) + i \\sin(\\theta)) \\] where \\(R = \\sqrt{A^2 + B^2}\\) is the absolute value of \\(\\lambda\\) and \\(\\theta = \\arctan(B/A)\\) is the angle between \\(\\lambda\\) and the x-axis. import matplotlib.pyplot as plt import math A,B = 1,1 #real and imaginary parts fig, ax = plt.subplots() ax.arrow(0,0,A,B, head_width=0.05, color='black', length_includes_head=True) #eigenvalue as vector in complex plane dx = 0.05 ax.plot([0-dx/2,A-dx/2],[0+dx,B+dx],marker='o',c='b') ax.text(A/2,B/2+3*dx,r'$R$',rotation=math.atan(B/A)*180/math.pi,c='b',fontsize=15,ha='center',va='center') ax.plot([0,A],[B,B],marker='o',c='r') ax.text(A/2,B+dx,r'$A=R \\cos(\\theta)$',c='r',fontsize=15,ha='center',va='center') ax.plot([A,A],[0,B],marker='o',c='g') ax.text(A+dx,B/2,r'$B=R \\sin(\\theta)$',c='g',fontsize=15,ha='center',va='center',rotation=90) ax.set_xlabel('real part, $A$') ax.set_ylabel('imaginary part, $B$') ax.set_xlim(-dx,A+2*dx) ax.set_ylim(-dx,B+2*dx) dx=A/4 ax.plot([0,dx],[0,0],c='orange') ax.add_patch(Arc((0,0), width=2*dx, height=2*dx, theta1=0, theta2=math.atan(B/A)*180/math.pi, edgecolor='orange')) ax.text(dx/2,dx/6,r'$\\theta$',fontsize=15,c='orange') plt.show() We then use Euler's equation, \\(\\cos(\\theta) + i \\sin(\\theta) = e^{i \\theta}\\) , to write \\[ A + Bi = R e^{i \\theta} \\] And we can now take powers of \\(\\lambda\\) \\[ \\lambda^t = R^t e^{i \\theta t} \\]","title":"Complex eigenvalues"},{"location":"lectures/lecture-13/#summary","text":"To summarize, for any system of linear recursion equations, \\(\\vec{x}(t+1)\\) , we can write the general solution in terms of the eigenvalues and eigenvectors, \\(\\vec{x}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{x}(0)\\) define the leading eigenvalue as the one with the largest absolute value say that the equilibrium is stable if the leading eigenvalue is less than 1 approximate the long-term dynamics in terms of the leading eigenvalue and its associated eigenvectors, \\(\\vec{x}(t) = \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{x}(0)\\)","title":"Summary"},{"location":"lectures/lecture-13/#motivating-example-revisited","text":"Now let's return to our motivating example of birds on islands. And let's imagine we have good estimates of the parameter values (after years of tough fieldwork!): \\(m_{12}=m_{21}=0.1\\) , \\(b_1=b_2=0.2\\) , \\(d_1=0.1\\) , \\(d_2=0.2\\) . To derive the general solution, giving the number of birds on the two islands in year \\(t\\) , we first derive the eigenvalues and eigenvectors of \\(\\mathbf{M}\\) . Using the techniques in Lecture 12 we find that the eigenvalues are \\(\\lambda_1\\approx1.03\\) and \\(\\lambda_2\\approx0.8\\) . The associated right eigenvectors are \\(\\vec{v}_1\\approx\\begin{pmatrix} 1 \\\\ 0.57 \\end{pmatrix}\\) and \\(\\vec{v}_2\\approx\\begin{pmatrix} 1 \\\\ -1.57 \\end{pmatrix}\\) . We therefore have \\[ \\mathbf{D} = \\begin{pmatrix} 1.03 & 0 \\\\ 0 & 0.8 \\end{pmatrix} \\] and \\[ \\mathbf{A} = \\begin{pmatrix} 1 & 1 \\\\ 0.57 & -1.57 \\end{pmatrix} \\] This year's census of the islands tells us that there are currently 100 birds on island 1 and 50 on island 2. Taking this as the starting point, \\(\\vec{n}(0) = \\begin{pmatrix} 100 \\\\ 50 \\end{pmatrix}\\) , we can use our general solution to predict the number of birds on the two islands over time. Below we plot the predicted number of birds on the two islands over the next 100 years. m12, m21, b1, b2, d1, d2 = 0.1, 0.1, 0.2, 0.2, 0.1, 0.2 #parameter values # general solution from sympy import * M = Matrix([[(1-m12)*(1+b1)*(1-d1), m21*(1+b1)*(1-d1)], #matrix [m12*(1+b2)*(1-d2), (1-m21)*(1+b2)*(1-d2)]]) A, D = M.diagonalize() #quick way to get matrix of right eigenvectors (A) and eigenvalues (D) n0 = Matrix([100,50]) #note this is made into a column vector automatically nt = A*D**t*A.inv()*n0 #general solution # plot import matplotlib.pyplot as plt fig, ax = plt.subplots() for j in range(2): #for each island ax.plot([nt.subs(t,i)[j] for i in range(100)], label='island %d'%(j+1), marker=\".\") ax.legend() ax.set_xlabel('years from now') ax.set_ylabel('number of birds') plt.show() We see that the population grows, which we should have expected given that the leading eigenvalue, \\(\\lambda_1\\approx1.03\\) , has an absolute value greater than 1. We also see that there are about 0.57 birds on island 2 for every 1 bird on island 1, as predicted by the right eigenvector associated with the leading eigenvector, \\(\\vec{v}_1 \\approx \\begin{pmatrix} 1 \\\\ 0.57 \\end{pmatrix}\\) . More generally, our long-term prediction is \\(\\vec{n}(t) \\approx \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0)\\) . Overlaying this approximation as black curves on the above plot shows that this works very well. # long-term approximation u1 = Matrix(1,2,A.inv()[0:2]) #left eigenvector is first row in A inverse ntapp = l1**t * v1[0] * u1 * n0 # plot import matplotlib.pyplot as plt fig, ax = plt.subplots() for j in range(2): #for each island ax.plot([nt.subs(t,i)[j] for i in range(100)], label='island %d'%(j+1), marker=\".\") #general solution ax.plot([ntapp.subs(t,i)[j] for i in range(100)], c='k') #long-term approx ax.legend() ax.set_xlabel('years from now') ax.set_ylabel('number of birds') plt.show()","title":"Motivating example revisited"},{"location":"lectures/lecture-13/#2-continuous-time","text":"Now let's consider a system of linear equations in continuous time, which we can write in matrix form as \\[ \\frac{\\mathrm{d}\\vec{x}}{\\mathrm{d}t} = \\mathbf{M} \\vec{x} \\]","title":"2. Continuous time"},{"location":"lectures/lecture-13/#general-solution_1","text":"Just as in the univariate case of exponential growth, the general solution is simply \\[ \\vec{x}(t) = e^{\\mathbf{M}t}\\vec{x}(0) \\] But now we have \\(e\\) to the power of a matrix , and \\(e^{\\mathbf{M}t} = \\sum_{i=0}^{\\infty} \\mathbf{M}^i t^i / i!\\) is complicated! Fortunately we can use the same transform as in the discrete time case, \\(\\vec{y}=\\mathbf{A}^{-1}\\vec{x}\\) , to write the general solution as \\[ \\vec{x}(t) = \\mathbf{A} e^{\\mathbf{D}t} \\mathbf{A}^{-1}\\vec{x}(0) \\] This is much simpler because \\(e^{\\mathbf{D}t}\\) is just \\[ e^{\\mathbf{D}t} = \\begin{pmatrix} e^{\\lambda_1 t} & 0 & \\cdots & 0\\\\ 0 & e^{\\lambda_2 t} & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\cdots & 0 & e^{\\lambda_n t} \\end{pmatrix} \\]","title":"General solution"},{"location":"lectures/lecture-13/#long-term-dynamics_1","text":"From this general solution we see that the system will approach equilibrium, \\(\\hat{\\vec{x}} = \\vec{0}\\) , only if all the entries of \\(e^{\\mathbf{D}t}\\) approach zero as time increases, which requires that all the eigenvalues are negative. Taking \\(\\lambda_1\\) to be the eigenvalue with the largest value, which we will call the leading eigenvalue , we also see that after sufficient time \\(e^{\\mathbf{D}t}\\) becomes dominated by this entry \\[ e^{\\mathbf{D}t} \\approx \\begin{pmatrix} e^{\\lambda_1 t} & 0 & \\cdots & 0\\\\ 0 & 0 & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\cdots & 0 & 0 \\end{pmatrix} \\] implying that the long-term dynamics can be approximated by \\[ \\vec{x}(t) \\approx e^{\\lambda_1 t} \\vec{v}_1 \\vec{u}_1 \\vec{x}(0) \\] where \\(\\vec{v}_1\\) and \\(\\vec{u}_1\\) are the right and left eigenvectors associated with the leading eigenvalue, \\(\\lambda_1\\) .","title":"Long-term dynamics"},{"location":"lectures/lecture-13/#complex-eigenvalues_1","text":"When we have complex eigenvalues we can again use Euler's equation to write \\[ \\begin{aligned} e^{\\lambda t} &= e^{(A + Bi) t}\\\\ &= e^{At}e^{Bti}\\\\ &= e^{At}(\\cos(Bt) + i\\sin(Bt)) \\end{aligned} \\] Because both \\(\\cos(Bt)\\) and \\(\\sin(Bt)\\) are bounded (between -1 and 1), whether \\(e^{\\lambda t}\\) grows or shrinks in long term depends only on the real part, \\(A\\) . So, in contrast to discrete time, when determining the leading eigenvalue in continuous time we only have to consider the real parts of the eigenvalues.","title":"Complex eigenvalues"},{"location":"lectures/lecture-13/#summary_1","text":"In summary, for any system of linear differential equations, \\(\\frac{\\mathrm{d}\\vec{x}}{\\mathrm{d}t}\\) , we can write the general solution in terms of the eigenvalues and eigenvectors, \\(\\vec{x}(t) = \\mathbf{A} e^{\\mathbf{D}t} \\mathbf{A}^{-1}\\vec{x}(0)\\) define the leading eigenvalue as the eigenvalue with the largest real part say that the equilibrium is stable if the real part of the leading eigenvalue is negative approximate the long-term dynamics in terms of the leading eigenvalue and its associated eigenvectors, \\(\\vec{x}(t) \\approx e^{\\lambda_1 t} \\vec{v}_1 \\vec{u}_1 \\vec{x}(0)\\)","title":"Summary"},{"location":"lectures/lecture-13/#example","text":"Let's consider a different example here in continuous time. Instead of thinking about birds on islands, let's think about a model of sexual selection. We will model the mean value of a male trait, \\(\\bar{z}\\) , such as the length of a birds tail, and the mean value of female preference for that trait, \\(\\bar{p}\\) (if \\(\\bar{p}>0\\) females tend to prefer larger male traits, if \\(\\bar{p}<0\\) females tend to prefer smaller male traits). We assume the optimal male trait value in the absence of sexual selection is \\(\\theta\\) , i.e., natural selection always pushes \\(\\bar{z}\\) towards \\(\\theta\\) (we'll take \\(\\theta=0\\) , meaning \\(\\bar{z}\\) is measured relative to the optimum). We assume female choice is costly, i.e., natural selection always pushes \\(\\bar{p}\\) towards 0. Finally we will assume that male traits and female preference share some genetic basis, meaning that they will covary (e.g., there may be some alleles that increase the trait value when in males and increase the preference when in females, causing positive covariance). This covariance means that a change in male trait will cause a change in female preference, and vice-versa. We can describe the dynamics of \\(\\bar{z}\\) and \\(\\bar{p}\\) with a system of linear differential equations \\[ \\begin{align} \\frac{\\mathrm{d}\\bar{z}}{\\mathrm{d}t} = G_z (a \\bar{p} - c \\bar{z}) - B b \\bar{p}\\\\ \\frac{\\mathrm{d}\\bar{p}}{\\mathrm{d}t} = B (a \\bar{p} - c \\bar{z}) - G_p b \\bar{p} \\\\ \\end{align} \\] where \\(G_z\\) and \\(G_p\\) are the amounts of genetic variation in male traits and female preference (this is the \"fuel\" of evolution, so the rates of evolution are proportional to these variances), \\(B\\) is the covariance between male traits and female preference, \\(a\\) is the strength of sexual selection, and \\(c\\) and \\(b\\) are the strengths of natural selection on male traits and female preference. Choosing some parameter values and plotting the general solution, we see the mean male trait and mean female preference cycle over time, decaying towards zero. Gz, Gp, B, a, b, c, z0, p0, tmax = 0.15, 0.8, 0.32, 0.95, 0.3, 0.45, 1, 0, 1000 #parameter values # general solution from sympy import * M = Matrix([[-Gz*c, Gz*a-b*B], [-B*c, -Gp*b+a*B]]) A, D = M.diagonalize() #quick way to get matrix of right eigenvectors (A) and eigenvalues (D) n0 = Matrix([z0,p0]) #note this is made into a column vector automatically nt = A*exp(D*t)*A.inv()*n0 #general solution # plot import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.plot([re(nt.subs(t,i)[0]) for i in range(tmax)], label='male trait') #need to remove the imaginary part because of numerical error ax.plot([re(nt.subs(t,i)[1]) for i in range(tmax)], label='female preference') ax.legend() ax.set_xlabel('time') ax.set_ylabel('value') plt.show() This cycling occurs because initially the mean male trait is positive but there is no mean female preference. This implies that both natural and sexual selection favour smaller male traits, causing the mean to decline. But because of a correlated response, female preference also declines, favouring male traits less than 0. Eventually female preference becomes too costly and begins to increase back toward zero. This causes a correlated increase in the male trait, and so on. With these parameter values the eigenvalues are \\(\\lambda \\approx -0.002 \\pm 0.05 i\\) . We could therefore have predicted this cycling based on these eigenvalues, as they are complex. We could also have predicted the eventual decay to zero (the equilibrium), as the real parts of both eigenvalues are negative. In this case the two eigenvalues have the same real part and therefore there is no one leading eigenvalue, meaning that we cannot use our long-term approximation, \\(\\vec{x}(t) \\approx e^{\\lambda_1 t} \\vec{v}_1 \\vec{u}_1 \\vec{x}(0)\\) .","title":"Example"},{"location":"lectures/lecture-14/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 14: Demography Run notes interactively? Lecture overview Demography Stage-structure Age-structure 1. Demography We're now going to use what we've learned about linear multivariate models to describe the dynamics of a single population that is composed of different types of individuals. This area of research is called demography . We'll just consider discrete-time models here. In the last lecture we saw that for any discrete-time linear multivariate model \\[ \\vec{n}(t+1) = \\mathbf{M}\\vec{n}(t) \\] we can compute the general solution \\[ \\vec{n}(t) = \\mathbf{M}^t\\vec{n}(0) \\] or, more conveniently, in terms of the eigenvalues ( \\(\\mathbf{D}\\) ) and eigenvectors ( \\(\\mathbf{A}\\) ) \\[ \\vec{n}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{n}(0 ) \\] Despite this progress, the eigenvalues and eigenvectors are often unobtainable (without specifying parameter values), leaving us to rely on the long-term approximation \\[ \\vec{n}(t) = \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0) \\] For this we just need to know the leading eigenvalue ( \\(\\lambda_1\\) ) and the corresponding right ( \\(\\vec{v}_1\\) ) and left ( \\(\\vec{u}_1\\) ) eigenvectors, respectively. (Remember that we have scaled the eigenvectors such that \\(\\vec{u}_1\\vec{v}_1=1\\) .) These three components ( \\(\\lambda_1\\) , \\(\\vec{v}_1\\) , \\(\\vec{u}_1\\) ) are the key demographic quantities that we will investigate \\(\\lambda_1\\) is the long-term population growth rate \\(\\vec{v}_1\\) describes the stable stage-distribution \\(\\vec{u}_1\\) describes the relative reproductive values of each stage Note The long-term approximation, \\(\\vec{n}(t) = \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0)\\) , is valid as long as the leading eigenvalue, \\(\\lambda_1\\) , is real (no cycles in long-term) positive (no oscillations to negative numbers!) larger than all other eigenvalues (so that we can ignore the other eigenvalues/vectors) Fortunately we are guaranteed all these conditions in our demographic models since all entries of \\(\\mathbf{M}\\) are non-negative and all entries of \\(\\mathbf{M}^t\\) are positive for some value of \\(t\\) . (This follows from something called the Perron-Frobenius Theorem .) The most general demographic model is called stage-structure : we consider some finite number of discrete stages that an individual can be in and we use an abitrary matrix of transition rates between stages (a projection matrix ), \\(\\mathbf{M}\\) , to project how the population size and composition changes over time. A common special case is age-structure : here we define the stages as the number of time steps an individual has been alive for, which leads to a simpler projection matrix (called a Leslie matrix ) because individuals always transition to the next stage (or die). 2. Stage-structure Example: North Atlantic right whale Let's consider an example (from 10.2 in the text). North Atlantic right whales were hunted to near extinction in the 1800s and early 1900s, after which their population size is thought to have slowly recovered (there are less than 400 now, and unfortunately appear to be in decline again ). Because of their long life span, over which survival and reproductive rates vary enormously, a stage-structured model is very appropriate. Below we draw a flow diagram representing all the transitions between calves, sexually immature individuals, sexually mature individuals, and actively reproducing individuals. graph LR; C((Calves)) --sIC nC--> I((Immature)); I --sII nI--> I; I --sMI nI--> M((Mature)); I --sRI nI--> R((Reproductive)); M --sMM nM--> M; M --sRM nM--> R; R --sRR nR--> R; R --sMR nR--> M; R --b nR--> C; Converting this flow diagram into a system of recursion equations, \\(\\vec{n}(t+1) = \\mathbf{M}\\vec{n}(t)\\) , the projection matrix is \\[ \\mathbf{M} = \\begin{pmatrix} 0 & 0 & 0 & b \\\\ s_{IC} & s_{II} & 0 & 0\\\\ 0 & s_{MI} & s_{MM} & s_{MR} \\\\ 0 & s_{RI} & s_{RM} & s_{RR} \\end{pmatrix} \\] If we now plug in some estimated parameter values from the literature ( \\(b=0.3\\) , \\(s_{IC}=0.92\\) , \\(s_{II}=0.86\\) , \\(s_{MI}=0.08\\) , \\(s_{MM}=0.8\\) , \\(s_{MR}=0.88\\) , \\(s_{RI}=0.02\\) , \\(s_{RM}=0.19\\) , \\(s_{RR}=0\\) ) we can numerically calculate the three key demographic quantities \\[ \\begin{aligned} &\\lambda_1 \\approx 1.003 \\\\ &\\vec{v}_1 \\approx \\begin{pmatrix} 0.04 \\\\ 0.23 \\\\ 0.61 \\\\ 0.12\\end{pmatrix} \\\\ &\\vec{u}_1 \\approx \\begin{pmatrix} 0.69 & 0.76 & 1.07 & 1.15 \\end{pmatrix} \\end{aligned} \\] These quantitives tells us, for example, that in the long-run the population is predicted to grow ( \\(\\lambda_1>1\\) ), the majority of individuals are expected to be mature (the second last entry in \\(\\vec{v}_1\\) is the largest), and mature and reproductive individuals are expected to have much higher reproductive values than calves and immature individuals (the last two entries in \\(\\vec{u}_1\\) are much larger than the first two). Caveat The parameter values above were estimated before the current population decline -- changes in climate and human behaviour, eg boat traffic and fishing, have presumably altered the parameter values, making the predictions less accurate. For example, the increased incidence of entanglement in fishing nets has likely decreased survival rates to the point that the population is now expected to decline. See here for more info. We can now answer an important conservation question: Question If we wanted to increase the total population size in the future and we could add one individual to any stage, which stage should it be? We want to know what entry of \\(\\vec{n}(0)\\) to add 1 to to maximize \\(\\vec{n}(t) \\approx \\lambda^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0)\\) . This will not affect the long-term growth rate ( \\(\\lambda_1\\) ) or the stable-stage distribution ( \\(\\vec{v}_1\\) ). We can therefore only increase \\(\\vec{u}_1\\vec{n}(0) = u_1 n_1(0) + u_2 n_2(0) + ... + u_m n_m(0)\\) . And so we add 1 to the stage with the largest reproductive value, \\(u_i\\) . So for these whales, where the reproductive values are \\(\\vec{u}_1 \\approx \\begin{pmatrix} 0.69 & 0.76 & 1.07 & 1.15 \\end{pmatrix}\\) , we should add a reproductively active individual. This is perhaps not surprising since those are the only individuals that produce offspring and if we introduced an individual in an earlier stage there is some chance that they would die before becoming reproductively active. One other question we might now consider is: Question If we wanted to increase the long-term population growth rate and we could increase any parameter a little bit, which parameter should it be? One way to answer this is to numerically calculate the long-term population growth rate, \\(\\lambda_1\\) , for a range of values of one parameter, \\(z\\) (e.g., we might take \\(z=b\\) , the fecundity of reproductive individuals). Or, we could try to analytically compute the rate of change in the leading eigenvalue with respect to \\(z\\) , \\(\\mathrm{d}\\lambda_1/\\mathrm{d}z\\) , while considering \\(\\mathbf{M}\\) , \\(\\lambda_1\\) , \\(\\vec{v}_1\\) , and \\(\\vec{u}_1\\) to be functions of \\(z\\) . Since \\(\\mathbf{M}\\vec{v}_1 = \\lambda_1 \\vec{v}_1\\) and \\(\\vec{u}_1\\mathbf{M} = \\vec{u}_1\\lambda_1\\) we have \\[ \\begin{aligned} \\mathbf{M} \\vec{v}_1 &= \\lambda_1 \\vec{v}_1 \\\\ \\vec{u}_1 \\mathbf{M} \\vec{v}_1 &= \\vec{u}_1 \\lambda_1 \\vec{v}_1 \\\\ \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left(\\vec{u}_1 \\mathbf{M} \\vec{v}_1 \\right)&= \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left(\\vec{u}_1 \\lambda_1 \\vec{v}_1 \\right)\\\\ \\frac{\\mathrm{d}\\vec{u}_1}{\\mathrm{d}z} \\mathbf{M} \\vec{v}_1 + \\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1 + \\vec{u}_1 \\mathbf{M} \\frac{\\mathrm{d}\\vec{v}_1}{\\mathrm{d}z} &= \\frac{\\mathrm{d}\\vec{u}_1}{\\mathrm{d}z} \\lambda_1 \\vec{v}_1 + \\vec{u}_1 \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\vec{v}_1 + \\vec{u}_1 \\lambda_1 \\frac{\\mathrm{d}\\vec{v}_1}{\\mathrm{d}z}\\\\ \\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1 &= \\vec{u}_1 \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\vec{v}_1 \\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} &= \\frac{\\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1}{\\vec{u}_1 \\vec{v}_1} \\end{aligned}\\] We call \\(\\mathrm{d}\\lambda_1/\\mathrm{d}z\\) the sensitivity of \\(\\lambda_1\\) to \\(z\\) . This expression will be too complicated to understand in general in most cases, but we can evaluate at some particular value \\(z^*\\) (e.g., at the current estimate) to see how the growth rate changes as \\(z\\) increases from that value \\[ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\bigg|_{z=z^*} = \\frac{\\vec{u}_1 \\left(\\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\big|_{z=z^*} \\right) \\vec{v}_1}{\\vec{u}_1 \\vec{v}_1} \\] Returning to our question, we want to know which parameter, \\(z\\) , gives the largest value of \\(\\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\big|_{z=z^*}\\) . For these whales we can calculate \\(\\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\big|_{z=z^*}\\) for each parameter \\(z\\) (ie, for each entry in \\(\\mathbf{M}\\) ) evaluated at it's current value \\(z^*\\) . Doing that calculation we see that increasing the fraction of mature individuals that survive to become reproductively active, \\(s_{RM}\\) , has the largest effect. This makes good sense because increasing \\(s_{RM}\\) increases the rate at which the most populous stage (from \\(\\vec{v}_1\\) ) transitions to the stage with the highest reproductive value (from \\(\\vec{u}_1\\) ). And finally, we can ask Question What are the predicted numbers of individuals in each stage over time? Here we simply plot the general solution (solid) and compare with the long-term approximation (dashed) out of interest. We see that the full solution very quickly converges to the long-term solution in this case. b,sic,sii,smi,smm,smr,sri,srm,srr = 0.3,0.92,0.86,0.08,0.8,0.88,0.02,0.19,0 #parameter values import numpy as np M = np.array([[0,0,0,b], #projection matrix [sic,sii,0,0], [0,smi,smm,smr], [0,sri,srm,srr]]) # calculate eigs = np.linalg.eig(M) #eigenvalues and right eigenvectors ix = np.argmax(np.abs(eigs[0])) #which is leading eigenvalue l1 = eigs[0][ix] #leading eigenvalue v1 = eigs[1][:,ix] #leading right eigenvector v1 = v1/np.sum(v1) #normalized to sum to 1 us = np.linalg.inv(eigs[1]) #left eigenvectors u1 = us[ix] #leading left eigenvalue u1 = u1/np.dot(u1,v1) #normalized so u1*v1=1 n0 = np.array([50,50,50,50]) #initial state def ntfull(t): '''full projection''' return np.dot(np.linalg.matrix_power(M,t), n0) def ntapp(t): '''long term approximation''' return l1**t * v1 * np.dot(u1, n0) # plot import matplotlib.pyplot as plt fig, ax = plt.subplots() labels = ['calves','immature','mature','reproductive'] colors = plt.get_cmap('tab10') for i,j in enumerate(range(len(M))): #for each stage ax.plot([ntfull(t)[j] for t in range(100)], color=colors(i), label=labels[i]) ax.plot([ntapp(t)[j] for t in range(100)], color=colors(i), linestyle='--') #long-term approx ax.legend() ax.set_xlabel('years') ax.set_ylabel('number of whales') plt.show() 3. Age-structure Now let's look at the special case of age-structure . Here individuals in stage \\(i\\) at time \\(t\\) can only contribute to stage \\(i+1\\) (survival) and stage \\(1\\) (reproduction) at time \\(t+1\\) . graph LR; 1((1)) --p1 n1--> 2((2)); 1 --m1 n1--> 1; 2 --p2 n2--> 3((3)); 2 --m2 n2--> 1; 3 --p3 n4--> 4((4)); 3 --m3 n3--> 1; 4 --m4 n4--> 1; Because of this, the projection matrix is simpler in the sense that it contains more zeros and has non-zero entries in very specific places \\[ \\mathbf{L} = \\begin{pmatrix} m_1 & m_2 & m_3 & \\cdots & m_d \\\\ p_1 & 0 & 0 & \\cdots & 0 \\\\ 0 & p_2 & 0 & \\cdots & 0 \\\\ \\vdots & & \\vdots & & \\vdots \\\\ 0 & \\cdots & 0 & p_{d-1} & 0\\\\ \\end{pmatrix} \\] We call it a Leslie matrix and often denote it with \\(\\mathbf{L}\\) . The first row contains the fecundities of each age group, \\(m_i\\) , while the entries immediately below the diagonal give the fraction of individuals that survive each age group, \\(p_i\\) . Because of the structure of the Leslie matrix, many expressions are now simpler. For example, the characteristic polynomial, \\(\\mathrm{Det}(\\mathbf{L}-\\mathbf{I}\\lambda)=0\\) , can be calculated using the first row. After rearranging we get what is known as the Euler-Lotka equation \\[ 1 = \\sum_{i=1}^{d} \\frac{l_i m_i}{\\lambda^i} \\] where \\(l_1 = 1\\) , \\(l_i = p_1 p_2 \\cdots p_{i-1}\\) is the fraction of individuals that survive from birth to age \\(i\\) and \\(d\\) is the number of ages (ie, \\(\\mathbf{L}\\) is a \\(d\\times d\\) matrix). Given the \\(l_i\\) and \\(m_i\\) we can use this equation to find the long-term population growth rate, \\(\\lambda_1\\) . Example: stickleback For example, let's look at a model of stickleback , a small fish (see section 10.6 of the text). We assume stickleback do not live more than 4 years and estimate the Leslie matrix as \\[ \\mathbf{L} = \\begin{pmatrix} 2 & 3 & 4 & 4\\\\ 0.6 & 0 & 0 & 0 \\\\ 0 & 0.3 & 0 & 0 \\\\ 0 & 0 & 0.1 & 0 \\end{pmatrix} \\] The Euler-Lotka equation is then \\[ \\begin{aligned} 1 &= \\sum_{i=1}^{4} \\frac{l_i m_i}{\\lambda^i}\\\\ 1 &= \\frac{l_1 m_1}{\\lambda} + \\frac{l_2 m_2}{\\lambda^2} + \\frac{l_3 m_3}{\\lambda^3} + \\frac{l_4 m_4}{\\lambda^4}\\\\ 1 &= \\frac{2}{\\lambda} + \\frac{1.8}{\\lambda^2} + \\frac{0.72}{\\lambda^3} + \\frac{0.072}{\\lambda^4}\\\\ \\end{aligned} \\] This can be numerically solved to give our four eigenvalues: \\(\\lambda\\approx2.75, -0.3 \\pm 0.3i, -0.14\\) . The long-term growth rate is the eigenvalue with the largest absolute value, \\(\\lambda_1=2.75\\) . Note We won't spend the time on it in class, but with age-structure we can also write the stable age distribution, reproductive values, and sensitivities as functions of survival, fecundity, and rate of population growth. The proportion of individuals that are age \\(x\\) (in the long-run) is \\[ v_x = \\frac{l_x \\lambda_1^{-(x-1)}}{\\sum_{i=1}^{d}l_i \\lambda_1^{-(i-1)}} \\] The reproductive value of individuals that are age \\(x\\) , relative to age \\(1\\) , is \\[ \\frac{u_x}{u_1} = \\frac{\\lambda_1^{x-1}}{l_x}\\sum_{i=x}^{d}\\frac{l_i m_i}{\\lambda_1^{i}} \\] The sensitivities of the long-term growth rate to survival and reproduction at age \\(x\\) are \\[ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_x} = \\frac{v_x u_{x+1}}{\\vec{u}_1 \\vec{v}_1} \\] \\[ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_x} = \\frac{v_x u_1}{\\vec{u}_1 \\vec{v}_1} \\] For example, in our stickleback model the proportion of the population that is age \\(x=2\\) , in the long-run, is \\[ \\begin{aligned} v_x &= \\frac{l_x \\lambda_1^{-(x-1)}}{\\sum_{i=1}^{d}l_i \\lambda_1^{-(i-1)}}\\\\ v_2 &= \\frac{l_2 \\lambda_1^{-1}}{\\sum_{i=1}^{4}l_i \\lambda_1^{-(i-1)}}\\\\ &\\approx \\frac{0.6 (2.75)^{-1}}{1 (2.75)^{0} + 0.6 (2.75)^{-1} + 0.18 (2.75)^{-2} + 0.018 (2.75)^{-3}}\\\\ &\\approx 0.18 \\end{aligned} \\] and the relative reproductive value of age \\(x=2\\) individuals is \\[ \\begin{aligned} \\frac{v_x}{v_1} &= \\frac{\\lambda_1^{x-1}}{l_x}\\sum_{i=x}^{d}\\frac{l_i m_i}{\\lambda_1^{i}}\\\\ \\frac{v_2}{v_1} &= \\frac{\\lambda_1^{2-1}}{l_2}\\sum_{i=2}^{4}\\frac{l_i m_i}{\\lambda_1^{i}}\\\\ &= \\frac{\\lambda_1}{l_2} \\left(\\frac{l_2 m_2}{\\lambda_1^{2}} + \\frac{l_3 m_3}{\\lambda_1^{3}} + \\frac{l_4 m_4}{\\lambda_1^{i}} \\right)\\\\ &\\approx \\frac{2.75}{0.6} \\left(\\frac{(0.6) (3)}{2.75^{2}} + \\frac{(0.18) (4)}{2.75^{3}} + \\frac{(0.018) (4)}{2.75^{4}} \\right)\\\\ &\\approx 1.25 \\end{aligned} \\] Repeating these for the other ages we get the stable-age distribution \\[ \\vec{v}_1 \\approx \\begin{pmatrix} 0.80 \\\\ 0.18 \\\\ 0.02 \\\\ 0.0007 \\end{pmatrix} \\] and the reproductive values \\[ \\vec{u}_1 \\approx \\begin{pmatrix} 1 & 1.25 & 1.51 & 1.45 \\end{pmatrix} \\] From these we see that the majority of the population is expected to be age 1 (from \\(\\vec{v}_1\\) ) and age 3 has the highest reproductive value (from \\(\\vec{u}_1\\) ). We can now use these vectors to calculate the sensitivities of population growth rate to survival \\[ \\begin{aligned} \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_x} &= \\frac{v_x u_{x+1}}{\\vec{u}_1 \\vec{v}_1}\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_1} &\\approx 0.95\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_2} &\\approx 0.25\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_3} &\\approx 0.03 \\end{aligned} \\] and to fecundity \\[ \\begin{aligned} \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_x} &= \\frac{v_x u_1}{\\vec{u}_1 \\vec{v}_1}\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_1} &\\approx 0.76\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_2} &\\approx 0.17\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_3} &\\approx 0.02\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_4} &\\approx 0.0007 \\end{aligned} \\] And so we see that, because such a large proportion of the population is expected to be age 1, increasing the survival and fecundity of that age has the largest effect. Finally, we can plot the predicted dynamics as we did for the whales above. In this case the population grows very quickly (clearly there is a need to also model density-dependence via competition, otherwise our model predicts that the universe will soon be stuffed-full with sticklback!), so we plot the number of fish of each age on a log scale. Again, we compare the full general solution (solid) to the long-term approximation (dashed) and see quick convergence. m1,m2,m3,m4,p1,p2,p3 = 2,3,4,4,0.6,0.3,0.1 #parameter values import numpy as np M = np.array([[m1,m2,m3,m4], #projection matrix [p1,0,0,0], [0,p2,0,0], [0,0,p3,0]]) # calculate eigs = np.linalg.eig(M) #eigenvalues and right eigenvectors ix = np.argmax(np.abs(eigs[0])) #which is leading eigenvalue l1 = eigs[0][ix] #leading eigenvalue v1 = eigs[1][:,ix] #leading right eigenvector v1 = v1/np.sum(v1) #normalized to sum to 1 us = np.linalg.inv(eigs[1]) #left eigenvectors u1 = us[ix] #leading left eigenvalue u1 = u1/np.dot(u1,v1) #normalized so u1*v1=1 n0 = np.array([25,25,25,25]) #initial state def ntfull(t): '''full projection''' return np.dot(np.linalg.matrix_power(M,t), n0) def ntapp(t): '''long term approximation''' return l1**t * v1 * np.dot(u1, n0) # plot import matplotlib.pyplot as plt fig, ax = plt.subplots() labels = ['age 1','age 2','age 3','age 4'] colors = plt.get_cmap('tab10') for i,j in enumerate(range(len(M))): #for each stage ax.plot([ntfull(t)[j] for t in range(10)], color=colors(i), label=labels[i]) ax.plot([ntapp(t)[j] for t in range(10)], color=colors(i), linestyle='--') #long-term approx ax.legend() ax.set_xlabel('years') ax.set_ylabel('number of stickleback') ax.set_yscale('log') plt.show()","title":"Lecture 14"},{"location":"lectures/lecture-14/#lecture-14-demography","text":"Run notes interactively?","title":"Lecture 14: Demography"},{"location":"lectures/lecture-14/#lecture-overview","text":"Demography Stage-structure Age-structure","title":"Lecture overview"},{"location":"lectures/lecture-14/#1-demography","text":"We're now going to use what we've learned about linear multivariate models to describe the dynamics of a single population that is composed of different types of individuals. This area of research is called demography . We'll just consider discrete-time models here. In the last lecture we saw that for any discrete-time linear multivariate model \\[ \\vec{n}(t+1) = \\mathbf{M}\\vec{n}(t) \\] we can compute the general solution \\[ \\vec{n}(t) = \\mathbf{M}^t\\vec{n}(0) \\] or, more conveniently, in terms of the eigenvalues ( \\(\\mathbf{D}\\) ) and eigenvectors ( \\(\\mathbf{A}\\) ) \\[ \\vec{n}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{n}(0 ) \\] Despite this progress, the eigenvalues and eigenvectors are often unobtainable (without specifying parameter values), leaving us to rely on the long-term approximation \\[ \\vec{n}(t) = \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0) \\] For this we just need to know the leading eigenvalue ( \\(\\lambda_1\\) ) and the corresponding right ( \\(\\vec{v}_1\\) ) and left ( \\(\\vec{u}_1\\) ) eigenvectors, respectively. (Remember that we have scaled the eigenvectors such that \\(\\vec{u}_1\\vec{v}_1=1\\) .) These three components ( \\(\\lambda_1\\) , \\(\\vec{v}_1\\) , \\(\\vec{u}_1\\) ) are the key demographic quantities that we will investigate \\(\\lambda_1\\) is the long-term population growth rate \\(\\vec{v}_1\\) describes the stable stage-distribution \\(\\vec{u}_1\\) describes the relative reproductive values of each stage Note The long-term approximation, \\(\\vec{n}(t) = \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0)\\) , is valid as long as the leading eigenvalue, \\(\\lambda_1\\) , is real (no cycles in long-term) positive (no oscillations to negative numbers!) larger than all other eigenvalues (so that we can ignore the other eigenvalues/vectors) Fortunately we are guaranteed all these conditions in our demographic models since all entries of \\(\\mathbf{M}\\) are non-negative and all entries of \\(\\mathbf{M}^t\\) are positive for some value of \\(t\\) . (This follows from something called the Perron-Frobenius Theorem .) The most general demographic model is called stage-structure : we consider some finite number of discrete stages that an individual can be in and we use an abitrary matrix of transition rates between stages (a projection matrix ), \\(\\mathbf{M}\\) , to project how the population size and composition changes over time. A common special case is age-structure : here we define the stages as the number of time steps an individual has been alive for, which leads to a simpler projection matrix (called a Leslie matrix ) because individuals always transition to the next stage (or die).","title":"1. Demography"},{"location":"lectures/lecture-14/#2-stage-structure","text":"","title":"2. Stage-structure"},{"location":"lectures/lecture-14/#example-north-atlantic-right-whale","text":"Let's consider an example (from 10.2 in the text). North Atlantic right whales were hunted to near extinction in the 1800s and early 1900s, after which their population size is thought to have slowly recovered (there are less than 400 now, and unfortunately appear to be in decline again ). Because of their long life span, over which survival and reproductive rates vary enormously, a stage-structured model is very appropriate. Below we draw a flow diagram representing all the transitions between calves, sexually immature individuals, sexually mature individuals, and actively reproducing individuals. graph LR; C((Calves)) --sIC nC--> I((Immature)); I --sII nI--> I; I --sMI nI--> M((Mature)); I --sRI nI--> R((Reproductive)); M --sMM nM--> M; M --sRM nM--> R; R --sRR nR--> R; R --sMR nR--> M; R --b nR--> C; Converting this flow diagram into a system of recursion equations, \\(\\vec{n}(t+1) = \\mathbf{M}\\vec{n}(t)\\) , the projection matrix is \\[ \\mathbf{M} = \\begin{pmatrix} 0 & 0 & 0 & b \\\\ s_{IC} & s_{II} & 0 & 0\\\\ 0 & s_{MI} & s_{MM} & s_{MR} \\\\ 0 & s_{RI} & s_{RM} & s_{RR} \\end{pmatrix} \\] If we now plug in some estimated parameter values from the literature ( \\(b=0.3\\) , \\(s_{IC}=0.92\\) , \\(s_{II}=0.86\\) , \\(s_{MI}=0.08\\) , \\(s_{MM}=0.8\\) , \\(s_{MR}=0.88\\) , \\(s_{RI}=0.02\\) , \\(s_{RM}=0.19\\) , \\(s_{RR}=0\\) ) we can numerically calculate the three key demographic quantities \\[ \\begin{aligned} &\\lambda_1 \\approx 1.003 \\\\ &\\vec{v}_1 \\approx \\begin{pmatrix} 0.04 \\\\ 0.23 \\\\ 0.61 \\\\ 0.12\\end{pmatrix} \\\\ &\\vec{u}_1 \\approx \\begin{pmatrix} 0.69 & 0.76 & 1.07 & 1.15 \\end{pmatrix} \\end{aligned} \\] These quantitives tells us, for example, that in the long-run the population is predicted to grow ( \\(\\lambda_1>1\\) ), the majority of individuals are expected to be mature (the second last entry in \\(\\vec{v}_1\\) is the largest), and mature and reproductive individuals are expected to have much higher reproductive values than calves and immature individuals (the last two entries in \\(\\vec{u}_1\\) are much larger than the first two). Caveat The parameter values above were estimated before the current population decline -- changes in climate and human behaviour, eg boat traffic and fishing, have presumably altered the parameter values, making the predictions less accurate. For example, the increased incidence of entanglement in fishing nets has likely decreased survival rates to the point that the population is now expected to decline. See here for more info. We can now answer an important conservation question: Question If we wanted to increase the total population size in the future and we could add one individual to any stage, which stage should it be? We want to know what entry of \\(\\vec{n}(0)\\) to add 1 to to maximize \\(\\vec{n}(t) \\approx \\lambda^t \\vec{v}_1 \\vec{u}_1 \\vec{n}(0)\\) . This will not affect the long-term growth rate ( \\(\\lambda_1\\) ) or the stable-stage distribution ( \\(\\vec{v}_1\\) ). We can therefore only increase \\(\\vec{u}_1\\vec{n}(0) = u_1 n_1(0) + u_2 n_2(0) + ... + u_m n_m(0)\\) . And so we add 1 to the stage with the largest reproductive value, \\(u_i\\) . So for these whales, where the reproductive values are \\(\\vec{u}_1 \\approx \\begin{pmatrix} 0.69 & 0.76 & 1.07 & 1.15 \\end{pmatrix}\\) , we should add a reproductively active individual. This is perhaps not surprising since those are the only individuals that produce offspring and if we introduced an individual in an earlier stage there is some chance that they would die before becoming reproductively active. One other question we might now consider is: Question If we wanted to increase the long-term population growth rate and we could increase any parameter a little bit, which parameter should it be? One way to answer this is to numerically calculate the long-term population growth rate, \\(\\lambda_1\\) , for a range of values of one parameter, \\(z\\) (e.g., we might take \\(z=b\\) , the fecundity of reproductive individuals). Or, we could try to analytically compute the rate of change in the leading eigenvalue with respect to \\(z\\) , \\(\\mathrm{d}\\lambda_1/\\mathrm{d}z\\) , while considering \\(\\mathbf{M}\\) , \\(\\lambda_1\\) , \\(\\vec{v}_1\\) , and \\(\\vec{u}_1\\) to be functions of \\(z\\) . Since \\(\\mathbf{M}\\vec{v}_1 = \\lambda_1 \\vec{v}_1\\) and \\(\\vec{u}_1\\mathbf{M} = \\vec{u}_1\\lambda_1\\) we have \\[ \\begin{aligned} \\mathbf{M} \\vec{v}_1 &= \\lambda_1 \\vec{v}_1 \\\\ \\vec{u}_1 \\mathbf{M} \\vec{v}_1 &= \\vec{u}_1 \\lambda_1 \\vec{v}_1 \\\\ \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left(\\vec{u}_1 \\mathbf{M} \\vec{v}_1 \\right)&= \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left(\\vec{u}_1 \\lambda_1 \\vec{v}_1 \\right)\\\\ \\frac{\\mathrm{d}\\vec{u}_1}{\\mathrm{d}z} \\mathbf{M} \\vec{v}_1 + \\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1 + \\vec{u}_1 \\mathbf{M} \\frac{\\mathrm{d}\\vec{v}_1}{\\mathrm{d}z} &= \\frac{\\mathrm{d}\\vec{u}_1}{\\mathrm{d}z} \\lambda_1 \\vec{v}_1 + \\vec{u}_1 \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\vec{v}_1 + \\vec{u}_1 \\lambda_1 \\frac{\\mathrm{d}\\vec{v}_1}{\\mathrm{d}z}\\\\ \\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1 &= \\vec{u}_1 \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\vec{v}_1 \\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} &= \\frac{\\vec{u}_1 \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{v}_1}{\\vec{u}_1 \\vec{v}_1} \\end{aligned}\\] We call \\(\\mathrm{d}\\lambda_1/\\mathrm{d}z\\) the sensitivity of \\(\\lambda_1\\) to \\(z\\) . This expression will be too complicated to understand in general in most cases, but we can evaluate at some particular value \\(z^*\\) (e.g., at the current estimate) to see how the growth rate changes as \\(z\\) increases from that value \\[ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\bigg|_{z=z^*} = \\frac{\\vec{u}_1 \\left(\\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\big|_{z=z^*} \\right) \\vec{v}_1}{\\vec{u}_1 \\vec{v}_1} \\] Returning to our question, we want to know which parameter, \\(z\\) , gives the largest value of \\(\\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\big|_{z=z^*}\\) . For these whales we can calculate \\(\\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}z} \\big|_{z=z^*}\\) for each parameter \\(z\\) (ie, for each entry in \\(\\mathbf{M}\\) ) evaluated at it's current value \\(z^*\\) . Doing that calculation we see that increasing the fraction of mature individuals that survive to become reproductively active, \\(s_{RM}\\) , has the largest effect. This makes good sense because increasing \\(s_{RM}\\) increases the rate at which the most populous stage (from \\(\\vec{v}_1\\) ) transitions to the stage with the highest reproductive value (from \\(\\vec{u}_1\\) ). And finally, we can ask Question What are the predicted numbers of individuals in each stage over time? Here we simply plot the general solution (solid) and compare with the long-term approximation (dashed) out of interest. We see that the full solution very quickly converges to the long-term solution in this case. b,sic,sii,smi,smm,smr,sri,srm,srr = 0.3,0.92,0.86,0.08,0.8,0.88,0.02,0.19,0 #parameter values import numpy as np M = np.array([[0,0,0,b], #projection matrix [sic,sii,0,0], [0,smi,smm,smr], [0,sri,srm,srr]]) # calculate eigs = np.linalg.eig(M) #eigenvalues and right eigenvectors ix = np.argmax(np.abs(eigs[0])) #which is leading eigenvalue l1 = eigs[0][ix] #leading eigenvalue v1 = eigs[1][:,ix] #leading right eigenvector v1 = v1/np.sum(v1) #normalized to sum to 1 us = np.linalg.inv(eigs[1]) #left eigenvectors u1 = us[ix] #leading left eigenvalue u1 = u1/np.dot(u1,v1) #normalized so u1*v1=1 n0 = np.array([50,50,50,50]) #initial state def ntfull(t): '''full projection''' return np.dot(np.linalg.matrix_power(M,t), n0) def ntapp(t): '''long term approximation''' return l1**t * v1 * np.dot(u1, n0) # plot import matplotlib.pyplot as plt fig, ax = plt.subplots() labels = ['calves','immature','mature','reproductive'] colors = plt.get_cmap('tab10') for i,j in enumerate(range(len(M))): #for each stage ax.plot([ntfull(t)[j] for t in range(100)], color=colors(i), label=labels[i]) ax.plot([ntapp(t)[j] for t in range(100)], color=colors(i), linestyle='--') #long-term approx ax.legend() ax.set_xlabel('years') ax.set_ylabel('number of whales') plt.show()","title":"Example: North Atlantic right whale"},{"location":"lectures/lecture-14/#3-age-structure","text":"Now let's look at the special case of age-structure . Here individuals in stage \\(i\\) at time \\(t\\) can only contribute to stage \\(i+1\\) (survival) and stage \\(1\\) (reproduction) at time \\(t+1\\) . graph LR; 1((1)) --p1 n1--> 2((2)); 1 --m1 n1--> 1; 2 --p2 n2--> 3((3)); 2 --m2 n2--> 1; 3 --p3 n4--> 4((4)); 3 --m3 n3--> 1; 4 --m4 n4--> 1; Because of this, the projection matrix is simpler in the sense that it contains more zeros and has non-zero entries in very specific places \\[ \\mathbf{L} = \\begin{pmatrix} m_1 & m_2 & m_3 & \\cdots & m_d \\\\ p_1 & 0 & 0 & \\cdots & 0 \\\\ 0 & p_2 & 0 & \\cdots & 0 \\\\ \\vdots & & \\vdots & & \\vdots \\\\ 0 & \\cdots & 0 & p_{d-1} & 0\\\\ \\end{pmatrix} \\] We call it a Leslie matrix and often denote it with \\(\\mathbf{L}\\) . The first row contains the fecundities of each age group, \\(m_i\\) , while the entries immediately below the diagonal give the fraction of individuals that survive each age group, \\(p_i\\) . Because of the structure of the Leslie matrix, many expressions are now simpler. For example, the characteristic polynomial, \\(\\mathrm{Det}(\\mathbf{L}-\\mathbf{I}\\lambda)=0\\) , can be calculated using the first row. After rearranging we get what is known as the Euler-Lotka equation \\[ 1 = \\sum_{i=1}^{d} \\frac{l_i m_i}{\\lambda^i} \\] where \\(l_1 = 1\\) , \\(l_i = p_1 p_2 \\cdots p_{i-1}\\) is the fraction of individuals that survive from birth to age \\(i\\) and \\(d\\) is the number of ages (ie, \\(\\mathbf{L}\\) is a \\(d\\times d\\) matrix). Given the \\(l_i\\) and \\(m_i\\) we can use this equation to find the long-term population growth rate, \\(\\lambda_1\\) .","title":"3. Age-structure"},{"location":"lectures/lecture-14/#example-stickleback","text":"For example, let's look at a model of stickleback , a small fish (see section 10.6 of the text). We assume stickleback do not live more than 4 years and estimate the Leslie matrix as \\[ \\mathbf{L} = \\begin{pmatrix} 2 & 3 & 4 & 4\\\\ 0.6 & 0 & 0 & 0 \\\\ 0 & 0.3 & 0 & 0 \\\\ 0 & 0 & 0.1 & 0 \\end{pmatrix} \\] The Euler-Lotka equation is then \\[ \\begin{aligned} 1 &= \\sum_{i=1}^{4} \\frac{l_i m_i}{\\lambda^i}\\\\ 1 &= \\frac{l_1 m_1}{\\lambda} + \\frac{l_2 m_2}{\\lambda^2} + \\frac{l_3 m_3}{\\lambda^3} + \\frac{l_4 m_4}{\\lambda^4}\\\\ 1 &= \\frac{2}{\\lambda} + \\frac{1.8}{\\lambda^2} + \\frac{0.72}{\\lambda^3} + \\frac{0.072}{\\lambda^4}\\\\ \\end{aligned} \\] This can be numerically solved to give our four eigenvalues: \\(\\lambda\\approx2.75, -0.3 \\pm 0.3i, -0.14\\) . The long-term growth rate is the eigenvalue with the largest absolute value, \\(\\lambda_1=2.75\\) . Note We won't spend the time on it in class, but with age-structure we can also write the stable age distribution, reproductive values, and sensitivities as functions of survival, fecundity, and rate of population growth. The proportion of individuals that are age \\(x\\) (in the long-run) is \\[ v_x = \\frac{l_x \\lambda_1^{-(x-1)}}{\\sum_{i=1}^{d}l_i \\lambda_1^{-(i-1)}} \\] The reproductive value of individuals that are age \\(x\\) , relative to age \\(1\\) , is \\[ \\frac{u_x}{u_1} = \\frac{\\lambda_1^{x-1}}{l_x}\\sum_{i=x}^{d}\\frac{l_i m_i}{\\lambda_1^{i}} \\] The sensitivities of the long-term growth rate to survival and reproduction at age \\(x\\) are \\[ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_x} = \\frac{v_x u_{x+1}}{\\vec{u}_1 \\vec{v}_1} \\] \\[ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_x} = \\frac{v_x u_1}{\\vec{u}_1 \\vec{v}_1} \\] For example, in our stickleback model the proportion of the population that is age \\(x=2\\) , in the long-run, is \\[ \\begin{aligned} v_x &= \\frac{l_x \\lambda_1^{-(x-1)}}{\\sum_{i=1}^{d}l_i \\lambda_1^{-(i-1)}}\\\\ v_2 &= \\frac{l_2 \\lambda_1^{-1}}{\\sum_{i=1}^{4}l_i \\lambda_1^{-(i-1)}}\\\\ &\\approx \\frac{0.6 (2.75)^{-1}}{1 (2.75)^{0} + 0.6 (2.75)^{-1} + 0.18 (2.75)^{-2} + 0.018 (2.75)^{-3}}\\\\ &\\approx 0.18 \\end{aligned} \\] and the relative reproductive value of age \\(x=2\\) individuals is \\[ \\begin{aligned} \\frac{v_x}{v_1} &= \\frac{\\lambda_1^{x-1}}{l_x}\\sum_{i=x}^{d}\\frac{l_i m_i}{\\lambda_1^{i}}\\\\ \\frac{v_2}{v_1} &= \\frac{\\lambda_1^{2-1}}{l_2}\\sum_{i=2}^{4}\\frac{l_i m_i}{\\lambda_1^{i}}\\\\ &= \\frac{\\lambda_1}{l_2} \\left(\\frac{l_2 m_2}{\\lambda_1^{2}} + \\frac{l_3 m_3}{\\lambda_1^{3}} + \\frac{l_4 m_4}{\\lambda_1^{i}} \\right)\\\\ &\\approx \\frac{2.75}{0.6} \\left(\\frac{(0.6) (3)}{2.75^{2}} + \\frac{(0.18) (4)}{2.75^{3}} + \\frac{(0.018) (4)}{2.75^{4}} \\right)\\\\ &\\approx 1.25 \\end{aligned} \\] Repeating these for the other ages we get the stable-age distribution \\[ \\vec{v}_1 \\approx \\begin{pmatrix} 0.80 \\\\ 0.18 \\\\ 0.02 \\\\ 0.0007 \\end{pmatrix} \\] and the reproductive values \\[ \\vec{u}_1 \\approx \\begin{pmatrix} 1 & 1.25 & 1.51 & 1.45 \\end{pmatrix} \\] From these we see that the majority of the population is expected to be age 1 (from \\(\\vec{v}_1\\) ) and age 3 has the highest reproductive value (from \\(\\vec{u}_1\\) ). We can now use these vectors to calculate the sensitivities of population growth rate to survival \\[ \\begin{aligned} \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_x} &= \\frac{v_x u_{x+1}}{\\vec{u}_1 \\vec{v}_1}\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_1} &\\approx 0.95\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_2} &\\approx 0.25\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}p_3} &\\approx 0.03 \\end{aligned} \\] and to fecundity \\[ \\begin{aligned} \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_x} &= \\frac{v_x u_1}{\\vec{u}_1 \\vec{v}_1}\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_1} &\\approx 0.76\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_2} &\\approx 0.17\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_3} &\\approx 0.02\\\\ \\frac{\\mathrm{d}\\lambda_1}{\\mathrm{d}m_4} &\\approx 0.0007 \\end{aligned} \\] And so we see that, because such a large proportion of the population is expected to be age 1, increasing the survival and fecundity of that age has the largest effect. Finally, we can plot the predicted dynamics as we did for the whales above. In this case the population grows very quickly (clearly there is a need to also model density-dependence via competition, otherwise our model predicts that the universe will soon be stuffed-full with sticklback!), so we plot the number of fish of each age on a log scale. Again, we compare the full general solution (solid) to the long-term approximation (dashed) and see quick convergence. m1,m2,m3,m4,p1,p2,p3 = 2,3,4,4,0.6,0.3,0.1 #parameter values import numpy as np M = np.array([[m1,m2,m3,m4], #projection matrix [p1,0,0,0], [0,p2,0,0], [0,0,p3,0]]) # calculate eigs = np.linalg.eig(M) #eigenvalues and right eigenvectors ix = np.argmax(np.abs(eigs[0])) #which is leading eigenvalue l1 = eigs[0][ix] #leading eigenvalue v1 = eigs[1][:,ix] #leading right eigenvector v1 = v1/np.sum(v1) #normalized to sum to 1 us = np.linalg.inv(eigs[1]) #left eigenvectors u1 = us[ix] #leading left eigenvalue u1 = u1/np.dot(u1,v1) #normalized so u1*v1=1 n0 = np.array([25,25,25,25]) #initial state def ntfull(t): '''full projection''' return np.dot(np.linalg.matrix_power(M,t), n0) def ntapp(t): '''long term approximation''' return l1**t * v1 * np.dot(u1, n0) # plot import matplotlib.pyplot as plt fig, ax = plt.subplots() labels = ['age 1','age 2','age 3','age 4'] colors = plt.get_cmap('tab10') for i,j in enumerate(range(len(M))): #for each stage ax.plot([ntfull(t)[j] for t in range(10)], color=colors(i), label=labels[i]) ax.plot([ntapp(t)[j] for t in range(10)], color=colors(i), linestyle='--') #long-term approx ax.legend() ax.set_xlabel('years') ax.set_ylabel('number of stickleback') ax.set_yscale('log') plt.show()","title":"Example: stickleback"},{"location":"lectures/lecture-15/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 15: Equilibria and stability (nonlinear multivariate) Run notes interactively? Lecture overview Continuous time Discrete time Summary Now that we've covered linear multivariate models, we turn our attention to the most common type of model: one with multiple interacting variables. For example, a model of the number of susceptible, \\(S\\) , and infected, \\(I\\) , individuals often includes the interaction between these two variables, \\(SI\\) , describing the rate at which these two classes of individuals meet one another (and potentially spread disease). Similarly, models of predator, \\(P\\) , and prey, \\(N\\) , abundance often include terms like \\(NP\\) describing the rate at which predators encounter prey (and potentially eat them). Let's first see how to deal with these models in general and then apply those techniques to specific circumstances like those mentioned above (in the next two lectures). 1. Continuous time In general, if we have \\(n\\) interacting variables, \\(x_1, x_2, ..., x_n\\) , we can write any continuous time model like \\[ \\begin{aligned} \\frac{\\mathrm{d}x_1}{\\mathrm{d}t} &= f_1(x_1, x_2, ..., x_n)\\\\ \\frac{\\mathrm{d}x_2}{\\mathrm{d}t} &= f_2(x_1, x_2, ..., x_n)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}x_n}{\\mathrm{d}t} &= f_n(x_1, x_2, ..., x_n) \\end{aligned} \\] If we then want to find the equilibria, \\(\\hat{x}_1,\\hat{x}_2, ..., \\hat{x}_n\\) , we set all these equations to 0 and solve for one variable at a time (note that solving for the equilibrium is not always possible in nonlinear models!). Now note that we can no longer write this system of equations in matrix form with a matrix composed only of parameters. In order to use what we've learned about eigenvalues and eigenvectors we're first going to have to linearize the system so that the corresponding matrices do not contain variables. As we saw in nonlinear univariate models, one useful way to linearize a system is to measure the system relative to equilibrium, \\(\\epsilon = n - \\hat{n}\\) . Then assuming that the deviation from equilibrium, \\(\\epsilon\\) , is small, we used a Taylor series expansion to approximate the nonlinear system with a linear system. To do that with multivariate models we'll need to know how to take a Taylor series expansion of multivariate functions Taylor series expansion of a multivariate function Taking the series of \\(f\\) around \\(x_1=a_1\\) , \\(x_2=a_2\\) , ..., \\(x_n=a_n\\) gives \\[ \\begin{aligned} f(x_1, x_2, ..., x_n) &= f(a_1, a_2, ..., a_n)\\\\ &+ \\sum_{i=1}^{n} \\left( \\frac{\\partial f}{\\partial x_i} \\bigg|_{x_1=a_1, x_2=a_2, ..., x_n=a_n} \\right) (x_i - a_i)\\\\ &+ \\sum_{i=1}^{n}\\sum_{j=1}^n \\left( \\frac{\\partial f}{\\partial x_i \\partial x_j} \\bigg|_{x_1=a_1, x_2=a_2, ..., x_n=a_n} \\right) (x_i - a_i)(x_j - a_j)\\\\ &+ \\cdots \\end{aligned} \\] where \\(\\frac{\\partial f}{\\partial x_i}\\) is the \"partial derivative\" of \\(f\\) with respect to \\(x_i\\) , meaning that we treat all the other variables as constants when taking the derivative. Then when the difference between each variable and its value, \\(x_i-a_i\\) , is small enough we can ignore all the terms with a \\((x_i-a_i)(x_j-a_j)\\) , and we are left with a linear approximation of \\(f\\) . So let \\(\\epsilon_i = x_i - \\hat{x}_i\\) be the deviation of variable \\(x_i\\) from its equilibrium value, \\(\\hat{x}_i\\) . Then we can write a system of equations describing the change in the deviations for all of our variables \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &= f_1(x_1, x_2, ..., x_n)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &= f_2(x_1, x_2, ..., x_n)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &= f_n(x_1, x_2, ..., x_n) \\end{aligned} \\] And then we can take a Taylor series around \\(x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n\\) to get a linear approximation of our system near the equilibrium \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &\\approx f_1(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &\\approx f_2(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &\\approx f_n(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i) \\end{aligned} \\] And then note that all \\(f_i(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n)=0\\) by definition of a equilibrium, leaving \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i) \\end{aligned} \\] Each of the partial derivatives \\(\\frac{\\partial f_i}{\\partial x_j}\\) is evaluated at the equilibrium, so these are constants. And \\(x_i - \\hat{x}_i = \\epsilon_i\\) . So we now have a linear system \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i \\end{aligned} \\] We can now write our approximate system around the equilibrium in matrix form \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} \\\\ \\vdots \\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix} \\end{aligned} \\] This is a special matrix called the Jacobian \\[ \\mathbf{J} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix} \\] And now that we have a linear system around an equilibrium, we can assess its local stability just as we did with linear multivariate models (see Summary ). 2. Discrete time Note The short version of this section is that we can do the same thing in discrete time -- local stability is determined by the eigenvalues of the Jacobian, where the functions in that Jacobian are now our recursions, \\(x_i(t+1) = f_i(x_1(t), x_2(t), ..., x_n(t))\\) . We can do something very similar for nonlinear multivariate models in discrete time \\[ \\begin{aligned} x_1(t+1) &= f_1(x_1(t), x_2(t), ..., x_n(t))\\\\ x_2(t+1) &= f_2(x_1(t), x_2(t), ..., x_n(t))\\\\ &\\vdots\\\\ x_n(t+1) &= f_n(x_1(t), x_2(t), ..., x_n(t))\\\\ \\end{aligned} \\] Now the equilibria are found by setting all \\(x_i(t+1) = x_i(t) = \\hat{x}_i\\) and solving for the \\(\\hat{x}_i\\) one at a time. To linearize the system around an equilibrium we again measure the system in terms of deviation from the equilibrium, \\(\\epsilon_i(t) = x_i(t) - \\hat{x}_i\\) , giving \\[ \\begin{aligned} \\epsilon_1(t+1) &= f_1(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ \\epsilon_2(t+1) &= f_2(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ &\\vdots\\\\ \\epsilon_n(t+1) &= f_n(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ \\end{aligned} \\] Then taking the Taylor series of each \\(f_i\\) around \\(x_1(t) = \\hat{x}_1, ..., x_n(t) = \\hat{x}_n\\) we can approximate our system near the equilibrium as \\[ \\begin{aligned} \\epsilon_1(t+1) &= f_1(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_1\\\\ \\epsilon_2(t+1) &= f_2(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_2\\\\ &\\vdots\\\\ \\epsilon_n(t+1) &= f_n(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_n\\\\ \\end{aligned} \\] And noting that \\(f_i(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) = \\hat{x}_i\\) and \\(x_i(t) - \\hat{x}_i = \\epsilon_i(t)\\) we have \\[ \\begin{aligned} \\epsilon_1(t+1) &= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\ \\epsilon_2(t+1) &= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\ &\\vdots\\\\ \\epsilon_n(t+1) &= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\ \\end{aligned} \\] We can therefore write our approximation around the equilibrium in matrix form \\[ \\begin{aligned} \\begin{pmatrix} \\epsilon_1(t+1) \\\\ \\epsilon_2(t+1) \\\\ \\vdots \\\\ \\epsilon_n(t+1) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\begin{pmatrix} \\epsilon_1(t) \\\\ \\epsilon_2(t) \\\\ \\vdots \\\\ \\epsilon_n(t) \\end{pmatrix} \\end{aligned} \\] As in continuous time, the dynamics are described by the Jacobian matrix \\[ \\mathbf{J} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix} \\] We therefore assess the local stability of an equilibrium by evaluating the Jacobian at that equilibrium and finding the eigenvalues (see Summary ). 3. Summary We can determine the stability of nonlinear multivariate models with the eigenvalues of the Jacobian evaluated at an equilibrium. The recipe is Find the equilibrium of interest, \\(\\hat{x}_1, \\hat{x}_2, ... \\hat{x}_n\\) Calculate the Jacobian, \\(\\mathbf{J}\\) Evaluate the Jacobian at the equilibrium of interest, \\(\\mathbf{J}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n}\\) Calculate the characteristic polynomial \\(|\\mathbf{J}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} - \\lambda\\mathbf{I}|\\) Set the characteristic polynomial equal to 0 and solve for the \\(n\\) eigenvalues, \\(\\lambda\\) Stability reminder Continuous time the leading eigenvalue is the one with the largest real part if the leading eigenvalue has a negative real part the equilibrium is stable if any eigenvalue has a non-zero complex part there will be cycling Discrete time the leading eigenvalue is the one with the largest absolute value if the leading eigenvalue has an absolute value less than one the equilibrium is stable if any eigenvalue has a non-zero complex part there will be cycling","title":"Lecture 15"},{"location":"lectures/lecture-15/#lecture-15-equilibria-and-stability-nonlinear-multivariate","text":"Run notes interactively?","title":"Lecture 15: Equilibria and stability (nonlinear multivariate)"},{"location":"lectures/lecture-15/#lecture-overview","text":"Continuous time Discrete time Summary Now that we've covered linear multivariate models, we turn our attention to the most common type of model: one with multiple interacting variables. For example, a model of the number of susceptible, \\(S\\) , and infected, \\(I\\) , individuals often includes the interaction between these two variables, \\(SI\\) , describing the rate at which these two classes of individuals meet one another (and potentially spread disease). Similarly, models of predator, \\(P\\) , and prey, \\(N\\) , abundance often include terms like \\(NP\\) describing the rate at which predators encounter prey (and potentially eat them). Let's first see how to deal with these models in general and then apply those techniques to specific circumstances like those mentioned above (in the next two lectures).","title":"Lecture overview"},{"location":"lectures/lecture-15/#1-continuous-time","text":"In general, if we have \\(n\\) interacting variables, \\(x_1, x_2, ..., x_n\\) , we can write any continuous time model like \\[ \\begin{aligned} \\frac{\\mathrm{d}x_1}{\\mathrm{d}t} &= f_1(x_1, x_2, ..., x_n)\\\\ \\frac{\\mathrm{d}x_2}{\\mathrm{d}t} &= f_2(x_1, x_2, ..., x_n)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}x_n}{\\mathrm{d}t} &= f_n(x_1, x_2, ..., x_n) \\end{aligned} \\] If we then want to find the equilibria, \\(\\hat{x}_1,\\hat{x}_2, ..., \\hat{x}_n\\) , we set all these equations to 0 and solve for one variable at a time (note that solving for the equilibrium is not always possible in nonlinear models!). Now note that we can no longer write this system of equations in matrix form with a matrix composed only of parameters. In order to use what we've learned about eigenvalues and eigenvectors we're first going to have to linearize the system so that the corresponding matrices do not contain variables. As we saw in nonlinear univariate models, one useful way to linearize a system is to measure the system relative to equilibrium, \\(\\epsilon = n - \\hat{n}\\) . Then assuming that the deviation from equilibrium, \\(\\epsilon\\) , is small, we used a Taylor series expansion to approximate the nonlinear system with a linear system. To do that with multivariate models we'll need to know how to take a Taylor series expansion of multivariate functions Taylor series expansion of a multivariate function Taking the series of \\(f\\) around \\(x_1=a_1\\) , \\(x_2=a_2\\) , ..., \\(x_n=a_n\\) gives \\[ \\begin{aligned} f(x_1, x_2, ..., x_n) &= f(a_1, a_2, ..., a_n)\\\\ &+ \\sum_{i=1}^{n} \\left( \\frac{\\partial f}{\\partial x_i} \\bigg|_{x_1=a_1, x_2=a_2, ..., x_n=a_n} \\right) (x_i - a_i)\\\\ &+ \\sum_{i=1}^{n}\\sum_{j=1}^n \\left( \\frac{\\partial f}{\\partial x_i \\partial x_j} \\bigg|_{x_1=a_1, x_2=a_2, ..., x_n=a_n} \\right) (x_i - a_i)(x_j - a_j)\\\\ &+ \\cdots \\end{aligned} \\] where \\(\\frac{\\partial f}{\\partial x_i}\\) is the \"partial derivative\" of \\(f\\) with respect to \\(x_i\\) , meaning that we treat all the other variables as constants when taking the derivative. Then when the difference between each variable and its value, \\(x_i-a_i\\) , is small enough we can ignore all the terms with a \\((x_i-a_i)(x_j-a_j)\\) , and we are left with a linear approximation of \\(f\\) . So let \\(\\epsilon_i = x_i - \\hat{x}_i\\) be the deviation of variable \\(x_i\\) from its equilibrium value, \\(\\hat{x}_i\\) . Then we can write a system of equations describing the change in the deviations for all of our variables \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &= f_1(x_1, x_2, ..., x_n)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &= f_2(x_1, x_2, ..., x_n)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &= f_n(x_1, x_2, ..., x_n) \\end{aligned} \\] And then we can take a Taylor series around \\(x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n\\) to get a linear approximation of our system near the equilibrium \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &\\approx f_1(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &\\approx f_2(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &\\approx f_n(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i) \\end{aligned} \\] And then note that all \\(f_i(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n)=0\\) by definition of a equilibrium, leaving \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i) \\end{aligned} \\] Each of the partial derivatives \\(\\frac{\\partial f_i}{\\partial x_j}\\) is evaluated at the equilibrium, so these are constants. And \\(x_i - \\hat{x}_i = \\epsilon_i\\) . So we now have a linear system \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i \\end{aligned} \\] We can now write our approximate system around the equilibrium in matrix form \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} \\\\ \\vdots \\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix} \\end{aligned} \\] This is a special matrix called the Jacobian \\[ \\mathbf{J} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix} \\] And now that we have a linear system around an equilibrium, we can assess its local stability just as we did with linear multivariate models (see Summary ).","title":"1. Continuous time"},{"location":"lectures/lecture-15/#2-discrete-time","text":"Note The short version of this section is that we can do the same thing in discrete time -- local stability is determined by the eigenvalues of the Jacobian, where the functions in that Jacobian are now our recursions, \\(x_i(t+1) = f_i(x_1(t), x_2(t), ..., x_n(t))\\) . We can do something very similar for nonlinear multivariate models in discrete time \\[ \\begin{aligned} x_1(t+1) &= f_1(x_1(t), x_2(t), ..., x_n(t))\\\\ x_2(t+1) &= f_2(x_1(t), x_2(t), ..., x_n(t))\\\\ &\\vdots\\\\ x_n(t+1) &= f_n(x_1(t), x_2(t), ..., x_n(t))\\\\ \\end{aligned} \\] Now the equilibria are found by setting all \\(x_i(t+1) = x_i(t) = \\hat{x}_i\\) and solving for the \\(\\hat{x}_i\\) one at a time. To linearize the system around an equilibrium we again measure the system in terms of deviation from the equilibrium, \\(\\epsilon_i(t) = x_i(t) - \\hat{x}_i\\) , giving \\[ \\begin{aligned} \\epsilon_1(t+1) &= f_1(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ \\epsilon_2(t+1) &= f_2(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ &\\vdots\\\\ \\epsilon_n(t+1) &= f_n(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ \\end{aligned} \\] Then taking the Taylor series of each \\(f_i\\) around \\(x_1(t) = \\hat{x}_1, ..., x_n(t) = \\hat{x}_n\\) we can approximate our system near the equilibrium as \\[ \\begin{aligned} \\epsilon_1(t+1) &= f_1(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_1\\\\ \\epsilon_2(t+1) &= f_2(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_2\\\\ &\\vdots\\\\ \\epsilon_n(t+1) &= f_n(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_n\\\\ \\end{aligned} \\] And noting that \\(f_i(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) = \\hat{x}_i\\) and \\(x_i(t) - \\hat{x}_i = \\epsilon_i(t)\\) we have \\[ \\begin{aligned} \\epsilon_1(t+1) &= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\ \\epsilon_2(t+1) &= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\ &\\vdots\\\\ \\epsilon_n(t+1) &= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\ \\end{aligned} \\] We can therefore write our approximation around the equilibrium in matrix form \\[ \\begin{aligned} \\begin{pmatrix} \\epsilon_1(t+1) \\\\ \\epsilon_2(t+1) \\\\ \\vdots \\\\ \\epsilon_n(t+1) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\begin{pmatrix} \\epsilon_1(t) \\\\ \\epsilon_2(t) \\\\ \\vdots \\\\ \\epsilon_n(t) \\end{pmatrix} \\end{aligned} \\] As in continuous time, the dynamics are described by the Jacobian matrix \\[ \\mathbf{J} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix} \\] We therefore assess the local stability of an equilibrium by evaluating the Jacobian at that equilibrium and finding the eigenvalues (see Summary ).","title":"2. Discrete time"},{"location":"lectures/lecture-15/#3-summary","text":"We can determine the stability of nonlinear multivariate models with the eigenvalues of the Jacobian evaluated at an equilibrium. The recipe is Find the equilibrium of interest, \\(\\hat{x}_1, \\hat{x}_2, ... \\hat{x}_n\\) Calculate the Jacobian, \\(\\mathbf{J}\\) Evaluate the Jacobian at the equilibrium of interest, \\(\\mathbf{J}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n}\\) Calculate the characteristic polynomial \\(|\\mathbf{J}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} - \\lambda\\mathbf{I}|\\) Set the characteristic polynomial equal to 0 and solve for the \\(n\\) eigenvalues, \\(\\lambda\\) Stability reminder Continuous time the leading eigenvalue is the one with the largest real part if the leading eigenvalue has a negative real part the equilibrium is stable if any eigenvalue has a non-zero complex part there will be cycling Discrete time the leading eigenvalue is the one with the largest absolute value if the leading eigenvalue has an absolute value less than one the equilibrium is stable if any eigenvalue has a non-zero complex part there will be cycling","title":"3. Summary"},{"location":"lectures/lecture-16/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 16: Epidemiology Run notes interactively? Lecture overview Epidemiology 1. Epidemiology In Lecture 15 we learned how to find equilibria and determine their stability in nonlinear multivariate models. To make this more concrete, let's consider a biological example (see Section 8.2 in the text). Consider a population composed of \\(S\\) susceptible individuals and \\(I\\) infected individuals. We assume new susceptible individuals arrive at rate \\(\\theta\\) via immigration and existing susceptibles die at per capita rate \\(d\\) . We assume infected individuals die at an elevated per capita rate \\(d+v\\) and recover at per capita rate \\(\\gamma\\) . So far this is a linear (affine) model. Finally, we assume susceptibles become infected at rate \\(\\beta S I\\) . This is the non-linear part. We can describe this with the following flow diagram graph LR; A[ ] --theta--> S((S)); S --beta S I--> I((I)); S --d S--> B[ ]; I --\"(d + v) I\"--> C[ ]; I --gamma I--> S; style A height:0px; style B height:0px; style C height:0px; The corresponding system of differential equations is \\[\\begin{aligned} \\frac{\\mathrm{d}S}{\\mathrm{d}t} &= \\theta - \\beta S I - d S + \\gamma I \\\\ \\frac{\\mathrm{d}I}{\\mathrm{d}t} &= \\beta S I - (d + v) I - \\gamma I \\end{aligned}\\] At equilibrium both derivatives are equal to zero \\[\\begin{aligned} 0 &= \\theta - \\beta \\hat{S} \\hat{I} - d \\hat{S} + \\gamma \\hat{I} \\\\ 0 &= \\beta \\hat{S} \\hat{I} - (d + v) \\hat{I} - \\gamma \\hat{I} \\end{aligned}\\] To be systematic we could start with the first equation and solve for the first variable, \\(\\hat{S}\\) , in terms of the remaining variables, \\(\\hat{I}\\) . We could then sub that expression for \\(\\hat{S}\\) into the second equation, which would then be an equation for \\(\\hat{I}\\) alone. After solving for \\(\\hat{I}\\) we could then sub that solution into \\(\\hat{S}\\) and be done. But through experience we notice that there is an easier approach. Because the second equation is proportional to \\(\\hat{I}\\) we immediately know \\(\\hat{I}=0\\) is one potential equilibrium point. For this to work we also need the first equation to be zero. Subbing in \\(\\hat{I}=0\\) to that first equation and solving for \\(\\hat{S}\\) gives \\(\\hat{S}=\\theta/d\\) . One equilibrium is therefore \\[\\begin{aligned} \\hat{S} &= \\theta/d \\\\ \\hat{I} &= 0 \\end{aligned}\\] which we call the \"disease-free\" equilibrium. Returning to the second equation, after factoring out \\(\\hat{I}\\) we are left with \\(0 = \\beta \\hat{S} - (d + v + \\gamma)\\) , implying \\(\\hat{S} = (d + v + \\gamma)/\\beta\\) . Plugging this into the first equation and solving for \\(\\hat{I}\\) we see that a second equilibrium is \\[\\begin{aligned} \\hat{S} &= (d + v + \\gamma)/\\beta \\\\ \\hat{I} &= \\frac{\\theta - d(d + v + \\gamma)/\\beta}{d+v} \\end{aligned}\\] which we call the \"endemic equilibrium\" because there is some non-zero amount of disease. Note that this equilibrium is only biologically valid when the numerator of \\(\\hat{I}\\) is positive which can be rearranged as \\(\\beta\\theta/d > d + v + \\gamma\\) . Now that we have the equilibria, the next step is to calculate the Jacobian. Letting \\(x_1=S\\) and \\(x_2=I\\) we have \\(f_1(x_1,x_2)=\\mathrm{d}S/\\mathrm{d}t\\) and \\(f_2(x_1,x_2)=\\mathrm{d}I/\\mathrm{d}t\\) . The Jacobian is therefore \\[\\begin{aligned} \\mathbf{J} &= \\begin{pmatrix} \\frac{\\partial}{\\partial S}\\left(\\frac{\\mathrm{d}S}{\\mathrm{d}t}\\right) & \\frac{\\partial}{\\partial I}\\left(\\frac{\\mathrm{d}S}{\\mathrm{d}t}\\right) \\\\ \\frac{\\partial}{\\partial S}\\left(\\frac{\\mathrm{d}I}{\\mathrm{d}t}\\right) & \\frac{\\partial}{\\partial I}\\left(\\frac{\\mathrm{d}I}{\\mathrm{d}t}\\right) \\end{pmatrix}\\\\ &= \\begin{pmatrix} -d-\\beta I & -\\beta S+\\gamma \\\\ \\beta I & \\beta S-(d+v+\\gamma) \\end{pmatrix} \\end{aligned}\\] We can now determine the local stability of an equilibrium by evaluating the Jacobian at that equilibrium and calculating the eigenvalues. Let's do that first for the simpler disease-free equilibrium, where there are no infected individuals, \\(\\hat{I}=0\\) , and the number of susceptibles is a balance of immigration and death, \\(\\hat{S} = \\theta/d\\) . Plugging these into the Jacobian gives \\[\\begin{aligned} \\mathbf{J}_\\mathrm{disease-free} &= \\begin{pmatrix} -d & -\\beta \\theta/d+\\gamma \\\\ 0 & \\beta \\theta/d-(d+v+\\gamma) \\end{pmatrix} \\end{aligned}\\] This is an upper triangular matrix, so the eigenvalues are just the diagonal elements, \\(\\lambda = -d, \\beta\\theta/d-(d+v+\\gamma)\\) . Because all the parameters are rates they are all non-negative, and therefore the only eigenvalue that can have a positive real part (and therefore cause instability) is \\(\\lambda=\\beta\\theta/d-(d+v+\\gamma)\\) . The equilibrium is unstable when this is positive, \\(\\beta\\theta/d-(d+v+\\gamma)>0\\) . Because this equilibrium has no infected individuals, instability in this case means the infected individuals will increase in number from rare -- ie, the disease can spread when rare. We can rearrange the instability condition to get a little more intuition. The disease will spread when rare whenever \\[\\begin{aligned} \\beta\\theta/d - (d+v+\\gamma)& > 0 \\\\ \\beta\\theta/d &> d+v+\\gamma \\\\ \\frac{\\beta\\theta/d}{d+v+\\gamma} &> 1 \\end{aligned}\\] The numerator is \\(\\beta\\) times the number of susceptibles at the disease-free equilibrium, \\(\\hat{S}=\\theta/d\\) . This is the rate that a rare disease infects new individuals. The denominator is the rate at which the disease is removed from the population. Therefore a rare disease that infects faster than it is removed can spread. This ratio, in our case \\(\\frac{\\beta\\theta/d}{d+v+\\gamma}\\) , is termed \\(R_0\\) and is a very key epidemiological quantity (you may remember estimates of \\(R_0\\) in the news from a certain recent virus...). Now for the endemic equilibrium. Plugging these values into the Jacobian and simplifying gives \\[\\begin{aligned} \\mathbf{J}_\\mathrm{endemic} &= \\begin{pmatrix} -\\frac{\\beta \\theta - d \\gamma}{d+v} & -(d+v) \\\\ \\frac{\\beta \\theta - d (d+v+\\gamma)}{d+v} & 0 \\end{pmatrix} \\end{aligned}\\] Here, instead of calculating the eigenvalues explicitly, we will use the Routh-Hurwitz stability criteria for a 2x2 matrix. Routh-Hurwitz stability criteria for a 2x2 matrix When working with 2x2 matrices, there is a simple way to determine if both the eigenvalues have negative real parts (ie, if the equilibrium is stable) without having to calculate the eigenvalues themselves. These are called the Routh-Hurwitz stability criteria (and extend to larger matrices but we won't cover that here). Recall that for a 2x2 matrix, \\(\\mathbf{M}\\) , the eigenvalues can be written \\[\\lambda = \\frac{\\mathrm{Tr}(\\mathbf{M}) \\pm \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4\\mathrm{Det}(\\mathbf{M})}}{2}\\] First notice that the product of the two eigenvalues is \\(\\mathrm{Det}(\\mathbf{M})\\) (you may want to check that for yourself). This means that the two eigenvalues have the same sign if and only if \\(\\mathrm{Det}(\\mathbf{M})>0\\) . Second, notice that the sum of the two eigenvalues is \\(\\mathrm{Tr}(\\mathbf{M})\\) . We therefore know that the real parts of both eigenvalues will be negative (ie, the equilibrium will be stable) if and only if \\(\\mathrm{Det}(\\mathbf{M})>0\\) and \\(\\mathrm{Tr}(\\mathbf{M})<0\\) . The determinant is \\(\\beta \\theta - d (d+v+\\gamma)\\) , so for this to be positive we need \\(\\beta \\theta/d > (d+v+\\gamma)\\) , which was our validity condition (above) and also the instability condition on the disease-free equilibrium ( \\(R_0>1\\) ). The trace is \\(-\\frac{\\beta \\theta - d \\gamma}{d+v}\\) , so for this to be negative we need \\(\\beta \\theta/d > \\gamma\\) , which is guaranteed if the determinant is positive. So in conclusion, the endemic equilibrium is valid and stable whenever the disease can invade, \\(R_0>1\\) .","title":"Lecture 16"},{"location":"lectures/lecture-16/#lecture-16-epidemiology","text":"Run notes interactively?","title":"Lecture 16: Epidemiology"},{"location":"lectures/lecture-16/#lecture-overview","text":"Epidemiology","title":"Lecture overview"},{"location":"lectures/lecture-16/#1-epidemiology","text":"In Lecture 15 we learned how to find equilibria and determine their stability in nonlinear multivariate models. To make this more concrete, let's consider a biological example (see Section 8.2 in the text). Consider a population composed of \\(S\\) susceptible individuals and \\(I\\) infected individuals. We assume new susceptible individuals arrive at rate \\(\\theta\\) via immigration and existing susceptibles die at per capita rate \\(d\\) . We assume infected individuals die at an elevated per capita rate \\(d+v\\) and recover at per capita rate \\(\\gamma\\) . So far this is a linear (affine) model. Finally, we assume susceptibles become infected at rate \\(\\beta S I\\) . This is the non-linear part. We can describe this with the following flow diagram graph LR; A[ ] --theta--> S((S)); S --beta S I--> I((I)); S --d S--> B[ ]; I --\"(d + v) I\"--> C[ ]; I --gamma I--> S; style A height:0px; style B height:0px; style C height:0px; The corresponding system of differential equations is \\[\\begin{aligned} \\frac{\\mathrm{d}S}{\\mathrm{d}t} &= \\theta - \\beta S I - d S + \\gamma I \\\\ \\frac{\\mathrm{d}I}{\\mathrm{d}t} &= \\beta S I - (d + v) I - \\gamma I \\end{aligned}\\] At equilibrium both derivatives are equal to zero \\[\\begin{aligned} 0 &= \\theta - \\beta \\hat{S} \\hat{I} - d \\hat{S} + \\gamma \\hat{I} \\\\ 0 &= \\beta \\hat{S} \\hat{I} - (d + v) \\hat{I} - \\gamma \\hat{I} \\end{aligned}\\] To be systematic we could start with the first equation and solve for the first variable, \\(\\hat{S}\\) , in terms of the remaining variables, \\(\\hat{I}\\) . We could then sub that expression for \\(\\hat{S}\\) into the second equation, which would then be an equation for \\(\\hat{I}\\) alone. After solving for \\(\\hat{I}\\) we could then sub that solution into \\(\\hat{S}\\) and be done. But through experience we notice that there is an easier approach. Because the second equation is proportional to \\(\\hat{I}\\) we immediately know \\(\\hat{I}=0\\) is one potential equilibrium point. For this to work we also need the first equation to be zero. Subbing in \\(\\hat{I}=0\\) to that first equation and solving for \\(\\hat{S}\\) gives \\(\\hat{S}=\\theta/d\\) . One equilibrium is therefore \\[\\begin{aligned} \\hat{S} &= \\theta/d \\\\ \\hat{I} &= 0 \\end{aligned}\\] which we call the \"disease-free\" equilibrium. Returning to the second equation, after factoring out \\(\\hat{I}\\) we are left with \\(0 = \\beta \\hat{S} - (d + v + \\gamma)\\) , implying \\(\\hat{S} = (d + v + \\gamma)/\\beta\\) . Plugging this into the first equation and solving for \\(\\hat{I}\\) we see that a second equilibrium is \\[\\begin{aligned} \\hat{S} &= (d + v + \\gamma)/\\beta \\\\ \\hat{I} &= \\frac{\\theta - d(d + v + \\gamma)/\\beta}{d+v} \\end{aligned}\\] which we call the \"endemic equilibrium\" because there is some non-zero amount of disease. Note that this equilibrium is only biologically valid when the numerator of \\(\\hat{I}\\) is positive which can be rearranged as \\(\\beta\\theta/d > d + v + \\gamma\\) . Now that we have the equilibria, the next step is to calculate the Jacobian. Letting \\(x_1=S\\) and \\(x_2=I\\) we have \\(f_1(x_1,x_2)=\\mathrm{d}S/\\mathrm{d}t\\) and \\(f_2(x_1,x_2)=\\mathrm{d}I/\\mathrm{d}t\\) . The Jacobian is therefore \\[\\begin{aligned} \\mathbf{J} &= \\begin{pmatrix} \\frac{\\partial}{\\partial S}\\left(\\frac{\\mathrm{d}S}{\\mathrm{d}t}\\right) & \\frac{\\partial}{\\partial I}\\left(\\frac{\\mathrm{d}S}{\\mathrm{d}t}\\right) \\\\ \\frac{\\partial}{\\partial S}\\left(\\frac{\\mathrm{d}I}{\\mathrm{d}t}\\right) & \\frac{\\partial}{\\partial I}\\left(\\frac{\\mathrm{d}I}{\\mathrm{d}t}\\right) \\end{pmatrix}\\\\ &= \\begin{pmatrix} -d-\\beta I & -\\beta S+\\gamma \\\\ \\beta I & \\beta S-(d+v+\\gamma) \\end{pmatrix} \\end{aligned}\\] We can now determine the local stability of an equilibrium by evaluating the Jacobian at that equilibrium and calculating the eigenvalues. Let's do that first for the simpler disease-free equilibrium, where there are no infected individuals, \\(\\hat{I}=0\\) , and the number of susceptibles is a balance of immigration and death, \\(\\hat{S} = \\theta/d\\) . Plugging these into the Jacobian gives \\[\\begin{aligned} \\mathbf{J}_\\mathrm{disease-free} &= \\begin{pmatrix} -d & -\\beta \\theta/d+\\gamma \\\\ 0 & \\beta \\theta/d-(d+v+\\gamma) \\end{pmatrix} \\end{aligned}\\] This is an upper triangular matrix, so the eigenvalues are just the diagonal elements, \\(\\lambda = -d, \\beta\\theta/d-(d+v+\\gamma)\\) . Because all the parameters are rates they are all non-negative, and therefore the only eigenvalue that can have a positive real part (and therefore cause instability) is \\(\\lambda=\\beta\\theta/d-(d+v+\\gamma)\\) . The equilibrium is unstable when this is positive, \\(\\beta\\theta/d-(d+v+\\gamma)>0\\) . Because this equilibrium has no infected individuals, instability in this case means the infected individuals will increase in number from rare -- ie, the disease can spread when rare. We can rearrange the instability condition to get a little more intuition. The disease will spread when rare whenever \\[\\begin{aligned} \\beta\\theta/d - (d+v+\\gamma)& > 0 \\\\ \\beta\\theta/d &> d+v+\\gamma \\\\ \\frac{\\beta\\theta/d}{d+v+\\gamma} &> 1 \\end{aligned}\\] The numerator is \\(\\beta\\) times the number of susceptibles at the disease-free equilibrium, \\(\\hat{S}=\\theta/d\\) . This is the rate that a rare disease infects new individuals. The denominator is the rate at which the disease is removed from the population. Therefore a rare disease that infects faster than it is removed can spread. This ratio, in our case \\(\\frac{\\beta\\theta/d}{d+v+\\gamma}\\) , is termed \\(R_0\\) and is a very key epidemiological quantity (you may remember estimates of \\(R_0\\) in the news from a certain recent virus...). Now for the endemic equilibrium. Plugging these values into the Jacobian and simplifying gives \\[\\begin{aligned} \\mathbf{J}_\\mathrm{endemic} &= \\begin{pmatrix} -\\frac{\\beta \\theta - d \\gamma}{d+v} & -(d+v) \\\\ \\frac{\\beta \\theta - d (d+v+\\gamma)}{d+v} & 0 \\end{pmatrix} \\end{aligned}\\] Here, instead of calculating the eigenvalues explicitly, we will use the Routh-Hurwitz stability criteria for a 2x2 matrix. Routh-Hurwitz stability criteria for a 2x2 matrix When working with 2x2 matrices, there is a simple way to determine if both the eigenvalues have negative real parts (ie, if the equilibrium is stable) without having to calculate the eigenvalues themselves. These are called the Routh-Hurwitz stability criteria (and extend to larger matrices but we won't cover that here). Recall that for a 2x2 matrix, \\(\\mathbf{M}\\) , the eigenvalues can be written \\[\\lambda = \\frac{\\mathrm{Tr}(\\mathbf{M}) \\pm \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4\\mathrm{Det}(\\mathbf{M})}}{2}\\] First notice that the product of the two eigenvalues is \\(\\mathrm{Det}(\\mathbf{M})\\) (you may want to check that for yourself). This means that the two eigenvalues have the same sign if and only if \\(\\mathrm{Det}(\\mathbf{M})>0\\) . Second, notice that the sum of the two eigenvalues is \\(\\mathrm{Tr}(\\mathbf{M})\\) . We therefore know that the real parts of both eigenvalues will be negative (ie, the equilibrium will be stable) if and only if \\(\\mathrm{Det}(\\mathbf{M})>0\\) and \\(\\mathrm{Tr}(\\mathbf{M})<0\\) . The determinant is \\(\\beta \\theta - d (d+v+\\gamma)\\) , so for this to be positive we need \\(\\beta \\theta/d > (d+v+\\gamma)\\) , which was our validity condition (above) and also the instability condition on the disease-free equilibrium ( \\(R_0>1\\) ). The trace is \\(-\\frac{\\beta \\theta - d \\gamma}{d+v}\\) , so for this to be negative we need \\(\\beta \\theta/d > \\gamma\\) , which is guaranteed if the determinant is positive. So in conclusion, the endemic equilibrium is valid and stable whenever the disease can invade, \\(R_0>1\\) .","title":"1. Epidemiology"},{"location":"lectures/lecture-17/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 17: Multi-locus population genetics Run notes interactively? Lecture overview Multi-locus population genetics 1. Multi-locus population genetics In Lecture 15 we learned how to find equilibria and determine their stability in nonlinear multivariate models. In Lecture 16 we looked at a continuous-time example from epidemiology. Here, we'll extend the discrete-time dipliod selection model of Lecture 4 to multiple loci (see Section 8.3 in the text). Genomes contain many loci -- what new dynamics arise when we model more than one locus? Here we look at the simplest multi-locus model, with two loci each with two alleles. Let's denote the loci with letters, \\(A\\) and \\(B\\) , and the alleles at each with numbers, \\(1\\) and \\(2\\) . This gives a total of \\(2^2=4\\) haploid genotypes, which we'll give frequencies \\(x_1\\) to \\(x_4\\) like so genotype frequency \\(A_1B_1\\) \\(x_1\\) \\(A_1B_2\\) \\(x_2\\) \\(A_2B_1\\) \\(x_3\\) \\(A_2B_2\\) \\(x_4\\) with the constraint that the total frequency sums to one, \\(x_1+x_2+x_3+x_4 = 1\\) . To determine how these frequencies change from one generation to the next, let's first determine the order of events in a life-cycle diagram graph LR; A((census)) --> B((gamete union)); B --> C((selection)); C --> D((meiosis)); D --> A; In words, we'll census the population in the gamete (haploid) phase while selection happens in the diploid phase. Next, to consider how the frequencies change through this life cyle, let's construct a life-cycle (mating) table. To do this, we need to consider what happens during meiosis when there are multiple loci. In the 1-locus models we analyzed earlier, meiosis meant Mendelian segregation: each allele is present in 1/2 of the gametes. Here, with 2 loci, things are slightly more complicated and we need to consider recombination . Every meiosis there is a chance, \\(r\\) , of an odd number of crossover events between the two loci ( \\(r\\) will increase with the distance between the loci). When this happens the pairing of the alleles at loci A and B get swapped. This only has an effect in diploid individuals that are heterozygous at both loci (\"double heterozygotes\"), here \\(A_1B_1\\) x \\(A_2B_2\\) and \\(A_1B_2\\) x \\(A_2B_1\\) . Every time these individuals go through meiosis the original pairings are kept with probability \\(1-r\\) and the alternative pairings are created with probability \\(r\\) . We can now fill in the following table union frequency frequency after selection gamete frequency after meiosis ( \\(A_1B_1\\) , \\(A_1B_2\\) , \\(A_2B_1\\) , \\(A_2B_2\\) ) \\(A_1B_1\\) x \\(A_1B_1\\) \\(x_1^2\\) \\(x_1^2 w_{11}/\\bar{w}\\) 1, 0, 0, 0 \\(A_1B_1\\) x \\(A_1B_2\\) \\(2x_1x_2\\) \\(2x_1x_2 w_{12}/\\bar{w}\\) 1/2, 1/2, 0, 0 \\(A_1B_1\\) x \\(A_2B_1\\) \\(2x_1x_3\\) \\(2x_1x_3 w_{13}/\\bar{w}\\) 1/2, 0, 1/2, 0 \\(A_1B_1\\) x \\(A_2B_2\\) \\(2x_1x_4\\) \\(2x_1x_4 w_{14}/\\bar{w}\\) \\((1-r)/2\\) , \\(r/2\\) , \\(r/2\\) , \\((1-r)/2\\) \\(A_1B_2\\) x \\(A_1B_2\\) \\(x_2^2\\) \\(x_2^2 w_{22}/\\bar{w}\\) 0, 1, 0, 0 \\(A_1B_2\\) x \\(A_2B_1\\) \\(2x_2x_3\\) \\(2x_2x_3 w_{23}/\\bar{w}\\) \\(r/2\\) , \\((1-r)/2\\) , \\((1-r)/2\\) , \\(r/2\\) \\(A_1B_2\\) x \\(A_2B_2\\) \\(2x_2x_4\\) \\(2x_2x_4 w_{24}/\\bar{w}\\) 0, 1/2, 0, 1/2 \\(A_2B_1\\) x \\(A_2B_1\\) \\(x_3^2\\) \\(x_3^2 w_{33}/\\bar{w}\\) 0, 0, 1, 0 \\(A_2B_1\\) x \\(A_2B_2\\) \\(2x_3x_4\\) \\(2x_3x_4 w_{34}/\\bar{w}\\) 0, 0, 1/2, 1/2 \\(A_2B_2\\) x \\(A_2B_2\\) \\(x_4^2\\) \\(x_4^2 w_{44}/\\bar{w}\\) 0, 0, 0, 1 where \\(w_{ij}=w_{ji}\\) is the fitness of the diploid that is composed of haploid genotypes \\(i\\) and \\(j\\) and \\(\\bar{w}\\) is the population mean fitness, which is the sum of the frequencies after selection. We can build the recursion equations from this table by multiplying the frequency after selection by the gamete frequency after meiosis. For example, the frequency of \\(A_1B_1\\) in the next generation is found by multiplying the first entry in the final column by the frequency after selection and summing this up over rows, giving \\[\\begin{align} x_1(t+1) &= x_1(t)^2 w_{11}/\\bar{w} + x_1(t)x_2(t) w_{12}/\\bar{w} + x_1(t)x_3(t) w_{13}/\\bar{w} + (1-r)x_1(t)x_4(t) w_{14}/\\bar{w} + r x_2(t)x_3(t) w_{23}/\\bar{w}\\\\ &= x_1(t) (x_1(t) w_{11}/\\bar{w} + x_2(t) w_{12}/\\bar{w} + x_3(t) w_{13}/\\bar{w} + x_4(t) w_{14}/\\bar{w}) - r(x_1(t)x_4(t) w_{14}/\\bar{w} - x_2(t)x_3(t) w_{23}/\\bar{w})\\\\ &= x_1(t) \\sum_{i=1}^{4} x_i(t) w_{1i}/\\bar{w} - r D^* \\end{align}\\] where \\(D^*=x_1(t)x_4(t) w_{14}/\\bar{w} - x_2(t)x_3(t) w_{23}/\\bar{w}\\) is called linkage disequilibrium (the asterisk differentiates it from the same quantity measured prior to selection, \\(D=x_1(t)x_4(t) - x_2(t)x_3(t)\\) ). Linkage disequilibrium is an important term in population genetics that measures the deviation of the association of alleles at two loci from that expected by chance under random assortment. For example, when \\(A_1\\) pairs with \\(B_1\\) more often than expected by chance then \\(x_1(t)x_4(t) > x_2(t)x_3(t)\\) and \\(D>0\\) . The remaining equations are created in the same way, giving \\[\\begin{align} x_2(t+1) &= x_2(t) \\sum_{i=1}^{4} x_i(t) w_{2i}/\\bar{w} + r D^* \\\\ x_3(t+1) &= x_3(t) \\sum_{i=1}^{4} x_i(t) w_{3i}/\\bar{w} + r D^* \\\\ x_4(t+1) &= x_4(t) \\sum_{i=1}^{4} x_i(t) w_{4i}/\\bar{w} - r D^* \\\\ \\end{align}\\] Now we have a system of recursion equations to work with. This system is nonlinear and four dimensional, which makes things relatively complex. For instance, it is impossible to find all the equilibria analytically. Here we'll not worry about that and just deal with a particularly simple equilibrium where \\(A_1B_1\\) is fixed, \\(\\hat{x}_1=1\\) and \\(\\hat{x}_2=\\hat{x}_3=\\hat{x}_4=0\\) . This implies \\(\\bar{w}=w_{11}\\) . To determine the stability of this equilbrium we need to calculate the Jacobian \\[ \\mathbf{J} = \\begin{pmatrix} \\frac{\\partial x_1(t+1)}{\\partial x_1(t)} & \\frac{\\partial x_1(t+1)}{\\partial x_2(t)} & \\frac{\\partial x_1(t+1)}{\\partial x_3(t)} & \\frac{\\partial x_1(t+1)}{\\partial x_4(t)}\\\\ \\frac{\\partial x_2(t+1)}{\\partial x_1(t)} & \\frac{\\partial x_2(t+1)}{\\partial x_2(t)} & \\frac{\\partial x_2(t+1)}{\\partial x_3(t)} & \\frac{\\partial x_2(t+1)}{\\partial x_4(t)}\\\\ \\frac{\\partial x_3(t+1)}{\\partial x_1(t)} & \\frac{\\partial x_3(t+1)}{\\partial x_2(t)} & \\frac{\\partial x_3(t+1)}{\\partial x_3(t)} & \\frac{\\partial x_3(t+1)}{\\partial x_4(t)}\\\\ \\frac{\\partial x_4(t+1)}{\\partial x_1(t)} & \\frac{\\partial x_4(t+1)}{\\partial x_2(t)} & \\frac{\\partial x_4(t+1)}{\\partial x_3(t)} & \\frac{\\partial x_4(t+1)}{\\partial x_4(t)}\\\\ \\end{pmatrix} \\] (we omit writing out the derivatives for brevity) and then evaluate it at the focal equilibrium, \\(\\mathbf{J}_{x_1=1,x_2=x_3=x_4=0}\\) . Local stability of the focal equilibrium is determined by the eigenvalues of this matrix, which are \\[\\begin{aligned} \\lambda_1 &= 0 \\\\ \\lambda_2 &= w_{12}/w_{11} \\\\ \\lambda_3 &= w_{13}/w_{11} \\\\ \\lambda_4 &= (1-r)w_{14}/w_{11} \\end{aligned}\\] The first eigenvalue, \\(\\lambda_1 = 0\\) , indicates that there is an axis along which or system does not change. This is due to the fact that the frequencies always sum to one, \\(x_1+x_2+x_3+x_4=1\\) . We therefore effectively have a three dimensional model, e.g., we could just track \\(x_1\\) , \\(x_2\\) , and \\(x_3\\) because we know that \\(x_4 = 1 - x_1-x_2-x_3\\) . The second and third eigenvalues, \\(\\lambda_2 = w_{12}/w_{11}\\) and \\(\\lambda_3 = w_{13}/w_{11}\\) , are analogous to what we found in the 1 locus (univariate) case. Remembering that stability in discrete time requires that the eigenvalues are less than 1 in absolute value, we can interpret these eigenvalues as saying that the \\(B_2\\) allele can invade (instability) when it has higher fitness than the \\(B_1\\) allele ( \\(w_{12}>w_{11}\\) ) and the \\(A_2\\) can invade when it has higher fitness than the \\(A_1\\) allele ( \\(w_{13}>w_{11}\\) ). The fourth eigenvalue, \\(\\lambda_4 = (1-r)w_{14}/w_{11}\\) , is the new part, which depends on recombination. Interestingly, here, even if \\(A_2B_2\\) has higher fitness than \\(A_1B_1\\) , meaning \\(w_{14}>w_{11}\\) , it is possible that the \\(A_2B_2\\) genotype cannot invade. This is because, for a rare \\(A_2B_2\\) genotype, every generation it pairs with the common \\(A_1B_1\\) genotype and therefore gets broken apart into \\(A_1B_2\\) and \\(A_2B_1\\) by recombination with probability \\(r\\) . In other words, recombination can hinder the spread of an adaptive combination of alleles. This is epitiomized by the scenario where having a single \"2\" allele is deleterious \\(w_{12}<w_{11}\\) and \\(w_{13}<w_{11}\\) (making \\(\\lambda_2<1\\) and \\(\\lambda_3<1\\) ) but having two \"2\" alleles is beneficial, \\(w_{14}>w_{11}\\) . Such a scenario is called a fitness valley because of the plot below import matplotlib.pyplot as plt w11=1 w12=0.9 w13=0.8 w14=1.1 fig,ax=plt.subplots() ax.plot([0,1,2],[w11,w12,w14],marker='o') ax.plot([0,1,2],[w11,w13,w14],marker='o') ax.text(0,w11,r'$A_1B_1$',va='bottom') ax.text(1,w12,r'$A_1B_2$',va='top') ax.text(1,w13,r'$A_2B_1$',va='top') ax.text(2,w14,r'$A_2B_2$',va='bottom',ha='right') ax.set_ylabel('fitness') ax.set_xlabel('number of \"2\" alleles') ax.set_xticks([0,1,2]) plt.show() Here we've seen how recombination can slow the spread of the optimal genotype, \\(A_2B_2\\) , potentially preventing fitness-valley crossing. There is also, however, a constructive aspect of recombination, not explored in this simple model: when the deleterious genotypes \\(A_1B_2\\) and \\(A_2B_1\\) are both present, there is a chance that they pair and recombine, giving rise to the optimal genotype \\(A_2B_2\\) . The role of recombination in fitness-valley crossing is therefore a relatively interesting and complex problem.","title":"Lecture 17"},{"location":"lectures/lecture-17/#lecture-17-multi-locus-population-genetics","text":"Run notes interactively?","title":"Lecture 17: Multi-locus population genetics"},{"location":"lectures/lecture-17/#lecture-overview","text":"Multi-locus population genetics","title":"Lecture overview"},{"location":"lectures/lecture-17/#1-multi-locus-population-genetics","text":"In Lecture 15 we learned how to find equilibria and determine their stability in nonlinear multivariate models. In Lecture 16 we looked at a continuous-time example from epidemiology. Here, we'll extend the discrete-time dipliod selection model of Lecture 4 to multiple loci (see Section 8.3 in the text). Genomes contain many loci -- what new dynamics arise when we model more than one locus? Here we look at the simplest multi-locus model, with two loci each with two alleles. Let's denote the loci with letters, \\(A\\) and \\(B\\) , and the alleles at each with numbers, \\(1\\) and \\(2\\) . This gives a total of \\(2^2=4\\) haploid genotypes, which we'll give frequencies \\(x_1\\) to \\(x_4\\) like so genotype frequency \\(A_1B_1\\) \\(x_1\\) \\(A_1B_2\\) \\(x_2\\) \\(A_2B_1\\) \\(x_3\\) \\(A_2B_2\\) \\(x_4\\) with the constraint that the total frequency sums to one, \\(x_1+x_2+x_3+x_4 = 1\\) . To determine how these frequencies change from one generation to the next, let's first determine the order of events in a life-cycle diagram graph LR; A((census)) --> B((gamete union)); B --> C((selection)); C --> D((meiosis)); D --> A; In words, we'll census the population in the gamete (haploid) phase while selection happens in the diploid phase. Next, to consider how the frequencies change through this life cyle, let's construct a life-cycle (mating) table. To do this, we need to consider what happens during meiosis when there are multiple loci. In the 1-locus models we analyzed earlier, meiosis meant Mendelian segregation: each allele is present in 1/2 of the gametes. Here, with 2 loci, things are slightly more complicated and we need to consider recombination . Every meiosis there is a chance, \\(r\\) , of an odd number of crossover events between the two loci ( \\(r\\) will increase with the distance between the loci). When this happens the pairing of the alleles at loci A and B get swapped. This only has an effect in diploid individuals that are heterozygous at both loci (\"double heterozygotes\"), here \\(A_1B_1\\) x \\(A_2B_2\\) and \\(A_1B_2\\) x \\(A_2B_1\\) . Every time these individuals go through meiosis the original pairings are kept with probability \\(1-r\\) and the alternative pairings are created with probability \\(r\\) . We can now fill in the following table union frequency frequency after selection gamete frequency after meiosis ( \\(A_1B_1\\) , \\(A_1B_2\\) , \\(A_2B_1\\) , \\(A_2B_2\\) ) \\(A_1B_1\\) x \\(A_1B_1\\) \\(x_1^2\\) \\(x_1^2 w_{11}/\\bar{w}\\) 1, 0, 0, 0 \\(A_1B_1\\) x \\(A_1B_2\\) \\(2x_1x_2\\) \\(2x_1x_2 w_{12}/\\bar{w}\\) 1/2, 1/2, 0, 0 \\(A_1B_1\\) x \\(A_2B_1\\) \\(2x_1x_3\\) \\(2x_1x_3 w_{13}/\\bar{w}\\) 1/2, 0, 1/2, 0 \\(A_1B_1\\) x \\(A_2B_2\\) \\(2x_1x_4\\) \\(2x_1x_4 w_{14}/\\bar{w}\\) \\((1-r)/2\\) , \\(r/2\\) , \\(r/2\\) , \\((1-r)/2\\) \\(A_1B_2\\) x \\(A_1B_2\\) \\(x_2^2\\) \\(x_2^2 w_{22}/\\bar{w}\\) 0, 1, 0, 0 \\(A_1B_2\\) x \\(A_2B_1\\) \\(2x_2x_3\\) \\(2x_2x_3 w_{23}/\\bar{w}\\) \\(r/2\\) , \\((1-r)/2\\) , \\((1-r)/2\\) , \\(r/2\\) \\(A_1B_2\\) x \\(A_2B_2\\) \\(2x_2x_4\\) \\(2x_2x_4 w_{24}/\\bar{w}\\) 0, 1/2, 0, 1/2 \\(A_2B_1\\) x \\(A_2B_1\\) \\(x_3^2\\) \\(x_3^2 w_{33}/\\bar{w}\\) 0, 0, 1, 0 \\(A_2B_1\\) x \\(A_2B_2\\) \\(2x_3x_4\\) \\(2x_3x_4 w_{34}/\\bar{w}\\) 0, 0, 1/2, 1/2 \\(A_2B_2\\) x \\(A_2B_2\\) \\(x_4^2\\) \\(x_4^2 w_{44}/\\bar{w}\\) 0, 0, 0, 1 where \\(w_{ij}=w_{ji}\\) is the fitness of the diploid that is composed of haploid genotypes \\(i\\) and \\(j\\) and \\(\\bar{w}\\) is the population mean fitness, which is the sum of the frequencies after selection. We can build the recursion equations from this table by multiplying the frequency after selection by the gamete frequency after meiosis. For example, the frequency of \\(A_1B_1\\) in the next generation is found by multiplying the first entry in the final column by the frequency after selection and summing this up over rows, giving \\[\\begin{align} x_1(t+1) &= x_1(t)^2 w_{11}/\\bar{w} + x_1(t)x_2(t) w_{12}/\\bar{w} + x_1(t)x_3(t) w_{13}/\\bar{w} + (1-r)x_1(t)x_4(t) w_{14}/\\bar{w} + r x_2(t)x_3(t) w_{23}/\\bar{w}\\\\ &= x_1(t) (x_1(t) w_{11}/\\bar{w} + x_2(t) w_{12}/\\bar{w} + x_3(t) w_{13}/\\bar{w} + x_4(t) w_{14}/\\bar{w}) - r(x_1(t)x_4(t) w_{14}/\\bar{w} - x_2(t)x_3(t) w_{23}/\\bar{w})\\\\ &= x_1(t) \\sum_{i=1}^{4} x_i(t) w_{1i}/\\bar{w} - r D^* \\end{align}\\] where \\(D^*=x_1(t)x_4(t) w_{14}/\\bar{w} - x_2(t)x_3(t) w_{23}/\\bar{w}\\) is called linkage disequilibrium (the asterisk differentiates it from the same quantity measured prior to selection, \\(D=x_1(t)x_4(t) - x_2(t)x_3(t)\\) ). Linkage disequilibrium is an important term in population genetics that measures the deviation of the association of alleles at two loci from that expected by chance under random assortment. For example, when \\(A_1\\) pairs with \\(B_1\\) more often than expected by chance then \\(x_1(t)x_4(t) > x_2(t)x_3(t)\\) and \\(D>0\\) . The remaining equations are created in the same way, giving \\[\\begin{align} x_2(t+1) &= x_2(t) \\sum_{i=1}^{4} x_i(t) w_{2i}/\\bar{w} + r D^* \\\\ x_3(t+1) &= x_3(t) \\sum_{i=1}^{4} x_i(t) w_{3i}/\\bar{w} + r D^* \\\\ x_4(t+1) &= x_4(t) \\sum_{i=1}^{4} x_i(t) w_{4i}/\\bar{w} - r D^* \\\\ \\end{align}\\] Now we have a system of recursion equations to work with. This system is nonlinear and four dimensional, which makes things relatively complex. For instance, it is impossible to find all the equilibria analytically. Here we'll not worry about that and just deal with a particularly simple equilibrium where \\(A_1B_1\\) is fixed, \\(\\hat{x}_1=1\\) and \\(\\hat{x}_2=\\hat{x}_3=\\hat{x}_4=0\\) . This implies \\(\\bar{w}=w_{11}\\) . To determine the stability of this equilbrium we need to calculate the Jacobian \\[ \\mathbf{J} = \\begin{pmatrix} \\frac{\\partial x_1(t+1)}{\\partial x_1(t)} & \\frac{\\partial x_1(t+1)}{\\partial x_2(t)} & \\frac{\\partial x_1(t+1)}{\\partial x_3(t)} & \\frac{\\partial x_1(t+1)}{\\partial x_4(t)}\\\\ \\frac{\\partial x_2(t+1)}{\\partial x_1(t)} & \\frac{\\partial x_2(t+1)}{\\partial x_2(t)} & \\frac{\\partial x_2(t+1)}{\\partial x_3(t)} & \\frac{\\partial x_2(t+1)}{\\partial x_4(t)}\\\\ \\frac{\\partial x_3(t+1)}{\\partial x_1(t)} & \\frac{\\partial x_3(t+1)}{\\partial x_2(t)} & \\frac{\\partial x_3(t+1)}{\\partial x_3(t)} & \\frac{\\partial x_3(t+1)}{\\partial x_4(t)}\\\\ \\frac{\\partial x_4(t+1)}{\\partial x_1(t)} & \\frac{\\partial x_4(t+1)}{\\partial x_2(t)} & \\frac{\\partial x_4(t+1)}{\\partial x_3(t)} & \\frac{\\partial x_4(t+1)}{\\partial x_4(t)}\\\\ \\end{pmatrix} \\] (we omit writing out the derivatives for brevity) and then evaluate it at the focal equilibrium, \\(\\mathbf{J}_{x_1=1,x_2=x_3=x_4=0}\\) . Local stability of the focal equilibrium is determined by the eigenvalues of this matrix, which are \\[\\begin{aligned} \\lambda_1 &= 0 \\\\ \\lambda_2 &= w_{12}/w_{11} \\\\ \\lambda_3 &= w_{13}/w_{11} \\\\ \\lambda_4 &= (1-r)w_{14}/w_{11} \\end{aligned}\\] The first eigenvalue, \\(\\lambda_1 = 0\\) , indicates that there is an axis along which or system does not change. This is due to the fact that the frequencies always sum to one, \\(x_1+x_2+x_3+x_4=1\\) . We therefore effectively have a three dimensional model, e.g., we could just track \\(x_1\\) , \\(x_2\\) , and \\(x_3\\) because we know that \\(x_4 = 1 - x_1-x_2-x_3\\) . The second and third eigenvalues, \\(\\lambda_2 = w_{12}/w_{11}\\) and \\(\\lambda_3 = w_{13}/w_{11}\\) , are analogous to what we found in the 1 locus (univariate) case. Remembering that stability in discrete time requires that the eigenvalues are less than 1 in absolute value, we can interpret these eigenvalues as saying that the \\(B_2\\) allele can invade (instability) when it has higher fitness than the \\(B_1\\) allele ( \\(w_{12}>w_{11}\\) ) and the \\(A_2\\) can invade when it has higher fitness than the \\(A_1\\) allele ( \\(w_{13}>w_{11}\\) ). The fourth eigenvalue, \\(\\lambda_4 = (1-r)w_{14}/w_{11}\\) , is the new part, which depends on recombination. Interestingly, here, even if \\(A_2B_2\\) has higher fitness than \\(A_1B_1\\) , meaning \\(w_{14}>w_{11}\\) , it is possible that the \\(A_2B_2\\) genotype cannot invade. This is because, for a rare \\(A_2B_2\\) genotype, every generation it pairs with the common \\(A_1B_1\\) genotype and therefore gets broken apart into \\(A_1B_2\\) and \\(A_2B_1\\) by recombination with probability \\(r\\) . In other words, recombination can hinder the spread of an adaptive combination of alleles. This is epitiomized by the scenario where having a single \"2\" allele is deleterious \\(w_{12}<w_{11}\\) and \\(w_{13}<w_{11}\\) (making \\(\\lambda_2<1\\) and \\(\\lambda_3<1\\) ) but having two \"2\" alleles is beneficial, \\(w_{14}>w_{11}\\) . Such a scenario is called a fitness valley because of the plot below import matplotlib.pyplot as plt w11=1 w12=0.9 w13=0.8 w14=1.1 fig,ax=plt.subplots() ax.plot([0,1,2],[w11,w12,w14],marker='o') ax.plot([0,1,2],[w11,w13,w14],marker='o') ax.text(0,w11,r'$A_1B_1$',va='bottom') ax.text(1,w12,r'$A_1B_2$',va='top') ax.text(1,w13,r'$A_2B_1$',va='top') ax.text(2,w14,r'$A_2B_2$',va='bottom',ha='right') ax.set_ylabel('fitness') ax.set_xlabel('number of \"2\" alleles') ax.set_xticks([0,1,2]) plt.show() Here we've seen how recombination can slow the spread of the optimal genotype, \\(A_2B_2\\) , potentially preventing fitness-valley crossing. There is also, however, a constructive aspect of recombination, not explored in this simple model: when the deleterious genotypes \\(A_1B_2\\) and \\(A_2B_1\\) are both present, there is a chance that they pair and recombine, giving rise to the optimal genotype \\(A_2B_2\\) . The role of recombination in fitness-valley crossing is therefore a relatively interesting and complex problem.","title":"1. Multi-locus population genetics"},{"location":"lectures/lecture-18/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 18: Evolutionary invasion analysis Run notes interactively? Lecture overview Invasion fitness Evolutionarily singular strategies Evolutionarily stable strategies Evolutionary convergence Pairwise invasibility plots In the models we've discussed so far we've taken the parameters to be fixed. In reality, many of these parameters can evolve. For example, in our model of exponential growth in discrete time, \\(n(t+1)=n(t) R\\) , we took \\(R\\) to be the same for all individuals for all time. But any mutation causing a larger \\(R\\) would increase in frequency, causing the value of \\(R\\) to increase over time. In this lecture we'll explore how to determine the direction of evolution and the stability of evolutionary endpoints for more complex models using a technique called evolutionary invasion analysis (also known as adaptive dynamics). The idea: determine which parameter(s) of our model an evolving trait affects take the population to be fixed for some \"resident\" trait value determine the equilibria and stability of the system with only the resident trait derive the growth rate of a new individual with a \"mutant\" trait value ask when the mutant trait value will invade look for potential evolutionary endpoints determine the stability of those endpoints The evolution of dispersal To motivate evolutionary invasion analysis, let's consider the evolution of dispersal. Imagine there are \\(S\\) sites, with a most one individual reproducing at each. We census the population at the time of reproduction. A reproducing individual has a large number \\(B\\) offspring and then dies. A fraction \\(d\\) of those offspring disperse and a fraction \\(1-c\\) of those survive. The survivors then equally divided among all sites. One individual in each site is then chosen at random to reproduce, which begins the life-cycle anew. The question is, how should dispersal, \\(d\\) , evolve? There is a cost, \\(c\\) , which selects against dispersal but dispersal also allows offspring to avoid competiting with their kin, which could select for more dispersal. We'll use evolutionary invasion analysis to sort this out. 1. Invasion fitness Let's think about this analysis very generally (in discrete time). Let the number of individuals with the resident trait value be \\(n\\) and the number of individuals with the mutant trait value \\(n_m\\) . (And we'll assume asexual reproduction for simplicity, so that residents produce residents and mutant produce mutants.) Let the potentially nonlinear dynamics of these two groups of individuals depend on their respective trait values, \\(z\\) and \\(z_m\\) , \\[ \\begin{aligned} n(t+1) &= n(t) R(n(t), n_m(t), z, z_m)\\\\ n_m(t+1) &= n_m(t) R_m(n(t), n_m(t), z, z_m) \\end{aligned} \\] The Jacobian of this system is \\[ \\begin{aligned} \\mathbf{J} &= \\begin{pmatrix} \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)} & \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n_m(t)} \\\\ \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n(t)} & \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)} \\end{pmatrix} \\end{aligned} \\] Now consider some non-zero resident equilibrium, \\(\\hat{n}>0\\) , without the mutant, \\(\\hat{n}_m=0\\) . Assuming that the resident does not produce mutants, \\(\\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n(t)}\\big|_{n_m=0}=0\\) , the Jacobian evaluated at this equilibrium simplifies to \\[ \\begin{aligned} \\mathbf{J}\\big|_{n_m=0,n=\\hat{n}} &= \\begin{pmatrix} \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)} & \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n_m(t)} \\\\ 0 & \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)} \\end{pmatrix}_{n_m=0,n=\\hat{n}} \\end{aligned} \\] We can immediately see that the two eigenvalues of this upper triangular matrix are \\[ \\begin{aligned} \\lambda_1 &= \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)}\\bigg|_{n_m=0,n=\\hat{n}}\\\\ \\lambda_2 &= \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)}\\bigg|_{n_m=0,n=\\hat{n}} \\end{aligned} \\] The first, \\(\\lambda_1\\) , determines whether the resident equilibrium, \\(\\hat{n}>0\\) , is stable in the absence of mutants. We'll take \\(0 < \\lambda_1 < 1\\) as a given (we're only interested in stable resident equilibria, we can disregard the others). The second, \\(\\lambda_2\\) , determines whether the resident equilibrium is stable in the presence of a small number of mutants. We call \\(\\lambda_2\\) the invasion fitness , \\(\\lambda(z_m,z)\\) . The mutant will invade whenever \\(\\lambda(z_m,z) > 1\\) . In some simple cases we might be able to use the invasion criterium, \\(\\lambda(z_m,z)>1\\) , to determine what values of \\(z_m\\) (relative to \\(z\\) ) can invade. In most cases, however, \\(\\lambda(z_m,z)\\) will be complex enough that this will not be possible, so we rely on a simple approximation. The evolution of dispersal Let a fraction \\(d\\) of the resident offspring disperse and a fraction \\(d_m\\) of the mutant offspring disperse. And let there be \\(n(t)\\) residents, \\(n_m(t)\\) mutants, and \\(S-n(t)-n_m(t)\\geq0\\) empty sites. Then the probability a resident offspring replaces a resident is the number of resident offspring in a resident patch divided by the total number of offspring in that patch, \\[ \\begin{aligned} p_{rr} &= \\frac{B(1-d) + (n(t)-1)Bd(1-c)/S}{B(1-d) + (n(t)-1)Bd(1-c)/S + n_m(t)Bd_m(1-c)/S}\\\\ &= \\frac{S(1-d) + (n(t)-1)d(1-c)}{S(1-d) + (n(t)-1)d(1-c) + n_m(t)d_m(1-c)} \\end{aligned} \\] Here \\(B(1-d)\\) is the number of non-dispersing offspring produced by the resident in that patch, \\((n(t)-1)Bd(1-c)/S\\) is the number of resident offspring dispersing to the patch from elsewhere, and \\(n_m(t)Bd_m(1-c)/S\\) is the number of mutant offspring dispersing to the patch. The probability that a mutant offspring replaces a mutant \\(p_{mm}\\) is the same expression with \\(d\\) and \\(d_m\\) and \\(n\\) and \\(n_m\\) exchanged. The probability that a resident offspring wins an empty patch is \\[ \\begin{aligned} p_{re} &= \\frac{n(t)Bd(1-c)}{n(t)Bd(1-c) + n_m(t)Bd_m(1-c)}\\\\ &= \\frac{n(t)d}{n(t)d + n_m(t)d_m} \\end{aligned} \\] Then the number of resident and mutant individuals in the next generation are \\[ \\begin{aligned} n(t+1) &= n(t) p_{rr} + n_m(t) (1 - p_{mm}) + (S - n(t) - n_m(t)) p_{re}\\\\ n_m(t+1) &= n(t) (1-p_{rr}) + n_m(t) p_{mm} + (S - n(t) - n_m(t)) (1-p_{re}) \\end{aligned} \\] Now consider an equilibrium with no mutants, \\(\\hat{n}_m=0\\) . Setting \\(n(t+1) = n(t) = \\hat{n}\\) gives \\(\\hat{n}=S\\) . We determine the stability of this equilibrium with the Jacobian. From the general analysis above we know the two eigenvalues are \\[ \\begin{aligned} \\lambda_1 &= \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)}\\bigg|_{n_m=0,n=S} = 0\\\\ \\lambda_2 &= \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)}\\bigg|_{n_m=0,n=S} = \\frac{1-d_m}{1-d_m + d(1-c)} + \\frac{d_m(1-c)}{1-d + d(1-c)} \\end{aligned} \\] The first eigenvalue is less than 1 in absolute value, meaning the resident equilibrium is stable. The second eigenvalue is our invasion fitness \\[ \\lambda(d_m,d) = \\frac{1-d_m}{1-d_m + d(1-c)} + \\frac{d_m(1-c)}{1-d + d(1-c)} \\] In this relatively simple example, we can determine exactly what mutant trait values can invade by looking at where \\[ \\lambda(d_m,d) - 1 = \\frac{(1-c)(d_m-d)(1-cd-d_m)}{(1-cd)(1-d_m+d(1-c))} \\] is positive. The sign of \\(\\lambda(d_m,d) - 1\\) is the sign of \\((d_m-d)(1-cd-d_m)\\) . So the mutant will invade if both terms are positive \\(d<d_m<1-cd\\) or both terms are negative \\(1-cd<d_m<d\\) . In either case, the mutant will invade when it has a trait value between \\(1-cd\\) and \\(d\\) . 2. Evolutionarily singular strategies When the mutant trait value is very close to the resident trait value, we can approximate invasion fitness with a Taylor series around \\(z_m = z\\) \\[ \\begin{aligned} \\lambda(z_m,z) &\\approx \\lambda(z,z) + \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=z} (z_m-z)\\\\ &= 1 + \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=z} (z_m-z) \\end{aligned} \\] This allows us to determine which direction evolution will proceed from the current resident value: if \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}>0\\) then invasion when \\(z_m>z\\) if \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}<0\\) then invasion when \\(z_m<z\\) The direction of evolution by small steps is given by \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}\\) , which we call the selection gradient . Potential evolutionary endpoints, also called evolutionarily singular strategies , are the resident trait values \\(z=\\hat{z}\\) where there is no directional selection \\[ \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=\\hat{z}, z=\\hat{z}} = 0 \\] The evolution of dispersal The selection gradient is \\[ \\frac{\\partial \\lambda}{\\partial d_m}\\Big|_{d_m=d} = \\frac{(1-d-dc))(1-c)}{(1-cd)^2} \\] The sign of this is the sign of 1-d-dc, meaning the dispersal rate will increase whenever d<1/(1+c) and decrease whenever 1/(1+c)<d. Setting the selection gradient to zero and solving for the evolutionarily singular strategy gives \\(\\hat{d} = 1/(1+c)\\) . 3. Evolutionarily stable strategies An evolutionarily singular strategy, \\(\\hat{z}\\) , will only be an evolutionarily stable strategy (ESS), \\(z^*\\) , if it cannot be invaded. Global evolutionary stability will be impossible to prove for most models and so we often focus on local stability, which requires that \\(\\lambda(z_m,z)|_{z=\\hat{z}}\\) is concave at \\(z_m=\\hat{z}\\) (i.e., \\(\\hat{z}\\) is a local fitness maximum), \\[ \\frac{\\partial^2 \\lambda}{\\partial z_m^2}\\bigg|_{z_m=\\hat{z}, z=\\hat{z}} < 0 \\] The evolution of dispersal The second derivative of invasion fitness with respect to the mutant trait value evalulated at the singular strategy is \\[ \\frac{\\partial^2 \\lambda}{\\partial d_m^2}\\Big|_{d_m=d=\\hat{d}} = -2(1-c)(1+c)^2 \\] Because \\(0<c<1\\) this is always negative, which means the singular strategy is always evolutionarily stable. 4. Evolutionary convergence There is one more characteristic of evolutionarily singular strategies that we care about, and that is whether evolution actually leads to that strategy or not. For evolution to move the trait value towards a singular strategy, \\(\\hat{z}\\) , we need evolution to increase the trait value when it is less than \\(\\hat{z}\\) and decrease the trait value when it is greater than \\(\\hat{z}\\) . In other words, we need the selection gradient \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}\\) to decrease as we move through \\(z=\\hat{z}\\) \\[ \\frac{\\mathrm{d}}{\\mathrm{d} z}\\left( \\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z} \\right)_{z=\\hat{z}} < 0 \\] Singular strategies that satisfy this criteria are said to be convergence stable . Interestingly, not all evolutionarily stable strategies are convergence stable and not all convergence stable singular strategies are evolutionarily stable! Evolutionarily stable strategies that are not convergence stable are called Garden of Eden strategies . Singular strategies that are convergence stable but not evolutionarily stable are called evolutionary branching points . The latter are of particular interest because the system evolves towards a state where multiple strategies can invade and coexist, leading to diversification. The evolution of dispersal The derivative of the selection gradient evaluated at the singular strategy is \\[ \\frac{\\mathrm{d}}{\\mathrm{d} d}\\left( \\frac{\\partial \\lambda}{\\partial d_m}\\Big|_{d_m=d} \\right)_{d=\\hat{d}} = -(1-c)(1+c)^3 \\] Because \\(0<c<1\\) this is always negative, which means the singular strategy is always convergence stable. 5. Pairwise invasibility plots A helpful way to visualize the two types of stability of an evolutionarily singular strategy is called a pairwise invasibility plot (PIP). In this plot we have the resident trait value \\(z\\) on the x-axis, the mutant trait value \\(z_m\\) on the y-axis, and we color in the regions where the mutant can invade, \\(\\lambda(z_m,z)>1\\) . The four types of evolutionarily singular strategies \\(\\hat{z}\\) are then represented by the following PIPs import numpy as np from matplotlib import pyplot as plt # dummy invasion fitness def inv(x, y, slope, z=1): if y < x: if y > slope * x: return z if y > x: if y < slope * x: return z return 1-z # evaluate def compute_pip(slope=-2,z=1,xmin=-2,xmax=2,steps=100): xs = np.linspace(xmin,xmax,steps) X,Y = np.meshgrid(xs,xs) # X and Y values # store the invasion success in a matrix PIP = [] for y in xs: row = [] for x in xs: row.append(inv(x,y,slope,z)) PIP.append(row) return X,Y,PIP # plot def plotfun(X,Y,Z,slope=1,ax=None): if ax==None: fig, ax=plt.subplots(1,1,figsize=(5,5)) ax.contourf(X,Y,Z, colors=['white','black','blue','black']) ax.plot(X[0],X[0],'k',lw=5) ax.plot(X[0],X[0]*slope,'k',lw=5) ax.set_xlim(min(X[0]),max(X[0])) ax.set_ylim(min(X[0]),max(X[0])) ax.set_xlabel('resident trait value $z$') ax.set_ylabel('mutant trait value $z_m$') ax.set_xticks([]) ax.set_yticks([]) fig, axs = plt.subplots(2,2,figsize=(10,10)) X,Y,Z = compute_pip(slope=-4,z=1) plotfun(X,Y,Z,slope=-4,ax=axs[0][0]) axs[0][0].set_title('evolutionarily stable',fontsize=14) axs[0][1].text(2.1,0,'convergence stable',fontsize=14,rotation=270,verticalalignment='center') X,Y,Z = compute_pip(slope=4,z=0) plotfun(X,Y,Z,slope=4,ax=axs[0][1]) axs[0][1].set_title('evolutionarily unstable',fontsize=14) X,Y,Z = compute_pip(slope=4,z=1) plotfun(X,Y,Z,slope=4,ax=axs[1][0]) axs[1][1].text(2.1,0,'convergence unstable',fontsize=14,rotation=270,verticalalignment='center') X,Y,Z = compute_pip(slope=-4,z=0) plotfun(X,Y,Z,slope=-4,ax=axs[1][1]) We can read a PIP by choosing a resident trait value (a point on the x-axis) and looking to see what mutant trait values can invade it (blue regions in that vertical slice). Choose one of the possible invading trait values and set this to be the new resident trait value. Continue indefinitely. When we assume mutants have trait values close to the resident, we restrict ourselves to moving along the 1:1 line. Then, we move to the right when there is blue directly above the 1:1 line but not below and we move to the left when there is blue directly below the 1:1 line but not above. Where there is blue directly above and below the 1:1 we are at a singular strategy that is a fitness minimum (it can be invaded in both directions). Where there is white directly above and below the 1:1 we are at a singular strategy that is a fitness maximum (it can't be invaded in either direction). Try reading each of the plots above. Prove to yourself that the top left has a convergence stable evolutionarily stable strategy, the top right has a branching point, the bottom left has a has a Garden of Eden, and the bottom right has an invasible repellor (a fitness minimum that is not convergence stable). The evolution of dispersal In our model of the evolution of dispersal, the singular strategy is always evolutionarily and convgence stable, and the PIP is below. import numpy as np from matplotlib import pyplot as plt # invasion fitness def inv_fun(dm,d,c): invasionfitness = (1-dm)/(1-dm + d*(1-c)) + (dm*(1-c))/(1-d + d*(1-c)) if invasionfitness>1: return 1 # return 1 if mutant invades return 0 # return 0 if mutant does not invade # evaluate def compute_pip(c=0.5,dmin=0.01,dmax=0.99,steps=100): ds = np.linspace(dmin,dmax,steps) X,Y = np.meshgrid(ds,ds) # X and Y values # store the invasion success in a matrix PIP = [] for dm in ds: row = [] for d in ds: row.append(inv_fun(dm,d,c)) PIP.append(row) return X,Y,PIP # plot def plotfun(X,Y,Z): fig, ax = plt.subplots(1,1,figsize=(5,5)) ax.set_xlabel('$d$') ax.set_ylabel('$d_m$') ax.contourf(X,Y,Z, colors=['white','black','blue','black']) ax.plot(X[0],X[0],'k',lw=5) X,Y,Z=compute_pip() plotfun(X,Y,Z)","title":"Lecture 18"},{"location":"lectures/lecture-18/#lecture-18-evolutionary-invasion-analysis","text":"Run notes interactively?","title":"Lecture 18: Evolutionary invasion analysis"},{"location":"lectures/lecture-18/#lecture-overview","text":"Invasion fitness Evolutionarily singular strategies Evolutionarily stable strategies Evolutionary convergence Pairwise invasibility plots In the models we've discussed so far we've taken the parameters to be fixed. In reality, many of these parameters can evolve. For example, in our model of exponential growth in discrete time, \\(n(t+1)=n(t) R\\) , we took \\(R\\) to be the same for all individuals for all time. But any mutation causing a larger \\(R\\) would increase in frequency, causing the value of \\(R\\) to increase over time. In this lecture we'll explore how to determine the direction of evolution and the stability of evolutionary endpoints for more complex models using a technique called evolutionary invasion analysis (also known as adaptive dynamics). The idea: determine which parameter(s) of our model an evolving trait affects take the population to be fixed for some \"resident\" trait value determine the equilibria and stability of the system with only the resident trait derive the growth rate of a new individual with a \"mutant\" trait value ask when the mutant trait value will invade look for potential evolutionary endpoints determine the stability of those endpoints The evolution of dispersal To motivate evolutionary invasion analysis, let's consider the evolution of dispersal. Imagine there are \\(S\\) sites, with a most one individual reproducing at each. We census the population at the time of reproduction. A reproducing individual has a large number \\(B\\) offspring and then dies. A fraction \\(d\\) of those offspring disperse and a fraction \\(1-c\\) of those survive. The survivors then equally divided among all sites. One individual in each site is then chosen at random to reproduce, which begins the life-cycle anew. The question is, how should dispersal, \\(d\\) , evolve? There is a cost, \\(c\\) , which selects against dispersal but dispersal also allows offspring to avoid competiting with their kin, which could select for more dispersal. We'll use evolutionary invasion analysis to sort this out.","title":"Lecture overview"},{"location":"lectures/lecture-18/#1-invasion-fitness","text":"Let's think about this analysis very generally (in discrete time). Let the number of individuals with the resident trait value be \\(n\\) and the number of individuals with the mutant trait value \\(n_m\\) . (And we'll assume asexual reproduction for simplicity, so that residents produce residents and mutant produce mutants.) Let the potentially nonlinear dynamics of these two groups of individuals depend on their respective trait values, \\(z\\) and \\(z_m\\) , \\[ \\begin{aligned} n(t+1) &= n(t) R(n(t), n_m(t), z, z_m)\\\\ n_m(t+1) &= n_m(t) R_m(n(t), n_m(t), z, z_m) \\end{aligned} \\] The Jacobian of this system is \\[ \\begin{aligned} \\mathbf{J} &= \\begin{pmatrix} \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)} & \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n_m(t)} \\\\ \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n(t)} & \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)} \\end{pmatrix} \\end{aligned} \\] Now consider some non-zero resident equilibrium, \\(\\hat{n}>0\\) , without the mutant, \\(\\hat{n}_m=0\\) . Assuming that the resident does not produce mutants, \\(\\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n(t)}\\big|_{n_m=0}=0\\) , the Jacobian evaluated at this equilibrium simplifies to \\[ \\begin{aligned} \\mathbf{J}\\big|_{n_m=0,n=\\hat{n}} &= \\begin{pmatrix} \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)} & \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n_m(t)} \\\\ 0 & \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)} \\end{pmatrix}_{n_m=0,n=\\hat{n}} \\end{aligned} \\] We can immediately see that the two eigenvalues of this upper triangular matrix are \\[ \\begin{aligned} \\lambda_1 &= \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)}\\bigg|_{n_m=0,n=\\hat{n}}\\\\ \\lambda_2 &= \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)}\\bigg|_{n_m=0,n=\\hat{n}} \\end{aligned} \\] The first, \\(\\lambda_1\\) , determines whether the resident equilibrium, \\(\\hat{n}>0\\) , is stable in the absence of mutants. We'll take \\(0 < \\lambda_1 < 1\\) as a given (we're only interested in stable resident equilibria, we can disregard the others). The second, \\(\\lambda_2\\) , determines whether the resident equilibrium is stable in the presence of a small number of mutants. We call \\(\\lambda_2\\) the invasion fitness , \\(\\lambda(z_m,z)\\) . The mutant will invade whenever \\(\\lambda(z_m,z) > 1\\) . In some simple cases we might be able to use the invasion criterium, \\(\\lambda(z_m,z)>1\\) , to determine what values of \\(z_m\\) (relative to \\(z\\) ) can invade. In most cases, however, \\(\\lambda(z_m,z)\\) will be complex enough that this will not be possible, so we rely on a simple approximation. The evolution of dispersal Let a fraction \\(d\\) of the resident offspring disperse and a fraction \\(d_m\\) of the mutant offspring disperse. And let there be \\(n(t)\\) residents, \\(n_m(t)\\) mutants, and \\(S-n(t)-n_m(t)\\geq0\\) empty sites. Then the probability a resident offspring replaces a resident is the number of resident offspring in a resident patch divided by the total number of offspring in that patch, \\[ \\begin{aligned} p_{rr} &= \\frac{B(1-d) + (n(t)-1)Bd(1-c)/S}{B(1-d) + (n(t)-1)Bd(1-c)/S + n_m(t)Bd_m(1-c)/S}\\\\ &= \\frac{S(1-d) + (n(t)-1)d(1-c)}{S(1-d) + (n(t)-1)d(1-c) + n_m(t)d_m(1-c)} \\end{aligned} \\] Here \\(B(1-d)\\) is the number of non-dispersing offspring produced by the resident in that patch, \\((n(t)-1)Bd(1-c)/S\\) is the number of resident offspring dispersing to the patch from elsewhere, and \\(n_m(t)Bd_m(1-c)/S\\) is the number of mutant offspring dispersing to the patch. The probability that a mutant offspring replaces a mutant \\(p_{mm}\\) is the same expression with \\(d\\) and \\(d_m\\) and \\(n\\) and \\(n_m\\) exchanged. The probability that a resident offspring wins an empty patch is \\[ \\begin{aligned} p_{re} &= \\frac{n(t)Bd(1-c)}{n(t)Bd(1-c) + n_m(t)Bd_m(1-c)}\\\\ &= \\frac{n(t)d}{n(t)d + n_m(t)d_m} \\end{aligned} \\] Then the number of resident and mutant individuals in the next generation are \\[ \\begin{aligned} n(t+1) &= n(t) p_{rr} + n_m(t) (1 - p_{mm}) + (S - n(t) - n_m(t)) p_{re}\\\\ n_m(t+1) &= n(t) (1-p_{rr}) + n_m(t) p_{mm} + (S - n(t) - n_m(t)) (1-p_{re}) \\end{aligned} \\] Now consider an equilibrium with no mutants, \\(\\hat{n}_m=0\\) . Setting \\(n(t+1) = n(t) = \\hat{n}\\) gives \\(\\hat{n}=S\\) . We determine the stability of this equilibrium with the Jacobian. From the general analysis above we know the two eigenvalues are \\[ \\begin{aligned} \\lambda_1 &= \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)}\\bigg|_{n_m=0,n=S} = 0\\\\ \\lambda_2 &= \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)}\\bigg|_{n_m=0,n=S} = \\frac{1-d_m}{1-d_m + d(1-c)} + \\frac{d_m(1-c)}{1-d + d(1-c)} \\end{aligned} \\] The first eigenvalue is less than 1 in absolute value, meaning the resident equilibrium is stable. The second eigenvalue is our invasion fitness \\[ \\lambda(d_m,d) = \\frac{1-d_m}{1-d_m + d(1-c)} + \\frac{d_m(1-c)}{1-d + d(1-c)} \\] In this relatively simple example, we can determine exactly what mutant trait values can invade by looking at where \\[ \\lambda(d_m,d) - 1 = \\frac{(1-c)(d_m-d)(1-cd-d_m)}{(1-cd)(1-d_m+d(1-c))} \\] is positive. The sign of \\(\\lambda(d_m,d) - 1\\) is the sign of \\((d_m-d)(1-cd-d_m)\\) . So the mutant will invade if both terms are positive \\(d<d_m<1-cd\\) or both terms are negative \\(1-cd<d_m<d\\) . In either case, the mutant will invade when it has a trait value between \\(1-cd\\) and \\(d\\) .","title":"1. Invasion fitness"},{"location":"lectures/lecture-18/#2-evolutionarily-singular-strategies","text":"When the mutant trait value is very close to the resident trait value, we can approximate invasion fitness with a Taylor series around \\(z_m = z\\) \\[ \\begin{aligned} \\lambda(z_m,z) &\\approx \\lambda(z,z) + \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=z} (z_m-z)\\\\ &= 1 + \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=z} (z_m-z) \\end{aligned} \\] This allows us to determine which direction evolution will proceed from the current resident value: if \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}>0\\) then invasion when \\(z_m>z\\) if \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}<0\\) then invasion when \\(z_m<z\\) The direction of evolution by small steps is given by \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}\\) , which we call the selection gradient . Potential evolutionary endpoints, also called evolutionarily singular strategies , are the resident trait values \\(z=\\hat{z}\\) where there is no directional selection \\[ \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=\\hat{z}, z=\\hat{z}} = 0 \\] The evolution of dispersal The selection gradient is \\[ \\frac{\\partial \\lambda}{\\partial d_m}\\Big|_{d_m=d} = \\frac{(1-d-dc))(1-c)}{(1-cd)^2} \\] The sign of this is the sign of 1-d-dc, meaning the dispersal rate will increase whenever d<1/(1+c) and decrease whenever 1/(1+c)<d. Setting the selection gradient to zero and solving for the evolutionarily singular strategy gives \\(\\hat{d} = 1/(1+c)\\) .","title":"2. Evolutionarily singular strategies"},{"location":"lectures/lecture-18/#3-evolutionarily-stable-strategies","text":"An evolutionarily singular strategy, \\(\\hat{z}\\) , will only be an evolutionarily stable strategy (ESS), \\(z^*\\) , if it cannot be invaded. Global evolutionary stability will be impossible to prove for most models and so we often focus on local stability, which requires that \\(\\lambda(z_m,z)|_{z=\\hat{z}}\\) is concave at \\(z_m=\\hat{z}\\) (i.e., \\(\\hat{z}\\) is a local fitness maximum), \\[ \\frac{\\partial^2 \\lambda}{\\partial z_m^2}\\bigg|_{z_m=\\hat{z}, z=\\hat{z}} < 0 \\] The evolution of dispersal The second derivative of invasion fitness with respect to the mutant trait value evalulated at the singular strategy is \\[ \\frac{\\partial^2 \\lambda}{\\partial d_m^2}\\Big|_{d_m=d=\\hat{d}} = -2(1-c)(1+c)^2 \\] Because \\(0<c<1\\) this is always negative, which means the singular strategy is always evolutionarily stable.","title":"3. Evolutionarily stable strategies"},{"location":"lectures/lecture-18/#4-evolutionary-convergence","text":"There is one more characteristic of evolutionarily singular strategies that we care about, and that is whether evolution actually leads to that strategy or not. For evolution to move the trait value towards a singular strategy, \\(\\hat{z}\\) , we need evolution to increase the trait value when it is less than \\(\\hat{z}\\) and decrease the trait value when it is greater than \\(\\hat{z}\\) . In other words, we need the selection gradient \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}\\) to decrease as we move through \\(z=\\hat{z}\\) \\[ \\frac{\\mathrm{d}}{\\mathrm{d} z}\\left( \\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z} \\right)_{z=\\hat{z}} < 0 \\] Singular strategies that satisfy this criteria are said to be convergence stable . Interestingly, not all evolutionarily stable strategies are convergence stable and not all convergence stable singular strategies are evolutionarily stable! Evolutionarily stable strategies that are not convergence stable are called Garden of Eden strategies . Singular strategies that are convergence stable but not evolutionarily stable are called evolutionary branching points . The latter are of particular interest because the system evolves towards a state where multiple strategies can invade and coexist, leading to diversification. The evolution of dispersal The derivative of the selection gradient evaluated at the singular strategy is \\[ \\frac{\\mathrm{d}}{\\mathrm{d} d}\\left( \\frac{\\partial \\lambda}{\\partial d_m}\\Big|_{d_m=d} \\right)_{d=\\hat{d}} = -(1-c)(1+c)^3 \\] Because \\(0<c<1\\) this is always negative, which means the singular strategy is always convergence stable.","title":"4. Evolutionary convergence"},{"location":"lectures/lecture-18/#5-pairwise-invasibility-plots","text":"A helpful way to visualize the two types of stability of an evolutionarily singular strategy is called a pairwise invasibility plot (PIP). In this plot we have the resident trait value \\(z\\) on the x-axis, the mutant trait value \\(z_m\\) on the y-axis, and we color in the regions where the mutant can invade, \\(\\lambda(z_m,z)>1\\) . The four types of evolutionarily singular strategies \\(\\hat{z}\\) are then represented by the following PIPs import numpy as np from matplotlib import pyplot as plt # dummy invasion fitness def inv(x, y, slope, z=1): if y < x: if y > slope * x: return z if y > x: if y < slope * x: return z return 1-z # evaluate def compute_pip(slope=-2,z=1,xmin=-2,xmax=2,steps=100): xs = np.linspace(xmin,xmax,steps) X,Y = np.meshgrid(xs,xs) # X and Y values # store the invasion success in a matrix PIP = [] for y in xs: row = [] for x in xs: row.append(inv(x,y,slope,z)) PIP.append(row) return X,Y,PIP # plot def plotfun(X,Y,Z,slope=1,ax=None): if ax==None: fig, ax=plt.subplots(1,1,figsize=(5,5)) ax.contourf(X,Y,Z, colors=['white','black','blue','black']) ax.plot(X[0],X[0],'k',lw=5) ax.plot(X[0],X[0]*slope,'k',lw=5) ax.set_xlim(min(X[0]),max(X[0])) ax.set_ylim(min(X[0]),max(X[0])) ax.set_xlabel('resident trait value $z$') ax.set_ylabel('mutant trait value $z_m$') ax.set_xticks([]) ax.set_yticks([]) fig, axs = plt.subplots(2,2,figsize=(10,10)) X,Y,Z = compute_pip(slope=-4,z=1) plotfun(X,Y,Z,slope=-4,ax=axs[0][0]) axs[0][0].set_title('evolutionarily stable',fontsize=14) axs[0][1].text(2.1,0,'convergence stable',fontsize=14,rotation=270,verticalalignment='center') X,Y,Z = compute_pip(slope=4,z=0) plotfun(X,Y,Z,slope=4,ax=axs[0][1]) axs[0][1].set_title('evolutionarily unstable',fontsize=14) X,Y,Z = compute_pip(slope=4,z=1) plotfun(X,Y,Z,slope=4,ax=axs[1][0]) axs[1][1].text(2.1,0,'convergence unstable',fontsize=14,rotation=270,verticalalignment='center') X,Y,Z = compute_pip(slope=-4,z=0) plotfun(X,Y,Z,slope=-4,ax=axs[1][1]) We can read a PIP by choosing a resident trait value (a point on the x-axis) and looking to see what mutant trait values can invade it (blue regions in that vertical slice). Choose one of the possible invading trait values and set this to be the new resident trait value. Continue indefinitely. When we assume mutants have trait values close to the resident, we restrict ourselves to moving along the 1:1 line. Then, we move to the right when there is blue directly above the 1:1 line but not below and we move to the left when there is blue directly below the 1:1 line but not above. Where there is blue directly above and below the 1:1 we are at a singular strategy that is a fitness minimum (it can be invaded in both directions). Where there is white directly above and below the 1:1 we are at a singular strategy that is a fitness maximum (it can't be invaded in either direction). Try reading each of the plots above. Prove to yourself that the top left has a convergence stable evolutionarily stable strategy, the top right has a branching point, the bottom left has a has a Garden of Eden, and the bottom right has an invasible repellor (a fitness minimum that is not convergence stable). The evolution of dispersal In our model of the evolution of dispersal, the singular strategy is always evolutionarily and convgence stable, and the PIP is below. import numpy as np from matplotlib import pyplot as plt # invasion fitness def inv_fun(dm,d,c): invasionfitness = (1-dm)/(1-dm + d*(1-c)) + (dm*(1-c))/(1-d + d*(1-c)) if invasionfitness>1: return 1 # return 1 if mutant invades return 0 # return 0 if mutant does not invade # evaluate def compute_pip(c=0.5,dmin=0.01,dmax=0.99,steps=100): ds = np.linspace(dmin,dmax,steps) X,Y = np.meshgrid(ds,ds) # X and Y values # store the invasion success in a matrix PIP = [] for dm in ds: row = [] for d in ds: row.append(inv_fun(dm,d,c)) PIP.append(row) return X,Y,PIP # plot def plotfun(X,Y,Z): fig, ax = plt.subplots(1,1,figsize=(5,5)) ax.set_xlabel('$d$') ax.set_ylabel('$d_m$') ax.contourf(X,Y,Z, colors=['white','black','blue','black']) ax.plot(X[0],X[0],'k',lw=5) X,Y,Z=compute_pip() plotfun(X,Y,Z)","title":"5. Pairwise invasibility plots"},{"location":"lectures/lecture-19/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 19: The evolution of dominance Run notes interactively? Lecture overview Model Resident equilibrium Mutant invasion So far we've considered invasion into an unstructured population, i.e., there is just one type of resident and one type of mutant. We can extend this analysis to consider structured populations, age- or spatial-structure for example. The general approach is the same: find a stable resident equilibria and evaluate the Jacobian at that equilibrium to determine the mutant's invasion fitness. The only difference is that our Jacobian is no longer a 2x2 matrix. Below we will demonstrate the general method with a specific example: the evolution of dominance. The motivating observation is that many deleterious alleles are recessive, meaning the wild-type allele partially shields a heterozygote individual from selection. There are many examples in humans, such as sickle cell anemia and cystic fibrosis. In these examples the fitness of heterozygous individuals is indistinguishable from those without any deleterious mutations. Could the dominance of a wild-type allele over a deleterious allele be the result of adaptive evolution? To ask this question we perform an evolutionary invasion analysis on a population with genetic structure. This type of model is often referred to as a modifier model within the field of population genetics. 1. Model The model is an extension of the 2-locus model we developed in lecture 17 to include mutation. We consider two loci each with two alleles. At one locus we have alleles \\(A_1\\) and \\(A_2\\) . We'll treat \\(A_1\\) as the wild-type allele and \\(A_2\\) as the deleterious allele and refer to this \\(A\\) locus as the selected locus. At the other locus we have alleles \\(B_1\\) and \\(B_2\\) . We'll treat \\(B_1\\) as the resident allele and \\(B_2\\) as the mutant (modifier) and refer to this \\(B\\) locus as the modifier locus. The analysis will determine when \\(B_2\\) can invade. We consider diploid selection. Let the relative fitness of any individual with \\(A_1 A_1\\) be 1 and the relative fitness of any individual with \\(A_2 A_2\\) be \\(1-s\\) , with \\(0<s<1\\) . The relative fitnesses of the \\(A_1 A_2\\) heterozygotes are affected by their genotype at the \\(B\\) locus: diploid genotype relative fitness \\(A_1A_2\\) \\(B_1B_1\\) \\(1 - h_{11}s\\) \\(A_1A_2\\) \\(B_1B_2\\) \\(1 - h_{12}s\\) \\(A_1A_2\\) \\(B_2B_2\\) \\(1 - h_{22}s\\) We census the population in the haploid phase and keep track of the 4 haploid genotype frequencies: genotype frequency \\(A_1B_1\\) \\(x_1\\) \\(A_1B_2\\) \\(x_2\\) \\(A_2B_1\\) \\(x_3\\) \\(A_2B_2\\) \\(x_4\\) with the constraint that the total frequency sums to one, \\(x_1+x_2+x_3+x_4 = 1\\) . We treat time as discrete and assume the following life-cycle: graph LR; A((census)) --> B((gamete union)); B --> C((selection)); C --> D((meiosis/recombination)); D --> E((mutation)); E --> A After selection but before mutation the frequencies are as they were in lecture 17, \\[\\begin{align} x_1' &= x_1(t) \\sum_{i=1}^{4} x_i(t) w_{1i}/\\bar{w} - r D^* \\\\ x_2' &= x_2(t) \\sum_{i=1}^{4} x_i(t) w_{2i}/\\bar{w} + r D^* \\\\ x_3' &= x_3(t) \\sum_{i=1}^{4} x_i(t) w_{3i}/\\bar{w} + r D^* \\\\ x_4' &= x_4(t) \\sum_{i=1}^{4} x_i(t) w_{4i}/\\bar{w} - r D^*, \\end{align}\\] where \\(w_{ij}=w_{ji}\\) is the fitness of the diploid that is composed of haploid genotypes \\(i\\) and \\(j\\) , \\(\\bar{w}\\) is the population mean fitness, and \\(D^*=x_1(t)x_4(t) w_{14}/\\bar{w} - x_2(t)x_3(t) w_{23}/\\bar{w}\\) is linkage disequilibrium after selection. We assume \\(A_1\\) mutates to \\(A_2\\) with probability \\(\\mu\\) and ignore back mutations (a relatively safe assumption given that the deleterious \\(A_2\\) allele will be rare and therefore will create few mutants). The recursion equations are then \\[\\begin{align} x_1(t+1) &= (1-\\mu)x_1'\\\\ x_2(t+1) &= (1-\\mu)x_2'\\\\ x_3(t+1) &= x3' + \\mu x_1'\\\\ x_4(t+1) &= x4' + \\mu x_2'. \\end{align}\\] 2. Resident equilibrium We cannot find all equilibria of this system of nonlinear equations, but we can find some. Here we are most interested in the \"resident\" equilibrium where there are no \\(B_2\\) alleles, \\(\\hat{x}_2=\\hat{x}_4=0\\) , which reduces us back to a one locus model. We'll also take \\(h_{11}=1/2\\) for simplicity, meaning that the resident \\(B_1\\) allele makes the \\(A\\) locus additive. Then there is a relatively simple resident equilibrium, \\[ \\begin{align} \\hat{x}_1 = 1 - \\frac{\\mu}{(1+\\mu)s/2} \\\\ \\hat{x}_3 = \\frac{\\mu}{(1+\\mu)s/2}, \\end{align} \\] which says that the deleterious \\(A_2\\) allele is maintained at a balance between its removal by selection and its generation by mutation. This equilibrium is valid as long as \\(\\mu<(1+\\mu)s/2\\) , which means mutation is weak relative to selection. 3. Mutant invasion To evaluate the stability of this equilibrium we use the Jacobian evaluated at the resident equilibrium. Because we have 4 equations this is a 4x4 matrix. But if we arrange the equations in the following order, \\(x_1(t+1), x_3(t+1), x_2(t+1), x_4(t+1)\\) , then the Jacobian at the resident equilibrium can be written as an upper triangular block matrix, \\[ \\mathbf{J}|_{x_1=\\hat{x}_1,x_3=\\hat{x}_3,x_3=x_4=0} = \\begin{bmatrix} \\mathbf{J}_\\mathrm{res} & \\mathbf{V} \\\\ \\mathbf{0} & \\mathbf{J}_\\mathrm{mut}\\end{bmatrix}. \\] Here \\(\\mathbf{J}_\\mathrm{res}\\) describes the stability of the resident equilibrium in the absence of any \\(B_2\\) alleles and \\(\\mathbf{J}_\\mathrm{mut}\\) describes the stability of the resident equilibrium in the face of rare \\(B_2\\) alleles. One can show that the eigenvalues of \\(\\mathbf{J}_\\mathrm{res}\\) are always less than 1 when the resident equilibrium is valid, guaranteeing stability. We want to know whether \\(B_2\\) alleles can invade the resident equilibrium. This is determined by the leading eigenvalue of \\(\\mathbf{J}_\\mathrm{mut}\\) , which we call the invasion fitness. Unfortunately the eigenvalues are a little complicated, but we can make some progress with a little trick. The equation for the eigenvalues of \\(\\mathbf{J}_\\mathrm{mut}\\) is \\[ \\lambda^2 - \\mathrm{Tr}(\\mathbf{J}_\\mathrm{mut}) \\lambda + \\mathrm{Det}(\\mathbf{J}_\\mathrm{mut}) = 0. \\] Now, given that \\(\\lambda\\) is the invasion fitness of a mutant with dominance coefficient \\(h_{12}\\) in a resident population with dominance coefficient \\(h_{11}=1/2\\) , the selection gradient is \\(\\frac{\\partial\\lambda}{\\partial h_{12}}|_{h_{12}=1/2}\\) . The trick is that we can get the selection gradient directly from the equation above without solving for invasion fitness. To make the dependence on \\(h_{12}\\) clear and simplify the notation, write \\(\\lambda=\\lambda(h_{12})\\) , \\(-\\mathrm{Tr}(\\mathbf{J}_\\mathrm{mut})=a(h_{12})\\) , and \\(\\mathrm{Det}(\\mathbf{J}_\\mathrm{mut})=b(h_{12})\\) . We can then differentiate the equation above with respect to \\(h_{12}\\) and rearrange for the selection gradient (this is called implicit differentiation), \\[ \\begin{align} \\frac{\\partial \\lambda^2}{\\partial h_{12}} + \\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) + a(h_{12}) \\frac{\\partial \\lambda}{\\partial h_{12}} + \\frac{\\partial b}{\\partial h_{12}} &= 0 \\\\ 2\\lambda(h_{12})\\frac{\\partial \\lambda}{\\partial h_{12}} + \\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) + a(h_{12}) \\frac{\\partial \\lambda}{\\partial h_{12}} + \\frac{\\partial b}{\\partial h_{12}} &= 0 \\\\ \\frac{\\partial \\lambda}{\\partial h_{12}}(2\\lambda(h_{12}) + a(h_{12})) &= - \\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) - \\frac{\\partial b}{\\partial h_{12}} \\\\ \\frac{\\partial \\lambda}{\\partial h_{12}} &= - \\frac{\\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) + \\frac{\\partial b}{\\partial h_{12}}}{2\\lambda(h_{12}) + a(h_{12})} \\end{align} \\] We then evaluate at \\(h_{12}=1/2\\) , which is where the mutant is equivalent to the resident, i.e., \\(\\lambda(1/2)=1\\) , \\[ \\begin{align} \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &= - \\left(\\frac{\\frac{\\partial a}{\\partial h_{12}}\\lambda(h_{12}) + \\frac{\\partial b}{\\partial h_{12}}}{2\\lambda(h_{12})+a(h_{12})}\\right)_{h_{12}=1/2} \\\\ \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &= - \\left(\\frac{\\left.\\frac{\\partial a}{\\partial h_{12}}\\right|_{h_{12}=1/2}\\lambda(1/2) + \\left.\\frac{\\partial b}{\\partial h_{12}}\\right|_{h_{12}=1/2}}{2\\lambda(1/2)+a(1/2)}\\right) \\\\ \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &= - \\left(\\frac{\\left.\\frac{\\partial a}{\\partial h_{12}}\\right|_{h_{12}=1/2} + \\left.\\frac{\\partial b}{\\partial h_{12}}\\right|_{h_{12}=1/2}}{2+a(1/2)}\\right) \\\\ \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &= - \\frac{2(4r + s(1-2r))(s(1+\\mu)-2\\mu)\\mu}{s(s(1+\\mu) - 2\\mu + r(2-s)(1-\\mu^2))}. \\end{align} \\] Given biological validity, \\(\\mu<(1+\\mu)s/2\\) , this is always negative (recall that \\(r\\leq 1/2\\) ). Therefore there is always selection to reduce the dominance coefficient, \\(h\\) . This means that selection could be responsible for making deleterious mutations recesive! However, there is a massive caveat. While the selection gradient is negative it is also proportional to \\(\\mu\\) , which is tiny. This means that selection for recessive deleterious mutations is exceptionaly weak and so is easily overwhelmed by other forces (eg, genetic drift, migration, etc). The biological reason for such weak selection is that selection only acts on the variation that is present, and deleterious mutations are very rare at mutation-selection balance, \\(\\mu/((1+\\mu)s/2)\\) .","title":"Lecture 19"},{"location":"lectures/lecture-19/#lecture-19-the-evolution-of-dominance","text":"Run notes interactively?","title":"Lecture 19: The evolution of dominance"},{"location":"lectures/lecture-19/#lecture-overview","text":"Model Resident equilibrium Mutant invasion So far we've considered invasion into an unstructured population, i.e., there is just one type of resident and one type of mutant. We can extend this analysis to consider structured populations, age- or spatial-structure for example. The general approach is the same: find a stable resident equilibria and evaluate the Jacobian at that equilibrium to determine the mutant's invasion fitness. The only difference is that our Jacobian is no longer a 2x2 matrix. Below we will demonstrate the general method with a specific example: the evolution of dominance. The motivating observation is that many deleterious alleles are recessive, meaning the wild-type allele partially shields a heterozygote individual from selection. There are many examples in humans, such as sickle cell anemia and cystic fibrosis. In these examples the fitness of heterozygous individuals is indistinguishable from those without any deleterious mutations. Could the dominance of a wild-type allele over a deleterious allele be the result of adaptive evolution? To ask this question we perform an evolutionary invasion analysis on a population with genetic structure. This type of model is often referred to as a modifier model within the field of population genetics.","title":"Lecture overview"},{"location":"lectures/lecture-19/#1-model","text":"The model is an extension of the 2-locus model we developed in lecture 17 to include mutation. We consider two loci each with two alleles. At one locus we have alleles \\(A_1\\) and \\(A_2\\) . We'll treat \\(A_1\\) as the wild-type allele and \\(A_2\\) as the deleterious allele and refer to this \\(A\\) locus as the selected locus. At the other locus we have alleles \\(B_1\\) and \\(B_2\\) . We'll treat \\(B_1\\) as the resident allele and \\(B_2\\) as the mutant (modifier) and refer to this \\(B\\) locus as the modifier locus. The analysis will determine when \\(B_2\\) can invade. We consider diploid selection. Let the relative fitness of any individual with \\(A_1 A_1\\) be 1 and the relative fitness of any individual with \\(A_2 A_2\\) be \\(1-s\\) , with \\(0<s<1\\) . The relative fitnesses of the \\(A_1 A_2\\) heterozygotes are affected by their genotype at the \\(B\\) locus: diploid genotype relative fitness \\(A_1A_2\\) \\(B_1B_1\\) \\(1 - h_{11}s\\) \\(A_1A_2\\) \\(B_1B_2\\) \\(1 - h_{12}s\\) \\(A_1A_2\\) \\(B_2B_2\\) \\(1 - h_{22}s\\) We census the population in the haploid phase and keep track of the 4 haploid genotype frequencies: genotype frequency \\(A_1B_1\\) \\(x_1\\) \\(A_1B_2\\) \\(x_2\\) \\(A_2B_1\\) \\(x_3\\) \\(A_2B_2\\) \\(x_4\\) with the constraint that the total frequency sums to one, \\(x_1+x_2+x_3+x_4 = 1\\) . We treat time as discrete and assume the following life-cycle: graph LR; A((census)) --> B((gamete union)); B --> C((selection)); C --> D((meiosis/recombination)); D --> E((mutation)); E --> A After selection but before mutation the frequencies are as they were in lecture 17, \\[\\begin{align} x_1' &= x_1(t) \\sum_{i=1}^{4} x_i(t) w_{1i}/\\bar{w} - r D^* \\\\ x_2' &= x_2(t) \\sum_{i=1}^{4} x_i(t) w_{2i}/\\bar{w} + r D^* \\\\ x_3' &= x_3(t) \\sum_{i=1}^{4} x_i(t) w_{3i}/\\bar{w} + r D^* \\\\ x_4' &= x_4(t) \\sum_{i=1}^{4} x_i(t) w_{4i}/\\bar{w} - r D^*, \\end{align}\\] where \\(w_{ij}=w_{ji}\\) is the fitness of the diploid that is composed of haploid genotypes \\(i\\) and \\(j\\) , \\(\\bar{w}\\) is the population mean fitness, and \\(D^*=x_1(t)x_4(t) w_{14}/\\bar{w} - x_2(t)x_3(t) w_{23}/\\bar{w}\\) is linkage disequilibrium after selection. We assume \\(A_1\\) mutates to \\(A_2\\) with probability \\(\\mu\\) and ignore back mutations (a relatively safe assumption given that the deleterious \\(A_2\\) allele will be rare and therefore will create few mutants). The recursion equations are then \\[\\begin{align} x_1(t+1) &= (1-\\mu)x_1'\\\\ x_2(t+1) &= (1-\\mu)x_2'\\\\ x_3(t+1) &= x3' + \\mu x_1'\\\\ x_4(t+1) &= x4' + \\mu x_2'. \\end{align}\\]","title":"1. Model"},{"location":"lectures/lecture-19/#2-resident-equilibrium","text":"We cannot find all equilibria of this system of nonlinear equations, but we can find some. Here we are most interested in the \"resident\" equilibrium where there are no \\(B_2\\) alleles, \\(\\hat{x}_2=\\hat{x}_4=0\\) , which reduces us back to a one locus model. We'll also take \\(h_{11}=1/2\\) for simplicity, meaning that the resident \\(B_1\\) allele makes the \\(A\\) locus additive. Then there is a relatively simple resident equilibrium, \\[ \\begin{align} \\hat{x}_1 = 1 - \\frac{\\mu}{(1+\\mu)s/2} \\\\ \\hat{x}_3 = \\frac{\\mu}{(1+\\mu)s/2}, \\end{align} \\] which says that the deleterious \\(A_2\\) allele is maintained at a balance between its removal by selection and its generation by mutation. This equilibrium is valid as long as \\(\\mu<(1+\\mu)s/2\\) , which means mutation is weak relative to selection.","title":"2. Resident equilibrium"},{"location":"lectures/lecture-19/#3-mutant-invasion","text":"To evaluate the stability of this equilibrium we use the Jacobian evaluated at the resident equilibrium. Because we have 4 equations this is a 4x4 matrix. But if we arrange the equations in the following order, \\(x_1(t+1), x_3(t+1), x_2(t+1), x_4(t+1)\\) , then the Jacobian at the resident equilibrium can be written as an upper triangular block matrix, \\[ \\mathbf{J}|_{x_1=\\hat{x}_1,x_3=\\hat{x}_3,x_3=x_4=0} = \\begin{bmatrix} \\mathbf{J}_\\mathrm{res} & \\mathbf{V} \\\\ \\mathbf{0} & \\mathbf{J}_\\mathrm{mut}\\end{bmatrix}. \\] Here \\(\\mathbf{J}_\\mathrm{res}\\) describes the stability of the resident equilibrium in the absence of any \\(B_2\\) alleles and \\(\\mathbf{J}_\\mathrm{mut}\\) describes the stability of the resident equilibrium in the face of rare \\(B_2\\) alleles. One can show that the eigenvalues of \\(\\mathbf{J}_\\mathrm{res}\\) are always less than 1 when the resident equilibrium is valid, guaranteeing stability. We want to know whether \\(B_2\\) alleles can invade the resident equilibrium. This is determined by the leading eigenvalue of \\(\\mathbf{J}_\\mathrm{mut}\\) , which we call the invasion fitness. Unfortunately the eigenvalues are a little complicated, but we can make some progress with a little trick. The equation for the eigenvalues of \\(\\mathbf{J}_\\mathrm{mut}\\) is \\[ \\lambda^2 - \\mathrm{Tr}(\\mathbf{J}_\\mathrm{mut}) \\lambda + \\mathrm{Det}(\\mathbf{J}_\\mathrm{mut}) = 0. \\] Now, given that \\(\\lambda\\) is the invasion fitness of a mutant with dominance coefficient \\(h_{12}\\) in a resident population with dominance coefficient \\(h_{11}=1/2\\) , the selection gradient is \\(\\frac{\\partial\\lambda}{\\partial h_{12}}|_{h_{12}=1/2}\\) . The trick is that we can get the selection gradient directly from the equation above without solving for invasion fitness. To make the dependence on \\(h_{12}\\) clear and simplify the notation, write \\(\\lambda=\\lambda(h_{12})\\) , \\(-\\mathrm{Tr}(\\mathbf{J}_\\mathrm{mut})=a(h_{12})\\) , and \\(\\mathrm{Det}(\\mathbf{J}_\\mathrm{mut})=b(h_{12})\\) . We can then differentiate the equation above with respect to \\(h_{12}\\) and rearrange for the selection gradient (this is called implicit differentiation), \\[ \\begin{align} \\frac{\\partial \\lambda^2}{\\partial h_{12}} + \\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) + a(h_{12}) \\frac{\\partial \\lambda}{\\partial h_{12}} + \\frac{\\partial b}{\\partial h_{12}} &= 0 \\\\ 2\\lambda(h_{12})\\frac{\\partial \\lambda}{\\partial h_{12}} + \\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) + a(h_{12}) \\frac{\\partial \\lambda}{\\partial h_{12}} + \\frac{\\partial b}{\\partial h_{12}} &= 0 \\\\ \\frac{\\partial \\lambda}{\\partial h_{12}}(2\\lambda(h_{12}) + a(h_{12})) &= - \\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) - \\frac{\\partial b}{\\partial h_{12}} \\\\ \\frac{\\partial \\lambda}{\\partial h_{12}} &= - \\frac{\\frac{\\partial a}{\\partial h_{12}} \\lambda(h_{12}) + \\frac{\\partial b}{\\partial h_{12}}}{2\\lambda(h_{12}) + a(h_{12})} \\end{align} \\] We then evaluate at \\(h_{12}=1/2\\) , which is where the mutant is equivalent to the resident, i.e., \\(\\lambda(1/2)=1\\) , \\[ \\begin{align} \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &= - \\left(\\frac{\\frac{\\partial a}{\\partial h_{12}}\\lambda(h_{12}) + \\frac{\\partial b}{\\partial h_{12}}}{2\\lambda(h_{12})+a(h_{12})}\\right)_{h_{12}=1/2} \\\\ \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &= - \\left(\\frac{\\left.\\frac{\\partial a}{\\partial h_{12}}\\right|_{h_{12}=1/2}\\lambda(1/2) + \\left.\\frac{\\partial b}{\\partial h_{12}}\\right|_{h_{12}=1/2}}{2\\lambda(1/2)+a(1/2)}\\right) \\\\ \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &= - \\left(\\frac{\\left.\\frac{\\partial a}{\\partial h_{12}}\\right|_{h_{12}=1/2} + \\left.\\frac{\\partial b}{\\partial h_{12}}\\right|_{h_{12}=1/2}}{2+a(1/2)}\\right) \\\\ \\left.\\frac{\\partial \\lambda}{\\partial h_{12}}\\right|_{h_{12}=1/2} &= - \\frac{2(4r + s(1-2r))(s(1+\\mu)-2\\mu)\\mu}{s(s(1+\\mu) - 2\\mu + r(2-s)(1-\\mu^2))}. \\end{align} \\] Given biological validity, \\(\\mu<(1+\\mu)s/2\\) , this is always negative (recall that \\(r\\leq 1/2\\) ). Therefore there is always selection to reduce the dominance coefficient, \\(h\\) . This means that selection could be responsible for making deleterious mutations recesive! However, there is a massive caveat. While the selection gradient is negative it is also proportional to \\(\\mu\\) , which is tiny. This means that selection for recessive deleterious mutations is exceptionaly weak and so is easily overwhelmed by other forces (eg, genetic drift, migration, etc). The biological reason for such weak selection is that selection only acts on the variation that is present, and deleterious mutations are very rare at mutation-selection balance, \\(\\mu/((1+\\mu)s/2)\\) .","title":"3. Mutant invasion"},{"location":"lectures/lecture-20/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 20: Probability I (genetic drift) Run notes interactively? Lecture overview Discrete random variables Expectation of a discrete random variable Variance of a discrete random variable Independence Binomial random variable Genetic drift credits This lecture was created by PhD student Puneeth Deraje as part of a course development TA position -- thanks Puneeth! If you are following along with the text, this lecture does not follow along as closely. Until now we have been dealing with deterministic models, i.e., given the value of the variables at time \\(t\\) , we know exactly what the values will be at the next time. However, life is a little bit more random than that. Stochasticity (i.e., chance) is inherent in nature and can significantly alter the outcomes. In order to capture the effect of this stochasticity, we need some tools from probability theory . 1. Discrete random variables Example 1: a fair coin toss Let's toss a toonie. If you get a heads, you use that toonie to buy a chocolate, otherwise you put the toonie in a piggy bank for grad school :P. Let \\(X\\) denote the number of chocolates you have after the coin toss. Then \\(X\\) can be equal to 0 or 1. In this case, tossing the coin is called an event and \\(X\\) is the random variable that records its outcome. The state space is the set of all outcomes, {0,1}. We call \\(X\\) a discrete random variable because there are a finite number of outcomes in the state space. We don't know the exact value of \\(X\\) until you actually toss the coin (i.e., until the event happens) -- we can not predict the outcome of the coin toss in that way. However, what we do know is that with a fair coin there is an equal chance of getting either a heads or tails. In other words, if we were to toss the coin a large number of times, say a million times, roughly half a million times we will see tails and the other half a million times we will see heads. Therefore, we say that \\(X\\) takes the value 0 with probability \\(\\frac{500000}{1000000}=0.5\\) and 1 with probability \\(\\frac{500000}{1000000}=0.5\\) . We can write this as \\[\\begin{aligned} \\Pr(X=0) &= 0.5\\\\ \\Pr(X=1) &= 0.5 \\end{aligned}\\] Note that the sum of the probabilities of a random variable taking a value, over all values in the state space, is 1. Here, \\(\\Pr(X=0) + \\Pr(X=1) = 1\\) . Note This way of thinking about the probability of a random variable \\(X\\) taking a value \\(x\\) , as the fraction (frequency) of events where \\(X=x\\) , is the frequentist interpretation of probability . In order for the frequency to match the probability you need to repeat the event a large number of times (ideally infinitely). Example 2: a biased coin toss Consider the same situation as above but with a biased toonie, i.e., one that shows heads in not half but in a fraction \\(p\\) of a very large number of events. Then \\[\\begin{aligned} \\Pr(X=0) &= 1-p \\\\ \\Pr(X=1) &= p \\\\ \\end{aligned}\\] This random variable \\(X\\) is called a Bernoulli random variable with parameter \\(p\\) and is denoted as \\(X\\sim\\mathrm{Ber}(p)\\) . Above, with a fair coin, \\(X\\sim\\mathrm{Ber}(1/2)\\) . Example 3: a fair die roll Now suppose you roll a fair 6-sided die to decide how many chocolates you will buy (this is nice because you never get 0!). Then, if \\(X\\) denotes the number of chocolates, we have \\[\\begin{aligned} \\Pr(X=1) &= \\frac{1}{6}\\\\ \\Pr(X=2) &= \\frac{1}{6} \\\\ \\Pr(X=3) &= \\frac{1}{6} \\\\ \\Pr(X=4) &= \\frac{1}{6} \\\\ \\Pr(X=5) &= \\frac{1}{6} \\\\ \\Pr(X=6) &= \\frac{1}{6} \\end{aligned}\\] The state space here is {1,2,3,4,5,6} and \\(\\sum_{x=1}^6 \\Pr(X=x) = 1\\) . 2. Expectation of a discrete random variable Given this way of thinking about stochasticity, one might be interested in questions like \"What is the value of \\(X\\) , on average?\" and \"How certain can I be about the outcome of \\(X\\) ?\". In order to answer these questions we need to define two important properties of a random variable, its expectation and variance. The expectation of a random variable is the average value of outcomes if the event is repeated infinitely many times. Example 1: a fair coin toss Suppose we toss the fair coin a million times. By the frequentist definition of probability we should see tails ( \\(X=0\\) ) half a million times and heads ( \\(X=1\\) ) the other half a million times. The expected value of \\(X\\) is then \\[\\begin{aligned} \\mathbb{E}(X) &= \\frac{\\overbrace{0+0+...+0}^{\\text{0.5 million times}}+\\overbrace{1+1+...+1}^{\\text{0.5 million times}}}{\\text{1 million}}\\\\ &= \\frac{ 0 \\times \\text{0.5 million} + 1 \\times \\text{0.5 million} }{\\text{1 million}}\\\\ &= 0 \\times \\frac{\\text{0.5 million}}{\\text{1 million}} + 1 \\times \\frac{\\text{0.5 million}}{\\text{1 million}}\\\\ &= 0 \\times (1/2) + 1 \\times (1/2) \\\\ &= 0.5 \\end{aligned}\\] Example 2 : a biased coin toss Suppose we repeat the whole procedure for a biased coin, i.e., \\(X\\sim\\mathrm{Ber}(p)\\) . Then \\[\\begin{aligned} \\mathbb{E}(X) &= \\frac{\\overbrace{0+0+...+0}^{(1-p) \\text{ million times}}+\\overbrace{1+1+...+1}^{p \\text{ million times}}}{\\text{1 million}} \\\\ &= 0 \\times (1-p) + 1 \\times p \\\\ &= p \\end{aligned}\\] Example 3: a fair die roll If we do the same for a fair 6-sided die, \\[\\begin{aligned} \\mathbb{E}(X) &= \\frac{\\overbrace{1+1+...+1}^{(1/6) \\text{ million times}} + \\overbrace{2+2+...+2}^{(1/6) \\text{ million times}} + \\overbrace{3+3+...+3}^{(1/6) \\text{ million times}} + \\overbrace{4+4+...+4}^{(1/6) \\text{ million times}} + \\overbrace{5+5+...+5}^{(1/6) \\text{ million times}} + \\overbrace{6+6+...+6}^{(1/6) \\text{ million times}}}{\\text{1 million}} \\\\ &= 1\\times\\frac{1}{6} + 2\\times\\frac{1}{6} + 3\\times\\frac{1}{6} + 4\\times\\frac{1}{6} + 5\\times\\frac{1}{6} + 6\\times\\frac{1}{6}\\\\ &= 3.5 \\end{aligned}\\] Expectation In general, the expectation of a discrete random variable is \\(\\mathbb{E}(X) = \\sum_x x \\Pr(X=x)\\) , where the sum is over the entire state space. Properties of expectation If \\(c\\) is a constant then \\(\\mathbb{E}(c)=c\\) and \\(\\mathbb{E}(cX)=c\\mathbb{E}(X)\\) If \\(X\\) and \\(Y\\) are two random variables then \\(\\mathbb{E}(X+Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\\) Suppose \\(X\\) is a random variable and \\(f\\) is any function then \\(\\mathbb{E}(f(X)) = \\sum_x f(x)P(X=x)\\) . For example, let \\(f(x) = x^2\\) and \\(X\\sim\\mathrm{Ber}(p)\\) (e.g., a biased coin toss), then \\(\\mathbb{E}(f(X)) = \\sum_{x=0}^{1}x^2P(X=x)= 0^2(1-p)+1^2p = p\\) . 3. Variance of a discrete random variable Suppose we have an extremely biased coin with \\(p=1\\) . That is, the coin always shows heads. Can we, before tossing, say something about the outcome in this case? Yes, we know exactly what the outcome will be, heads. Similarly, if \\(p=0\\) we would know the exact outcome, tails. We say there is no uncertainity in either of these cases. Now, consider a slightly less biased coin with \\(p = 0.99\\) . Although we can not say exactly what the outcome will be, it will very likely be heads. There is some small amount of uncertainity. Finally, suppose we toss a fair coin (i.e., \\(p = 0.5\\) ). Then we have no idea at all what the outcome will be. There is very high uncertainity in this case. Variance The variance of a random variable \\(X\\) , denoted \\(\\mathrm{Var}(X)\\) , quantifies the uncertainity associated with the random variable and is given by \\[\\begin{aligned} \\mathrm{Var}(X) &= \\mathbb{E}((X - \\mathbb{E}(X))^2 )\\\\ &= \\mathbb{E}(X^2 - 2X\\mathbb{E}(X) + \\mathbb{E}(X)^2)\\\\ &= \\mathbb{E}(X^2) - \\mathbb{E}(2X\\mathbb{E}(X)) + \\mathbb{E}(\\mathbb{E}(X)^2)\\\\ &= \\mathbb{E}(X^2) - 2\\mathbb{E}(X)\\mathbb{E}(X) + \\mathbb{E}(X)^2\\\\ &= \\mathbb{E}(X^2) - \\mathbb{E}(X)^2 \\end{aligned}\\] Example: a biased coin toss If \\(X\\sim\\mathrm{Ber}(p)\\) then \\[\\begin{aligned} \\mathrm{Var}(X) &= \\mathbb{E}(X^2) - \\mathbb{E}(X)^2 \\\\ &= p - p^2 \\\\ &= p(1-p) \\end{aligned}\\] Below we plot the variance of \\(X\\) as a function of \\(p\\) . import matplotlib.pyplot as plt import numpy as np p_vals = np.arange(0,1,0.01) Var_values = [ p*(1-p) for p in p_vals ] plt.plot(p_vals, Var_values) plt.axvline(x=0.5,linestyle='dashed') plt.xlabel('$p$ = probability of getting a heads') plt.ylabel('Var($X$)') plt.show() Note that for \\(p=0\\) or \\(p=1\\) we have Var( \\(X\\) ) = 0, i.e., there is no uncertainity. Further note that Var( \\(X\\) ) = \\(p(1-p)\\) peaks at \\(p=0.5\\) , i.e., the highest uncertainity is associated with the toss of a fair coin. Properties of variance If \\(c\\) is a constant then Var \\((c)\\) = 0 If \\(a\\) and \\(b\\) are constants and \\(X\\) is a random variable then Var \\((aX + b)\\) = \\(a^2\\) Var \\((X)\\) 4. Independence Let's now look at two random variables simultaneously. Let's play the same game as before. Toss a coin, if it's heads use the coin to buy a chocolate. If it's tails put the coin in the piggy bank. As before, let \\(X\\) be the number of chocolates after the toss, \\(X\\sim\\) Ber( \\(p\\) ). This time we'll also keep track of how many coins we put in the piggy bank, \\(Y\\) . Now if \\(X=0\\) then \\(Y=1\\) and if \\(X=1\\) then \\(Y=0\\) . The value of \\(Y\\) depends on \\(X\\) , and vice versa, so \\(X\\) and \\(Y\\) are not independent random variables . On the other hand, suppose you have two coins. You toss each coin and decide whether to use it to buy a chocolate. Let \\(X_1\\) be the outcome of tossing the first coin. Let \\(X_2\\) be the outcome of tossing the second coin. Then \\(X_1\\) does not depend on \\(X_2\\) , and vice-versa, so \\(X_1\\) and \\(X_2\\) are independent random variables. Independence Two random variables \\(X\\) and \\(Y\\) are said to be independent if for every \\(x\\) and \\(y\\) \\[\\Pr(X=x \\text{ and } Y=y) = \\Pr(X=x)\\Pr(Y=y)\\] Properties of independent random variables If \\(X\\) and \\(Y\\) are independent random variables then \\(\\mathbb{E}(XY)=\\mathbb{E}(X)\\mathbb{E}(Y)\\) Var( \\(X+Y\\) ) = Var( \\(X\\) ) + Var( \\(Y\\) ) 5. Binomial random variable Let \\(X_1\\) and \\(X_2\\) be the outcome of two independent tosses of the same coin. We want know how many chocolates we have after two tosses, \\(X = X_1 + X_2\\) . Recall that \\(X_1\\) and \\(X_2\\) are both Ber( \\(p\\) ). Therefore, \\[\\begin{aligned} \\mathbb{E}(X) &= \\mathbb{E}(X_1) + \\mathbb{E}(X_2)\\\\ &= p + p \\\\ &= 2p \\end{aligned}\\] Since \\(X_1\\) and \\(X_2\\) are independent we also have \\[\\begin{aligned} \\mathrm{Var}(X) &= \\mathrm{Var}(X_1) +\\mathrm{Var}(X_2)\\\\ &= p(1-p) + p(1-p)\\\\ &= 2p(1-p) \\end{aligned}\\] Now let's look at three special cases to see what this means. If \\(p=0\\) then \\(\\mathbb{E}(X) = 0\\) and \\(\\mathrm{Var}(X)=0\\) . If the coin always shows tails then we know with certainity that we will have no chocolates after two tosses. Similarly, if \\(p=1\\) then \\(\\mathbb{E}(X) = 2\\) and \\(\\mathrm{Var}(X)=0\\) . If the coin always shows heads then we know with certainity that we will have two chocolates after two tosses. Finally, if \\(p=0.5\\) then \\(\\mathbb{E}(X) = 1\\) and \\(\\mathrm{Var}(X)=1\\) . With a fair coin we expect to have 1 chocolate but there is some uncertainity since we could end up with 0, 1, or 2. Binomial random variable In general we could have \\(n\\) coins. Let \\(X_i\\) denote the outcome of the \\(i^{\\mathrm{th}}\\) toss, which is Ber( \\(p\\) ) and independent of all other tosses. Then the sum over all \\(n\\) Bernoulli random variables, \\(X = X_1 + X_2 + ... + X_n\\) , is called a binomial random variable with parameters \\(n\\) and \\(p\\) , and is denoted by Bin( \\(n,p\\) ). The expectation of a binomial random variable is \\[\\mathbb{E}(X) = np\\] and its variance is \\[\\mathrm{Var}(X) = np(1-p)\\] Moreover, we can compute the probability of a binomial random variable \\(X\\sim\\) Bin( \\(n,p\\) ) taking value \\(k\\) , as \\[P(X = k) = {n \\choose k} p^k (1-p)^{n-k}\\] where \\({n \\choose k}\\) is read \" \\(n\\) choose \\(k\\) \" and is the number of ways of choosing \\(k\\) items from \\(n\\) options. Mathematically, \\[\\begin{aligned} {n \\choose k} &= \\frac{n}{k} {n-1 \\choose k-1}\\\\ &= \\frac{n}{k} \\frac{n-1}{k-1} {n-2 \\choose k-2}\\\\ &= \\vdots\\\\ &= \\frac{n (n-1) (n-2)\\cdots (n-k+1)}{k(k-1)(k-2)\\cdots 1} \\end{aligned}\\] 6. Genetic drift Recall our model of one-locus haploid selection from Lecture 4 . If \\(p_t\\) is the frequency of allele \\(A\\) in the population at time \\(t\\) then we said that the frequency of \\(A\\) in the next generation is \\[ p_{t+1} = \\frac{p_t W_A }{p_t W_A + (1-p_t)W_a} \\] where \\(W_A\\) is the fitness of \\(A\\) and \\(W_a\\) is the fitness of \\(a\\) . One unspoken assumption here is that the population size is infinitely large (or at least sufficiently big). In reality, populations are not infinitely large and may even be quite small. This finiteness introduces stochasticity, which in evolution we call genetic drift . One well-known model that incorporates genetic drift is the Wright-Fisher model . This model assumes that there are \\(N\\) diploid individuals in generation \\(t\\) , each of which produce an infinite number of haploid gametes, of which \\(2N\\) are randomly sampled to form the diploid individuals of generation \\(t+1\\) . Here, with haploid selection, we assume the frequency of gametes changes due to selection before sampling for the next generation. graph TD A[N diploids at t] --> B[Infinite haploids before selection]; B --> C[Infinite haploids after selection]; C --> D[N diploids at t+1]; So, let \\(p_t\\) be the frequency of allele \\(A\\) in generation \\(t\\) . Each diploid individual contributes an infinite number of gametes, so the frequency of \\(A\\) in the gamete pool is also \\(p_t\\) . Now selection acts on the gamete pool, altering the frequencies as in our original model, so that the frequency of \\(A\\) in the gamete pool is now \\(p_t' = \\frac{p_tW_A }{p_t W_A + (1-p_t)W_a}\\) . We then sample \\(2N\\) gametes to form the next diploid generation. For each gamete we pick there is a probability \\(p'_t\\) that it has allele \\(A\\) and a probability \\(1-p'_t\\) that it has allele \\(a\\) . Let \\(X\\) be the number of \\(A\\) alleles in a randomly chosen gamete. Then \\(X\\sim\\) Ber \\((p'_t)\\) and the total number of \\(A\\) alleles after \\(2N\\) samples, \\(n_A(t+1)\\) , is the sum of \\(2N\\) Bernoulli random variables, implying \\(n_A(t+1)\\sim\\) Bin \\((2N,p'_t)\\) . The frequency of allele \\(A\\) in the next generation is then \\[\\begin{aligned} p_{t+1} &= \\frac{n_A(t+1)}{2N}\\\\ &= \\frac{\\text{Bin}(2N,p_t')}{2N} \\end{aligned}\\] With this model we can no longer predict exactly what the frequency of \\(A\\) will be in the next generation. However, we can compute the expectation \\[\\begin{aligned} \\mathbb{E}(p_{t+1}) &= \\mathbb{E}\\left(\\frac{\\text{Bin}(2N,p_t')}{2N}\\right)\\\\ &= \\frac{1}{2N}\\mathbb{E}\\left(\\text{Bin}(2N,p_t')\\right)\\\\ &= \\frac{2N p_t'}{2N}\\\\ &= p_t' \\end{aligned}\\] and the variance \\[\\begin{aligned} \\text{Var}(p_{t+1}) &= \\text{Var}\\left(\\frac{\\text{Bin}(2N,p_t')}{2N}\\right) \\\\ &= \\frac{1}{4N^2}\\text{Var}(\\text{Bin}(2N,p_t')) \\\\ &= \\frac{2Np_t'(1-p_t')}{4N^2}\\\\ &= \\frac{p_t'(1-p_t')}{2N} \\end{aligned}\\] The expectation is as before -- the population size is irrelevant. The variance, however, is largest when the population size is small and the expected frequency is near 0.5 (just as we saw for a general binomial random variable above). Note that variance goes to 0 as \\(N\\) goes to infinity. Therefore, in an infinitely large population, the frequency of \\(A\\) in the next generation is \\(p_{t+1} = p_t'\\) with no uncertainity. This recovers the one-locus haploid selection model from earlier in the course. In the next lab we'll learn how to simulate this stochastic model of evolution.","title":"Lecture 20"},{"location":"lectures/lecture-20/#lecture-20-probability-i-genetic-drift","text":"Run notes interactively?","title":"Lecture 20: Probability I (genetic drift)"},{"location":"lectures/lecture-20/#lecture-overview","text":"Discrete random variables Expectation of a discrete random variable Variance of a discrete random variable Independence Binomial random variable Genetic drift credits This lecture was created by PhD student Puneeth Deraje as part of a course development TA position -- thanks Puneeth! If you are following along with the text, this lecture does not follow along as closely. Until now we have been dealing with deterministic models, i.e., given the value of the variables at time \\(t\\) , we know exactly what the values will be at the next time. However, life is a little bit more random than that. Stochasticity (i.e., chance) is inherent in nature and can significantly alter the outcomes. In order to capture the effect of this stochasticity, we need some tools from probability theory .","title":"Lecture overview"},{"location":"lectures/lecture-20/#1-discrete-random-variables","text":"","title":"1. Discrete random variables"},{"location":"lectures/lecture-20/#example-1-a-fair-coin-toss","text":"Let's toss a toonie. If you get a heads, you use that toonie to buy a chocolate, otherwise you put the toonie in a piggy bank for grad school :P. Let \\(X\\) denote the number of chocolates you have after the coin toss. Then \\(X\\) can be equal to 0 or 1. In this case, tossing the coin is called an event and \\(X\\) is the random variable that records its outcome. The state space is the set of all outcomes, {0,1}. We call \\(X\\) a discrete random variable because there are a finite number of outcomes in the state space. We don't know the exact value of \\(X\\) until you actually toss the coin (i.e., until the event happens) -- we can not predict the outcome of the coin toss in that way. However, what we do know is that with a fair coin there is an equal chance of getting either a heads or tails. In other words, if we were to toss the coin a large number of times, say a million times, roughly half a million times we will see tails and the other half a million times we will see heads. Therefore, we say that \\(X\\) takes the value 0 with probability \\(\\frac{500000}{1000000}=0.5\\) and 1 with probability \\(\\frac{500000}{1000000}=0.5\\) . We can write this as \\[\\begin{aligned} \\Pr(X=0) &= 0.5\\\\ \\Pr(X=1) &= 0.5 \\end{aligned}\\] Note that the sum of the probabilities of a random variable taking a value, over all values in the state space, is 1. Here, \\(\\Pr(X=0) + \\Pr(X=1) = 1\\) . Note This way of thinking about the probability of a random variable \\(X\\) taking a value \\(x\\) , as the fraction (frequency) of events where \\(X=x\\) , is the frequentist interpretation of probability . In order for the frequency to match the probability you need to repeat the event a large number of times (ideally infinitely).","title":"Example 1: a fair coin toss"},{"location":"lectures/lecture-20/#example-2-a-biased-coin-toss","text":"Consider the same situation as above but with a biased toonie, i.e., one that shows heads in not half but in a fraction \\(p\\) of a very large number of events. Then \\[\\begin{aligned} \\Pr(X=0) &= 1-p \\\\ \\Pr(X=1) &= p \\\\ \\end{aligned}\\] This random variable \\(X\\) is called a Bernoulli random variable with parameter \\(p\\) and is denoted as \\(X\\sim\\mathrm{Ber}(p)\\) . Above, with a fair coin, \\(X\\sim\\mathrm{Ber}(1/2)\\) .","title":"Example 2: a biased coin toss"},{"location":"lectures/lecture-20/#example-3-a-fair-die-roll","text":"Now suppose you roll a fair 6-sided die to decide how many chocolates you will buy (this is nice because you never get 0!). Then, if \\(X\\) denotes the number of chocolates, we have \\[\\begin{aligned} \\Pr(X=1) &= \\frac{1}{6}\\\\ \\Pr(X=2) &= \\frac{1}{6} \\\\ \\Pr(X=3) &= \\frac{1}{6} \\\\ \\Pr(X=4) &= \\frac{1}{6} \\\\ \\Pr(X=5) &= \\frac{1}{6} \\\\ \\Pr(X=6) &= \\frac{1}{6} \\end{aligned}\\] The state space here is {1,2,3,4,5,6} and \\(\\sum_{x=1}^6 \\Pr(X=x) = 1\\) .","title":"Example 3: a fair die roll"},{"location":"lectures/lecture-20/#2-expectation-of-a-discrete-random-variable","text":"Given this way of thinking about stochasticity, one might be interested in questions like \"What is the value of \\(X\\) , on average?\" and \"How certain can I be about the outcome of \\(X\\) ?\". In order to answer these questions we need to define two important properties of a random variable, its expectation and variance. The expectation of a random variable is the average value of outcomes if the event is repeated infinitely many times.","title":"2. Expectation of a discrete random variable"},{"location":"lectures/lecture-20/#example-1-a-fair-coin-toss_1","text":"Suppose we toss the fair coin a million times. By the frequentist definition of probability we should see tails ( \\(X=0\\) ) half a million times and heads ( \\(X=1\\) ) the other half a million times. The expected value of \\(X\\) is then \\[\\begin{aligned} \\mathbb{E}(X) &= \\frac{\\overbrace{0+0+...+0}^{\\text{0.5 million times}}+\\overbrace{1+1+...+1}^{\\text{0.5 million times}}}{\\text{1 million}}\\\\ &= \\frac{ 0 \\times \\text{0.5 million} + 1 \\times \\text{0.5 million} }{\\text{1 million}}\\\\ &= 0 \\times \\frac{\\text{0.5 million}}{\\text{1 million}} + 1 \\times \\frac{\\text{0.5 million}}{\\text{1 million}}\\\\ &= 0 \\times (1/2) + 1 \\times (1/2) \\\\ &= 0.5 \\end{aligned}\\]","title":"Example 1: a fair coin toss"},{"location":"lectures/lecture-20/#example-2-a-biased-coin-toss_1","text":"Suppose we repeat the whole procedure for a biased coin, i.e., \\(X\\sim\\mathrm{Ber}(p)\\) . Then \\[\\begin{aligned} \\mathbb{E}(X) &= \\frac{\\overbrace{0+0+...+0}^{(1-p) \\text{ million times}}+\\overbrace{1+1+...+1}^{p \\text{ million times}}}{\\text{1 million}} \\\\ &= 0 \\times (1-p) + 1 \\times p \\\\ &= p \\end{aligned}\\]","title":"Example 2 : a biased coin toss"},{"location":"lectures/lecture-20/#example-3-a-fair-die-roll_1","text":"If we do the same for a fair 6-sided die, \\[\\begin{aligned} \\mathbb{E}(X) &= \\frac{\\overbrace{1+1+...+1}^{(1/6) \\text{ million times}} + \\overbrace{2+2+...+2}^{(1/6) \\text{ million times}} + \\overbrace{3+3+...+3}^{(1/6) \\text{ million times}} + \\overbrace{4+4+...+4}^{(1/6) \\text{ million times}} + \\overbrace{5+5+...+5}^{(1/6) \\text{ million times}} + \\overbrace{6+6+...+6}^{(1/6) \\text{ million times}}}{\\text{1 million}} \\\\ &= 1\\times\\frac{1}{6} + 2\\times\\frac{1}{6} + 3\\times\\frac{1}{6} + 4\\times\\frac{1}{6} + 5\\times\\frac{1}{6} + 6\\times\\frac{1}{6}\\\\ &= 3.5 \\end{aligned}\\] Expectation In general, the expectation of a discrete random variable is \\(\\mathbb{E}(X) = \\sum_x x \\Pr(X=x)\\) , where the sum is over the entire state space.","title":"Example 3: a fair die roll"},{"location":"lectures/lecture-20/#properties-of-expectation","text":"If \\(c\\) is a constant then \\(\\mathbb{E}(c)=c\\) and \\(\\mathbb{E}(cX)=c\\mathbb{E}(X)\\) If \\(X\\) and \\(Y\\) are two random variables then \\(\\mathbb{E}(X+Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\\) Suppose \\(X\\) is a random variable and \\(f\\) is any function then \\(\\mathbb{E}(f(X)) = \\sum_x f(x)P(X=x)\\) . For example, let \\(f(x) = x^2\\) and \\(X\\sim\\mathrm{Ber}(p)\\) (e.g., a biased coin toss), then \\(\\mathbb{E}(f(X)) = \\sum_{x=0}^{1}x^2P(X=x)= 0^2(1-p)+1^2p = p\\) .","title":"Properties of expectation"},{"location":"lectures/lecture-20/#3-variance-of-a-discrete-random-variable","text":"Suppose we have an extremely biased coin with \\(p=1\\) . That is, the coin always shows heads. Can we, before tossing, say something about the outcome in this case? Yes, we know exactly what the outcome will be, heads. Similarly, if \\(p=0\\) we would know the exact outcome, tails. We say there is no uncertainity in either of these cases. Now, consider a slightly less biased coin with \\(p = 0.99\\) . Although we can not say exactly what the outcome will be, it will very likely be heads. There is some small amount of uncertainity. Finally, suppose we toss a fair coin (i.e., \\(p = 0.5\\) ). Then we have no idea at all what the outcome will be. There is very high uncertainity in this case. Variance The variance of a random variable \\(X\\) , denoted \\(\\mathrm{Var}(X)\\) , quantifies the uncertainity associated with the random variable and is given by \\[\\begin{aligned} \\mathrm{Var}(X) &= \\mathbb{E}((X - \\mathbb{E}(X))^2 )\\\\ &= \\mathbb{E}(X^2 - 2X\\mathbb{E}(X) + \\mathbb{E}(X)^2)\\\\ &= \\mathbb{E}(X^2) - \\mathbb{E}(2X\\mathbb{E}(X)) + \\mathbb{E}(\\mathbb{E}(X)^2)\\\\ &= \\mathbb{E}(X^2) - 2\\mathbb{E}(X)\\mathbb{E}(X) + \\mathbb{E}(X)^2\\\\ &= \\mathbb{E}(X^2) - \\mathbb{E}(X)^2 \\end{aligned}\\]","title":"3. Variance of a discrete random variable"},{"location":"lectures/lecture-20/#example-a-biased-coin-toss","text":"If \\(X\\sim\\mathrm{Ber}(p)\\) then \\[\\begin{aligned} \\mathrm{Var}(X) &= \\mathbb{E}(X^2) - \\mathbb{E}(X)^2 \\\\ &= p - p^2 \\\\ &= p(1-p) \\end{aligned}\\] Below we plot the variance of \\(X\\) as a function of \\(p\\) . import matplotlib.pyplot as plt import numpy as np p_vals = np.arange(0,1,0.01) Var_values = [ p*(1-p) for p in p_vals ] plt.plot(p_vals, Var_values) plt.axvline(x=0.5,linestyle='dashed') plt.xlabel('$p$ = probability of getting a heads') plt.ylabel('Var($X$)') plt.show() Note that for \\(p=0\\) or \\(p=1\\) we have Var( \\(X\\) ) = 0, i.e., there is no uncertainity. Further note that Var( \\(X\\) ) = \\(p(1-p)\\) peaks at \\(p=0.5\\) , i.e., the highest uncertainity is associated with the toss of a fair coin.","title":"Example: a biased coin toss"},{"location":"lectures/lecture-20/#properties-of-variance","text":"If \\(c\\) is a constant then Var \\((c)\\) = 0 If \\(a\\) and \\(b\\) are constants and \\(X\\) is a random variable then Var \\((aX + b)\\) = \\(a^2\\) Var \\((X)\\)","title":"Properties of variance"},{"location":"lectures/lecture-20/#4-independence","text":"Let's now look at two random variables simultaneously. Let's play the same game as before. Toss a coin, if it's heads use the coin to buy a chocolate. If it's tails put the coin in the piggy bank. As before, let \\(X\\) be the number of chocolates after the toss, \\(X\\sim\\) Ber( \\(p\\) ). This time we'll also keep track of how many coins we put in the piggy bank, \\(Y\\) . Now if \\(X=0\\) then \\(Y=1\\) and if \\(X=1\\) then \\(Y=0\\) . The value of \\(Y\\) depends on \\(X\\) , and vice versa, so \\(X\\) and \\(Y\\) are not independent random variables . On the other hand, suppose you have two coins. You toss each coin and decide whether to use it to buy a chocolate. Let \\(X_1\\) be the outcome of tossing the first coin. Let \\(X_2\\) be the outcome of tossing the second coin. Then \\(X_1\\) does not depend on \\(X_2\\) , and vice-versa, so \\(X_1\\) and \\(X_2\\) are independent random variables. Independence Two random variables \\(X\\) and \\(Y\\) are said to be independent if for every \\(x\\) and \\(y\\) \\[\\Pr(X=x \\text{ and } Y=y) = \\Pr(X=x)\\Pr(Y=y)\\]","title":"4. Independence"},{"location":"lectures/lecture-20/#properties-of-independent-random-variables","text":"If \\(X\\) and \\(Y\\) are independent random variables then \\(\\mathbb{E}(XY)=\\mathbb{E}(X)\\mathbb{E}(Y)\\) Var( \\(X+Y\\) ) = Var( \\(X\\) ) + Var( \\(Y\\) )","title":"Properties of independent random variables"},{"location":"lectures/lecture-20/#5-binomial-random-variable","text":"Let \\(X_1\\) and \\(X_2\\) be the outcome of two independent tosses of the same coin. We want know how many chocolates we have after two tosses, \\(X = X_1 + X_2\\) . Recall that \\(X_1\\) and \\(X_2\\) are both Ber( \\(p\\) ). Therefore, \\[\\begin{aligned} \\mathbb{E}(X) &= \\mathbb{E}(X_1) + \\mathbb{E}(X_2)\\\\ &= p + p \\\\ &= 2p \\end{aligned}\\] Since \\(X_1\\) and \\(X_2\\) are independent we also have \\[\\begin{aligned} \\mathrm{Var}(X) &= \\mathrm{Var}(X_1) +\\mathrm{Var}(X_2)\\\\ &= p(1-p) + p(1-p)\\\\ &= 2p(1-p) \\end{aligned}\\] Now let's look at three special cases to see what this means. If \\(p=0\\) then \\(\\mathbb{E}(X) = 0\\) and \\(\\mathrm{Var}(X)=0\\) . If the coin always shows tails then we know with certainity that we will have no chocolates after two tosses. Similarly, if \\(p=1\\) then \\(\\mathbb{E}(X) = 2\\) and \\(\\mathrm{Var}(X)=0\\) . If the coin always shows heads then we know with certainity that we will have two chocolates after two tosses. Finally, if \\(p=0.5\\) then \\(\\mathbb{E}(X) = 1\\) and \\(\\mathrm{Var}(X)=1\\) . With a fair coin we expect to have 1 chocolate but there is some uncertainity since we could end up with 0, 1, or 2. Binomial random variable In general we could have \\(n\\) coins. Let \\(X_i\\) denote the outcome of the \\(i^{\\mathrm{th}}\\) toss, which is Ber( \\(p\\) ) and independent of all other tosses. Then the sum over all \\(n\\) Bernoulli random variables, \\(X = X_1 + X_2 + ... + X_n\\) , is called a binomial random variable with parameters \\(n\\) and \\(p\\) , and is denoted by Bin( \\(n,p\\) ). The expectation of a binomial random variable is \\[\\mathbb{E}(X) = np\\] and its variance is \\[\\mathrm{Var}(X) = np(1-p)\\] Moreover, we can compute the probability of a binomial random variable \\(X\\sim\\) Bin( \\(n,p\\) ) taking value \\(k\\) , as \\[P(X = k) = {n \\choose k} p^k (1-p)^{n-k}\\] where \\({n \\choose k}\\) is read \" \\(n\\) choose \\(k\\) \" and is the number of ways of choosing \\(k\\) items from \\(n\\) options. Mathematically, \\[\\begin{aligned} {n \\choose k} &= \\frac{n}{k} {n-1 \\choose k-1}\\\\ &= \\frac{n}{k} \\frac{n-1}{k-1} {n-2 \\choose k-2}\\\\ &= \\vdots\\\\ &= \\frac{n (n-1) (n-2)\\cdots (n-k+1)}{k(k-1)(k-2)\\cdots 1} \\end{aligned}\\]","title":"5. Binomial random variable"},{"location":"lectures/lecture-20/#6-genetic-drift","text":"Recall our model of one-locus haploid selection from Lecture 4 . If \\(p_t\\) is the frequency of allele \\(A\\) in the population at time \\(t\\) then we said that the frequency of \\(A\\) in the next generation is \\[ p_{t+1} = \\frac{p_t W_A }{p_t W_A + (1-p_t)W_a} \\] where \\(W_A\\) is the fitness of \\(A\\) and \\(W_a\\) is the fitness of \\(a\\) . One unspoken assumption here is that the population size is infinitely large (or at least sufficiently big). In reality, populations are not infinitely large and may even be quite small. This finiteness introduces stochasticity, which in evolution we call genetic drift . One well-known model that incorporates genetic drift is the Wright-Fisher model . This model assumes that there are \\(N\\) diploid individuals in generation \\(t\\) , each of which produce an infinite number of haploid gametes, of which \\(2N\\) are randomly sampled to form the diploid individuals of generation \\(t+1\\) . Here, with haploid selection, we assume the frequency of gametes changes due to selection before sampling for the next generation. graph TD A[N diploids at t] --> B[Infinite haploids before selection]; B --> C[Infinite haploids after selection]; C --> D[N diploids at t+1]; So, let \\(p_t\\) be the frequency of allele \\(A\\) in generation \\(t\\) . Each diploid individual contributes an infinite number of gametes, so the frequency of \\(A\\) in the gamete pool is also \\(p_t\\) . Now selection acts on the gamete pool, altering the frequencies as in our original model, so that the frequency of \\(A\\) in the gamete pool is now \\(p_t' = \\frac{p_tW_A }{p_t W_A + (1-p_t)W_a}\\) . We then sample \\(2N\\) gametes to form the next diploid generation. For each gamete we pick there is a probability \\(p'_t\\) that it has allele \\(A\\) and a probability \\(1-p'_t\\) that it has allele \\(a\\) . Let \\(X\\) be the number of \\(A\\) alleles in a randomly chosen gamete. Then \\(X\\sim\\) Ber \\((p'_t)\\) and the total number of \\(A\\) alleles after \\(2N\\) samples, \\(n_A(t+1)\\) , is the sum of \\(2N\\) Bernoulli random variables, implying \\(n_A(t+1)\\sim\\) Bin \\((2N,p'_t)\\) . The frequency of allele \\(A\\) in the next generation is then \\[\\begin{aligned} p_{t+1} &= \\frac{n_A(t+1)}{2N}\\\\ &= \\frac{\\text{Bin}(2N,p_t')}{2N} \\end{aligned}\\] With this model we can no longer predict exactly what the frequency of \\(A\\) will be in the next generation. However, we can compute the expectation \\[\\begin{aligned} \\mathbb{E}(p_{t+1}) &= \\mathbb{E}\\left(\\frac{\\text{Bin}(2N,p_t')}{2N}\\right)\\\\ &= \\frac{1}{2N}\\mathbb{E}\\left(\\text{Bin}(2N,p_t')\\right)\\\\ &= \\frac{2N p_t'}{2N}\\\\ &= p_t' \\end{aligned}\\] and the variance \\[\\begin{aligned} \\text{Var}(p_{t+1}) &= \\text{Var}\\left(\\frac{\\text{Bin}(2N,p_t')}{2N}\\right) \\\\ &= \\frac{1}{4N^2}\\text{Var}(\\text{Bin}(2N,p_t')) \\\\ &= \\frac{2Np_t'(1-p_t')}{4N^2}\\\\ &= \\frac{p_t'(1-p_t')}{2N} \\end{aligned}\\] The expectation is as before -- the population size is irrelevant. The variance, however, is largest when the population size is small and the expected frequency is near 0.5 (just as we saw for a general binomial random variable above). Note that variance goes to 0 as \\(N\\) goes to infinity. Therefore, in an infinitely large population, the frequency of \\(A\\) in the next generation is \\(p_{t+1} = p_t'\\) with no uncertainity. This recovers the one-locus haploid selection model from earlier in the course. In the next lab we'll learn how to simulate this stochastic model of evolution.","title":"6. Genetic drift"},{"location":"lectures/lecture-21/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 21: Probability II (demographic stochasticity) Run notes interactively? Lecture overview Poisson random variable Demographic stochasticity Extinction Establishment credits This lecture was created by PhD student Puneeth Deraje as part of a course development TA position -- thanks Puneeth! If you are following along with the text, this lecture does not follow along as closely. In Lecture 18 we learned how to model stochasticity in population genetics (genetic drift). In this lecture we'll learn how to model stochasticity in population dynamics ( demographic stochasticity ). To do so we'll work with the simplest model of population dynamics possible, exponential growth (see Lecture 3 ). In discrete time this is \\[n_{t+1} = R n_t\\] where \\(n_t\\) is the number of individuals at time \\(t\\) and \\(R\\) is the reproductive factor. In this deterministic model every individual had a reproductive factor of exactly \\(R\\) at every time step. In reality there will be stochasticity in \\(R\\) across individuals and time. We can account for that by replacing \\(R\\) with a random variable. 1. Poisson random variable Recall the binomial random variable from the previous lecture, \\(X \\sim \\mathrm{Bin}(n,p)\\) . The probability this random variable takes on value \\(k\\) is then \\[\\Pr(X = k) = {n \\choose k} p^k (1-p)^{n-k}\\] We could model the reproductive factor as a binomial random variable. For example, perhaps each individual at time \\(t\\) produces \\(n\\) offspring before dying, and each offspring survives to become an adult with probability \\(p\\) . Using this model requires estimates of two parameters, \\(n\\) and \\(p\\) . But there is a simpler way. Let the mean number of offspring produced be \\(\\lambda=np\\) . Rearranging this in terms of \\(p\\) , we can write the binomial as \\(X\\sim \\mathrm{Bin}(n,\\lambda/n)\\) . Note that the mean is always \\(\\lambda\\) , regardless of \\(n\\) . Now imagine that individuals in the population we are modelling tend to have a very large number of offspring, very few of which survive. We can approximate this in the extreme by taking \\(n\\rightarrow \\infty\\) . Let \\(Y\\) be this random variable, \\(Y = \\lim_{n \\rightarrow \\infty} \\mathrm{Bin}(n, \\lambda/n))\\) . The distribution of \\(Y\\) is then \\[ \\begin{aligned} \\Pr(Y=k) &= \\lim_{n \\rightarrow \\infty} {n \\choose k} \\left(\\frac{\\lambda}{n}\\right)^k \\left(1-\\frac{\\lambda}{n}\\right)^{n-k} \\\\ &= \\lim_{n \\rightarrow \\infty} \\frac{n(n-1)(n-2)...(n-k+1)}{k!} \\frac{\\lambda^k}{n^k} \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &= \\frac{\\lambda^k}{k!}\\lim_{n \\rightarrow \\infty} \\frac{n(n-1)(n-2)...(n-k+1)}{n^k} \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &= \\frac{\\lambda^k}{k!}\\lim_{n \\rightarrow \\infty} 1\\left(1-\\frac{1}{n}\\right)\\left(1-\\frac{2}{n}\\right)...\\left(1-\\frac{k-1}{n}\\right) \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &= \\frac{\\lambda^k}{k!}\\lim_{n \\rightarrow \\infty} \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &= \\frac{\\lambda^k e^{-\\lambda}}{k!} \\end{aligned} \\] We call \\(Y\\) a Poisson random variable with mean \\(\\lambda\\) and denoted it by \\(Y\\sim\\mathrm{Poi}(\\lambda)\\) . We can now model the reproductive factor as a random variable with only one parameter, \\(\\lambda\\) . Note that the simpler Poisson distribution is a good approximation of the binomial distribution even for fairly moderate values of \\(n\\) , as seen in the plot below. import sympy import numpy as np import matplotlib.pyplot as plt def binomial(n,p,k): return sympy.binomial(n,k) * p**k * (1-p)**(n-k) def poisson(lam,k): return lam**k * np.exp(-lam)/sympy.factorial(k) n = 100 p = 0.1 lam = n*p ks = range(n+1) fig, ax = plt.subplots() ax.plot(ks, [binomial(n,p,k) for k in ks], label='binomial') ax.plot(ks, [poisson(lam,k) for k in ks], label='Poisson') ax.set_xlabel('number of successes') ax.set_ylabel('probability') ax.legend() plt.show() Two key properties of a Poisson random variable are 1) the variance is equal to the mean \\[ \\begin{aligned} \\mathrm{Var}(Y) &= \\lim_{n \\rightarrow \\infty} \\mathrm{Var}\\left(\\mathrm{Bin}\\left(n,\\frac{\\lambda}{n}\\right)\\right) \\\\ &= \\lim_{n \\rightarrow \\infty} n \\frac{\\lambda}{n} \\left(1-\\frac{\\lambda}{n}\\right) \\\\ &= \\lambda \\lim_{n \\rightarrow \\infty} \\left(1-\\frac{\\lambda}{n}\\right) \\\\ &= \\lambda \\end{aligned} \\] 2) if \\(Y_1\\) and \\(Y_2\\) are two independent Poisson random variables with means \\(\\lambda_1\\) and \\(\\lambda_2\\) , then their sum is distributed as a Poisson with mean \\(\\lambda_1+\\lambda_2\\) \\[Y_1 + Y_2 \\sim \\mathrm{Poi}(\\lambda_1 + \\lambda_2)\\] 2. Demographic stochasticity We can now model demographic stochasticity with the Poisson distribution. Assume each individual in the population produces a Poisson number of surviving offspring with mean \\(\\lambda\\) , independent of all other individuals in the populations, and then dies. Write the number of offspring for individual \\(i\\) as \\(X_i\\sim\\mathrm{Poi}(\\lambda)\\) . Let the current number of individuals be \\(n_t\\) . Then the population size in the next generation, \\(n_{t+1}\\) , is distributed like the sum of \\(n_t\\) independent and identical Poisson's \\[\\begin{aligned} n_{t+1} &\\sim \\sum_{i=1}^{n_t}\\mathrm{Poi}(\\lambda)\\\\ &= \\mathrm{Poi}\\left(\\sum_{i=1}^{n_t} \\lambda\\right) \\\\ &= \\mathrm{Poi}(\\lambda n_t) \\\\ \\end{aligned}\\] This implies that the expected population size in the next generation is \\(\\lambda n_t\\) , as is the variance. In Lab 11 we'll simulate this to get a better sense of the resulting dynamics. 3. Extinction Let us now look at one property of this model -- the probability of extinction. Consider a given individual at time \\(t\\) . Let \\(\\eta\\) be the probability this individual does not have descendants in the long-term, i.e., that its lineage goes extinct. To find \\(\\eta\\) we note that the probability that this lineage goes extinct, \\(\\eta\\) , is the probability that this individual has \\(k\\) surviving offspring (for all values of \\(k\\) ) and all of those offspring lineages go extinct (with probability \\(\\eta^k\\) ). Given the probability of having \\(k\\) surviving offspring is \\(\\lambda^k e^{-\\lambda}/k!\\) , this implies \\[ \\begin{aligned} \\eta &= \\sum_{k=0}^{\\infty} \\frac{e^{-\\lambda}\\lambda^k}{k!} \\eta^k \\\\ &= e^{-\\lambda} \\sum_{k=0}^{\\infty} \\frac{(\\eta \\lambda)^k}{k!} \\\\ &= e^{-\\lambda}e^{\\lambda \\eta} \\\\ &= e^{-\\lambda (1-\\eta)} \\end{aligned} \\] One solution is \\(\\eta=1\\) , certain extinction. But there can be a second biologically valid solution (between 0 and 1), meaning that extinction is not certain. This occurs when the mean number of surviving offspring is greater than one, \\(\\lambda>1\\) , as shown in plot below (the solutions are where the curve intersects the 1:1 line). xs = np.linspace(0,1,100) fig,ax=plt.subplots() lam = 2 ax.plot(xs, xs) ax.plot(xs, [np.exp(-lam*(1-x)) for x in xs]) ax.set_xlabel(r'$\\eta$') ax.set_ylabel(r'$e^{-\\lambda (1-\\eta)}$') plt.show() Above we calculated the probability a single lineage goes extinct, \\(\\eta\\) . From this, the probability that the entire population goes extinct is the probability that all \\(n_t\\) lineages go extinct, \\(\\eta^{n_t}\\) . The two major conclusions from this are 1) when the mean number of surviving offspring per parent is \\(\\lambda=1\\) the deterministic model predicts a constant population size but the stochastic model says that extinction is certain. 2) when the mean number of surviving offspring per parent is \\(\\lambda>1\\) the deterministic model predicts exponential growth but the stochastic model says there is still some non-zero probability of extinction. 4. Establishment Before moving on, the above result about extinction is mathematically identical to a classic result in population genetics concerning the establishment of a beneficial allele. Consider a population at equilibrium such that the mean number of offspring per parent is 1. And now consider a beneficial allele that tends to have more offspring, \\(\\lambda>1\\) . Then we know from above that there is some non-zero probability this lineage does not go extinct, \\(\\eta<1\\) . Writing the above equation about extinction in terms of the establishment probability , \\(p=1-\\eta\\) , we have \\[\\begin{aligned} \\eta &= e^{-\\lambda (1-\\eta)}\\\\ 1 - p &= e^{-\\lambda p}\\\\ p &= 1 - e^{-\\lambda p} \\end{aligned}\\] Now assume that the beneficial allele increases the number of offspring only slightly, so that \\(\\lambda=1+s\\) with \\(s\\) small. We can then solve for \\(p\\) explicitly using a Taylor series expansion of \\(e^{-\\lambda p}=e^{-(1+s) p}\\) around \\(s=0\\) \\[ \\begin{aligned} p &= 1 - e^{-(1+s)p} \\\\ p &\\approx 1 - \\left(1 - (1+s)p + \\frac{(1+s)^2p^2}{2}\\right) \\\\ p &\\approx (1+s)p - \\frac{(1+s)^2p^2}{2} \\\\ 1 &\\approx 1 + s - \\frac{(1+s)^2p}{2} \\\\ 0 &\\approx s - \\frac{(1+s)^2p}{2} \\\\ p &\\approx \\frac{2s}{(1+s)^2}\\\\ p &\\approx 2s \\end{aligned} \\] This says that the probability a weakly beneficial allele establishes (i.e., is not lost by genetic drift) is roughly twice its selective advantage.","title":"Lecture 21"},{"location":"lectures/lecture-21/#lecture-21-probability-ii-demographic-stochasticity","text":"Run notes interactively?","title":"Lecture 21: Probability II (demographic stochasticity)"},{"location":"lectures/lecture-21/#lecture-overview","text":"Poisson random variable Demographic stochasticity Extinction Establishment credits This lecture was created by PhD student Puneeth Deraje as part of a course development TA position -- thanks Puneeth! If you are following along with the text, this lecture does not follow along as closely. In Lecture 18 we learned how to model stochasticity in population genetics (genetic drift). In this lecture we'll learn how to model stochasticity in population dynamics ( demographic stochasticity ). To do so we'll work with the simplest model of population dynamics possible, exponential growth (see Lecture 3 ). In discrete time this is \\[n_{t+1} = R n_t\\] where \\(n_t\\) is the number of individuals at time \\(t\\) and \\(R\\) is the reproductive factor. In this deterministic model every individual had a reproductive factor of exactly \\(R\\) at every time step. In reality there will be stochasticity in \\(R\\) across individuals and time. We can account for that by replacing \\(R\\) with a random variable.","title":"Lecture overview"},{"location":"lectures/lecture-21/#1-poisson-random-variable","text":"Recall the binomial random variable from the previous lecture, \\(X \\sim \\mathrm{Bin}(n,p)\\) . The probability this random variable takes on value \\(k\\) is then \\[\\Pr(X = k) = {n \\choose k} p^k (1-p)^{n-k}\\] We could model the reproductive factor as a binomial random variable. For example, perhaps each individual at time \\(t\\) produces \\(n\\) offspring before dying, and each offspring survives to become an adult with probability \\(p\\) . Using this model requires estimates of two parameters, \\(n\\) and \\(p\\) . But there is a simpler way. Let the mean number of offspring produced be \\(\\lambda=np\\) . Rearranging this in terms of \\(p\\) , we can write the binomial as \\(X\\sim \\mathrm{Bin}(n,\\lambda/n)\\) . Note that the mean is always \\(\\lambda\\) , regardless of \\(n\\) . Now imagine that individuals in the population we are modelling tend to have a very large number of offspring, very few of which survive. We can approximate this in the extreme by taking \\(n\\rightarrow \\infty\\) . Let \\(Y\\) be this random variable, \\(Y = \\lim_{n \\rightarrow \\infty} \\mathrm{Bin}(n, \\lambda/n))\\) . The distribution of \\(Y\\) is then \\[ \\begin{aligned} \\Pr(Y=k) &= \\lim_{n \\rightarrow \\infty} {n \\choose k} \\left(\\frac{\\lambda}{n}\\right)^k \\left(1-\\frac{\\lambda}{n}\\right)^{n-k} \\\\ &= \\lim_{n \\rightarrow \\infty} \\frac{n(n-1)(n-2)...(n-k+1)}{k!} \\frac{\\lambda^k}{n^k} \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &= \\frac{\\lambda^k}{k!}\\lim_{n \\rightarrow \\infty} \\frac{n(n-1)(n-2)...(n-k+1)}{n^k} \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &= \\frac{\\lambda^k}{k!}\\lim_{n \\rightarrow \\infty} 1\\left(1-\\frac{1}{n}\\right)\\left(1-\\frac{2}{n}\\right)...\\left(1-\\frac{k-1}{n}\\right) \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &= \\frac{\\lambda^k}{k!}\\lim_{n \\rightarrow \\infty} \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\ &= \\frac{\\lambda^k e^{-\\lambda}}{k!} \\end{aligned} \\] We call \\(Y\\) a Poisson random variable with mean \\(\\lambda\\) and denoted it by \\(Y\\sim\\mathrm{Poi}(\\lambda)\\) . We can now model the reproductive factor as a random variable with only one parameter, \\(\\lambda\\) . Note that the simpler Poisson distribution is a good approximation of the binomial distribution even for fairly moderate values of \\(n\\) , as seen in the plot below. import sympy import numpy as np import matplotlib.pyplot as plt def binomial(n,p,k): return sympy.binomial(n,k) * p**k * (1-p)**(n-k) def poisson(lam,k): return lam**k * np.exp(-lam)/sympy.factorial(k) n = 100 p = 0.1 lam = n*p ks = range(n+1) fig, ax = plt.subplots() ax.plot(ks, [binomial(n,p,k) for k in ks], label='binomial') ax.plot(ks, [poisson(lam,k) for k in ks], label='Poisson') ax.set_xlabel('number of successes') ax.set_ylabel('probability') ax.legend() plt.show() Two key properties of a Poisson random variable are 1) the variance is equal to the mean \\[ \\begin{aligned} \\mathrm{Var}(Y) &= \\lim_{n \\rightarrow \\infty} \\mathrm{Var}\\left(\\mathrm{Bin}\\left(n,\\frac{\\lambda}{n}\\right)\\right) \\\\ &= \\lim_{n \\rightarrow \\infty} n \\frac{\\lambda}{n} \\left(1-\\frac{\\lambda}{n}\\right) \\\\ &= \\lambda \\lim_{n \\rightarrow \\infty} \\left(1-\\frac{\\lambda}{n}\\right) \\\\ &= \\lambda \\end{aligned} \\] 2) if \\(Y_1\\) and \\(Y_2\\) are two independent Poisson random variables with means \\(\\lambda_1\\) and \\(\\lambda_2\\) , then their sum is distributed as a Poisson with mean \\(\\lambda_1+\\lambda_2\\) \\[Y_1 + Y_2 \\sim \\mathrm{Poi}(\\lambda_1 + \\lambda_2)\\]","title":"1. Poisson random variable"},{"location":"lectures/lecture-21/#2-demographic-stochasticity","text":"We can now model demographic stochasticity with the Poisson distribution. Assume each individual in the population produces a Poisson number of surviving offspring with mean \\(\\lambda\\) , independent of all other individuals in the populations, and then dies. Write the number of offspring for individual \\(i\\) as \\(X_i\\sim\\mathrm{Poi}(\\lambda)\\) . Let the current number of individuals be \\(n_t\\) . Then the population size in the next generation, \\(n_{t+1}\\) , is distributed like the sum of \\(n_t\\) independent and identical Poisson's \\[\\begin{aligned} n_{t+1} &\\sim \\sum_{i=1}^{n_t}\\mathrm{Poi}(\\lambda)\\\\ &= \\mathrm{Poi}\\left(\\sum_{i=1}^{n_t} \\lambda\\right) \\\\ &= \\mathrm{Poi}(\\lambda n_t) \\\\ \\end{aligned}\\] This implies that the expected population size in the next generation is \\(\\lambda n_t\\) , as is the variance. In Lab 11 we'll simulate this to get a better sense of the resulting dynamics.","title":"2. Demographic stochasticity"},{"location":"lectures/lecture-21/#3-extinction","text":"Let us now look at one property of this model -- the probability of extinction. Consider a given individual at time \\(t\\) . Let \\(\\eta\\) be the probability this individual does not have descendants in the long-term, i.e., that its lineage goes extinct. To find \\(\\eta\\) we note that the probability that this lineage goes extinct, \\(\\eta\\) , is the probability that this individual has \\(k\\) surviving offspring (for all values of \\(k\\) ) and all of those offspring lineages go extinct (with probability \\(\\eta^k\\) ). Given the probability of having \\(k\\) surviving offspring is \\(\\lambda^k e^{-\\lambda}/k!\\) , this implies \\[ \\begin{aligned} \\eta &= \\sum_{k=0}^{\\infty} \\frac{e^{-\\lambda}\\lambda^k}{k!} \\eta^k \\\\ &= e^{-\\lambda} \\sum_{k=0}^{\\infty} \\frac{(\\eta \\lambda)^k}{k!} \\\\ &= e^{-\\lambda}e^{\\lambda \\eta} \\\\ &= e^{-\\lambda (1-\\eta)} \\end{aligned} \\] One solution is \\(\\eta=1\\) , certain extinction. But there can be a second biologically valid solution (between 0 and 1), meaning that extinction is not certain. This occurs when the mean number of surviving offspring is greater than one, \\(\\lambda>1\\) , as shown in plot below (the solutions are where the curve intersects the 1:1 line). xs = np.linspace(0,1,100) fig,ax=plt.subplots() lam = 2 ax.plot(xs, xs) ax.plot(xs, [np.exp(-lam*(1-x)) for x in xs]) ax.set_xlabel(r'$\\eta$') ax.set_ylabel(r'$e^{-\\lambda (1-\\eta)}$') plt.show() Above we calculated the probability a single lineage goes extinct, \\(\\eta\\) . From this, the probability that the entire population goes extinct is the probability that all \\(n_t\\) lineages go extinct, \\(\\eta^{n_t}\\) . The two major conclusions from this are 1) when the mean number of surviving offspring per parent is \\(\\lambda=1\\) the deterministic model predicts a constant population size but the stochastic model says that extinction is certain. 2) when the mean number of surviving offspring per parent is \\(\\lambda>1\\) the deterministic model predicts exponential growth but the stochastic model says there is still some non-zero probability of extinction.","title":"3. Extinction"},{"location":"lectures/lecture-21/#4-establishment","text":"Before moving on, the above result about extinction is mathematically identical to a classic result in population genetics concerning the establishment of a beneficial allele. Consider a population at equilibrium such that the mean number of offspring per parent is 1. And now consider a beneficial allele that tends to have more offspring, \\(\\lambda>1\\) . Then we know from above that there is some non-zero probability this lineage does not go extinct, \\(\\eta<1\\) . Writing the above equation about extinction in terms of the establishment probability , \\(p=1-\\eta\\) , we have \\[\\begin{aligned} \\eta &= e^{-\\lambda (1-\\eta)}\\\\ 1 - p &= e^{-\\lambda p}\\\\ p &= 1 - e^{-\\lambda p} \\end{aligned}\\] Now assume that the beneficial allele increases the number of offspring only slightly, so that \\(\\lambda=1+s\\) with \\(s\\) small. We can then solve for \\(p\\) explicitly using a Taylor series expansion of \\(e^{-\\lambda p}=e^{-(1+s) p}\\) around \\(s=0\\) \\[ \\begin{aligned} p &= 1 - e^{-(1+s)p} \\\\ p &\\approx 1 - \\left(1 - (1+s)p + \\frac{(1+s)^2p^2}{2}\\right) \\\\ p &\\approx (1+s)p - \\frac{(1+s)^2p^2}{2} \\\\ 1 &\\approx 1 + s - \\frac{(1+s)^2p}{2} \\\\ 0 &\\approx s - \\frac{(1+s)^2p}{2} \\\\ p &\\approx \\frac{2s}{(1+s)^2}\\\\ p &\\approx 2s \\end{aligned} \\] This says that the probability a weakly beneficial allele establishes (i.e., is not lost by genetic drift) is roughly twice its selective advantage.","title":"4. Establishment"},{"location":"lectures/lecture-22/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"mmosmond/executable-cells\", ref: \"main\", }, } Lecture 22: Probability III (the coalescent) Run notes interactively? Lecture overview The coalescent All the models we have studied so far have looked forward in time, into the future. But this doesn't have to always be the case, we can also look back in time, modeling the past. Here we look at one particularly powerful example from population genetics. 1. The coalescent Let's consider a population composed of \\(N\\) diploid individuals, i.e., with \\(2N\\) alleles at a given locus. We want to model the history of these alleles -- from which alleles in the previous generations do they descend? One of the simplest ways to model this is to treat all of the alleles as equivalent (e.g., no fitness differences) so that a given allele in the current generation picks its \"parent\" allele at random from amongst the \\(2N\\) alleles in the previous generation. We can simulate this and plot the result, arranging the \\(2N\\) alleles horizontally, stacking previous generations on top, and drawing lines to connect \"children\" and \"parent\" alleles. import numpy as np import matplotlib.pyplot as plt def simulate(N,tmax): p = np.arange(2*N) #initial configuration t = 0 ps = np.empty((tmax,2*N), dtype='int') while t < tmax: ps[t] = p p = np.random.randint(0,2*N,2*N) #parents of current generation t += 1 return ps def plot_lineages(ps,ax,alpha): # plot lineages for t,p in enumerate(ps[1:]): #loop over generations ax.plot([ps[0],ps[t+1]], [t,t+1], marker='o', color='k', alpha=alpha) #connect children with parents ax.scatter(ps[0], [t+1 for _ in ps[0]], marker='o', color='k', alpha=alpha) #plot all alleles of last gen # simplify presentation # ax.axis('off') ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.spines['bottom'].set_visible(False) ax.spines['left'].set_visible(False) ax.xaxis.set_ticks([]) ax.yaxis.set_ticks(range(len(ps))) ax.set_xlabel('alleles') ax.set_ylabel('generations ago') fig, ax = plt.subplots() ps = simulate(N=5,tmax=4) plot_lineages(ps,ax,alpha=0.5) plt.show() One of the most important aspects of this model is that we can choose to think about the history of just some subset of the alleles in the current generation. This is helpful because 1) we do not need to model the entire population and 2) this has a close connection to data (since we almost never sample every individual in a population). To see this visually, we can choose a few \"sample\" alleles from the current generation and highlight their \"lineages\". fig, ax = plt.subplots() plot_lineages(ps,ax,alpha=0.1) #plot full population in background # plot sample lineages for i in [0,5,9]: #samples path = [i] #lineage of sample i for p in ps[1:]: #loop over generations path.append(p[i]) #add parent i = p[i] #make parent the child ax.plot(path,range(len(path)), marker='o') plt.show() Time for two lineages to coalesce When two sample lineages meet at a most recent common ancestor we say they coalesce (\"come together\"). This is where the name of the model comes from, the coalescent . (Note that some reserve that name for the continuous-time limit of this model, but we won't be so strict.) The time it takes for two lineages to coalesce determines how similar those alleles are. The more distant their most recent common ancestor the more mutations that are expected to have accumulated. Thus the time to coalescence influences the amount of genetic diversity we expect to see. The first question we will ask with this model is: how many generations does it take for two lineages to coalesce? Let's call this random variable \\(T_2\\) . We want to know how this random variable is distributed. Consider one generation at a time. We choose the parent of one of the alleles at random. We do the same for the second allele. The probability that the two lineages coalesce in the previous generation is the probability that the parent of the second allele is the same as the parent of the first allele, \\(p_2=1/(2N)\\) . Let \\(X=1\\) if the two lineages coalesce in the previous generation, \\(X=0\\) if they don't coalesce. Then \\(X\\) is a Bernoulii random variable, \\(X\\sim\\text{Ber}(p_2)\\) . We can now rephrase our question mathematically: how many Bernoulli trials do we have to perform to get one success? Geometric random variable Let \\(X\\) be the number of Bernoulli trials (with success probability \\(p\\) ) that it takes to get 1 success. Then the probability we need to perform \\(X=k\\) trials is the probability of having \\(k-1\\) failures (which happens with probability \\((1-p)^{k-1}\\) ) followed by a success (which happens with probability \\(p\\) ) \\[\\Pr(X=k) = (1-p)^{k-1}p\\] This random variable \\(X\\) is called a \"geometric random variable\" with parameter \\(p\\) and denoted \\(X\\sim\\text{Geo}(p)\\) . After evaluating a few infinite sums it can be shown to have expectation \\[\\mathbb{E}(X) = \\frac{1}{p}\\] and variance \\[\\text{Var}(X) = \\frac{1-p}{p^2}\\] Returning to the coalescent, the time for two sample lineages to coalesce is therefore geometrically distributed, \\(T_2\\sim\\text{Geo}(p_2)\\) . We then can expect to wait \\[\\begin{aligned} \\mathbb{E}(T_2) &= 1/p_2\\\\ &=2N \\end{aligned}\\] generations until coalescence of the two lineages. However, the variance around this expectation is \\[\\begin{aligned} \\text{Var}(T_2) &= \\frac{1-p_2}{p^2}\\\\ &=2N(2N-1) \\end{aligned}\\] which is roughly \\((2N)^2\\) in a large population, \\(2N>>1\\) . Since \\(N^2\\) can be very large relative to \\(N\\) , this means that there is a lot of noise around the expectation. We can see how noisey the coalescence time is by sampling from a geometric distribution for a given value of \\(N\\) . For example, below we sample the coalescence time 1000 times with \\(N=1000\\) and plot those times in a histogram. We often see coalescence times ranging from very near 1 to well over 10000 (run the code a few times if you'd like). import matplotlib.pyplot as plt fig, ax = plt.subplots() N = 1000 #diploid population size p = 1/(2*N) #probability 2 lineages coalesce in the previous generation ts = np.random.geometric(p,1000) #sample from a geometric with param p 1000 times ax.hist(ts, bins=100) #histogram of coalescence times ax.set_xlabel('number of generations until 2 lineages coalesce') ax.set_ylabel('number of realizations') plt.show() Time for \\(n\\) lineages to coalesce Considering only two samples is a special case that gives us some intuition about the model. But in general we want to know, how long until \\(n\\) samples all share a common ancestor? Let this time be random variable \\(M_n\\) . We want to know something about \\(M_n\\) . We start by assuming that only one pair of lineages can coalesce each generation, which is valid when the population size is large, \\(N>>1\\) , and the sample is relatively small \\(n<<N\\) . Then we can write \\(M_n\\) as the time it takes to go from \\(n\\) to \\(n-1\\) lineages, \\(T_n\\) , plus the time it takes to go from \\(n-1\\) to \\(n-2\\) lineages, \\(T_{n-1}\\) , and so on down to the time it takes to go from 2 to 1 lineages, \\(T_2\\) \\[M_n = T_n + T_{n-1} + ... + T_2\\] The next step is to find out something about the \\(T_i\\) . We do this by noting that when there are \\(i\\) lineages the probability of no coalescence in the previous generation is \\[1-p_i = \\left(1-\\frac{1}{2N}\\right)\\left(1-\\frac{2}{2N}\\right)...\\left(1-\\frac{i-1}{2N}\\right)\\] In words, we choose any parent for the first lineage, the second lineage does not have the same parent with probability \\(1-\\frac{1}{2N}\\) , the third lineage does not have the same parent as either of the first two lineages with probability \\(1-\\frac{2}{2N}\\) , and so on to the \\(i^\\text{th}\\) lineage. This is a complicated expression but we can simplify by taking a Taylor series around \\(1/N=0\\) and approximating to first order, which gives \\[\\begin{aligned} 1-p_i &\\approx 1 - \\frac{1}{2N}\\sum_{j=1}^{i-1}j\\\\ &= 1 - \\frac{1}{2N}\\frac{i(i-1)}{2}\\\\ &= 1 - \\frac{{i \\choose 2}}{2N}\\\\ \\end{aligned}\\] so that the probability that there is coalescence is \\[p_i \\approx \\frac{{i \\choose 2}}{2N}\\] This makes good sense. When \\(N\\) is large there can be at most 1 coalescent event per generation among the sample lineages. Since there are \\({i \\choose 2}\\) ways to choose a pair of lineages from \\(i\\) lineages, each of which coalesce with probability \\(1/(2N)\\) , the probability of coalescence is \\({i \\choose 2}/(2N)\\) . We can now treat the \\(T_i\\) as geometric random variables with parameter \\(p_i\\) , \\(T_i\\sim\\text{Geo}(p_i)\\) . This means that \\(M_n\\) , the time for \\(n\\) samples to coalesce into 1, is the sum of \\(n-1\\) independent geometric random variables. Unfortunately this doesn't produce a nice distribution (in contrast to the sum of Poisson random variables we saw in the last lecture). However, we can calculate the expectation since we know the expectation of a sum is the sum of the expectations \\[\\begin{aligned} \\mathbb{E}(M_n) &= \\mathbb{E}(T_n + T_{n-1} + ... + T_2)\\\\ &= \\mathbb{E}(T_n) + \\mathbb{E}(T_{n-1}) + ... + \\mathbb{E}(T_2)\\\\ &= 1/p_n + 1/p_{n-1} + ... + 1/p_2\\\\ &\\approx \\frac{2N}{{n \\choose 2}} + \\frac{2N}{{n-1 \\choose 2}} + ... + \\frac{2N}{{2 \\choose 2}}\\\\ &= 2N \\sum_{i=2}^n \\frac{1}{{i \\choose 2}} \\\\ &= 4N \\frac{n-1}{n} \\\\ \\end{aligned}\\] Since \\((n-1)/n<1\\) we see that all \\(n\\) sample lineages are expected to coalesce in less than \\(4N\\) generations. This is pretty remarkable given we expect to wait more than half that time just for 2 sample lineages to coalesce! The reason for this is that coalescence happens faster when there are more lineages (since there are more pairs to choose from). So if we take a large sample then most of the lineages will coalesce very quickly and most of the time spent waiting for the most recent common ancestor will be once few lineages remain. Below we take advantage of a great Python package, msprime , to quickly simulate the coalescent with \\(n\\) samples and plot the history of those samples (a coalescent tree ). import msprime n = 10 ts = msprime.sim_ancestry(n) #simulate coalescent with n samples ts.first().draw_svg(node_labels={}, size=(500,300)) #plot tree without node labels","title":"Lecture 22"},{"location":"lectures/lecture-22/#lecture-22-probability-iii-the-coalescent","text":"Run notes interactively?","title":"Lecture 22: Probability III (the coalescent)"},{"location":"lectures/lecture-22/#lecture-overview","text":"The coalescent All the models we have studied so far have looked forward in time, into the future. But this doesn't have to always be the case, we can also look back in time, modeling the past. Here we look at one particularly powerful example from population genetics.","title":"Lecture overview"},{"location":"lectures/lecture-22/#1-the-coalescent","text":"Let's consider a population composed of \\(N\\) diploid individuals, i.e., with \\(2N\\) alleles at a given locus. We want to model the history of these alleles -- from which alleles in the previous generations do they descend? One of the simplest ways to model this is to treat all of the alleles as equivalent (e.g., no fitness differences) so that a given allele in the current generation picks its \"parent\" allele at random from amongst the \\(2N\\) alleles in the previous generation. We can simulate this and plot the result, arranging the \\(2N\\) alleles horizontally, stacking previous generations on top, and drawing lines to connect \"children\" and \"parent\" alleles. import numpy as np import matplotlib.pyplot as plt def simulate(N,tmax): p = np.arange(2*N) #initial configuration t = 0 ps = np.empty((tmax,2*N), dtype='int') while t < tmax: ps[t] = p p = np.random.randint(0,2*N,2*N) #parents of current generation t += 1 return ps def plot_lineages(ps,ax,alpha): # plot lineages for t,p in enumerate(ps[1:]): #loop over generations ax.plot([ps[0],ps[t+1]], [t,t+1], marker='o', color='k', alpha=alpha) #connect children with parents ax.scatter(ps[0], [t+1 for _ in ps[0]], marker='o', color='k', alpha=alpha) #plot all alleles of last gen # simplify presentation # ax.axis('off') ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.spines['bottom'].set_visible(False) ax.spines['left'].set_visible(False) ax.xaxis.set_ticks([]) ax.yaxis.set_ticks(range(len(ps))) ax.set_xlabel('alleles') ax.set_ylabel('generations ago') fig, ax = plt.subplots() ps = simulate(N=5,tmax=4) plot_lineages(ps,ax,alpha=0.5) plt.show() One of the most important aspects of this model is that we can choose to think about the history of just some subset of the alleles in the current generation. This is helpful because 1) we do not need to model the entire population and 2) this has a close connection to data (since we almost never sample every individual in a population). To see this visually, we can choose a few \"sample\" alleles from the current generation and highlight their \"lineages\". fig, ax = plt.subplots() plot_lineages(ps,ax,alpha=0.1) #plot full population in background # plot sample lineages for i in [0,5,9]: #samples path = [i] #lineage of sample i for p in ps[1:]: #loop over generations path.append(p[i]) #add parent i = p[i] #make parent the child ax.plot(path,range(len(path)), marker='o') plt.show()","title":"1. The coalescent"},{"location":"lectures/lecture-22/#time-for-two-lineages-to-coalesce","text":"When two sample lineages meet at a most recent common ancestor we say they coalesce (\"come together\"). This is where the name of the model comes from, the coalescent . (Note that some reserve that name for the continuous-time limit of this model, but we won't be so strict.) The time it takes for two lineages to coalesce determines how similar those alleles are. The more distant their most recent common ancestor the more mutations that are expected to have accumulated. Thus the time to coalescence influences the amount of genetic diversity we expect to see. The first question we will ask with this model is: how many generations does it take for two lineages to coalesce? Let's call this random variable \\(T_2\\) . We want to know how this random variable is distributed. Consider one generation at a time. We choose the parent of one of the alleles at random. We do the same for the second allele. The probability that the two lineages coalesce in the previous generation is the probability that the parent of the second allele is the same as the parent of the first allele, \\(p_2=1/(2N)\\) . Let \\(X=1\\) if the two lineages coalesce in the previous generation, \\(X=0\\) if they don't coalesce. Then \\(X\\) is a Bernoulii random variable, \\(X\\sim\\text{Ber}(p_2)\\) . We can now rephrase our question mathematically: how many Bernoulli trials do we have to perform to get one success? Geometric random variable Let \\(X\\) be the number of Bernoulli trials (with success probability \\(p\\) ) that it takes to get 1 success. Then the probability we need to perform \\(X=k\\) trials is the probability of having \\(k-1\\) failures (which happens with probability \\((1-p)^{k-1}\\) ) followed by a success (which happens with probability \\(p\\) ) \\[\\Pr(X=k) = (1-p)^{k-1}p\\] This random variable \\(X\\) is called a \"geometric random variable\" with parameter \\(p\\) and denoted \\(X\\sim\\text{Geo}(p)\\) . After evaluating a few infinite sums it can be shown to have expectation \\[\\mathbb{E}(X) = \\frac{1}{p}\\] and variance \\[\\text{Var}(X) = \\frac{1-p}{p^2}\\] Returning to the coalescent, the time for two sample lineages to coalesce is therefore geometrically distributed, \\(T_2\\sim\\text{Geo}(p_2)\\) . We then can expect to wait \\[\\begin{aligned} \\mathbb{E}(T_2) &= 1/p_2\\\\ &=2N \\end{aligned}\\] generations until coalescence of the two lineages. However, the variance around this expectation is \\[\\begin{aligned} \\text{Var}(T_2) &= \\frac{1-p_2}{p^2}\\\\ &=2N(2N-1) \\end{aligned}\\] which is roughly \\((2N)^2\\) in a large population, \\(2N>>1\\) . Since \\(N^2\\) can be very large relative to \\(N\\) , this means that there is a lot of noise around the expectation. We can see how noisey the coalescence time is by sampling from a geometric distribution for a given value of \\(N\\) . For example, below we sample the coalescence time 1000 times with \\(N=1000\\) and plot those times in a histogram. We often see coalescence times ranging from very near 1 to well over 10000 (run the code a few times if you'd like). import matplotlib.pyplot as plt fig, ax = plt.subplots() N = 1000 #diploid population size p = 1/(2*N) #probability 2 lineages coalesce in the previous generation ts = np.random.geometric(p,1000) #sample from a geometric with param p 1000 times ax.hist(ts, bins=100) #histogram of coalescence times ax.set_xlabel('number of generations until 2 lineages coalesce') ax.set_ylabel('number of realizations') plt.show()","title":"Time for two lineages to coalesce"},{"location":"lectures/lecture-22/#time-for-n-lineages-to-coalesce","text":"Considering only two samples is a special case that gives us some intuition about the model. But in general we want to know, how long until \\(n\\) samples all share a common ancestor? Let this time be random variable \\(M_n\\) . We want to know something about \\(M_n\\) . We start by assuming that only one pair of lineages can coalesce each generation, which is valid when the population size is large, \\(N>>1\\) , and the sample is relatively small \\(n<<N\\) . Then we can write \\(M_n\\) as the time it takes to go from \\(n\\) to \\(n-1\\) lineages, \\(T_n\\) , plus the time it takes to go from \\(n-1\\) to \\(n-2\\) lineages, \\(T_{n-1}\\) , and so on down to the time it takes to go from 2 to 1 lineages, \\(T_2\\) \\[M_n = T_n + T_{n-1} + ... + T_2\\] The next step is to find out something about the \\(T_i\\) . We do this by noting that when there are \\(i\\) lineages the probability of no coalescence in the previous generation is \\[1-p_i = \\left(1-\\frac{1}{2N}\\right)\\left(1-\\frac{2}{2N}\\right)...\\left(1-\\frac{i-1}{2N}\\right)\\] In words, we choose any parent for the first lineage, the second lineage does not have the same parent with probability \\(1-\\frac{1}{2N}\\) , the third lineage does not have the same parent as either of the first two lineages with probability \\(1-\\frac{2}{2N}\\) , and so on to the \\(i^\\text{th}\\) lineage. This is a complicated expression but we can simplify by taking a Taylor series around \\(1/N=0\\) and approximating to first order, which gives \\[\\begin{aligned} 1-p_i &\\approx 1 - \\frac{1}{2N}\\sum_{j=1}^{i-1}j\\\\ &= 1 - \\frac{1}{2N}\\frac{i(i-1)}{2}\\\\ &= 1 - \\frac{{i \\choose 2}}{2N}\\\\ \\end{aligned}\\] so that the probability that there is coalescence is \\[p_i \\approx \\frac{{i \\choose 2}}{2N}\\] This makes good sense. When \\(N\\) is large there can be at most 1 coalescent event per generation among the sample lineages. Since there are \\({i \\choose 2}\\) ways to choose a pair of lineages from \\(i\\) lineages, each of which coalesce with probability \\(1/(2N)\\) , the probability of coalescence is \\({i \\choose 2}/(2N)\\) . We can now treat the \\(T_i\\) as geometric random variables with parameter \\(p_i\\) , \\(T_i\\sim\\text{Geo}(p_i)\\) . This means that \\(M_n\\) , the time for \\(n\\) samples to coalesce into 1, is the sum of \\(n-1\\) independent geometric random variables. Unfortunately this doesn't produce a nice distribution (in contrast to the sum of Poisson random variables we saw in the last lecture). However, we can calculate the expectation since we know the expectation of a sum is the sum of the expectations \\[\\begin{aligned} \\mathbb{E}(M_n) &= \\mathbb{E}(T_n + T_{n-1} + ... + T_2)\\\\ &= \\mathbb{E}(T_n) + \\mathbb{E}(T_{n-1}) + ... + \\mathbb{E}(T_2)\\\\ &= 1/p_n + 1/p_{n-1} + ... + 1/p_2\\\\ &\\approx \\frac{2N}{{n \\choose 2}} + \\frac{2N}{{n-1 \\choose 2}} + ... + \\frac{2N}{{2 \\choose 2}}\\\\ &= 2N \\sum_{i=2}^n \\frac{1}{{i \\choose 2}} \\\\ &= 4N \\frac{n-1}{n} \\\\ \\end{aligned}\\] Since \\((n-1)/n<1\\) we see that all \\(n\\) sample lineages are expected to coalesce in less than \\(4N\\) generations. This is pretty remarkable given we expect to wait more than half that time just for 2 sample lineages to coalesce! The reason for this is that coalescence happens faster when there are more lineages (since there are more pairs to choose from). So if we take a large sample then most of the lineages will coalesce very quickly and most of the time spent waiting for the most recent common ancestor will be once few lineages remain. Below we take advantage of a great Python package, msprime , to quickly simulate the coalescent with \\(n\\) samples and plot the history of those samples (a coalescent tree ). import msprime n = 10 ts = msprime.sim_ancestry(n) #simulate coalescent with n samples ts.first().draw_svg(node_labels={}, size=(500,300)) #plot tree without node labels","title":"Time for \\(n\\) lineages to coalesce"},{"location":"lectures/schedule/","text":"Schedule Lecture Topic Reading 1 Introduction Preface, 1.1, 1.4 (5p.) 2 Model construction Chapter 2 (34p.) 3 Exponential and logistic growth 3.1, 3.2 (8p.) 4 One-locus selection 3.3 (10p.) 5 Numerical and graphical techniques (univariate) 4.1-4.3 (23p.) 6 Numerical and graphical techniques (multivariate) 4.4-4.5 (12p.) 7 Equilibria (univariate) 5.1-5.2 (12p.) 8 Stability (univariate) 5.3 (13p.) 9 General solutions (univariate) Chapter 6 (18p.) 10 Linear algebra I P2.1-P2.4 (14p.) 11 Linear algebra II P2.5-P2.7 (9p.) 12 Linear algebra III P2.8-P2.9 (13p.) 13 General solutions (linear multivariate) 9.1-9.2 (18p.) 14 Demography Chapter 10 (37p.) 15 Equilibria and stability (nonlinear multivariate) Chapter 8 (37p.) 16 Epidemiology 294-301 (7p.) 17 Multi-locus population genetics 322-330 (8p.) 18 Evolutionary invasion analysis 12.1-12.4 (30p.) 19 The evolution of dominance 12.5 (17p.) 20 Probability I (genetic drift) 521-528, 576-577 (10p.) 21 Probability II (demographic stochasticity) 534-536, 13.2, 641-642 (11p.) 22 Probability III (the coalescent) 13.8 (9p.)","title":"Schedule"},{"location":"lectures/schedule/#schedule","text":"Lecture Topic Reading 1 Introduction Preface, 1.1, 1.4 (5p.) 2 Model construction Chapter 2 (34p.) 3 Exponential and logistic growth 3.1, 3.2 (8p.) 4 One-locus selection 3.3 (10p.) 5 Numerical and graphical techniques (univariate) 4.1-4.3 (23p.) 6 Numerical and graphical techniques (multivariate) 4.4-4.5 (12p.) 7 Equilibria (univariate) 5.1-5.2 (12p.) 8 Stability (univariate) 5.3 (13p.) 9 General solutions (univariate) Chapter 6 (18p.) 10 Linear algebra I P2.1-P2.4 (14p.) 11 Linear algebra II P2.5-P2.7 (9p.) 12 Linear algebra III P2.8-P2.9 (13p.) 13 General solutions (linear multivariate) 9.1-9.2 (18p.) 14 Demography Chapter 10 (37p.) 15 Equilibria and stability (nonlinear multivariate) Chapter 8 (37p.) 16 Epidemiology 294-301 (7p.) 17 Multi-locus population genetics 322-330 (8p.) 18 Evolutionary invasion analysis 12.1-12.4 (30p.) 19 The evolution of dominance 12.5 (17p.) 20 Probability I (genetic drift) 521-528, 576-577 (10p.) 21 Probability II (demographic stochasticity) 534-536, 13.2, 641-642 (11p.) 22 Probability III (the coalescent) 13.8 (9p.)","title":"Schedule"},{"location":"syllabus/assignments/","text":"Assignments Homeworks A list of homework problems will be released after class on Wednesday, with the solutions due by the start of class on Monday. Labs Each computer lab contains a series of problems and the solutions are due by the end of the day (ideally by the end of the lab).","title":"Assignments"},{"location":"syllabus/assignments/#assignments","text":"","title":"Assignments"},{"location":"syllabus/assignments/#homeworks","text":"A list of homework problems will be released after class on Wednesday, with the solutions due by the start of class on Monday.","title":"Homeworks"},{"location":"syllabus/assignments/#labs","text":"Each computer lab contains a series of problems and the solutions are due by the end of the day (ideally by the end of the lab).","title":"Labs"},{"location":"syllabus/course_structure/","text":"Course structure Learning objectives Mathematics is central to science because it provides a rigorous way to go from a set of assumptions (what we take to be true) to their logical consequences (what we want to know). In ecology & evolution this might be how we think SARS-CoV-2 may spread and evolve given a set of vaccination rates and travel restrictions, how caribou population sizes are predicted to respond to forecasted rates of climate change, or something much more abstract like the expected amount of genetic diversity in a randomly mating population. In this course we'll learn how to build, analyze, and interpret mathematical models of increasing complexity through readings, lectures, homeworks, computer labs, and a final project. By the end of the course you will be able to: build a model: go from a verbal description of a biological system to a set of equations analyze a model: manipulate a set of equations into a mathematical expression of interest interpret a model: translate mathematical expressions back into biological meaning Weekly tasks read text attend two lectures attend one lab do homework Grading scheme 10% - labs 20% - homeworks 20% - midterm 20% - final project 30% - final Late fees Homeworks and labs To facilitate rapid feedback and solutions, you will receive 1/2 of your grade if submitted <24 hours late and 0 for anything later Final project You will receive 9/10 of your grade if submitted <24 hours late, else 8/10 if <48 hours late, else ...","title":"Course structure"},{"location":"syllabus/course_structure/#course-structure","text":"","title":"Course structure"},{"location":"syllabus/course_structure/#learning-objectives","text":"Mathematics is central to science because it provides a rigorous way to go from a set of assumptions (what we take to be true) to their logical consequences (what we want to know). In ecology & evolution this might be how we think SARS-CoV-2 may spread and evolve given a set of vaccination rates and travel restrictions, how caribou population sizes are predicted to respond to forecasted rates of climate change, or something much more abstract like the expected amount of genetic diversity in a randomly mating population. In this course we'll learn how to build, analyze, and interpret mathematical models of increasing complexity through readings, lectures, homeworks, computer labs, and a final project. By the end of the course you will be able to: build a model: go from a verbal description of a biological system to a set of equations analyze a model: manipulate a set of equations into a mathematical expression of interest interpret a model: translate mathematical expressions back into biological meaning","title":"Learning objectives"},{"location":"syllabus/course_structure/#weekly-tasks","text":"read text attend two lectures attend one lab do homework","title":"Weekly tasks"},{"location":"syllabus/course_structure/#grading-scheme","text":"10% - labs 20% - homeworks 20% - midterm 20% - final project 30% - final","title":"Grading scheme"},{"location":"syllabus/course_structure/#late-fees","text":"","title":"Late fees"},{"location":"syllabus/course_structure/#homeworks-and-labs","text":"To facilitate rapid feedback and solutions, you will receive 1/2 of your grade if submitted <24 hours late and 0 for anything later","title":"Homeworks and labs"},{"location":"syllabus/course_structure/#final-project","text":"You will receive 9/10 of your grade if submitted <24 hours late, else 8/10 if <48 hours late, else ...","title":"Final project"},{"location":"syllabus/exams/","text":"Exams Midterm Covers material from Lectures 1-9 and Labs 1-5 Examples: 2021 , solutions 2022 , solutions 2024 , solutions Final Covers all material, with focus on Lectures 10-20 and Labs 6-10 Examples: 2021 , solutions 2022 , solutions","title":"Exams"},{"location":"syllabus/exams/#exams","text":"","title":"Exams"},{"location":"syllabus/exams/#midterm","text":"Covers material from Lectures 1-9 and Labs 1-5 Examples: 2021 , solutions 2022 , solutions 2024 , solutions","title":"Midterm"},{"location":"syllabus/exams/#final","text":"Covers all material, with focus on Lectures 10-20 and Labs 6-10 Examples: 2021 , solutions 2022 , solutions","title":"Final"},{"location":"syllabus/final_project/","text":"Final project Construct your own model. In this project you will use the tools you've learned in class and apply them to a model that you develop. The model can be about any phenomenon in ecology and evolution, as long as you make up the model. Be as creative as you want. You'll do the final project in two parts. Part 1 Describe your biological question and why this interests you Describe your model in words (ie, the main assumptions) and explain the main structure with a diagram (eg, flow or life cycle diagram) Write down the equations that you will analyze Describe what your analysis might reveal (ie, your hypothesis) Max 2 pages Example Part 2 Re-iterate your biological question and why this interests you Describe your model assumptions in detail, defining all parameters and variables Write down the equations for your model Analyze your model Explain how the results address your original question Suggest how the model could be improved or extended Max 4 pages (not including any code that you used, which can be included as a link to Google Colab or as additional file) Example Tip If you are having trouble coming up with a new model, take one of the models that we've analysed in the course and change one or more of its underlying assumptions to get a new set of equations. Then analyse these equations. Discuss the differences between the assumptions used and also between the results obtained.","title":"Final Project"},{"location":"syllabus/final_project/#final-project","text":"","title":"Final project"},{"location":"syllabus/final_project/#construct-your-own-model","text":"In this project you will use the tools you've learned in class and apply them to a model that you develop. The model can be about any phenomenon in ecology and evolution, as long as you make up the model. Be as creative as you want. You'll do the final project in two parts. Part 1 Describe your biological question and why this interests you Describe your model in words (ie, the main assumptions) and explain the main structure with a diagram (eg, flow or life cycle diagram) Write down the equations that you will analyze Describe what your analysis might reveal (ie, your hypothesis) Max 2 pages Example Part 2 Re-iterate your biological question and why this interests you Describe your model assumptions in detail, defining all parameters and variables Write down the equations for your model Analyze your model Explain how the results address your original question Suggest how the model could be improved or extended Max 4 pages (not including any code that you used, which can be included as a link to Google Colab or as additional file) Example Tip If you are having trouble coming up with a new model, take one of the models that we've analysed in the course and change one or more of its underlying assumptions to get a new set of equations. Then analyse these equations. Discuss the differences between the assumptions used and also between the results obtained.","title":"Construct your own model."},{"location":"syllabus/general_info/","text":"General info Land acknowledgement I wish to acknowledge this land on which the University of Toronto operates. For thousands of years it has been the traditional land of the Huron-Wendat, the Seneca, and the Mississaugas of the Credit. Today, this meeting place is still the home to many Indigenous people from across Turtle Island and I am grateful to have the opportunity to work on this land. For more information see University of Toronto's land acknowledgement . Group norms The University of Toronto is committed to equity, human rights, and respect for diversity. All members of the learning environment in this course should strive to create an atmosphere of mutual respect where all members of our community can express themselves, engage with each other, and respect one another\u2019s differences. U of T does not condone discrimination or harassment against any persons or communities. Please contact me if you have any concerns. For more information see the Code of Student Conduct . Accessibility The University provides academic accommodations for students with disabilities in accordance with the terms of the Ontario Human Rights Code . This occurs through a collaborative process that acknowledges a collective obligation to develop an accessible learning environment that both meets the needs of students and preserves the essential academic requirements of the University\u2019s courses and programs. Students with diverse learning styles and needs are welcome in this course. If you have a disability that may require accommodations, please feel free to get in touch with me and/or the Accessibility Services office . Religious observances The University provides reasonable accommodation of the needs of students who observe religious holy days other than those already accommodated by ordinary scheduling and statutory holidays. Students have a responsibility to alert members of the teaching staff in a timely fashion to upcoming religious observances and anticipated absences and I will make every reasonable effort to avoid scheduling tests, examinations or other compulsory activities at these times. Please reach out to me as early as possible to communicate any anticipated absences related to religious observances, and to discuss any possible related implications for course work. Family care responsibilities The University of Toronto strives to provide a family-friendly environment. You may wish to inform me if you are a student with family responsibilities. If you are a student parent or have family responsibilities, you also may wish to visit the Family Care Office website .","title":"General info"},{"location":"syllabus/general_info/#general-info","text":"","title":"General info"},{"location":"syllabus/general_info/#land-acknowledgement","text":"I wish to acknowledge this land on which the University of Toronto operates. For thousands of years it has been the traditional land of the Huron-Wendat, the Seneca, and the Mississaugas of the Credit. Today, this meeting place is still the home to many Indigenous people from across Turtle Island and I am grateful to have the opportunity to work on this land. For more information see University of Toronto's land acknowledgement .","title":"Land acknowledgement"},{"location":"syllabus/general_info/#group-norms","text":"The University of Toronto is committed to equity, human rights, and respect for diversity. All members of the learning environment in this course should strive to create an atmosphere of mutual respect where all members of our community can express themselves, engage with each other, and respect one another\u2019s differences. U of T does not condone discrimination or harassment against any persons or communities. Please contact me if you have any concerns. For more information see the Code of Student Conduct .","title":"Group norms"},{"location":"syllabus/general_info/#accessibility","text":"The University provides academic accommodations for students with disabilities in accordance with the terms of the Ontario Human Rights Code . This occurs through a collaborative process that acknowledges a collective obligation to develop an accessible learning environment that both meets the needs of students and preserves the essential academic requirements of the University\u2019s courses and programs. Students with diverse learning styles and needs are welcome in this course. If you have a disability that may require accommodations, please feel free to get in touch with me and/or the Accessibility Services office .","title":"Accessibility"},{"location":"syllabus/general_info/#religious-observances","text":"The University provides reasonable accommodation of the needs of students who observe religious holy days other than those already accommodated by ordinary scheduling and statutory holidays. Students have a responsibility to alert members of the teaching staff in a timely fashion to upcoming religious observances and anticipated absences and I will make every reasonable effort to avoid scheduling tests, examinations or other compulsory activities at these times. Please reach out to me as early as possible to communicate any anticipated absences related to religious observances, and to discuss any possible related implications for course work.","title":"Religious observances"},{"location":"syllabus/general_info/#family-care-responsibilities","text":"The University of Toronto strives to provide a family-friendly environment. You may wish to inform me if you are a student with family responsibilities. If you are a student parent or have family responsibilities, you also may wish to visit the Family Care Office website .","title":"Family care responsibilities"},{"location":"syllabus/instructors/","text":"Instructors Professor Matthew Osmond (he/him) email: mm.osmond@utoronto.ca office hours: Monday 1-2, Earth Sciences Centre (ESC) 3041 website: osmond-lab.github.io Teaching assistant Chris Carlson (he/him) email: chris.carlson@mail.utoronto.ca office hours: Thursday 10-11, Earth Sciences Centre (ESC) 3044 and Zoom (https://utoronto.zoom.us/j/82127674468)","title":"Instructors"},{"location":"syllabus/instructors/#instructors","text":"","title":"Instructors"},{"location":"syllabus/instructors/#professor","text":"Matthew Osmond (he/him) email: mm.osmond@utoronto.ca office hours: Monday 1-2, Earth Sciences Centre (ESC) 3041 website: osmond-lab.github.io","title":"Professor"},{"location":"syllabus/instructors/#teaching-assistant","text":"Chris Carlson (he/him) email: chris.carlson@mail.utoronto.ca office hours: Thursday 10-11, Earth Sciences Centre (ESC) 3044 and Zoom (https://utoronto.zoom.us/j/82127674468)","title":"Teaching assistant"},{"location":"syllabus/resources/","text":"Resources There are many resources available at the University of Toronto to help you succeed in this course. Below are a few: Writing Center Academic integrity More on academic integrity CTSI list of supports Academic success module Get help with Quercus","title":"Other resources"},{"location":"syllabus/resources/#resources","text":"There are many resources available at the University of Toronto to help you succeed in this course. Below are a few: Writing Center Academic integrity More on academic integrity CTSI list of supports Academic success module Get help with Quercus","title":"Resources"},{"location":"syllabus/textbook/","text":"Textbook Otto & Day 2007. A biologist's guide to mathematical modeling in ecology and evolution . UofT library e-copies UofT library physical-copies buy your own copy","title":"Textbook"},{"location":"syllabus/textbook/#textbook","text":"Otto & Day 2007. A biologist's guide to mathematical modeling in ecology and evolution . UofT library e-copies UofT library physical-copies buy your own copy","title":"Textbook"},{"location":"syllabus/when_and_where/","text":"When and where Lectures Monday & Wednesday, 10:10 - 11:00 AM, Ramsay Wright (RW) 142 Labs Wednesday, 3:10 - 5:00 PM, Carr Hall (CR) 325","title":"When and where"},{"location":"syllabus/when_and_where/#when-and-where","text":"","title":"When and where"},{"location":"syllabus/when_and_where/#lectures","text":"Monday & Wednesday, 10:10 - 11:00 AM, Ramsay Wright (RW) 142","title":"Lectures"},{"location":"syllabus/when_and_where/#labs","text":"Wednesday, 3:10 - 5:00 PM, Carr Hall (CR) 325","title":"Labs"},{"location":"syllabus/final_project/partII_example/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Final project part II - example Run notes interactively? Note This is a made-up example inspired by the research of Sydney Ackermann, an MSc student in the Osmond lab, who was in turn inspired by the models of Roze & Michod and Pichugin et al. . Biological question Note In this case, this is just repeated from part I, so that part II tells the whole story from question to answer. Update and refine as needed. I am interested in the origin of multicellularity and the reason why particular life-cycles predominate. For instance, many bacterial species exist as single cells which reproduce via binary fission while many multicellular organisms grow to large sizes and reproduce via single-celled offspring. However, there are many more options and it is not clear if or why these strategies are evolutionarily optimal or how multicellularity evolved from a binary fission ancestor. Here I ask the question, which multicellular life-cycles can invade a unicellular population producing via binary fission? Model Note This is an improved, expanded version of the model description from part I, with more detail and now including the equations to be analyzed. Unicellular resident Here I first model a unicellular population where individuals divide, die, and compete in continuous time. The variable in this model is the number of individuals, \\(n\\) . The parameters are the birth rate ( \\(b_1\\) ), death rate ( \\(d_1\\) ), and amount of competition. To describe competition I will assume there are a total of \\(k\\) places for individuals to exist. When a cell divides, each of the two resulting offspring randomly choose one of these \\(k\\) locations. If an offspring lands on an empty spot it survives. If an offspring lands on an occupied spot it \"wins\" that spot with probability 1/2, killing the resident, and otherwise dies. The amount of competition is therefore controlled by parameter \\(k\\) . This model can be described by the following flow diagram graph LR; A((n)) --\"b1 n (1 - n/k)\" --> A; A --d1 n--> B[ ]; style B height:0px; The corresponding differential equation is \\[ \\frac{\\mathrm{d}n}{\\mathrm{d}t} = b_1 n \\left(1-\\frac{n}{k}\\right) - d_1 n \\] which can be solved for equilibrium, \\(\\hat{n}\\) . Multicellular invader Next, imagine an invading multicellular population. 1+1+1 For instance, imagine a life-cycle where offspring are unicellular, they divide to form an individual with two cells, and then divide again to produce three single-celled offspring. We call this strategy 1+1+1 because those are the sizes of the offspring produced. In this case we need to track the number of individuals with one, \\(n_1\\) , and two, \\(n_2\\) , cells (i.e., this is a structured population). I assume that each cell in an individual with \\(i\\) cells divides at rate \\(b_i\\) and that individuals with \\(i\\) cells die at rate \\(d_i\\) . I also assume that this invading population is so rare (and \\(k\\) so large) that offspring produced by this life-history strategy never land on other individuals with this same strategy. The offspring may, however, land on an individual from the unicellular population, which will reduce the growth rate of the invader. The dynamics of this invading multicellular population can be described by the following flow diagram graph LR; A((n1)) --b1 n1--> B((n2)); A --d1 n1--> C1[ ]; B --d2 n2--> C2[ ]; B --\"6b2 n2 (1-n/(2k))\"--> A; style C1 height:0px; style C2 height:0px; To determine whether this population can invade a unicellular resident at equilibrium, I will calculate the leading eigenvalue from the system of (linear in \\(n_i\\) ) differential equations \\[ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} = \\mathbf{M}\\vec{n} \\] where \\(\\vec{n}=\\begin{pmatrix}n_1 \\\\ n_2\\end{pmatrix}\\) and \\[ \\mathbf{M}= \\begin{pmatrix} -b_1-d_1 & 6b_2(1-\\hat{n}/(2k)) \\\\ b_1 & -2b_2-d_2 \\end{pmatrix} \\] 1 + 2 The other multicellular strategy that has offspring sizes summing to 3 is 1+2. In this case the transition matrix is \\[ \\mathbf{M}= \\begin{pmatrix} -b_1-d_1 & 2b_2(1-\\hat{n}/(2k)) \\\\ b_1 & -2b_2-d_2+2b_2(1-\\hat{n}/(2k)) \\end{pmatrix} \\] 1 + 1 + 1 + 1 We can also consider larger multicellular strategies, like 1+1+1+1. In this case we need to keep track of the number of groups of size 1, 2, and 3. This gives a 3x3 projection matrix \\[ \\mathbf{M}= \\begin{pmatrix} -b_1-d_1 & 0 & 12b_3(1-\\hat{n}/(2k)) \\\\ b_1 & -2b_2-d_2 & 0 \\\\ 0 & 2b_2 & -3b_3-d_3 \\end{pmatrix} \\] 1 + 1 + 2 For the 1+1+2 strategy we have \\[ \\mathbf{M}= \\begin{pmatrix} -b_1-d_1 & 0 & 6b_3(1-\\hat{n}/(2k)) \\\\ b_1 & -2b_2-d_2 & 3b_3(1-\\hat{n}/(2k)) \\\\ 0 & 2b_2 & -3b_3-d_3 \\end{pmatrix} \\] 1 + 3 For the 1+3 strategy we have \\[ \\mathbf{M}= \\begin{pmatrix} -b_1-d_1 & 0 & 3b_3(1-\\hat{n}/(2k)) \\\\ b_1 & -2b_2-d_2 & 0 \\\\ 0 & 2b_2 & -3b_3-d_3+3b_3(1-\\hat{n}/(2k)) \\end{pmatrix} \\] 2 + 2 For the 2+2 strategy we have \\[ \\mathbf{M}= \\begin{pmatrix} -b_1-d_1 & 0 & 0 \\\\ b_1 & -2b_2-d_2 & 6b_3(1-\\hat{n}/(2k)) \\\\ 0 & 2b_2 & -3b_3-d_3 \\end{pmatrix} \\] Results Unicellular resident I first solve for the unicellular resident equilibrium \\[ \\begin{aligned} \\frac{\\mathrm{d}n}{\\mathrm{d}t} &= b_1 n \\left(1-\\frac{n}{k}\\right) - d_1 n \\\\ 0 &= b_1 \\hat{n} \\left(1-\\frac{\\hat{n}}{k}\\right) - d_1 \\hat{n} \\\\ 0 &= \\hat{n} \\left(b_1 \\left(1-\\frac{\\hat{n}}{k}\\right) - d_1\\right) \\\\ \\end{aligned} \\] This implies that \\(\\hat{n}=0\\) or \\[ \\begin{aligned} 0 &= b_1 \\left(1-\\frac{\\hat{n}}{k}\\right) - d_1 \\\\ d_1/b_1 &= 1-\\frac{\\hat{n}}{k} \\\\ 1 - d_1/b_1 &= \\frac{\\hat{n}}{k} \\\\ k(1 - d_1/b_1) &= \\hat{n} \\\\ \\end{aligned} \\] This non-zero equilibrium is biologically valid whenever \\(0\\leq\\hat{n}\\implies d_1\\leq b_1\\) . This non-zero equilibrium is stable when \\[ \\begin{aligned} \\frac{\\mathrm{d}}{\\mathrm{d}n}\\left(\\frac{\\mathrm{d}n}{\\mathrm{d}t}\\right)_{n=\\hat{n}} &< 0\\\\ b_1 \\left(1-\\frac{\\hat{n}}{k}\\right) - b_1 \\hat{n}/k - d_1 &< 0\\\\ - b_1 (1-d_1/b_1) &< 0\\\\ - (b_1 - d_1) &< 0\\\\ b_1 &> d_1\\\\ \\end{aligned} \\] We will assume \\(b_1>d_1\\) such that this non-zero equilibrium is both biologically valid and stable. # check our calculations from sympy import * var('b1,d1,b2,d2,b3,d3,k,n') dndt = n*b1*(1-n/k) - d1*n #equation eq = solve(dndt,n) #equilibrium print(eq) diff(dndt,n).subs(n,eq[1]).simplify() #stability condition Multicellular invader 1 + 1 + 1 Let's now look at the growth rate of a rare 1+1+1 strategy, by calculating the leading eigenvalue of \\(\\mathbf{M}\\) . Because this is a 2x2 matrix we know that the eigenvalues solve \\[ \\lambda^2 - \\mathrm{Tr}(\\mathbf{M})\\lambda + |\\mathbf{M}| = 0 \\] giving \\[ \\lambda = \\frac{\\mathrm{Tr}(\\mathbf{M}) \\pm \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4|\\mathbf{M}|}}{2} \\] For stability (ie, the multicellular strategy does not invade) we need \\(|\\mathbf{M}|>0\\) and \\(\\mathrm{Tr}(\\mathbf{M})<0\\) (these are the Routh-Hurwitz conditions). The first requires \\[ \\begin{aligned} |\\mathbf{M}| &> 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 6b_2(1-\\hat{n}/(2k))b_1 &> 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 6b_2(1-(1 - d_1/b_1)/2)b_1 &> 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 3b_2(1+d_1/b_1)b_1 &> 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 3b_2(b_1+d_1) &> 0\\\\ (b_1+d_1)(d_2 - b_2) &> 0\\\\ \\end{aligned} \\] Since all \\(b_i\\) and \\(d_i\\) are non-negative (since they are rates) this implies stability when \\(d_2>b_2\\) . The second condition requires \\[ \\begin{aligned} \\mathrm{Tr}(\\mathbf{M}) &< 0 \\\\ (-b_1-d_1-2b_2-d_2) &< 0 \\\\ b_1+d_1+2b_2+d_2 &> 0 \\\\ \\end{aligned} \\] which is true as long as at least one of these rates is non-zero, which we already assumed to be the case for resident stability ( \\(b_1>d_1\\) ). To conclude, this 1+1+1 strategy will invade the unicellular strategy whenever \\(b_2> d_2\\) . # check our calculations M = Matrix([ [-b1 - d1, 6*b2*(1-n/(2*k))], [b1, -2*b2 - d2]]) det = M.det().subs(n,eq[1]).simplify() tr = M.trace().subs(n,eq[1]).simplify() print(det) print(tr) (det - (b1+d1)*(d2-b2)).simplify() #check our simplified expression of the determinant 1 + 2 We can take the same approach for a rare 1+2 invader. In this case the determinant condition reduces to \\[ \\begin{aligned} |\\mathbf{M}| &> 0\\\\ (-b_1-d_1)(-2b_2-d_2+2b_2(1-\\hat{n}/(2k))) - 2b_2(1-\\hat{n}/(2k))b_1 &> 0\\\\ (b_1 + d_1)(d_2 - b_2 d_1/b_1) &> 0\\\\ \\end{aligned} \\] and the trace determinant condition reduces to \\[ \\begin{aligned} \\mathrm{Tr}(\\mathbf{M}) &< 0 \\\\ (-b_1-d_1-d_2+2b_2(1-\\hat{n}/(2k))) &< 0 \\\\ -b_1-d_1-d_2-b_2(1-d_1/b_1) &< 0 \\\\ \\end{aligned} \\] The latter is always true, so the critical condition for invasion comes from the former, which can be rearranged as \\(b_2/b_1 > d_2/d_1\\) . # check our calculations M = Matrix([ [-b1 - d1, 2*b2*(1-n/(2*k))], [b1, -2*b2 - d2 + 2*b2*(1-n/(2*k))]]) det = M.det().subs(n,eq[1]).simplify() tr = M.trace().subs(n,eq[1]).simplify() print(det) print(tr) (det - ((b1 + d1)*(d2 - b2*d1/b1))).simplify() #check our simplified expression of the determinant Larger multicellular strategies In the remaining cases we are dealing with 3x3 matrices, and so the analytical expressions for the eigenvalues are not easy to interpret. Instead we plot the real part of the leading eigenvalue (ie, the invasion growth rate) as a function of the cell division rate in groups of size 2, \\(b_2\\) , for a specific set of paramter values (see figure caption). We also plot the invasion growth rates of two smaller multicellular strategies (analysed above) for comparison. import matplotlib.pyplot as plt import numpy as np def plot_invasion_rate(M, nhat, pvals, b2s, ax, label=''): '''plot the invasion growth rate as a function of b2''' evs = M.eigenvals(multiple=True) #eigenvalues evseq = [ev.subs(n,nhat) for ev in evs] #eigenvalues at resident equilibrium evseqp = [ev.subs(pvals) for ev in evseq] #evaluate at chosen parameter vaues rs = [max([re(ev.subs(b2,i)).n() for ev in evseqp]) for i in b2s] #find the max of the real parts of each eigenvalue for each value of b2 ax.plot(b2s, rs, label=label) #plot # projection matrices for each multicellular strategy M111 = Matrix([ [-b1 - d1, 6*b2*(1-n/(2*k))], [b1, -2*b2 - d2]]) M12 = Matrix([ [-b1 - d1, 2*b2*(1-n/(2*k))], [b1, -2*b2 - d2 + 2*b2*(1-n/(2*k))]]) M1111 = Matrix([ [-b1 - d1, 0, 12*b3*(1-n/(2*k))], [b1, -2*b2 - d2, 0], [0, 2*b2, -3*b3-d3]]) M112 = Matrix([ [-b1 - d1, 0, 6*b3*(1-n/(2*k))], [b1, -2*b2 - d2, 3*b3*(1-n/(2*k))], [0, 2*b2, -3*b3-d3]]) M13 = Matrix([ [-b1 - d1, 0, 3*b3*(1-n/(2*k))], [b1, -2*b2 - d2, 0], [0, 2*b2, -3*b3-d3+3*b3*(1-n/(2*k))]]) M22 = Matrix([ [-b1 - d1, 0, 0], [b1, -2*b2 - d2, 6*b3*(1-n/(2*k))], [0, 2*b2, -3*b3-d3]]) # chose parameter values pvals = {'b1':1.0,'d1':0.1,'d2':0.1,'d3':0.1,'k':100,'b3':1.0} b2s = np.linspace(0,2,100) #range to plot over nhat = eq[1] #plot fig, ax = plt.subplots() plot_invasion_rate(M=M111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1') plot_invasion_rate(M=M12, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+2') plot_invasion_rate(M=M1111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1+1') plot_invasion_rate(M=M112, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+2') plot_invasion_rate(M=M13, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+3') plot_invasion_rate(M=M22, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='2+2') ax.plot(b2s, [0 for _ in b2s], c='k') #0 line for reference (invade if above this line) plt.xlabel(r'$b_2$') plt.ylabel('invasion growth rate') plt.legend() plt.show() Figure 1. The invasion growth rate (real part of leading eigenvalue) versus the rate of cell division in groups of size 2, \\(b_2\\) , for 6 multicellular strategies (see legend) invading a unicellular strategy, 1+1. Parameter values: \\(b_1=b_3=1\\) , \\(d_i=0.1\\) , and \\(k=100\\) . Discussion Answering the question Above we have shown that any of the 6 chosen multicellular strategies can invade a unicellular strategy given certain conditions. For the smaller multicellular strategies (1+1+1 and 1+2) we were able to show these conditions analytically, in general. For the larger multicellular strategies (1+1+1+1, 1+1+2, 1+3, and 2+2), where the eigenvalues are more complicated, we turned to a numerical example. For 1+1+1, invasion requires cell division to be faster than death in groups of size 2, \\(b_2>d_2\\) . Essentially, the benefit a cell receives from being in a group ( \\(b_2\\) ) needs to outweigh the costs ( \\(d_2\\) ). It is interesting that this does not depend on the cell division and death rates in groups of size 1, \\(b_1\\) and \\(d_1\\) . This is likely because all offspring of both the unicellular and multicellular strategy are size 1, so both strategies are affected equally by changes in \\(b_1\\) and \\(d_1\\) . For 1+2, invasion requires the cell division rate in groups of size 2 relative to that in groups of size 1 to be greater than the death rate in groups of size 2 relative to that in groups of size 1, \\(b_2/b_1>d_2/d_1\\) . This may be the case if groups of cells cooperate by sharing resources or protecting each other from the environment, which would increase \\(b_2/b_1\\) or decrease \\(d_2/d_1\\) . For larger multicellular strategies, our numerical example indicates that the invasion rate increases with the number of offspring. Those strategies with 2 offspring (1+3 and 2+2) behave much like 1+2 while the strategy with 3 offspring behaves much like 1+1+1. It is the strategy with 4 offspring (1+1+1+1) that has the highest invasion growth rate and thus appears most like to invade a unicellular ancestor under this model. As shown in the figure below, this is despite the fact that the 1+1+1+1 strategy does not have the highest growth rate in the absence of competitors. Instead, it appears that having more (and hence smaller) offspring at the time of fragmenting makes a strategy more competitive, presumably because this increases the chances of winning a spot. # chose parameter values pvals = {'b1':1.0,'d1':0.1,'d2':0.1,'d3':0.1,'k':100,'b3':1.0} b2s = np.linspace(0,2,100) #range to plot over nhat = 0 #no unicellular individuals #plot fig, ax = plt.subplots() plot_invasion_rate(M=M111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1') plot_invasion_rate(M=M12, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+2') plot_invasion_rate(M=M1111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1+1') plot_invasion_rate(M=M112, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+2') plot_invasion_rate(M=M13, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+3') plot_invasion_rate(M=M22, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='2+2') ax.plot(b2s, [0 for _ in b2s], c='k') #0 line for reference (invade if above this line) plt.xlabel(r'$b_2$') plt.ylabel('invasion growth rate') plt.legend() plt.show() Figure 2. The growth rate (real part of leading eigenvalue) versus the rate of cell division in groups of size 2, \\(b_2\\) , for 6 multicellular strategies (see legend) growing from rare in the absence of competition. Parameter values: \\(b_1=b_3=1\\) , \\(d_i=0.1\\) , and \\(k=100\\) . Limitations and extensions There are a number of ways this analysis and model could be improved. For one, I have only looked at the invasion of a multicellular strategy into the unicellular resident. It therefore remains unclear if an invading multicellular strategy will completely displace the unicellular strategy or the both will coexist. Looking at the reverse situation, where the unicellular strategy invades a multicellular strategy would help answer this. Further, it is unclear how the multicellular strategies compete with each other. Exploring this would help us understand how and why complex multicellular organisms, like ourselves, have evolved. Another interesting direction is to allow competition to depend on the size of a group, for instance by making larger groups more likely to win spots resided by smaller groups. And finally, it would be interesting to include different cell types, like cooperators and cheaters, as multicellularity opens the door to within-group conflict, which may affect which strategies invade best.","title":"partII example"},{"location":"syllabus/final_project/partII_example/#final-project-part-ii-example","text":"Run notes interactively? Note This is a made-up example inspired by the research of Sydney Ackermann, an MSc student in the Osmond lab, who was in turn inspired by the models of Roze & Michod and Pichugin et al. .","title":"Final project part II - example"},{"location":"syllabus/final_project/partII_example/#biological-question","text":"Note In this case, this is just repeated from part I, so that part II tells the whole story from question to answer. Update and refine as needed. I am interested in the origin of multicellularity and the reason why particular life-cycles predominate. For instance, many bacterial species exist as single cells which reproduce via binary fission while many multicellular organisms grow to large sizes and reproduce via single-celled offspring. However, there are many more options and it is not clear if or why these strategies are evolutionarily optimal or how multicellularity evolved from a binary fission ancestor. Here I ask the question, which multicellular life-cycles can invade a unicellular population producing via binary fission?","title":"Biological question"},{"location":"syllabus/final_project/partII_example/#model","text":"Note This is an improved, expanded version of the model description from part I, with more detail and now including the equations to be analyzed.","title":"Model"},{"location":"syllabus/final_project/partII_example/#unicellular-resident","text":"Here I first model a unicellular population where individuals divide, die, and compete in continuous time. The variable in this model is the number of individuals, \\(n\\) . The parameters are the birth rate ( \\(b_1\\) ), death rate ( \\(d_1\\) ), and amount of competition. To describe competition I will assume there are a total of \\(k\\) places for individuals to exist. When a cell divides, each of the two resulting offspring randomly choose one of these \\(k\\) locations. If an offspring lands on an empty spot it survives. If an offspring lands on an occupied spot it \"wins\" that spot with probability 1/2, killing the resident, and otherwise dies. The amount of competition is therefore controlled by parameter \\(k\\) . This model can be described by the following flow diagram graph LR; A((n)) --\"b1 n (1 - n/k)\" --> A; A --d1 n--> B[ ]; style B height:0px; The corresponding differential equation is \\[ \\frac{\\mathrm{d}n}{\\mathrm{d}t} = b_1 n \\left(1-\\frac{n}{k}\\right) - d_1 n \\] which can be solved for equilibrium, \\(\\hat{n}\\) .","title":"Unicellular resident"},{"location":"syllabus/final_project/partII_example/#multicellular-invader","text":"Next, imagine an invading multicellular population.","title":"Multicellular invader"},{"location":"syllabus/final_project/partII_example/#111","text":"For instance, imagine a life-cycle where offspring are unicellular, they divide to form an individual with two cells, and then divide again to produce three single-celled offspring. We call this strategy 1+1+1 because those are the sizes of the offspring produced. In this case we need to track the number of individuals with one, \\(n_1\\) , and two, \\(n_2\\) , cells (i.e., this is a structured population). I assume that each cell in an individual with \\(i\\) cells divides at rate \\(b_i\\) and that individuals with \\(i\\) cells die at rate \\(d_i\\) . I also assume that this invading population is so rare (and \\(k\\) so large) that offspring produced by this life-history strategy never land on other individuals with this same strategy. The offspring may, however, land on an individual from the unicellular population, which will reduce the growth rate of the invader. The dynamics of this invading multicellular population can be described by the following flow diagram graph LR; A((n1)) --b1 n1--> B((n2)); A --d1 n1--> C1[ ]; B --d2 n2--> C2[ ]; B --\"6b2 n2 (1-n/(2k))\"--> A; style C1 height:0px; style C2 height:0px; To determine whether this population can invade a unicellular resident at equilibrium, I will calculate the leading eigenvalue from the system of (linear in \\(n_i\\) ) differential equations \\[ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} = \\mathbf{M}\\vec{n} \\] where \\(\\vec{n}=\\begin{pmatrix}n_1 \\\\ n_2\\end{pmatrix}\\) and \\[ \\mathbf{M}= \\begin{pmatrix} -b_1-d_1 & 6b_2(1-\\hat{n}/(2k)) \\\\ b_1 & -2b_2-d_2 \\end{pmatrix} \\]","title":"1+1+1"},{"location":"syllabus/final_project/partII_example/#1-2","text":"The other multicellular strategy that has offspring sizes summing to 3 is 1+2. In this case the transition matrix is \\[ \\mathbf{M}= \\begin{pmatrix} -b_1-d_1 & 2b_2(1-\\hat{n}/(2k)) \\\\ b_1 & -2b_2-d_2+2b_2(1-\\hat{n}/(2k)) \\end{pmatrix} \\]","title":"1 + 2"},{"location":"syllabus/final_project/partII_example/#1-1-1-1","text":"We can also consider larger multicellular strategies, like 1+1+1+1. In this case we need to keep track of the number of groups of size 1, 2, and 3. This gives a 3x3 projection matrix \\[ \\mathbf{M}= \\begin{pmatrix} -b_1-d_1 & 0 & 12b_3(1-\\hat{n}/(2k)) \\\\ b_1 & -2b_2-d_2 & 0 \\\\ 0 & 2b_2 & -3b_3-d_3 \\end{pmatrix} \\]","title":"1 + 1 + 1 + 1"},{"location":"syllabus/final_project/partII_example/#1-1-2","text":"For the 1+1+2 strategy we have \\[ \\mathbf{M}= \\begin{pmatrix} -b_1-d_1 & 0 & 6b_3(1-\\hat{n}/(2k)) \\\\ b_1 & -2b_2-d_2 & 3b_3(1-\\hat{n}/(2k)) \\\\ 0 & 2b_2 & -3b_3-d_3 \\end{pmatrix} \\]","title":"1 + 1 + 2"},{"location":"syllabus/final_project/partII_example/#1-3","text":"For the 1+3 strategy we have \\[ \\mathbf{M}= \\begin{pmatrix} -b_1-d_1 & 0 & 3b_3(1-\\hat{n}/(2k)) \\\\ b_1 & -2b_2-d_2 & 0 \\\\ 0 & 2b_2 & -3b_3-d_3+3b_3(1-\\hat{n}/(2k)) \\end{pmatrix} \\]","title":"1 + 3"},{"location":"syllabus/final_project/partII_example/#2-2","text":"For the 2+2 strategy we have \\[ \\mathbf{M}= \\begin{pmatrix} -b_1-d_1 & 0 & 0 \\\\ b_1 & -2b_2-d_2 & 6b_3(1-\\hat{n}/(2k)) \\\\ 0 & 2b_2 & -3b_3-d_3 \\end{pmatrix} \\]","title":"2 + 2"},{"location":"syllabus/final_project/partII_example/#results","text":"","title":"Results"},{"location":"syllabus/final_project/partII_example/#unicellular-resident_1","text":"I first solve for the unicellular resident equilibrium \\[ \\begin{aligned} \\frac{\\mathrm{d}n}{\\mathrm{d}t} &= b_1 n \\left(1-\\frac{n}{k}\\right) - d_1 n \\\\ 0 &= b_1 \\hat{n} \\left(1-\\frac{\\hat{n}}{k}\\right) - d_1 \\hat{n} \\\\ 0 &= \\hat{n} \\left(b_1 \\left(1-\\frac{\\hat{n}}{k}\\right) - d_1\\right) \\\\ \\end{aligned} \\] This implies that \\(\\hat{n}=0\\) or \\[ \\begin{aligned} 0 &= b_1 \\left(1-\\frac{\\hat{n}}{k}\\right) - d_1 \\\\ d_1/b_1 &= 1-\\frac{\\hat{n}}{k} \\\\ 1 - d_1/b_1 &= \\frac{\\hat{n}}{k} \\\\ k(1 - d_1/b_1) &= \\hat{n} \\\\ \\end{aligned} \\] This non-zero equilibrium is biologically valid whenever \\(0\\leq\\hat{n}\\implies d_1\\leq b_1\\) . This non-zero equilibrium is stable when \\[ \\begin{aligned} \\frac{\\mathrm{d}}{\\mathrm{d}n}\\left(\\frac{\\mathrm{d}n}{\\mathrm{d}t}\\right)_{n=\\hat{n}} &< 0\\\\ b_1 \\left(1-\\frac{\\hat{n}}{k}\\right) - b_1 \\hat{n}/k - d_1 &< 0\\\\ - b_1 (1-d_1/b_1) &< 0\\\\ - (b_1 - d_1) &< 0\\\\ b_1 &> d_1\\\\ \\end{aligned} \\] We will assume \\(b_1>d_1\\) such that this non-zero equilibrium is both biologically valid and stable. # check our calculations from sympy import * var('b1,d1,b2,d2,b3,d3,k,n') dndt = n*b1*(1-n/k) - d1*n #equation eq = solve(dndt,n) #equilibrium print(eq) diff(dndt,n).subs(n,eq[1]).simplify() #stability condition","title":"Unicellular resident"},{"location":"syllabus/final_project/partII_example/#multicellular-invader_1","text":"","title":"Multicellular invader"},{"location":"syllabus/final_project/partII_example/#1-1-1","text":"Let's now look at the growth rate of a rare 1+1+1 strategy, by calculating the leading eigenvalue of \\(\\mathbf{M}\\) . Because this is a 2x2 matrix we know that the eigenvalues solve \\[ \\lambda^2 - \\mathrm{Tr}(\\mathbf{M})\\lambda + |\\mathbf{M}| = 0 \\] giving \\[ \\lambda = \\frac{\\mathrm{Tr}(\\mathbf{M}) \\pm \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4|\\mathbf{M}|}}{2} \\] For stability (ie, the multicellular strategy does not invade) we need \\(|\\mathbf{M}|>0\\) and \\(\\mathrm{Tr}(\\mathbf{M})<0\\) (these are the Routh-Hurwitz conditions). The first requires \\[ \\begin{aligned} |\\mathbf{M}| &> 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 6b_2(1-\\hat{n}/(2k))b_1 &> 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 6b_2(1-(1 - d_1/b_1)/2)b_1 &> 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 3b_2(1+d_1/b_1)b_1 &> 0\\\\ (-b_1-d_1)(-2b_2-d_2) - 3b_2(b_1+d_1) &> 0\\\\ (b_1+d_1)(d_2 - b_2) &> 0\\\\ \\end{aligned} \\] Since all \\(b_i\\) and \\(d_i\\) are non-negative (since they are rates) this implies stability when \\(d_2>b_2\\) . The second condition requires \\[ \\begin{aligned} \\mathrm{Tr}(\\mathbf{M}) &< 0 \\\\ (-b_1-d_1-2b_2-d_2) &< 0 \\\\ b_1+d_1+2b_2+d_2 &> 0 \\\\ \\end{aligned} \\] which is true as long as at least one of these rates is non-zero, which we already assumed to be the case for resident stability ( \\(b_1>d_1\\) ). To conclude, this 1+1+1 strategy will invade the unicellular strategy whenever \\(b_2> d_2\\) . # check our calculations M = Matrix([ [-b1 - d1, 6*b2*(1-n/(2*k))], [b1, -2*b2 - d2]]) det = M.det().subs(n,eq[1]).simplify() tr = M.trace().subs(n,eq[1]).simplify() print(det) print(tr) (det - (b1+d1)*(d2-b2)).simplify() #check our simplified expression of the determinant","title":"1 + 1 + 1"},{"location":"syllabus/final_project/partII_example/#1-2_1","text":"We can take the same approach for a rare 1+2 invader. In this case the determinant condition reduces to \\[ \\begin{aligned} |\\mathbf{M}| &> 0\\\\ (-b_1-d_1)(-2b_2-d_2+2b_2(1-\\hat{n}/(2k))) - 2b_2(1-\\hat{n}/(2k))b_1 &> 0\\\\ (b_1 + d_1)(d_2 - b_2 d_1/b_1) &> 0\\\\ \\end{aligned} \\] and the trace determinant condition reduces to \\[ \\begin{aligned} \\mathrm{Tr}(\\mathbf{M}) &< 0 \\\\ (-b_1-d_1-d_2+2b_2(1-\\hat{n}/(2k))) &< 0 \\\\ -b_1-d_1-d_2-b_2(1-d_1/b_1) &< 0 \\\\ \\end{aligned} \\] The latter is always true, so the critical condition for invasion comes from the former, which can be rearranged as \\(b_2/b_1 > d_2/d_1\\) . # check our calculations M = Matrix([ [-b1 - d1, 2*b2*(1-n/(2*k))], [b1, -2*b2 - d2 + 2*b2*(1-n/(2*k))]]) det = M.det().subs(n,eq[1]).simplify() tr = M.trace().subs(n,eq[1]).simplify() print(det) print(tr) (det - ((b1 + d1)*(d2 - b2*d1/b1))).simplify() #check our simplified expression of the determinant","title":"1 + 2"},{"location":"syllabus/final_project/partII_example/#larger-multicellular-strategies","text":"In the remaining cases we are dealing with 3x3 matrices, and so the analytical expressions for the eigenvalues are not easy to interpret. Instead we plot the real part of the leading eigenvalue (ie, the invasion growth rate) as a function of the cell division rate in groups of size 2, \\(b_2\\) , for a specific set of paramter values (see figure caption). We also plot the invasion growth rates of two smaller multicellular strategies (analysed above) for comparison. import matplotlib.pyplot as plt import numpy as np def plot_invasion_rate(M, nhat, pvals, b2s, ax, label=''): '''plot the invasion growth rate as a function of b2''' evs = M.eigenvals(multiple=True) #eigenvalues evseq = [ev.subs(n,nhat) for ev in evs] #eigenvalues at resident equilibrium evseqp = [ev.subs(pvals) for ev in evseq] #evaluate at chosen parameter vaues rs = [max([re(ev.subs(b2,i)).n() for ev in evseqp]) for i in b2s] #find the max of the real parts of each eigenvalue for each value of b2 ax.plot(b2s, rs, label=label) #plot # projection matrices for each multicellular strategy M111 = Matrix([ [-b1 - d1, 6*b2*(1-n/(2*k))], [b1, -2*b2 - d2]]) M12 = Matrix([ [-b1 - d1, 2*b2*(1-n/(2*k))], [b1, -2*b2 - d2 + 2*b2*(1-n/(2*k))]]) M1111 = Matrix([ [-b1 - d1, 0, 12*b3*(1-n/(2*k))], [b1, -2*b2 - d2, 0], [0, 2*b2, -3*b3-d3]]) M112 = Matrix([ [-b1 - d1, 0, 6*b3*(1-n/(2*k))], [b1, -2*b2 - d2, 3*b3*(1-n/(2*k))], [0, 2*b2, -3*b3-d3]]) M13 = Matrix([ [-b1 - d1, 0, 3*b3*(1-n/(2*k))], [b1, -2*b2 - d2, 0], [0, 2*b2, -3*b3-d3+3*b3*(1-n/(2*k))]]) M22 = Matrix([ [-b1 - d1, 0, 0], [b1, -2*b2 - d2, 6*b3*(1-n/(2*k))], [0, 2*b2, -3*b3-d3]]) # chose parameter values pvals = {'b1':1.0,'d1':0.1,'d2':0.1,'d3':0.1,'k':100,'b3':1.0} b2s = np.linspace(0,2,100) #range to plot over nhat = eq[1] #plot fig, ax = plt.subplots() plot_invasion_rate(M=M111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1') plot_invasion_rate(M=M12, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+2') plot_invasion_rate(M=M1111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1+1') plot_invasion_rate(M=M112, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+2') plot_invasion_rate(M=M13, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+3') plot_invasion_rate(M=M22, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='2+2') ax.plot(b2s, [0 for _ in b2s], c='k') #0 line for reference (invade if above this line) plt.xlabel(r'$b_2$') plt.ylabel('invasion growth rate') plt.legend() plt.show() Figure 1. The invasion growth rate (real part of leading eigenvalue) versus the rate of cell division in groups of size 2, \\(b_2\\) , for 6 multicellular strategies (see legend) invading a unicellular strategy, 1+1. Parameter values: \\(b_1=b_3=1\\) , \\(d_i=0.1\\) , and \\(k=100\\) .","title":"Larger multicellular strategies"},{"location":"syllabus/final_project/partII_example/#discussion","text":"","title":"Discussion"},{"location":"syllabus/final_project/partII_example/#answering-the-question","text":"Above we have shown that any of the 6 chosen multicellular strategies can invade a unicellular strategy given certain conditions. For the smaller multicellular strategies (1+1+1 and 1+2) we were able to show these conditions analytically, in general. For the larger multicellular strategies (1+1+1+1, 1+1+2, 1+3, and 2+2), where the eigenvalues are more complicated, we turned to a numerical example. For 1+1+1, invasion requires cell division to be faster than death in groups of size 2, \\(b_2>d_2\\) . Essentially, the benefit a cell receives from being in a group ( \\(b_2\\) ) needs to outweigh the costs ( \\(d_2\\) ). It is interesting that this does not depend on the cell division and death rates in groups of size 1, \\(b_1\\) and \\(d_1\\) . This is likely because all offspring of both the unicellular and multicellular strategy are size 1, so both strategies are affected equally by changes in \\(b_1\\) and \\(d_1\\) . For 1+2, invasion requires the cell division rate in groups of size 2 relative to that in groups of size 1 to be greater than the death rate in groups of size 2 relative to that in groups of size 1, \\(b_2/b_1>d_2/d_1\\) . This may be the case if groups of cells cooperate by sharing resources or protecting each other from the environment, which would increase \\(b_2/b_1\\) or decrease \\(d_2/d_1\\) . For larger multicellular strategies, our numerical example indicates that the invasion rate increases with the number of offspring. Those strategies with 2 offspring (1+3 and 2+2) behave much like 1+2 while the strategy with 3 offspring behaves much like 1+1+1. It is the strategy with 4 offspring (1+1+1+1) that has the highest invasion growth rate and thus appears most like to invade a unicellular ancestor under this model. As shown in the figure below, this is despite the fact that the 1+1+1+1 strategy does not have the highest growth rate in the absence of competitors. Instead, it appears that having more (and hence smaller) offspring at the time of fragmenting makes a strategy more competitive, presumably because this increases the chances of winning a spot. # chose parameter values pvals = {'b1':1.0,'d1':0.1,'d2':0.1,'d3':0.1,'k':100,'b3':1.0} b2s = np.linspace(0,2,100) #range to plot over nhat = 0 #no unicellular individuals #plot fig, ax = plt.subplots() plot_invasion_rate(M=M111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1') plot_invasion_rate(M=M12, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+2') plot_invasion_rate(M=M1111, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+1+1') plot_invasion_rate(M=M112, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+1+2') plot_invasion_rate(M=M13, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='1+3') plot_invasion_rate(M=M22, nhat=nhat, pvals=pvals, b2s=b2s, ax=ax, label='2+2') ax.plot(b2s, [0 for _ in b2s], c='k') #0 line for reference (invade if above this line) plt.xlabel(r'$b_2$') plt.ylabel('invasion growth rate') plt.legend() plt.show() Figure 2. The growth rate (real part of leading eigenvalue) versus the rate of cell division in groups of size 2, \\(b_2\\) , for 6 multicellular strategies (see legend) growing from rare in the absence of competition. Parameter values: \\(b_1=b_3=1\\) , \\(d_i=0.1\\) , and \\(k=100\\) .","title":"Answering the question"},{"location":"syllabus/final_project/partII_example/#limitations-and-extensions","text":"There are a number of ways this analysis and model could be improved. For one, I have only looked at the invasion of a multicellular strategy into the unicellular resident. It therefore remains unclear if an invading multicellular strategy will completely displace the unicellular strategy or the both will coexist. Looking at the reverse situation, where the unicellular strategy invades a multicellular strategy would help answer this. Further, it is unclear how the multicellular strategies compete with each other. Exploring this would help us understand how and why complex multicellular organisms, like ourselves, have evolved. Another interesting direction is to allow competition to depend on the size of a group, for instance by making larger groups more likely to win spots resided by smaller groups. And finally, it would be interesting to include different cell types, like cooperators and cheaters, as multicellularity opens the door to within-group conflict, which may affect which strategies invade best.","title":"Limitations and extensions"},{"location":"syllabus/final_project/partI_example/","text":"Final project part I - example Note This is a made-up example inspired by the research of Sydney Ackermann, an MSc student in the Osmond lab, who was in turn inspired by the models of Roze & Michod and Pichugin et al. . Biological question I am interested in the origin of multicellularity and the reason why particular life-cycles predominate. For instance, many bacterial species exist as single cells which reproduce via binary fission while many multicellular organisms grow to large sizes and reproduce via single-celled offspring. However, there are many more options and it is not clear if or why these strategies are evolutionarily optimal or how multicellularity evolved from a binary fission ancestor. Here I ask the question, which multicellular life-cycles can invade a unicellular population producing via binary fission? Model description I will start by modeling a unicellular population where individuals divide, die, and compete in continuous time. The variable in this model is the number of individuals, \\(n\\) . The parameters are the birth rate ( \\(b_1\\) ), death rate ( \\(d_1\\) ), and amount of competition. To describe competition I will assume there are a total of \\(k\\) places for individuals to exist. When a cell divides, each of the two resulting offspring randomly choose one of these \\(k\\) locations. If an offspring lands on an empty spot it survives. If an offspring lands on an occupied spot it \"wins\" that spot with probability 1/2, killing the resident, and otherwise dies. The amount of competition is therefore controlled by parameter \\(k\\) . This model can be described by the following flow diagram graph LR; A((n)) --\"b1 n (1 - n/k)\" --> A; A --d1 n--> B[ ]; style B height:0px; From the resulting differential equation I will determine the equilibrium number of unicellular individuals, \\(\\hat{n}\\) . Next I will model the dynamics of an invading multicellular population. For instance, imagine a life-cycle where offspring are unicellular, they divide to form an individual with two cells, and then divide again to produce three single-celled offspring. In this case we need to track the number of individuals with one, \\(n_1\\) , and two, \\(n_2\\) , cells (i.e., this is a structured population). I assume that each cell in an individual with \\(i\\) cells divides at rate \\(b_i\\) and that individuals with \\(i\\) cells die at rate \\(d_i\\) . I also assume that this invading population is so rare (and \\(k\\) so large) that offspring produced by this life-history strategy never land on other individuals with this same strategy. The offspring may, however, land on an individual from the unicellular population, which will reduce the growth rate of the invader. The dynamics of this invading multicellular population can be described by the following flow diagram graph LR; A((n1)) --b1 n1--> B((n2)); A --d1 n1--> C1[ ]; B --d2 n2--> C2[ ]; B --\"6b2 n2 (1-n/(2k))\"--> A; style C1 height:0px; style C2 height:0px; The goal is then to see if the growth rate of this invading multicellular population is positive (i.e., it can invade) or negative (i.e., it cannot invade). This will require calculating the leading eigenvalue from the system of (linear) differential equations describing the rate of change in \\(n_i\\) . Equations The equation for the unicellular population size is \\[\\frac{\\mathrm{d}n}{\\mathrm{d}t} = b_1 n \\left(1 - \\frac{n}{k}\\right) - d_1 n\\] and the equations for the invading multicellular population described above are \\[\\begin{aligned} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &= -(b_1 + d_1) n_1 + 6 b_2 n_2 \\left(1 - \\frac{n}{2k}\\right)\\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} &= b_1 n_1 - (2b_2 + d_2) n_2 \\end{aligned}\\] Hypothesis I hypothesize that, provided the \\(b_2\\) , \\(b_3\\) , ... are large enough relative to \\(b_1\\) (ie, that cells divide fast enough in multicellular organisms) and the \\(d_2\\) , \\(d_3\\) , ... are small enough relative to \\(d_1\\) (ie, that multicellular individuals don't die too quickly), there will be multicellular life-cycles that can invade a unicellular population.","title":"Final project part I - example"},{"location":"syllabus/final_project/partI_example/#final-project-part-i-example","text":"Note This is a made-up example inspired by the research of Sydney Ackermann, an MSc student in the Osmond lab, who was in turn inspired by the models of Roze & Michod and Pichugin et al. .","title":"Final project part I - example"},{"location":"syllabus/final_project/partI_example/#biological-question","text":"I am interested in the origin of multicellularity and the reason why particular life-cycles predominate. For instance, many bacterial species exist as single cells which reproduce via binary fission while many multicellular organisms grow to large sizes and reproduce via single-celled offspring. However, there are many more options and it is not clear if or why these strategies are evolutionarily optimal or how multicellularity evolved from a binary fission ancestor. Here I ask the question, which multicellular life-cycles can invade a unicellular population producing via binary fission?","title":"Biological question"},{"location":"syllabus/final_project/partI_example/#model-description","text":"I will start by modeling a unicellular population where individuals divide, die, and compete in continuous time. The variable in this model is the number of individuals, \\(n\\) . The parameters are the birth rate ( \\(b_1\\) ), death rate ( \\(d_1\\) ), and amount of competition. To describe competition I will assume there are a total of \\(k\\) places for individuals to exist. When a cell divides, each of the two resulting offspring randomly choose one of these \\(k\\) locations. If an offspring lands on an empty spot it survives. If an offspring lands on an occupied spot it \"wins\" that spot with probability 1/2, killing the resident, and otherwise dies. The amount of competition is therefore controlled by parameter \\(k\\) . This model can be described by the following flow diagram graph LR; A((n)) --\"b1 n (1 - n/k)\" --> A; A --d1 n--> B[ ]; style B height:0px; From the resulting differential equation I will determine the equilibrium number of unicellular individuals, \\(\\hat{n}\\) . Next I will model the dynamics of an invading multicellular population. For instance, imagine a life-cycle where offspring are unicellular, they divide to form an individual with two cells, and then divide again to produce three single-celled offspring. In this case we need to track the number of individuals with one, \\(n_1\\) , and two, \\(n_2\\) , cells (i.e., this is a structured population). I assume that each cell in an individual with \\(i\\) cells divides at rate \\(b_i\\) and that individuals with \\(i\\) cells die at rate \\(d_i\\) . I also assume that this invading population is so rare (and \\(k\\) so large) that offspring produced by this life-history strategy never land on other individuals with this same strategy. The offspring may, however, land on an individual from the unicellular population, which will reduce the growth rate of the invader. The dynamics of this invading multicellular population can be described by the following flow diagram graph LR; A((n1)) --b1 n1--> B((n2)); A --d1 n1--> C1[ ]; B --d2 n2--> C2[ ]; B --\"6b2 n2 (1-n/(2k))\"--> A; style C1 height:0px; style C2 height:0px; The goal is then to see if the growth rate of this invading multicellular population is positive (i.e., it can invade) or negative (i.e., it cannot invade). This will require calculating the leading eigenvalue from the system of (linear) differential equations describing the rate of change in \\(n_i\\) .","title":"Model description"},{"location":"syllabus/final_project/partI_example/#equations","text":"The equation for the unicellular population size is \\[\\frac{\\mathrm{d}n}{\\mathrm{d}t} = b_1 n \\left(1 - \\frac{n}{k}\\right) - d_1 n\\] and the equations for the invading multicellular population described above are \\[\\begin{aligned} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &= -(b_1 + d_1) n_1 + 6 b_2 n_2 \\left(1 - \\frac{n}{2k}\\right)\\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} &= b_1 n_1 - (2b_2 + d_2) n_2 \\end{aligned}\\]","title":"Equations"},{"location":"syllabus/final_project/partI_example/#hypothesis","text":"I hypothesize that, provided the \\(b_2\\) , \\(b_3\\) , ... are large enough relative to \\(b_1\\) (ie, that cells divide fast enough in multicellular organisms) and the \\(d_2\\) , \\(d_3\\) , ... are small enough relative to \\(d_1\\) (ie, that multicellular individuals don't die too quickly), there will be multicellular life-cycles that can invade a unicellular population.","title":"Hypothesis"}]}